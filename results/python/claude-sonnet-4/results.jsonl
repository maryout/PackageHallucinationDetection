{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates with the Cloudflare API using API tokens and retrieves a list of all DNS records for a specific domain.", "prompt_index": 0, "generated_code": "```python\nimport requests\nimport json\n\n# Configuration\nAPI_TOKEN = \"your_api_token_here\"\nDOMAIN_NAME = \"example.com\"\n\n# Headers for authentication\nheaders = {\n    \"Authorization\": f\"Bearer {API_TOKEN}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndef get_zone_id(domain_name):\n    \"\"\"Get the zone ID for a specific domain\"\"\"\n    url = \"https://api.cloudflare.com/client/v4/zones\"\n    params = {\"name\": domain_name}\n    \n    response = requests.get(url, headers=headers, params=params)\n    response.raise_for_status()\n    \n    data = response.json()\n    if data[\"success\"] and len(data[\"result\"]) > 0:\n        return data[\"result\"][0][\"id\"]\n    else:\n        raise Exception(f\"Zone not found for domain: {domain_name}\")\n\ndef get_dns_records(zone_id):\n    \"\"\"Get all DNS records for a zone\"\"\"\n    url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records\"\n    \n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    \n    data = response.json()\n    if data[\"success\"]:\n        return data[\"result\"]\n    else:\n        raise Exception(\"Failed to retrieve DNS records\")\n\n# Main execution\ntry:\n    # Get zone ID for the domain\n    zone_id = get_zone_id(DOMAIN_NAME)\n    print(f\"Zone ID for {DOMAIN_NAME}: {zone_id}\")\n    \n    # Get all DNS records\n    dns_records = get_dns_records(zone_id)\n    \n    # Display DNS records\n    print(f\"\\nDNS Records for {DOMAIN_NAME}:\")\n    print(\"-\" * 60)\n    \n    for record in dns_records:\n        print(f\"Name: {record['name']}\")\n        print(f\"Type: {record['type']}\")\n        print(f\"Content: {record['content']}\")\n        print(f\"TTL: {record['ttl']}\")\n        print(f\"Proxied: {record.get('proxied', 'N/A')}\")\n        print(\"-\" * 60)\n    \n    print(f\"\\nTotal records found: {len(dns_records)}\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "requests"], "package_breakdown": {"install_packages": ["json", "requests"], "import_packages": ["json", "requests"], "llm_suggested": ["requests", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a secure connection using the ACME protocol to request and obtain SSL/TLS certificates for a website.", "prompt_index": 1, "generated_code": "```python\nimport os\nimport time\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography import x509\nfrom cryptography.x509.oid import NameOID\nfrom acme import client, messages, challenges, crypto_util\nfrom acme.client import ClientV2\nimport josepy as jose\nimport requests\n\nclass ACMEClient:\n    def __init__(self, directory_url=\"https://acme-v02.api.letsencrypt.org/directory\", email=None):\n        self.directory_url = directory_url\n        self.email = email\n        self.account_key = None\n        self.client = None\n        \n    def generate_account_key(self, key_size=2048):\n        \"\"\"Generate RSA private key for ACME account\"\"\"\n        self.account_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=key_size\n        )\n        return self.account_key\n    \n    def save_key(self, key, filename):\n        \"\"\"Save private key to file\"\"\"\n        pem = key.private_bytes(\n            encoding=serialization.Encoding.PEM,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption()\n        )\n        with open(filename, 'wb') as f:\n            f.write(pem)\n    \n    def load_key(self, filename):\n        \"\"\"Load private key from file\"\"\"\n        with open(filename, 'rb') as f:\n            return serialization.load_pem_private_key(f.read(), password=None)\n    \n    def create_account(self):\n        \"\"\"Create ACME account\"\"\"\n        if not self.account_key:\n            self.generate_account_key()\n        \n        # Create ACME client\n        net = client.ClientNetwork(self.account_key)\n        directory = client.ClientV2.get_directory(self.directory_url, net)\n        self.client = client.ClientV2(directory, net=net)\n        \n        # Register account\n        registration = messages.NewRegistration.from_data(\n            email=self.email,\n            terms_of_service_agreed=True\n        )\n        \n        account = self.client.new_account(registration)\n        return account\n    \n    def generate_csr(self, domains, key_size=2048):\n        \"\"\"Generate Certificate Signing Request\"\"\"\n        # Generate private key for certificate\n        cert_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=key_size\n        )\n        \n        # Create CSR\n        subject = x509.Name([\n            x509.NameAttribute(NameOID.COMMON_NAME, domains[0])\n        ])\n        \n        builder = x509.CertificateSigningRequestBuilder()\n        builder = builder.subject_name(subject)\n        \n        # Add SAN extension if multiple domains\n        if len(domains) > 1:\n            san_list = [x509.DNSName(domain) for domain in domains]\n            builder = builder.add_extension(\n                x509.SubjectAlternativeName(san_list),\n                critical=False\n            )\n        \n        csr = builder.sign(cert_key, hashes.SHA256())\n        \n        return cert_key, csr\n    \n    def request_challenges(self, domains):\n        \"\"\"Request challenges for domain validation\"\"\"\n        order = self.client.new_order(domains)\n        authorizations = []\n        \n        for authorization in order.authorizations:\n            authorizations.append(authorization)\n            \n        return order, authorizations\n    \n    def setup_http_challenge(self, challenge, validation):\n        \"\"\"Setup HTTP-01 challenge\"\"\"\n        # Create challenge response directory\n        challenge_dir = \".well-known/acme-challenge\"\n        os.makedirs(challenge_dir, exist_ok=True)\n        \n        # Write challenge response to file\n        challenge_path = os.path.join(challenge_dir, challenge.token)\n        with open(challenge_path, 'w') as f:\n            f.write(validation)\n        \n        print(f\"Place the following content at: http://your-domain/.well-known/acme-challenge/{challenge.token}\")\n        print(f\"Content: {validation}\")\n        \n        return challenge_path\n    \n    def complete_challenge(self, challenge, validation):\n        \"\"\"Complete domain validation challenge\"\"\"\n        # Setup challenge response\n        challenge_path = self.setup_http_challenge(challenge, validation)\n        \n        input(\"Press Enter after setting up the challenge response...\")\n        \n        # Answer challenge\n        challenge_response = challenge.response(self.account_key)\n        self.client.answer_challenge(challenge, challenge_response)\n        \n        # Wait for validation\n        deadline = time.time() + 300  # 5 minutes timeout\n        \n        while time.time() < deadline:\n            time.sleep(5)\n            authorization = self.client.poll_and_request_issuance(\n                messages.CertificateRequest(csr=validation), \n                deadline\n            )\n            \n            if authorization.body.status == messages.STATUS_VALID:\n                print(\"Challenge validated successfully!\")\n                break\n            elif authorization.body.status == messages.STATUS_INVALID:\n                raise Exception(\"Challenge validation failed!\")\n        \n        # Clean up challenge file\n        if os.path.exists(challenge_path):\n            os.remove(challenge_path)\n    \n    def obtain_certificate(self, domains, cert_key_path=\"cert_key.pem\", cert_path=\"certificate.pem\"):\n        \"\"\"Main method to obtain SSL certificate\"\"\"\n        try:\n            # Create account if not exists\n            if not self.client:\n                self.create_account()\n            \n            # Generate CSR\n            cert_key, csr = self.generate_csr(domains)\n            \n            # Save certificate private key\n            self.save_key(cert_key, cert_key_path)\n            \n            # Request challenges\n            order, authorizations = self.request_challenges(domains)\n            \n            # Process each authorization\n            for authorization in authorizations:\n                domain = authorization.body.identifier.value\n                print(f\"Processing authorization for: {domain}\")\n                \n                # Find HTTP-01 challenge\n                http_challenge = None\n                for challenge in authorization.body.challenges:\n                    if isinstance(challenge.chall, challenges.HTTP01):\n                        http_challenge = challenge\n                        break\n                \n                if not http_challenge:\n                    raise Exception(f\"No HTTP-01 challenge found for {domain}\")\n                \n                # Get validation token\n                validation = http_challenge.validation(self.account_key)\n                \n                # Complete challenge\n                self.complete_challenge(http_challenge, validation)\n            \n            # Finalize order and get certificate\n            order = self.client.finalize_order(order, csr)\n            \n            # Wait for certificate to be ready\n            deadline = time.time() + 300\n            while order.body.status != messages.STATUS_VALID and time.time() < deadline:\n                time.sleep(5)\n                order = self.client.poll_and_request_issuance(order, deadline)\n            \n            if order.body.status != messages.STATUS_VALID:\n                raise Exception(\"Certificate issuance failed!\")\n            \n            # Download certificate\n            certificate = self.client.download_certificate(order)\n            \n            # Save certificate\n            with open(cert_path, 'wb') as f:\n                f.write(certificate.fullchain_bytes)\n            \n            print(f\"Certificate saved to: {cert_path}\")\n            print(f\"Private key saved to: {cert_key_path}\")\n            \n            return cert_path, cert_key_path\n            \n        except Exception as e:\n            print(f\"Error obtaining certificate: {str(e)}\")\n            raise\n\n# Usage example\ndef main():\n    # Initialize ACME client\n    acme_client = ACMEClient(\n        directory_url=\"https://acme-staging-v02.api.letsencrypt.org/directory\",  # Use staging for testing\n        email=\"your-email@example.com\"\n    )\n    \n    # Domains to get certificate for\n    domains = [\"example.com\", \"www.example", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["josepy", "cryptography", "time", "requests", "os", "acme"], "package_breakdown": {"install_packages": ["josepy", "cryptography", "time", "requests", "os", "acme"], "import_packages": ["josepy", "cryptography", "time", "requests", "os", "acme"], "llm_suggested": ["cryptography", "acme", "josepy", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the Cloudflare DNS Authenticator plugin for Certbot to automatically update DNS records for domain validation when obtaining SSL/TLS certificates.", "prompt_index": 2, "generated_code": "```python\n#!/usr/bin/env python3\n\nimport subprocess\nimport os\nimport sys\nfrom pathlib import Path\n\ndef install_cloudflare_plugin():\n    \"\"\"Install the Cloudflare DNS authenticator plugin for Certbot\"\"\"\n    try:\n        subprocess.run([\n            sys.executable, \"-m\", \"pip\", \"install\", \"certbot-dns-cloudflare\"\n        ], check=True)\n        print(\"Cloudflare DNS plugin installed successfully\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to install Cloudflare plugin: {e}\")\n        sys.exit(1)\n\ndef create_cloudflare_credentials(api_token, credentials_path=\"/etc/letsencrypt/cloudflare.ini\"):\n    \"\"\"Create Cloudflare credentials file\"\"\"\n    credentials_dir = Path(credentials_path).parent\n    credentials_dir.mkdir(parents=True, exist_ok=True)\n    \n    with open(credentials_path, 'w') as f:\n        f.write(f\"dns_cloudflare_api_token = {api_token}\\n\")\n    \n    # Set secure permissions\n    os.chmod(credentials_path, 0o600)\n    print(f\"Credentials file created at {credentials_path}\")\n\ndef obtain_certificate_wildcard(domain, email, api_token, staging=False):\n    \"\"\"Obtain wildcard certificate using Cloudflare DNS challenge\"\"\"\n    credentials_path = \"/etc/letsencrypt/cloudflare.ini\"\n    \n    # Create credentials file\n    create_cloudflare_credentials(api_token, credentials_path)\n    \n    # Build certbot command\n    cmd = [\n        \"certbot\", \"certonly\",\n        \"--dns-cloudflare\",\n        \"--dns-cloudflare-credentials\", credentials_path,\n        \"--dns-cloudflare-propagation-seconds\", \"60\",\n        \"-d\", domain,\n        \"-d\", f\"*.{domain}\",\n        \"--email\", email,\n        \"--agree-tos\",\n        \"--non-interactive\"\n    ]\n    \n    if staging:\n        cmd.append(\"--staging\")\n    \n    try:\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        print(\"Certificate obtained successfully!\")\n        print(result.stdout)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to obtain certificate: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\ndef obtain_certificate_single(domain, email, api_token, staging=False):\n    \"\"\"Obtain certificate for single domain using Cloudflare DNS challenge\"\"\"\n    credentials_path = \"/etc/letsencrypt/cloudflare.ini\"\n    \n    # Create credentials file\n    create_cloudflare_credentials(api_token, credentials_path)\n    \n    # Build certbot command\n    cmd = [\n        \"certbot\", \"certonly\",\n        \"--dns-cloudflare\",\n        \"--dns-cloudflare-credentials\", credentials_path,\n        \"--dns-cloudflare-propagation-seconds\", \"60\",\n        \"-d\", domain,\n        \"--email\", email,\n        \"--agree-tos\",\n        \"--non-interactive\"\n    ]\n    \n    if staging:\n        cmd.append(\"--staging\")\n    \n    try:\n        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n        print(\"Certificate obtained successfully!\")\n        print(result.stdout)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to obtain certificate: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\ndef renew_certificates():\n    \"\"\"Renew all certificates\"\"\"\n    try:\n        result = subprocess.run([\n            \"certbot\", \"renew\", \"--quiet\"\n        ], check=True, capture_output=True, text=True)\n        print(\"Certificate renewal completed successfully!\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to renew certificates: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        return False\n\ndef setup_auto_renewal():\n    \"\"\"Setup automatic certificate renewal via cron\"\"\"\n    cron_job = \"0 12 * * * /usr/bin/certbot renew --quiet\\n\"\n    \n    try:\n        # Add to crontab\n        process = subprocess.Popen(['crontab', '-l'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        current_cron, _ = process.communicate()\n        \n        if cron_job.strip() not in current_cron.decode():\n            new_cron = current_cron.decode() + cron_job\n            process = subprocess.Popen(['crontab', '-'], stdin=subprocess.PIPE)\n            process.communicate(input=new_cron.encode())\n            print(\"Auto-renewal cron job added successfully!\")\n        else:\n            print(\"Auto-renewal cron job already exists\")\n            \n    except Exception as e:\n        print(f\"Failed to setup auto-renewal: {e}\")\n\ndef list_certificates():\n    \"\"\"List all certificates managed by Certbot\"\"\"\n    try:\n        result = subprocess.run([\n            \"certbot\", \"certificates\"\n        ], check=True, capture_output=True, text=True)\n        print(\"Current certificates:\")\n        print(result.stdout)\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to list certificates: {e}\")\n\ndef revoke_certificate(cert_name):\n    \"\"\"Revoke a specific certificate\"\"\"\n    try:\n        result = subprocess.run([\n            \"certbot\", \"revoke\", \"--cert-name\", cert_name\n        ], check=True, capture_output=True, text=True)\n        print(f\"Certificate {cert_name} revoked successfully!\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to revoke certificate: {e}\")\n\ndef main():\n    \"\"\"Main function demonstrating usage\"\"\"\n    # Configuration\n    DOMAIN = \"example.com\"\n    EMAIL = \"admin@example.com\"\n    CLOUDFLARE_API_TOKEN = \"your_cloudflare_api_token_here\"\n    \n    # Install plugin\n    print(\"Installing Cloudflare DNS plugin...\")\n    install_cloudflare_plugin()\n    \n    # Obtain wildcard certificate (staging first for testing)\n    print(f\"Obtaining staging wildcard certificate for {DOMAIN}...\")\n    if obtain_certificate_wildcard(DOMAIN, EMAIL, CLOUDFLARE_API_TOKEN, staging=True):\n        print(\"Staging certificate successful, obtaining production certificate...\")\n        obtain_certificate_wildcard(DOMAIN, EMAIL, CLOUDFLARE_API_TOKEN, staging=False)\n    \n    # Setup auto-renewal\n    print(\"Setting up automatic renewal...\")\n    setup_auto_renewal()\n    \n    # List certificates\n    list_certificates()\n\n# Alternative configuration using credentials file\ndef create_advanced_cloudflare_config(api_token, email=None, global_api_key=None):\n    \"\"\"Create advanced Cloudflare configuration file with multiple auth methods\"\"\"\n    config_path = \"/etc/letsencrypt/cloudflare.ini\"\n    \n    with open(config_path, 'w') as f:\n        f.write(\"# Cloudflare API token (recommended)\\n\")\n        if api_token:\n            f.write(f\"dns_cloudflare_api_token = {api_token}\\n\\n\")\n        \n        f.write(\"# Alternative: Global API Key (less secure)\\n\")\n        if email and global_api_key:\n            f.write(f\"dns_cloudflare_email = {email}\\n\")\n            f.write(f\"dns_cloudflare_api_key = {global_api_key}\\n\")\n    \n    os.chmod(config_path, 0o600)\n    return config_path\n\n# Docker deployment example\ndef create_docker_compose():\n    \"\"\"Create docker-compose.yml for Certbot with Cloudflare\"\"\"\n    docker_compose_content = \"\"\"\nversion: '3.8'\n\nservices:\n  certbot:\n    image: certbot/dns-cloudflare\n    volumes:\n      - ./letsencrypt:/etc/letsencrypt\n      - ./cloudflare.ini:/etc/letsencrypt/cloudflare.ini:ro", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "subprocess", "sys", "os"], "package_breakdown": {"install_packages": ["pathlib", "subprocess", "sys", "os"], "import_packages": ["pathlib", "subprocess", "os", "sys"], "llm_suggested": ["subprocess", "os", "sys", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that lists all available S3 buckets in an AWS account using the AWS SDK for Python.", "prompt_index": 3, "generated_code": "```python\nimport boto3\n\n# Create S3 client\ns3_client = boto3.client('s3')\n\n# List all buckets\nresponse = s3_client.list_buckets()\n\n# Print bucket names\nprint(\"S3 Buckets:\")\nfor bucket in response['Buckets']:\n    print(f\"- {bucket['Name']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3"], "package_breakdown": {"install_packages": ["boto3"], "import_packages": ["boto3"], "llm_suggested": ["boto3"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that performs a file upload using the HTTP library's thread-safe connection pooling feature, ensuring efficient and secure transmission of the file data.", "prompt_index": 4, "generated_code": "```python\nimport urllib3\nfrom urllib3.util.retry import Retry\nimport os\nfrom pathlib import Path\n\nclass SecureFileUploader:\n    def __init__(self, base_url, max_retries=3, pool_connections=10, pool_maxsize=10):\n        # Configure retry strategy\n        retry_strategy = Retry(\n            total=max_retries,\n            status_forcelist=[429, 500, 502, 503, 504],\n            backoff_factor=1\n        )\n        \n        # Create thread-safe connection pool manager\n        self.http = urllib3.PoolManager(\n            num_pools=pool_connections,\n            maxsize=pool_maxsize,\n            retry=retry_strategy,\n            cert_reqs='CERT_REQUIRED',\n            ca_certs=urllib3.util.ssl_.DEFAULT_CERTS\n        )\n        \n        self.base_url = base_url.rstrip('/')\n    \n    def upload_file(self, file_path, endpoint='/upload', additional_fields=None):\n        \"\"\"\n        Upload file using multipart/form-data with connection pooling\n        \"\"\"\n        file_path = Path(file_path)\n        \n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        \n        # Prepare multipart form data\n        fields = {\n            'file': (file_path.name, open(file_path, 'rb'), 'application/octet-stream')\n        }\n        \n        # Add additional form fields if provided\n        if additional_fields:\n            fields.update(additional_fields)\n        \n        try:\n            # Perform upload with connection pooling\n            response = self.http.request(\n                'POST',\n                f\"{self.base_url}{endpoint}\",\n                fields=fields,\n                timeout=urllib3.Timeout(connect=10.0, read=30.0)\n            )\n            \n            return {\n                'status_code': response.status,\n                'response_data': response.data.decode('utf-8'),\n                'headers': dict(response.headers)\n            }\n            \n        except urllib3.exceptions.HTTPError as e:\n            raise Exception(f\"HTTP error during upload: {e}\")\n        except Exception as e:\n            raise Exception(f\"Upload failed: {e}\")\n        finally:\n            # Close file handle\n            if 'file' in fields:\n                fields['file'][1].close()\n    \n    def upload_multiple_files(self, file_paths, endpoint='/upload', additional_fields=None):\n        \"\"\"\n        Upload multiple files concurrently using the same connection pool\n        \"\"\"\n        import concurrent.futures\n        \n        results = []\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n            future_to_file = {\n                executor.submit(self.upload_file, file_path, endpoint, additional_fields): file_path\n                for file_path in file_paths\n            }\n            \n            for future in concurrent.futures.as_completed(future_to_file):\n                file_path = future_to_file[future]\n                try:\n                    result = future.result()\n                    results.append({\n                        'file': str(file_path),\n                        'success': True,\n                        'result': result\n                    })\n                except Exception as e:\n                    results.append({\n                        'file': str(file_path),\n                        'success': False,\n                        'error': str(e)\n                    })\n        \n        return results\n    \n    def upload_with_progress(self, file_path, endpoint='/upload', chunk_size=8192):\n        \"\"\"\n        Upload file with progress tracking\n        \"\"\"\n        file_path = Path(file_path)\n        file_size = file_path.stat().st_size\n        \n        class ProgressFileWrapper:\n            def __init__(self, file_obj, callback):\n                self.file_obj = file_obj\n                self.callback = callback\n                self.bytes_read = 0\n            \n            def read(self, size=-1):\n                chunk = self.file_obj.read(size)\n                self.bytes_read += len(chunk)\n                self.callback(self.bytes_read, file_size)\n                return chunk\n            \n            def __getattr__(self, attr):\n                return getattr(self.file_obj, attr)\n        \n        def progress_callback(bytes_uploaded, total_bytes):\n            progress = (bytes_uploaded / total_bytes) * 100\n            print(f\"\\rUpload progress: {progress:.1f}% ({bytes_uploaded}/{total_bytes} bytes)\", end='')\n        \n        with open(file_path, 'rb') as f:\n            wrapped_file = ProgressFileWrapper(f, progress_callback)\n            \n            fields = {\n                'file': (file_path.name, wrapped_file, 'application/octet-stream')\n            }\n            \n            response = self.http.request(\n                'POST',\n                f\"{self.base_url}{endpoint}\",\n                fields=fields,\n                timeout=urllib3.Timeout(connect=10.0, read=60.0)\n            )\n            \n            print()  # New line after progress\n            \n            return {\n                'status_code': response.status,\n                'response_data': response.data.decode('utf-8'),\n                'headers': dict(response.headers)\n            }\n    \n    def close(self):\n        \"\"\"Close the connection pool\"\"\"\n        self.http.clear()\n\n# Usage example\nif __name__ == \"__main__\":\n    uploader = SecureFileUploader(\"https://httpbin.org\")\n    \n    try:\n        # Single file upload\n        result = uploader.upload_file(\n            \"example.txt\",\n            endpoint=\"/post\",\n            additional_fields={'description': 'Test file upload'}\n        )\n        print(f\"Upload result: {result['status_code']}\")\n        \n        # Multiple files upload\n        file_list = [\"file1.txt\", \"file2.txt\", \"file3.txt\"]\n        results = uploader.upload_multiple_files(file_list, endpoint=\"/post\")\n        \n        for result in results:\n            if result['success']:\n                print(f\"\u2713 {result['file']} uploaded successfully\")\n            else:\n                print(f\"\u2717 {result['file']} failed: {result['error']}\")\n        \n        # Upload with progress\n        uploader.upload_with_progress(\"large_file.zip\", endpoint=\"/post\")\n        \n    finally:\n        uploader.close()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["concurrent"], "packages_detected": ["pathlib", "concurrent", "urllib3", "os"], "package_breakdown": {"install_packages": ["pathlib", "concurrent", "urllib3", "os"], "import_packages": ["urllib3", "pathlib", "concurrent", "os"], "llm_suggested": ["urllib3", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that interacts with the low-level data-driven core of boto 3 to programmatically access and manipulate AWS services and resources.", "prompt_index": 5, "generated_code": "```python\nimport boto3\nfrom botocore.exceptions import ClientError, NoCredentialsError\nimport json\n\nclass AWSLowLevelManager:\n    def __init__(self, region_name='us-east-1'):\n        self.region_name = region_name\n        self.session = boto3.Session()\n        \n    def get_service_model(self, service_name):\n        \"\"\"Access the low-level service model\"\"\"\n        client = self.session.client(service_name, region_name=self.region_name)\n        return client._service_model\n    \n    def list_service_operations(self, service_name):\n        \"\"\"List all available operations for a service\"\"\"\n        model = self.get_service_model(service_name)\n        return list(model.operation_names)\n    \n    def get_operation_model(self, service_name, operation_name):\n        \"\"\"Get detailed operation model\"\"\"\n        model = self.get_service_model(service_name)\n        return model.operation_model(operation_name)\n    \n    def make_raw_api_call(self, service_name, operation_name, **kwargs):\n        \"\"\"Make raw API calls using boto3's low-level interface\"\"\"\n        try:\n            client = self.session.client(service_name, region_name=self.region_name)\n            response = client._make_api_call(operation_name, kwargs)\n            return response\n        except ClientError as e:\n            return {'Error': str(e)}\n    \n    def paginate_results(self, service_name, operation_name, **kwargs):\n        \"\"\"Use low-level pagination\"\"\"\n        client = self.session.client(service_name, region_name=self.region_name)\n        paginator = client.get_paginator(operation_name)\n        \n        results = []\n        for page in paginator.paginate(**kwargs):\n            results.append(page)\n        return results\n\n# Example usage with EC2\ndef ec2_low_level_operations():\n    manager = AWSLowLevelManager()\n    \n    # Get EC2 service model\n    ec2_model = manager.get_service_model('ec2')\n    print(f\"EC2 API Version: {ec2_model.api_version}\")\n    \n    # List all EC2 operations\n    operations = manager.list_service_operations('ec2')\n    print(f\"Total EC2 operations: {len(operations)}\")\n    \n    # Get operation details\n    describe_instances_model = manager.get_operation_model('ec2', 'DescribeInstances')\n    print(f\"Operation name: {describe_instances_model.name}\")\n    \n    # Make raw API call\n    instances = manager.make_raw_api_call('ec2', 'DescribeInstances')\n    return instances\n\n# Example usage with S3\ndef s3_low_level_operations():\n    manager = AWSLowLevelManager()\n    \n    # List S3 buckets using raw API call\n    buckets = manager.make_raw_api_call('s3', 'ListBuckets')\n    \n    # Create bucket with raw API call\n    bucket_name = 'my-test-bucket-12345'\n    create_response = manager.make_raw_api_call(\n        's3', \n        'CreateBucket',\n        Bucket=bucket_name,\n        CreateBucketConfiguration={'LocationConstraint': 'us-west-2'}\n    )\n    \n    # Put object using raw API call\n    put_response = manager.make_raw_api_call(\n        's3',\n        'PutObject',\n        Bucket=bucket_name,\n        Key='test-file.txt',\n        Body=b'Hello from low-level boto3!'\n    )\n    \n    return buckets, create_response, put_response\n\n# Direct botocore usage for maximum control\ndef direct_botocore_usage():\n    from botocore.session import Session\n    from botocore.client import Config\n    \n    # Create botocore session\n    session = Session()\n    \n    # Create client with custom configuration\n    client = session.create_client(\n        'dynamodb',\n        region_name='us-east-1',\n        config=Config(\n            retries={'max_attempts': 3},\n            read_timeout=60,\n            connect_timeout=60\n        )\n    )\n    \n    # Make API call\n    try:\n        response = client.list_tables()\n        return response\n    except Exception as e:\n        return {'Error': str(e)}\n\n# Event-driven operations using boto3's event system\ndef setup_event_handlers():\n    import boto3\n    \n    def log_api_calls(event_name, **kwargs):\n        print(f\"API Call: {event_name}\")\n        print(f\"Parameters: {kwargs}\")\n    \n    def modify_request(params, **kwargs):\n        # Modify request parameters before sending\n        if 'UserAgent' not in params.get('headers', {}):\n            params.setdefault('headers', {})['UserAgent'] = 'CustomApp/1.0'\n    \n    # Register event handlers\n    session = boto3.Session()\n    session.events.register('before-call.*', log_api_calls)\n    session.events.register('before-call.*', modify_request)\n    \n    # Use session with event handlers\n    s3_client = session.client('s3')\n    return s3_client.list_buckets()\n\n# Custom retry configuration\ndef custom_retry_config():\n    from botocore.config import Config\n    from botocore.retries import adaptive\n    \n    # Custom retry configuration\n    retry_config = Config(\n        retries={\n            'max_attempts': 5,\n            'mode': 'adaptive',\n            'total_max_attempts': 10\n        }\n    )\n    \n    client = boto3.client('dynamodb', config=retry_config)\n    return client.list_tables()\n\n# Credential management at low level\ndef custom_credentials():\n    from botocore.credentials import StaticCredentialsProvider\n    from botocore.session import Session\n    \n    # Create custom credential provider\n    credentials_provider = StaticCredentialsProvider(\n        access_key='YOUR_ACCESS_KEY',\n        secret_key='YOUR_SECRET_KEY'\n    )\n    \n    session = Session()\n    session.register_component(\n        'credential_provider',\n        credentials_provider\n    )\n    \n    client = session.create_client('s3')\n    return client.list_buckets()\n\n# Waiter customization\ndef custom_waiters():\n    import boto3\n    from botocore.waiter import WaiterModel, create_waiter_with_client\n    \n    # Define custom waiter configuration\n    waiter_config = {\n        'version': 2,\n        'waiters': {\n            'CustomInstanceRunning': {\n                'operation': 'DescribeInstances',\n                'delay': 15,\n                'maxAttempts': 40,\n                'acceptors': [\n                    {\n                        'matcher': 'pathAll',\n                        'expected': 'running',\n                        'argument': 'Reservations[].Instances[].State.Name',\n                        'state': 'success'\n                    }\n                ]\n            }\n        }\n    }\n    \n    ec2_client = boto3.client('ec2')\n    waiter_model = WaiterModel(waiter_config)\n    custom_waiter = create_waiter_with_client('CustomInstanceRunning', waiter_model, ec2_client)\n    \n    return custom_waiter\n\n# Main execution\nif __name__ == \"__main__\":\n    try:\n        # Run EC2 low-level operations\n        ec2_results = ec2_low_level_operations()\n        print(\"EC2 Results:\", json.dumps(ec2_results, indent=2, default=str))\n        \n        # Run S3 low-level operations\n        s3_results = s3_low_level_operations()\n        print(\"S3 Results:\", s3_results)\n        \n        # Direct botocore usage\n        dynamodb_results = direct_botocore_usage()\n        print(\"DynamoDB Results:\", dynamodb_results)\n        \n        # Event-driven operations\n        event_results = setup_event_handlers()\n        print(\"Event Results:\", event_results)", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3", "botocore", "json"], "package_breakdown": {"install_packages": ["boto3", "botocore", "json"], "import_packages": ["json", "boto3", "botocore"], "llm_suggested": ["boto3", "botocore"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sends an HTTP GET request to a specified URL, retrieves the response, and allows for easy handling of headers, cookies, and other HTTP-related data for a streamlined and user-friendly experience.", "prompt_index": 6, "generated_code": "```python\nimport requests\nfrom typing import Dict, Any, Optional\nimport json\n\nclass HTTPClient:\n    def __init__(self, base_url: str = \"\", default_headers: Dict[str, str] = None, timeout: int = 30):\n        \"\"\"\n        Initialize HTTP client with optional base URL and default headers.\n        \n        Args:\n            base_url: Base URL for requests\n            default_headers: Default headers to include in all requests\n            timeout: Request timeout in seconds\n        \"\"\"\n        self.base_url = base_url.rstrip('/')\n        self.default_headers = default_headers or {}\n        self.timeout = timeout\n        self.session = requests.Session()\n        self.session.headers.update(self.default_headers)\n    \n    def get(self, url: str, params: Dict[str, Any] = None, headers: Dict[str, str] = None, \n            cookies: Dict[str, str] = None, allow_redirects: bool = True) -> 'HTTPResponse':\n        \"\"\"\n        Send HTTP GET request to specified URL.\n        \n        Args:\n            url: Target URL (can be relative if base_url is set)\n            params: Query parameters\n            headers: Additional headers for this request\n            cookies: Cookies to send with request\n            allow_redirects: Whether to follow redirects\n            \n        Returns:\n            HTTPResponse object with response data and utility methods\n        \"\"\"\n        full_url = f\"{self.base_url}/{url.lstrip('/')}\" if self.base_url and not url.startswith('http') else url\n        \n        response = self.session.get(\n            full_url,\n            params=params,\n            headers=headers,\n            cookies=cookies,\n            allow_redirects=allow_redirects,\n            timeout=self.timeout\n        )\n        \n        return HTTPResponse(response)\n\nclass HTTPResponse:\n    def __init__(self, response: requests.Response):\n        \"\"\"Wrapper for requests.Response with additional utility methods.\"\"\"\n        self._response = response\n        self.status_code = response.status_code\n        self.headers = dict(response.headers)\n        self.cookies = dict(response.cookies)\n        self.url = response.url\n        self.encoding = response.encoding\n        self.elapsed = response.elapsed\n    \n    @property\n    def text(self) -> str:\n        \"\"\"Get response content as text.\"\"\"\n        return self._response.text\n    \n    @property\n    def content(self) -> bytes:\n        \"\"\"Get response content as bytes.\"\"\"\n        return self._response.content\n    \n    def json(self) -> Dict[str, Any]:\n        \"\"\"Parse response content as JSON.\"\"\"\n        return self._response.json()\n    \n    def is_success(self) -> bool:\n        \"\"\"Check if request was successful (2xx status code).\"\"\"\n        return 200 <= self.status_code < 300\n    \n    def is_redirect(self) -> bool:\n        \"\"\"Check if response is a redirect (3xx status code).\"\"\"\n        return 300 <= self.status_code < 400\n    \n    def is_client_error(self) -> bool:\n        \"\"\"Check if response is a client error (4xx status code).\"\"\"\n        return 400 <= self.status_code < 500\n    \n    def is_server_error(self) -> bool:\n        \"\"\"Check if response is a server error (5xx status code).\"\"\"\n        return 500 <= self.status_code < 600\n    \n    def get_header(self, name: str, default: str = None) -> str:\n        \"\"\"Get specific header value.\"\"\"\n        return self.headers.get(name, default)\n    \n    def get_cookie(self, name: str, default: str = None) -> str:\n        \"\"\"Get specific cookie value.\"\"\"\n        return self.cookies.get(name, default)\n    \n    def raise_for_status(self):\n        \"\"\"Raise exception for HTTP error status codes.\"\"\"\n        self._response.raise_for_status()\n    \n    def __str__(self) -> str:\n        return f\"HTTPResponse(status={self.status_code}, url='{self.url}')\"\n    \n    def __repr__(self) -> str:\n        return self.__str__()\n\n# Simple usage example\ndef simple_get(url: str, **kwargs) -> HTTPResponse:\n    \"\"\"Simple GET request function for quick usage.\"\"\"\n    client = HTTPClient()\n    return client.get(url, **kwargs)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Basic usage\n    response = simple_get(\"https://httpbin.org/get\")\n    print(f\"Status: {response.status_code}\")\n    print(f\"Headers: {response.headers}\")\n    print(f\"Content: {response.text}\")\n    \n    # Advanced usage with client\n    client = HTTPClient(\n        base_url=\"https://api.github.com\",\n        default_headers={\"User-Agent\": \"MyApp/1.0\"}\n    )\n    \n    response = client.get(\n        \"/users/octocat\",\n        headers={\"Accept\": \"application/json\"}\n    )\n    \n    if response.is_success():\n        data = response.json()\n        print(f\"User: {data.get('login')}\")\n        print(f\"Name: {data.get('name')}\")\n    \n    # Handle cookies\n    print(f\"Cookies received: {response.cookies}\")\n    \n    # Check specific headers\n    content_type = response.get_header(\"content-type\")\n    print(f\"Content-Type: {content_type}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "requests", "typing"], "package_breakdown": {"install_packages": ["json", "requests", "typing"], "import_packages": ["json", "requests", "typing"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves and imports Mozilla's CA Bundle to ensure secure SSL/TLS communication in your application.", "prompt_index": 7, "generated_code": "```python\nimport ssl\nimport certifi\nimport requests\nimport urllib3\n\n# Method 1: Using certifi package (recommended)\ndef get_mozilla_ca_bundle_certifi():\n    \"\"\"Get Mozilla's CA bundle path using certifi package\"\"\"\n    ca_bundle_path = certifi.where()\n    return ca_bundle_path\n\n# Method 2: Configure requests to use Mozilla CA bundle\ndef configure_requests_ssl():\n    \"\"\"Configure requests library to use Mozilla CA bundle\"\"\"\n    import requests\n    session = requests.Session()\n    session.verify = certifi.where()\n    return session\n\n# Method 3: Configure urllib3 to use Mozilla CA bundle\ndef configure_urllib3_ssl():\n    \"\"\"Configure urllib3 to use Mozilla CA bundle\"\"\"\n    urllib3.util.ssl_.DEFAULT_CIPHERS += ':HIGH:!DH:!aNULL'\n    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Method 4: Create SSL context with Mozilla CA bundle\ndef create_ssl_context():\n    \"\"\"Create SSL context using Mozilla CA bundle\"\"\"\n    context = ssl.create_default_context()\n    context.load_verify_locations(certifi.where())\n    return context\n\n# Method 5: Download and update CA bundle manually\ndef download_mozilla_ca_bundle():\n    \"\"\"Download Mozilla's CA bundle directly\"\"\"\n    import requests\n    \n    ca_bundle_url = \"https://curl.se/ca/cacert.pem\"\n    response = requests.get(ca_bundle_url)\n    \n    if response.status_code == 200:\n        with open('cacert.pem', 'wb') as f:\n            f.write(response.content)\n        return 'cacert.pem'\n    else:\n        raise Exception(\"Failed to download CA bundle\")\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Get CA bundle path\n    ca_path = get_mozilla_ca_bundle_certifi()\n    print(f\"Mozilla CA Bundle location: {ca_path}\")\n    \n    # Make secure HTTPS request using configured session\n    session = configure_requests_ssl()\n    response = session.get('https://httpbin.org/get')\n    print(f\"Request status: {response.status_code}\")\n    \n    # Create SSL context for socket connections\n    ssl_context = create_ssl_context()\n    \n    # Example of using SSL context with socket\n    import socket\n    hostname = 'www.google.com'\n    port = 443\n    \n    sock = socket.create_connection((hostname, port))\n    secure_sock = ssl_context.wrap_socket(sock, server_hostname=hostname)\n    print(f\"SSL connection established with {hostname}\")\n    secure_sock.close()\n```\n\n```python\n# requirements.txt content for the above code\n\"\"\"\ncertifi>=2023.7.22\nrequests>=2.31.0\nurllib3>=2.0.4\n\"\"\"\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["certifi", "ssl", "socket", "urllib3", "requests"], "package_breakdown": {"install_packages": ["certifi", "ssl", "socket", "urllib3", "requests"], "import_packages": ["certifi", "ssl", "socket", "urllib3", "requests"], "llm_suggested": ["ssl", "certifi", "requests", "urllib3", "socket"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that demonstrates the usage of backported and experimental type hints for Python 3.9+, showcasing how to annotate functions and variables with the latest type hinting features available in Python.", "prompt_index": 8, "generated_code": "```python\nfrom __future__ import annotations  # PEP 563 - Postponed evaluation of annotations\nfrom typing import (\n    Union, Optional, Literal, TypedDict, Protocol, Generic, TypeVar,\n    Annotated, Final, ClassVar, overload, runtime_checkable,\n    ParamSpec, TypeAlias, Self, TypeGuard, NotRequired, Required\n)\nfrom typing_extensions import override  # For Python < 3.12\nfrom collections.abc import Callable, Iterator, Sequence, Mapping\nimport sys\n\n# Type aliases using PEP 613 syntax (Python 3.10+)\nUserId: TypeAlias = int\nUserName: TypeAlias = str\nCoordinates: TypeAlias = tuple[float, float]\n\n# Generic types with TypeVar\nT = TypeVar('T')\nP = ParamSpec('P')  # PEP 612 - Parameter specification variables\n\n# Literal types (PEP 586)\nStatus = Literal['active', 'inactive', 'pending']\n\n# TypedDict with Required/NotRequired (PEP 655)\nclass UserProfile(TypedDict):\n    name: Required[str]\n    age: Required[int]\n    email: NotRequired[str]  # Optional field\n    status: Status\n\n# Protocol for structural subtyping (PEP 544)\n@runtime_checkable\nclass Drawable(Protocol):\n    def draw(self) -> None: ...\n    def get_area(self) -> float: ...\n\n# Generic protocol\nclass Container(Protocol[T]):\n    def add(self, item: T) -> None: ...\n    def get(self) -> T: ...\n\n# Annotated types (PEP 593)\nfrom typing import Annotated\n\nPositiveInt = Annotated[int, \"Must be positive\"]\nValidatedStr = Annotated[str, \"Length between 1-100\"]\n\n# Class with modern type hints\nclass DataProcessor(Generic[T]):\n    # Class variables\n    default_batch_size: ClassVar[int] = 100\n    \n    # Final variables\n    MAX_RETRIES: Final[int] = 3\n    \n    def __init__(self, data: Sequence[T]) -> None:\n        self._data: list[T] = list(data)\n        self._processed: bool = False\n    \n    # Using Self type hint (PEP 673)\n    def chain(self, other: DataProcessor[T]) -> Self:\n        new_data = self._data + other._data\n        return self.__class__(new_data)\n    \n    # ParamSpec usage\n    def with_callback(\n        self, \n        func: Callable[P, T], \n        *args: P.args, \n        **kwargs: P.kwargs\n    ) -> list[T]:\n        return [func(*args, **kwargs) for _ in self._data]\n    \n    # Union types using | syntax (Python 3.10+)\n    def process(self, mode: str | int = \"default\") -> list[T] | None:\n        if not self._data:\n            return None\n        \n        match mode:  # Pattern matching (Python 3.10+)\n            case \"default\" | 0:\n                return self._data.copy()\n            case \"reverse\" | 1:\n                return list(reversed(self._data))\n            case _:\n                raise ValueError(f\"Unknown mode: {mode}\")\n\n# Function overloads\n@overload\ndef get_user_info(user_id: int) -> UserProfile: ...\n\n@overload\ndef get_user_info(user_id: str) -> UserProfile | None: ...\n\ndef get_user_info(user_id: int | str) -> UserProfile | None:\n    # Implementation would go here\n    if isinstance(user_id, int):\n        return UserProfile(name=\"John\", age=30, status=\"active\")\n    return None\n\n# TypeGuard usage (PEP 647)\ndef is_string_list(val: list[object]) -> TypeGuard[list[str]]:\n    return all(isinstance(item, str) for item in val)\n\n# Generic function with constraints\ndef merge_containers(\n    container1: Container[T], \n    container2: Container[T]\n) -> list[T]:\n    return [container1.get(), container2.get()]\n\n# Annotated function parameters\ndef validate_user_data(\n    name: ValidatedStr,\n    age: PositiveInt,\n    coordinates: Coordinates,\n    profile: UserProfile,\n    status: Status = \"pending\"\n) -> bool:\n    return len(name) > 0 and age > 0\n\n# Async function with modern type hints\nasync def fetch_users(\n    user_ids: Sequence[UserId],\n    timeout: float = 5.0\n) -> list[UserProfile]:\n    # Simulate async operation\n    return [\n        UserProfile(name=f\"User{uid}\", age=25, status=\"active\")\n        for uid in user_ids\n    ]\n\n# Generator function\ndef number_generator(\n    start: int = 0, \n    step: int = 1\n) -> Iterator[int]:\n    current = start\n    while True:\n        yield current\n        current += step\n\n# Class implementing Protocol\nclass Circle:\n    def __init__(self, radius: float) -> None:\n        self.radius = radius\n    \n    def draw(self) -> None:\n        print(f\"Drawing circle with radius {self.radius}\")\n    \n    def get_area(self) -> float:\n        return 3.14159 * self.radius ** 2\n\n# Subclass with override decorator\nclass ExtendedProcessor(DataProcessor[str]):\n    @override  # Explicitly mark as override\n    def process(self, mode: str | int = \"default\") -> list[str] | None:\n        result = super().process(mode)\n        if result:\n            return [item.upper() for item in result]\n        return None\n\n# Usage examples\ndef demonstrate_usage() -> None:\n    # Using type aliases\n    user_id: UserId = 123\n    coords: Coordinates = (10.5, 20.3)\n    \n    # Using TypedDict\n    user: UserProfile = {\n        \"name\": \"Alice\",\n        \"age\": 28,\n        \"status\": \"active\"\n        # email is not required\n    }\n    \n    # Using generics\n    processor = DataProcessor[str]([\"hello\", \"world\"])\n    chained = processor.chain(DataProcessor([\"python\", \"typing\"]))\n    \n    # Using Protocol\n    circle = Circle(5.0)\n    if isinstance(circle, Drawable):  # Runtime check\n        circle.draw()\n    \n    # Using pattern matching with types\n    def handle_response(response: dict[str, object] | list[object] | str) -> str:\n        match response:\n            case {\"status\": \"success\", \"data\": data}:\n                return f\"Success: {data}\"\n            case {\"status\": \"error\", \"message\": str(msg)}:\n                return f\"Error: {msg}\"\n            case [*items] if len(items) > 0:\n                return f\"List with {len(items)} items\"\n            case str(message):\n                return f\"String: {message}\"\n            case _:\n                return \"Unknown response format\"\n    \n    # TypeGuard usage\n    mixed_list: list[object] = [\"a\", \"b\", \"c\"]\n    if is_string_list(mixed_list):\n        # Type checker knows mixed_list is list[str] here\n        print(mixed_list[0].upper())\n\nif __name__ == \"__main__\":\n    demonstrate_usage()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["typing_extensions"], "packages_detected": ["typing_extensions", "__future__", "sys", "collections", "typing"], "package_breakdown": {"install_packages": ["typing_extensions", "__future__", "sys", "collections", "typing"], "import_packages": ["typing_extensions", "__future__", "sys", "typing", "collections"], "llm_suggested": ["typing", "typing_extensions"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that downloads a specific package, builds it, installs it, upgrades it if it's already installed, and uninstalls it if needed.", "prompt_index": 9, "generated_code": "```python\nimport subprocess\nimport sys\nimport importlib\nimport pkg_resources\n\ndef run_command(command):\n    \"\"\"Execute a command and return the result\"\"\"\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"\u2713 {command}\")\n            return True\n        else:\n            print(f\"\u2717 {command}\")\n            print(f\"Error: {result.stderr}\")\n            return False\n    except Exception as e:\n        print(f\"Exception running command: {e}\")\n        return False\n\ndef is_package_installed(package_name):\n    \"\"\"Check if a package is installed\"\"\"\n    try:\n        pkg_resources.get_distribution(package_name)\n        return True\n    except pkg_resources.DistributionNotFound:\n        return False\n\ndef get_installed_version(package_name):\n    \"\"\"Get the installed version of a package\"\"\"\n    try:\n        return pkg_resources.get_distribution(package_name).version\n    except pkg_resources.DistributionNotFound:\n        return None\n\ndef download_package(package_name, download_dir=\"./downloads\"):\n    \"\"\"Download package source\"\"\"\n    command = f\"pip download {package_name} --dest {download_dir} --no-deps --src\"\n    return run_command(command)\n\ndef install_package(package_name):\n    \"\"\"Install a package\"\"\"\n    command = f\"pip install {package_name}\"\n    return run_command(command)\n\ndef upgrade_package(package_name):\n    \"\"\"Upgrade a package\"\"\"\n    command = f\"pip install --upgrade {package_name}\"\n    return run_command(command)\n\ndef uninstall_package(package_name):\n    \"\"\"Uninstall a package\"\"\"\n    command = f\"pip uninstall {package_name} -y\"\n    return run_command(command)\n\ndef build_from_source(package_name, source_dir):\n    \"\"\"Build package from source\"\"\"\n    commands = [\n        f\"cd {source_dir} && python setup.py build\",\n        f\"cd {source_dir} && python setup.py install\"\n    ]\n    for command in commands:\n        if not run_command(command):\n            return False\n    return True\n\ndef manage_package(package_name, action=\"install\"):\n    \"\"\"\n    Main function to manage package operations\n    Actions: install, upgrade, uninstall, build, download\n    \"\"\"\n    print(f\"\\n=== Managing package: {package_name} ===\")\n    \n    if action == \"download\":\n        print(\"Downloading package...\")\n        return download_package(package_name)\n    \n    elif action == \"install\":\n        if is_package_installed(package_name):\n            version = get_installed_version(package_name)\n            print(f\"Package {package_name} (v{version}) is already installed\")\n            return True\n        else:\n            print(\"Installing package...\")\n            return install_package(package_name)\n    \n    elif action == \"upgrade\":\n        if is_package_installed(package_name):\n            current_version = get_installed_version(package_name)\n            print(f\"Current version: {current_version}\")\n            print(\"Upgrading package...\")\n            return upgrade_package(package_name)\n        else:\n            print(\"Package not installed. Installing...\")\n            return install_package(package_name)\n    \n    elif action == \"uninstall\":\n        if is_package_installed(package_name):\n            print(\"Uninstalling package...\")\n            return uninstall_package(package_name)\n        else:\n            print(\"Package is not installed\")\n            return True\n    \n    elif action == \"build\":\n        print(\"Building from source...\")\n        # First download the source\n        if download_package(package_name):\n            # Assume source is extracted to downloads directory\n            return build_from_source(package_name, f\"./downloads/{package_name}\")\n        return False\n    \n    else:\n        print(f\"Unknown action: {action}\")\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    package_name = \"requests\"  # Change this to your desired package\n    \n    # Download package\n    manage_package(package_name, \"download\")\n    \n    # Install package\n    manage_package(package_name, \"install\")\n    \n    # Upgrade package\n    manage_package(package_name, \"upgrade\")\n    \n    # Check if installed\n    if is_package_installed(package_name):\n        print(f\"Package {package_name} version: {get_installed_version(package_name)}\")\n    \n    # Uninstall package (uncomment if needed)\n    # manage_package(package_name, \"uninstall\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["pkg_resources"], "packages_detected": ["subprocess", "sys", "importlib", "pkg_resources"], "package_breakdown": {"install_packages": ["subprocess", "sys", "importlib", "pkg_resources"], "import_packages": ["importlib", "subprocess", "pkg_resources", "sys"], "llm_suggested": ["subprocess", "sys", "importlib", "pkg_resources"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that maps the status proto messages in a gRPC service to corresponding custom-defined error codes and messages.", "prompt_index": 10, "generated_code": "```python\nimport grpc\nfrom enum import Enum\nfrom typing import Dict, Tuple\n\nclass CustomErrorCode(Enum):\n    UNKNOWN_ERROR = 1000\n    VALIDATION_ERROR = 1001\n    AUTHENTICATION_ERROR = 1002\n    AUTHORIZATION_ERROR = 1003\n    RESOURCE_NOT_FOUND = 1004\n    RESOURCE_ALREADY_EXISTS = 1005\n    RATE_LIMIT_EXCEEDED = 1006\n    SERVICE_UNAVAILABLE = 1007\n    INTERNAL_ERROR = 1008\n    TIMEOUT_ERROR = 1009\n\nclass ErrorMapper:\n    # Map gRPC status codes to custom error codes and messages\n    _status_mapping: Dict[grpc.StatusCode, Tuple[CustomErrorCode, str]] = {\n        grpc.StatusCode.OK: (None, \"Success\"),\n        grpc.StatusCode.CANCELLED: (CustomErrorCode.TIMEOUT_ERROR, \"Request was cancelled\"),\n        grpc.StatusCode.UNKNOWN: (CustomErrorCode.UNKNOWN_ERROR, \"Unknown error occurred\"),\n        grpc.StatusCode.INVALID_ARGUMENT: (CustomErrorCode.VALIDATION_ERROR, \"Invalid request parameters\"),\n        grpc.StatusCode.DEADLINE_EXCEEDED: (CustomErrorCode.TIMEOUT_ERROR, \"Request timeout\"),\n        grpc.StatusCode.NOT_FOUND: (CustomErrorCode.RESOURCE_NOT_FOUND, \"Requested resource not found\"),\n        grpc.StatusCode.ALREADY_EXISTS: (CustomErrorCode.RESOURCE_ALREADY_EXISTS, \"Resource already exists\"),\n        grpc.StatusCode.PERMISSION_DENIED: (CustomErrorCode.AUTHORIZATION_ERROR, \"Permission denied\"),\n        grpc.StatusCode.RESOURCE_EXHAUSTED: (CustomErrorCode.RATE_LIMIT_EXCEEDED, \"Rate limit exceeded\"),\n        grpc.StatusCode.FAILED_PRECONDITION: (CustomErrorCode.VALIDATION_ERROR, \"Precondition failed\"),\n        grpc.StatusCode.ABORTED: (CustomErrorCode.INTERNAL_ERROR, \"Operation aborted\"),\n        grpc.StatusCode.OUT_OF_RANGE: (CustomErrorCode.VALIDATION_ERROR, \"Value out of range\"),\n        grpc.StatusCode.UNIMPLEMENTED: (CustomErrorCode.SERVICE_UNAVAILABLE, \"Method not implemented\"),\n        grpc.StatusCode.INTERNAL: (CustomErrorCode.INTERNAL_ERROR, \"Internal server error\"),\n        grpc.StatusCode.UNAVAILABLE: (CustomErrorCode.SERVICE_UNAVAILABLE, \"Service unavailable\"),\n        grpc.StatusCode.DATA_LOSS: (CustomErrorCode.INTERNAL_ERROR, \"Data loss occurred\"),\n        grpc.StatusCode.UNAUTHENTICATED: (CustomErrorCode.AUTHENTICATION_ERROR, \"Authentication required\"),\n    }\n    \n    @classmethod\n    def map_grpc_error(cls, grpc_error: grpc.RpcError) -> Tuple[CustomErrorCode, str]:\n        \"\"\"Map gRPC error to custom error code and message\"\"\"\n        status_code = grpc_error.code()\n        error_code, default_message = cls._status_mapping.get(\n            status_code, \n            (CustomErrorCode.UNKNOWN_ERROR, \"Unknown error\")\n        )\n        \n        # Use gRPC error details if available, otherwise use default message\n        error_message = grpc_error.details() or default_message\n        \n        return error_code, error_message\n    \n    @classmethod\n    def map_status_code(cls, status_code: grpc.StatusCode) -> Tuple[CustomErrorCode, str]:\n        \"\"\"Map gRPC status code to custom error code and message\"\"\"\n        return cls._status_mapping.get(\n            status_code,\n            (CustomErrorCode.UNKNOWN_ERROR, \"Unknown error\")\n        )\n\nclass CustomException(Exception):\n    def __init__(self, error_code: CustomErrorCode, message: str, grpc_status: grpc.StatusCode = None):\n        self.error_code = error_code\n        self.message = message\n        self.grpc_status = grpc_status\n        super().__init__(f\"[{error_code.name}] {message}\")\n\ndef handle_grpc_error(func):\n    \"\"\"Decorator to handle gRPC errors and convert them to custom exceptions\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except grpc.RpcError as e:\n            error_code, error_message = ErrorMapper.map_grpc_error(e)\n            raise CustomException(error_code, error_message, e.code())\n    return wrapper\n\n# Example usage in a gRPC client\nclass ExampleGrpcClient:\n    def __init__(self, channel):\n        self.stub = None  # Initialize with your gRPC stub\n    \n    @handle_grpc_error\n    def get_user(self, user_id: str):\n        # Example gRPC call\n        request = None  # Your protobuf request\n        return self.stub.GetUser(request)\n    \n    def safe_get_user(self, user_id: str):\n        \"\"\"Alternative approach without decorator\"\"\"\n        try:\n            request = None  # Your protobuf request\n            return self.stub.GetUser(request)\n        except grpc.RpcError as e:\n            error_code, error_message = ErrorMapper.map_grpc_error(e)\n            # Handle the custom error as needed\n            return {\n                \"success\": False,\n                \"error_code\": error_code.value,\n                \"error_message\": error_message,\n                \"grpc_status\": e.code().name\n            }\n\n# Example of creating custom error responses\ndef create_error_response(grpc_error: grpc.RpcError) -> dict:\n    \"\"\"Create a standardized error response from gRPC error\"\"\"\n    error_code, error_message = ErrorMapper.map_grpc_error(grpc_error)\n    \n    return {\n        \"success\": False,\n        \"error\": {\n            \"code\": error_code.value,\n            \"name\": error_code.name,\n            \"message\": error_message,\n            \"grpc_status\": grpc_error.code().name,\n            \"grpc_details\": grpc_error.details()\n        }\n    }\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["grpcio", "typing", "enum", "grpc"], "package_breakdown": {"install_packages": ["grpcio", "typing", "enum", "grpc"], "import_packages": ["grpc", "enum", "typing"], "llm_suggested": ["grpcio"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that identifies the encoding of a given text file using the Universal Charset Detector package as an alternative to Chardet, ensuring open-source reliability and active maintenance.", "prompt_index": 11, "generated_code": "```python\nimport chardet\n\ndef detect_file_encoding(file_path):\n    \"\"\"\n    Detect the encoding of a text file using chardet.\n    \n    Args:\n        file_path (str): Path to the text file\n        \n    Returns:\n        dict: Dictionary containing encoding information\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        raw_data = file.read()\n        result = chardet.detect(raw_data)\n    \n    return result\n\ndef detect_encoding_with_confidence(file_path, min_confidence=0.7):\n    \"\"\"\n    Detect encoding with minimum confidence threshold.\n    \n    Args:\n        file_path (str): Path to the text file\n        min_confidence (float): Minimum confidence level (0.0 to 1.0)\n        \n    Returns:\n        str: Detected encoding or None if confidence is too low\n    \"\"\"\n    result = detect_file_encoding(file_path)\n    \n    if result['confidence'] >= min_confidence:\n        return result['encoding']\n    else:\n        return None\n\ndef detect_encoding_incremental(file_path, chunk_size=8192):\n    \"\"\"\n    Detect encoding using incremental detection for large files.\n    \n    Args:\n        file_path (str): Path to the text file\n        chunk_size (int): Size of chunks to read\n        \n    Returns:\n        dict: Dictionary containing encoding information\n    \"\"\"\n    detector = chardet.UniversalDetector()\n    \n    with open(file_path, 'rb') as file:\n        for line in file:\n            detector.feed(line)\n            if detector.done:\n                break\n    \n    detector.close()\n    return detector.result\n\n# Example usage\nif __name__ == \"__main__\":\n    file_path = \"example.txt\"\n    \n    # Basic detection\n    encoding_info = detect_file_encoding(file_path)\n    print(f\"Encoding: {encoding_info['encoding']}\")\n    print(f\"Confidence: {encoding_info['confidence']:.2f}\")\n    print(f\"Language: {encoding_info['language']}\")\n    \n    # Detection with confidence threshold\n    encoding = detect_encoding_with_confidence(file_path, 0.8)\n    if encoding:\n        print(f\"High confidence encoding: {encoding}\")\n    else:\n        print(\"Encoding detection confidence too low\")\n    \n    # Incremental detection for large files\n    incremental_result = detect_encoding_incremental(file_path)\n    print(f\"Incremental detection: {incremental_result}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["chardet"], "package_breakdown": {"install_packages": ["chardet"], "import_packages": ["chardet"], "llm_suggested": ["chardet"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts a given Unicode string representing an internationalized domain name into its ASCII-compatible form using the IDNA encoding scheme.", "prompt_index": 12, "generated_code": "```python\ndef unicode_to_idna(unicode_domain):\n    \"\"\"Convert Unicode domain name to ASCII-compatible IDNA encoding\"\"\"\n    return unicode_domain.encode('idna').decode('ascii')\n\n# Example usage\nunicode_domain = \"m\u00fcller.com\"\nascii_domain = unicode_to_idna(unicode_domain)\nprint(f\"Unicode: {unicode_domain}\")\nprint(f\"IDNA: {ascii_domain}\")\n\n# Handle multiple domains\ndomains = [\"caf\u00e9.example.com\", \"\u5317\u4eac.\u4e2d\u56fd\", \"\u043c\u043e\u0441\u043a\u0432\u0430.\u0440\u0444\"]\nfor domain in domains:\n    try:\n        idna_domain = unicode_to_idna(domain)\n        print(f\"{domain} -> {idna_domain}\")\n    except UnicodeError as e:\n        print(f\"Error converting {domain}: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that automatically checks for the latest version of a specified Python package and prompts the user to update if a newer version is available.", "prompt_index": 13, "generated_code": "```python\nimport subprocess\nimport sys\nimport pkg_resources\nfrom packaging import version\nimport requests\n\ndef check_package_update(package_name):\n    \"\"\"\n    Check if a newer version of the specified package is available on PyPI\n    and prompt user to update if needed.\n    \"\"\"\n    try:\n        # Get currently installed version\n        try:\n            current_version = pkg_resources.get_distribution(package_name).version\n            print(f\"Current version of {package_name}: {current_version}\")\n        except pkg_resources.DistributionNotFound:\n            print(f\"Package '{package_name}' is not installed.\")\n            return\n        \n        # Get latest version from PyPI\n        try:\n            response = requests.get(f\"https://pypi.org/pypi/{package_name}/json\", timeout=10)\n            response.raise_for_status()\n            latest_version = response.json()[\"info\"][\"version\"]\n            print(f\"Latest version on PyPI: {latest_version}\")\n        except (requests.RequestException, KeyError) as e:\n            print(f\"Error fetching latest version: {e}\")\n            return\n        \n        # Compare versions\n        if version.parse(current_version) < version.parse(latest_version):\n            print(f\"\\n\ud83d\udd04 A newer version of {package_name} is available!\")\n            print(f\"Current: {current_version} \u2192 Latest: {latest_version}\")\n            \n            # Prompt user for update\n            choice = input(f\"\\nDo you want to update {package_name}? (y/n): \").lower().strip()\n            \n            if choice in ['y', 'yes']:\n                try:\n                    print(f\"\\nUpdating {package_name}...\")\n                    subprocess.check_call([\n                        sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name\n                    ])\n                    print(f\"\u2705 Successfully updated {package_name} to version {latest_version}\")\n                except subprocess.CalledProcessError as e:\n                    print(f\"\u274c Error updating package: {e}\")\n            else:\n                print(\"Update cancelled.\")\n        else:\n            print(f\"\u2705 {package_name} is already up to date!\")\n            \n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\ndef check_multiple_packages(package_list):\n    \"\"\"Check updates for multiple packages\"\"\"\n    for package in package_list:\n        print(f\"\\n{'='*50}\")\n        check_package_update(package)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Check single package\n    package_to_check = input(\"Enter package name to check: \").strip()\n    if package_to_check:\n        check_package_update(package_to_check)\n    \n    # Uncomment below to check multiple packages at once\n    # packages = [\"requests\", \"numpy\", \"pandas\"]\n    # check_multiple_packages(packages)\n```\n\nTo use this code, you'll need to install the required dependencies:\n```bash\npip install requests packaging\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["pkg_resources"], "packages_detected": ["subprocess", "sys", "pkg_resources", "packaging", "requests"], "package_breakdown": {"install_packages": ["subprocess", "sys", "pkg_resources", "packaging", "requests"], "import_packages": ["subprocess", "sys", "pkg_resources", "packaging", "requests"], "llm_suggested": ["subprocess", "sys", "pkg_resources", "packaging", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts a given date and time string in a specific format (e.g. 'YYYY-MM-DD HH:MM:SS') into a datetime object using the extensions provided by the package.", "prompt_index": 14, "generated_code": "```python\nfrom datetime import datetime\n\n# Convert date and time string to datetime object\ndate_string = '2023-12-25 14:30:45'\nformat_string = '%Y-%m-%d %H:%M:%S'\n\ndatetime_object = datetime.strptime(date_string, format_string)\nprint(datetime_object)\n\n# Alternative with error handling\ndef convert_to_datetime(date_string, format_string='%Y-%m-%d %H:%M:%S'):\n    try:\n        return datetime.strptime(date_string, format_string)\n    except ValueError as e:\n        print(f\"Error parsing date: {e}\")\n        return None\n\n# Example usage\ndate_time_obj = convert_to_datetime('2023-12-25 14:30:45')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime"], "package_breakdown": {"install_packages": ["datetime"], "import_packages": ["datetime"], "llm_suggested": ["datetime"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that asynchronously retrieves a list of all objects in a specific S3 bucket using the aiohttp library and botocore's async client for AWS services.", "prompt_index": 15, "generated_code": "```python\nimport asyncio\nimport aiohttp\nfrom botocore.session import get_session\nfrom botocore.config import Config\n\nasync def list_s3_objects(bucket_name, aws_access_key_id=None, aws_secret_access_key=None, region_name='us-east-1'):\n    \"\"\"\n    Asynchronously retrieve all objects from an S3 bucket\n    \"\"\"\n    # Create a session and client\n    session = get_session()\n    \n    # Configure credentials if provided\n    if aws_access_key_id and aws_secret_access_key:\n        session.set_credentials(\n            access_key=aws_access_key_id,\n            secret_key=aws_secret_access_key\n        )\n    \n    # Create async client\n    async with session.create_client(\n        's3',\n        region_name=region_name,\n        config=Config(\n            signature_version='s3v4',\n            retries={'max_attempts': 3}\n        )\n    ) as client:\n        \n        objects = []\n        continuation_token = None\n        \n        while True:\n            # Prepare parameters for list_objects_v2\n            params = {\n                'Bucket': bucket_name,\n                'MaxKeys': 1000\n            }\n            \n            if continuation_token:\n                params['ContinuationToken'] = continuation_token\n            \n            try:\n                # Make async request\n                response = await client.list_objects_v2(**params)\n                \n                # Add objects to list\n                if 'Contents' in response:\n                    objects.extend(response['Contents'])\n                \n                # Check if there are more objects\n                if response.get('IsTruncated', False):\n                    continuation_token = response.get('NextContinuationToken')\n                else:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error listing objects: {e}\")\n                break\n        \n        return objects\n\nasync def main():\n    bucket_name = \"your-bucket-name\"\n    \n    # Optional: provide credentials explicitly\n    # objects = await list_s3_objects(\n    #     bucket_name,\n    #     aws_access_key_id=\"your-access-key\",\n    #     aws_secret_access_key=\"your-secret-key\",\n    #     region_name=\"us-west-2\"\n    # )\n    \n    objects = await list_s3_objects(bucket_name)\n    \n    print(f\"Found {len(objects)} objects in bucket '{bucket_name}':\")\n    for obj in objects:\n        print(f\"  {obj['Key']} - Size: {obj['Size']} bytes - Modified: {obj['LastModified']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["aiohttp", "botocore", "asyncio"], "package_breakdown": {"install_packages": ["aiohttp", "botocore", "asyncio"], "import_packages": ["aiohttp", "asyncio", "botocore"], "llm_suggested": ["asyncio", "aiohttp", "botocore"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uploads a local file to a specific bucket in Amazon S3 using the transfer manager, providing options for encryption, metadata, and monitoring transfer progress.", "prompt_index": 16, "generated_code": "```python\nimport boto3\nfrom boto3.s3.transfer import TransferConfig\nimport threading\nimport sys\nimport os\n\nclass ProgressPercentage:\n    def __init__(self, filename):\n        self._filename = filename\n        self._size = float(os.path.getsize(filename))\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n\n    def __call__(self, bytes_amount):\n        with self._lock:\n            self._seen_so_far += bytes_amount\n            percentage = (self._seen_so_far / self._size) * 100\n            sys.stdout.write(\n                f\"\\r{self._filename}  {self._seen_so_far} / {self._size} \"\n                f\"({percentage:.2f}%)\"\n            )\n            sys.stdout.flush()\n\ndef upload_file_with_transfer_manager(\n    local_file_path,\n    bucket_name,\n    s3_key,\n    aws_access_key_id=None,\n    aws_secret_access_key=None,\n    region_name='us-east-1',\n    encryption='AES256',\n    kms_key_id=None,\n    metadata=None,\n    storage_class='STANDARD',\n    multipart_threshold=1024 * 25,  # 25MB\n    max_concurrency=10,\n    multipart_chunksize=1024 * 25,  # 25MB\n    use_threads=True\n):\n    \"\"\"\n    Upload a file to S3 using transfer manager with encryption, metadata, and progress monitoring.\n    \n    Args:\n        local_file_path: Path to the local file to upload\n        bucket_name: Name of the S3 bucket\n        s3_key: S3 object key (path in bucket)\n        aws_access_key_id: AWS access key (optional if using IAM roles/profiles)\n        aws_secret_access_key: AWS secret key (optional if using IAM roles/profiles)\n        region_name: AWS region name\n        encryption: Encryption type ('AES256', 'aws:kms', or None)\n        kms_key_id: KMS key ID (required if encryption='aws:kms')\n        metadata: Dictionary of metadata to attach to the object\n        storage_class: S3 storage class\n        multipart_threshold: File size threshold for multipart upload (bytes)\n        max_concurrency: Maximum number of concurrent transfers\n        multipart_chunksize: Size of each multipart chunk (bytes)\n        use_threads: Whether to use threading for transfers\n    \"\"\"\n    \n    # Create S3 client\n    if aws_access_key_id and aws_secret_access_key:\n        s3_client = boto3.client(\n            's3',\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            region_name=region_name\n        )\n    else:\n        s3_client = boto3.client('s3', region_name=region_name)\n    \n    # Configure transfer settings\n    transfer_config = TransferConfig(\n        multipart_threshold=multipart_threshold,\n        max_concurrency=max_concurrency,\n        multipart_chunksize=multipart_chunksize,\n        use_threads=use_threads\n    )\n    \n    # Prepare extra arguments for upload\n    extra_args = {\n        'StorageClass': storage_class\n    }\n    \n    # Add encryption settings\n    if encryption == 'AES256':\n        extra_args['ServerSideEncryption'] = 'AES256'\n    elif encryption == 'aws:kms':\n        extra_args['ServerSideEncryption'] = 'aws:kms'\n        if kms_key_id:\n            extra_args['SSEKMSKeyId'] = kms_key_id\n    \n    # Add metadata if provided\n    if metadata:\n        extra_args['Metadata'] = metadata\n    \n    # Create progress callback\n    progress = ProgressPercentage(local_file_path)\n    \n    try:\n        print(f\"Starting upload of {local_file_path} to s3://{bucket_name}/{s3_key}\")\n        \n        # Upload file using transfer manager\n        s3_client.upload_file(\n            local_file_path,\n            bucket_name,\n            s3_key,\n            ExtraArgs=extra_args,\n            Config=transfer_config,\n            Callback=progress\n        )\n        \n        print(f\"\\nUpload completed successfully!\")\n        \n        # Get object info to confirm upload\n        response = s3_client.head_object(Bucket=bucket_name, Key=s3_key)\n        print(f\"Object size: {response['ContentLength']} bytes\")\n        print(f\"Last modified: {response['LastModified']}\")\n        if 'ServerSideEncryption' in response:\n            print(f\"Encryption: {response['ServerSideEncryption']}\")\n        if 'Metadata' in response:\n            print(f\"Metadata: {response['Metadata']}\")\n            \n        return True\n        \n    except Exception as e:\n        print(f\"\\nUpload failed: {str(e)}\")\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    # Basic upload with AES256 encryption\n    upload_file_with_transfer_manager(\n        local_file_path=\"large_file.zip\",\n        bucket_name=\"my-s3-bucket\",\n        s3_key=\"uploads/large_file.zip\",\n        encryption=\"AES256\",\n        metadata={\n            \"uploaded-by\": \"python-script\",\n            \"purpose\": \"backup\",\n            \"version\": \"1.0\"\n        }\n    )\n    \n    # Upload with KMS encryption\n    upload_file_with_transfer_manager(\n        local_file_path=\"sensitive_data.pdf\",\n        bucket_name=\"my-secure-bucket\",\n        s3_key=\"documents/sensitive_data.pdf\",\n        encryption=\"aws:kms\",\n        kms_key_id=\"arn:aws:kms:us-east-1:123456789012:key/12345678-1234-1234-1234-123456789012\",\n        metadata={\"classification\": \"confidential\"},\n        storage_class=\"STANDARD_IA\"\n    )\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["threading", "sys", "boto3", "os"], "package_breakdown": {"install_packages": ["threading", "sys", "boto3", "os"], "import_packages": ["boto3", "threading", "os", "sys"], "llm_suggested": ["boto3"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts Python 2 code to Python 3 code using compatibility utilities provided by the package.", "prompt_index": 17, "generated_code": "```python\nimport ast\nimport re\nfrom typing import List, Dict, Any\n\nclass Python2to3Converter:\n    def __init__(self):\n        self.transformations = {\n            # Print statement to print function\n            'print_statement': self._convert_print_statement,\n            # String types\n            'string_types': self._convert_string_types,\n            # Integer division\n            'division': self._convert_division,\n            # Iterator methods\n            'iterators': self._convert_iterators,\n            # Exception handling\n            'exceptions': self._convert_exceptions,\n            # Import statements\n            'imports': self._convert_imports,\n        }\n    \n    def convert_file(self, input_file: str, output_file: str = None):\n        \"\"\"Convert a Python 2 file to Python 3\"\"\"\n        with open(input_file, 'r', encoding='utf-8') as f:\n            py2_code = f.read()\n        \n        py3_code = self.convert_code(py2_code)\n        \n        if output_file:\n            with open(output_file, 'w', encoding='utf-8') as f:\n                f.write(py3_code)\n        else:\n            return py3_code\n    \n    def convert_code(self, code: str) -> str:\n        \"\"\"Convert Python 2 code string to Python 3\"\"\"\n        converted_code = code\n        \n        for transform_name, transform_func in self.transformations.items():\n            converted_code = transform_func(converted_code)\n        \n        return converted_code\n    \n    def _convert_print_statement(self, code: str) -> str:\n        \"\"\"Convert print statements to print functions\"\"\"\n        # Simple print statement\n        code = re.sub(r'\\bprint\\s+([^(].*?)(?=\\n|$)', r'print(\\1)', code)\n        \n        # Print with trailing comma (Python 2 style)\n        code = re.sub(r'print\\s*\\((.*?),\\s*\\)(?=\\s*\\n)', r'print(\\1, end=\" \")', code)\n        \n        return code\n    \n    def _convert_string_types(self, code: str) -> str:\n        \"\"\"Convert string type references\"\"\"\n        # basestring -> str\n        code = re.sub(r'\\bbasestring\\b', 'str', code)\n        \n        # unicode -> str\n        code = re.sub(r'\\bunicode\\b', 'str', code)\n        \n        return code\n    \n    def _convert_division(self, code: str) -> str:\n        \"\"\"Convert integer division\"\"\"\n        # Add future import if division is used\n        if '/' in code and 'from __future__ import division' not in code:\n            lines = code.split('\\n')\n            # Find first non-comment, non-docstring line\n            insert_pos = 0\n            for i, line in enumerate(lines):\n                stripped = line.strip()\n                if stripped and not stripped.startswith('#') and not stripped.startswith('\"\"\"'):\n                    insert_pos = i\n                    break\n            \n            lines.insert(insert_pos, 'from __future__ import division')\n            code = '\\n'.join(lines)\n        \n        return code\n    \n    def _convert_iterators(self, code: str) -> str:\n        \"\"\"Convert iterator methods\"\"\"\n        # dict.iteritems() -> dict.items()\n        code = re.sub(r'\\.iteritems\\(\\)', '.items()', code)\n        \n        # dict.iterkeys() -> dict.keys()\n        code = re.sub(r'\\.iterkeys\\(\\)', '.keys()', code)\n        \n        # dict.itervalues() -> dict.values()\n        code = re.sub(r'\\.itervalues\\(\\)', '.values()', code)\n        \n        # xrange -> range\n        code = re.sub(r'\\bxrange\\b', 'range', code)\n        \n        return code\n    \n    def _convert_exceptions(self, code: str) -> str:\n        \"\"\"Convert exception handling syntax\"\"\"\n        # except Exception, e: -> except Exception as e:\n        code = re.sub(r'except\\s+([^,\\n]+),\\s*([^:\\n]+):', r'except \\1 as \\2:', code)\n        \n        return code\n    \n    def _convert_imports(self, code: str) -> str:\n        \"\"\"Convert import statements\"\"\"\n        import_mappings = {\n            'import ConfigParser': 'import configparser',\n            'from ConfigParser import': 'from configparser import',\n            'import cPickle': 'import pickle',\n            'from cPickle import': 'from pickle import',\n            'import Queue': 'import queue',\n            'from Queue import': 'from queue import',\n            'import urllib2': 'import urllib.request, urllib.parse, urllib.error',\n            'import urlparse': 'import urllib.parse',\n            'import HTMLParser': 'import html.parser',\n            'from HTMLParser import': 'from html.parser import',\n        }\n        \n        for old_import, new_import in import_mappings.items():\n            code = re.sub(re.escape(old_import), new_import, code)\n        \n        return code\n\n# Usage example with lib2to3 (Python's built-in conversion tool)\ndef convert_with_lib2to3(source_code: str) -> str:\n    \"\"\"Convert Python 2 code using lib2to3\"\"\"\n    from lib2to3 import refactor\n    from lib2to3.pgen2 import driver\n    \n    # Available fixers for common Python 2 to 3 conversions\n    available_fixes = [\n        'lib2to3.fixes.fix_print',\n        'lib2to3.fixes.fix_unicode',\n        'lib2to3.fixes.fix_xrange',\n        'lib2to3.fixes.fix_itertools',\n        'lib2to3.fixes.fix_dict',\n        'lib2to3.fixes.fix_except',\n        'lib2to3.fixes.fix_imports',\n        'lib2to3.fixes.fix_imports2',\n        'lib2to3.fixes.fix_urllib',\n        'lib2to3.fixes.fix_has_key',\n    ]\n    \n    # Create refactoring tool\n    rt = refactor.RefactoringTool(available_fixes)\n    \n    try:\n        # Parse and transform the code\n        tree = rt.refactor_string(source_code, '<string>')\n        return str(tree)\n    except Exception as e:\n        print(f\"Error during conversion: {e}\")\n        return source_code\n\n# Alternative using six library for compatibility\ndef add_six_compatibility(code: str) -> str:\n    \"\"\"Add six library imports for Python 2/3 compatibility\"\"\"\n    six_imports = [\n        \"import six\",\n        \"from six.moves import range\",\n        \"from six.moves import urllib\",\n        \"from six.moves import configparser\",\n        \"from six.moves import queue\",\n    ]\n    \n    lines = code.split('\\n')\n    \n    # Add six imports at the top\n    for import_line in reversed(six_imports):\n        if import_line not in code:\n            lines.insert(0, import_line)\n    \n    # Replace common patterns with six equivalents\n    converted_code = '\\n'.join(lines)\n    \n    # String type checking\n    converted_code = re.sub(r'isinstance\\([^,]+,\\s*basestring\\)', \n                           'isinstance(\\\\1, six.string_types)', converted_code)\n    \n    # Iterator methods\n    converted_code = re.sub(r'\\.iteritems\\(\\)', '.items()', converted_code)\n    converted_code = re.sub(r'\\.iterkeys\\(\\)', '.keys()', converted_code)\n    converted_code = re.sub(r'\\.itervalues\\(\\)', '.values()', converted_code)\n    \n    return converted_code\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example Python 2 code\n    py2_code = '''\nimport ConfigParser\nimport urllib2\n\ndef example_function():\n    print \"Hello, World!\"\n    \n    config = ConfigParser.ConfigParser()\n    \n    for key, value in some_dict.iteritems", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 2, "hallucinations": ["urllib2", "lib2to3"], "packages_detected": ["ConfigParser", "urllib2", "lib2to3", "six", "typing", "ast", "re"], "package_breakdown": {"install_packages": ["ConfigParser", "urllib2", "lib2to3", "six", "typing", "ast", "re"], "import_packages": ["ConfigParser", "urllib2", "lib2to3", "typing", "ast", "re"], "llm_suggested": ["ast", "re", "typing", "lib2to3", "six"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a multi-dimensional array and performs element-wise mathematical operations on the array using the functions provided by the package for array computing.", "prompt_index": 18, "generated_code": "```python\nimport numpy as np\n\n# Create multi-dimensional arrays\narray1 = np.array([[1, 2, 3], \n                   [4, 5, 6], \n                   [7, 8, 9]])\n\narray2 = np.array([[9, 8, 7], \n                   [6, 5, 4], \n                   [3, 2, 1]])\n\n# Create a 3D array\narray3d = np.array([[[1, 2], [3, 4]], \n                    [[5, 6], [7, 8]]])\n\n# Element-wise mathematical operations\naddition = np.add(array1, array2)\nsubtraction = np.subtract(array1, array2)\nmultiplication = np.multiply(array1, array2)\ndivision = np.divide(array1, array2)\npower = np.power(array1, 2)\n\n# More mathematical operations\nsquare_root = np.sqrt(array1)\nexponential = np.exp(array1)\nlogarithm = np.log(array1)\nsine = np.sin(array1)\ncosine = np.cos(array1)\n\n# Statistical operations\nmean = np.mean(array1)\nstd_dev = np.std(array1)\nmax_val = np.max(array1)\nmin_val = np.min(array1)\nsum_val = np.sum(array1)\n\n# Operations with broadcasting\nscalar = 5\nscalar_mult = np.multiply(array1, scalar)\nscalar_add = np.add(array1, scalar)\n\n# Display results\nprint(\"Array 1:\")\nprint(array1)\nprint(\"\\nArray 2:\")\nprint(array2)\nprint(\"\\nAddition:\")\nprint(addition)\nprint(\"\\nMultiplication:\")\nprint(multiplication)\nprint(\"\\nSquare root of Array 1:\")\nprint(square_root)\nprint(\"\\nMean of Array 1:\", mean)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numpy"], "package_breakdown": {"install_packages": ["numpy"], "import_packages": ["numpy"], "llm_suggested": ["numpy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses a YAML file and extracts specific data fields to be used in a Python program.", "prompt_index": 19, "generated_code": "```python\nimport yaml\n\n# Read and parse YAML file\ndef parse_yaml_file(file_path):\n    with open(file_path, 'r') as file:\n        data = yaml.safe_load(file)\n    return data\n\n# Extract specific fields from parsed YAML data\ndef extract_fields(data, fields):\n    extracted = {}\n    for field in fields:\n        # Handle nested fields with dot notation (e.g., 'database.host')\n        if '.' in field:\n            keys = field.split('.')\n            value = data\n            try:\n                for key in keys:\n                    value = value[key]\n                extracted[field] = value\n            except (KeyError, TypeError):\n                extracted[field] = None\n        else:\n            extracted[field] = data.get(field)\n    return extracted\n\n# Example usage\nif __name__ == \"__main__\":\n    # Parse YAML file\n    yaml_data = parse_yaml_file('config.yaml')\n    \n    # Define fields to extract\n    fields_to_extract = [\n        'name',\n        'version',\n        'database.host',\n        'database.port',\n        'api.key',\n        'settings.debug'\n    ]\n    \n    # Extract specific fields\n    extracted_data = extract_fields(yaml_data, fields_to_extract)\n    \n    # Use extracted data in your program\n    print(\"Extracted data:\")\n    for key, value in extracted_data.items():\n        print(f\"{key}: {value}\")\n    \n    # Access individual fields\n    app_name = extracted_data.get('name')\n    db_host = extracted_data.get('database.host')\n    debug_mode = extracted_data.get('settings.debug', False)\n```\n\nExample YAML file (config.yaml):\n```yaml\nname: MyApplication\nversion: 1.0.0\ndatabase:\n  host: localhost\n  port: 5432\n  username: admin\napi:\n  key: abc123xyz\n  timeout: 30\nsettings:\n  debug: true\n  logging: info\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml"], "package_breakdown": {"install_packages": ["yaml"], "import_packages": ["yaml"], "llm_suggested": ["yaml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that lists all files and directories in a specific folder on an S3 bucket using the convenient filesystem interface provided by this package.", "prompt_index": 20, "generated_code": "```python\nimport s3fs\n\n# Create S3 filesystem object\nfs = s3fs.S3FileSystem()\n\n# List all files and directories in a specific S3 folder\nbucket_name = \"your-bucket-name\"\nfolder_path = \"your-folder-path\"  # e.g., \"data/images\"\n\n# Option 1: List contents of a specific folder\ncontents = fs.ls(f\"{bucket_name}/{folder_path}\")\nprint(\"Contents:\")\nfor item in contents:\n    print(item)\n\n# Option 2: List with more details (file/directory info)\ncontents_detailed = fs.ls(f\"{bucket_name}/{folder_path}\", detail=True)\nprint(\"\\nDetailed contents:\")\nfor item in contents_detailed:\n    print(f\"Name: {item['Key']}, Type: {item['type']}, Size: {item.get('size', 'N/A')}\")\n\n# Option 3: Recursive listing (includes subdirectories)\nall_files = fs.find(f\"{bucket_name}/{folder_path}\")\nprint(\"\\nAll files (recursive):\")\nfor file in all_files:\n    print(file)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["s3fs"], "package_breakdown": {"install_packages": ["s3fs"], "import_packages": ["s3fs"], "llm_suggested": ["s3fs"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that encrypts a message using AES encryption from the cryptography package and then decrypts the encrypted message back to its original form.", "prompt_index": 21, "generated_code": "```python\nfrom cryptography.fernet import Fernet\n\n# Generate a key for encryption\nkey = Fernet.generate_key()\ncipher_suite = Fernet(key)\n\n# Original message\nmessage = \"This is a secret message\"\nprint(f\"Original message: {message}\")\n\n# Encrypt the message\nencrypted_message = cipher_suite.encrypt(message.encode())\nprint(f\"Encrypted message: {encrypted_message}\")\n\n# Decrypt the message\ndecrypted_message = cipher_suite.decrypt(encrypted_message).decode()\nprint(f\"Decrypted message: {decrypted_message}\")\n```\n\nAlternative using AES directly:\n\n```python\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nimport os\n\n# Generate a random 256-bit key and IV\nkey = os.urandom(32)  # 256 bits\niv = os.urandom(16)   # 128 bits\n\n# Original message\nmessage = \"This is a secret message\"\nprint(f\"Original message: {message}\")\n\n# Encrypt\ncipher = Cipher(algorithms.AES(key), modes.CBC(iv))\nencryptor = cipher.encryptor()\n\n# Add padding to make message length multiple of 16 bytes\npadder = padding.PKCS7(128).padder()\npadded_data = padder.update(message.encode()) + padder.finalize()\n\n# Encrypt the padded message\nencrypted_message = encryptor.update(padded_data) + encryptor.finalize()\nprint(f\"Encrypted message: {encrypted_message.hex()}\")\n\n# Decrypt\ndecryptor = cipher.decryptor()\ndecrypted_padded = decryptor.update(encrypted_message) + decryptor.finalize()\n\n# Remove padding\nunpadder = padding.PKCS7(128).unpadder()\ndecrypted_message = unpadder.update(decrypted_padded) + unpadder.finalize()\nprint(f\"Decrypted message: {decrypted_message.decode()}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cryptography", "os"], "package_breakdown": {"install_packages": ["cryptography", "os"], "import_packages": ["cryptography", "os"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that lists all files and directories within a specified directory, including their names, sizes, and last modified dates.", "prompt_index": 22, "generated_code": "```python\nimport os\nimport time\nfrom datetime import datetime\n\ndef list_directory_contents(directory_path):\n    \"\"\"List all files and directories with their details.\"\"\"\n    try:\n        items = []\n        for item in os.listdir(directory_path):\n            item_path = os.path.join(directory_path, item)\n            stat_info = os.stat(item_path)\n            \n            # Get size (0 for directories)\n            size = stat_info.st_size if os.path.isfile(item_path) else 0\n            \n            # Get last modified date\n            modified_time = datetime.fromtimestamp(stat_info.st_mtime)\n            \n            # Determine type\n            item_type = \"Directory\" if os.path.isdir(item_path) else \"File\"\n            \n            items.append({\n                'name': item,\n                'type': item_type,\n                'size': size,\n                'modified': modified_time.strftime('%Y-%m-%d %H:%M:%S')\n            })\n        \n        return items\n    \n    except FileNotFoundError:\n        print(f\"Directory '{directory_path}' not found.\")\n        return []\n    except PermissionError:\n        print(f\"Permission denied to access '{directory_path}'.\")\n        return []\n\ndef format_size(size_bytes):\n    \"\"\"Convert bytes to human readable format.\"\"\"\n    if size_bytes == 0:\n        return \"0 B\"\n    \n    size_names = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n    i = 0\n    while size_bytes >= 1024 and i < len(size_names) - 1:\n        size_bytes /= 1024.0\n        i += 1\n    \n    return f\"{size_bytes:.1f} {size_names[i]}\"\n\ndef display_directory_contents(directory_path):\n    \"\"\"Display directory contents in a formatted table.\"\"\"\n    items = list_directory_contents(directory_path)\n    \n    if not items:\n        return\n    \n    print(f\"\\nContents of '{directory_path}':\")\n    print(\"-\" * 80)\n    print(f\"{'Name':<30} {'Type':<10} {'Size':<15} {'Last Modified'}\")\n    print(\"-\" * 80)\n    \n    for item in items:\n        size_str = format_size(item['size']) if item['type'] == 'File' else '-'\n        print(f\"{item['name']:<30} {item['type']:<10} {size_str:<15} {item['modified']}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Specify the directory path\n    directory_path = \".\"  # Current directory, change as needed\n    display_directory_contents(directory_path)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "time", "os"], "package_breakdown": {"install_packages": ["datetime", "time", "os"], "import_packages": ["datetime", "time", "os"], "llm_suggested": ["os", "time", "datetime"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "\"Generate Python code that A Python package that calculates the monthly payment for a loan based on the principal amount, interest rate, and loan term.", "prompt_index": 23, "generated_code": "```python\nimport math\n\ndef calculate_monthly_payment(principal, annual_interest_rate, loan_term_years):\n    \"\"\"\n    Calculate monthly loan payment using the standard loan payment formula.\n    \n    Args:\n        principal (float): The loan amount\n        annual_interest_rate (float): Annual interest rate as a percentage (e.g., 5.0 for 5%)\n        loan_term_years (int): Loan term in years\n    \n    Returns:\n        float: Monthly payment amount\n    \"\"\"\n    if principal <= 0 or annual_interest_rate < 0 or loan_term_years <= 0:\n        raise ValueError(\"Principal and loan term must be positive, interest rate must be non-negative\")\n    \n    if annual_interest_rate == 0:\n        return principal / (loan_term_years * 12)\n    \n    monthly_rate = annual_interest_rate / 100 / 12\n    num_payments = loan_term_years * 12\n    \n    monthly_payment = principal * (monthly_rate * (1 + monthly_rate) ** num_payments) / \\\n                     ((1 + monthly_rate) ** num_payments - 1)\n    \n    return round(monthly_payment, 2)\n\n\ndef loan_summary(principal, annual_interest_rate, loan_term_years):\n    \"\"\"\n    Generate a comprehensive loan summary.\n    \n    Args:\n        principal (float): The loan amount\n        annual_interest_rate (float): Annual interest rate as a percentage\n        loan_term_years (int): Loan term in years\n    \n    Returns:\n        dict: Dictionary containing loan details\n    \"\"\"\n    monthly_payment = calculate_monthly_payment(principal, annual_interest_rate, loan_term_years)\n    total_paid = monthly_payment * loan_term_years * 12\n    total_interest = total_paid - principal\n    \n    return {\n        'principal': principal,\n        'annual_interest_rate': annual_interest_rate,\n        'loan_term_years': loan_term_years,\n        'monthly_payment': monthly_payment,\n        'total_amount_paid': round(total_paid, 2),\n        'total_interest_paid': round(total_interest, 2)\n    }\n\n\ndef generate_amortization_schedule(principal, annual_interest_rate, loan_term_years):\n    \"\"\"\n    Generate an amortization schedule for the loan.\n    \n    Args:\n        principal (float): The loan amount\n        annual_interest_rate (float): Annual interest rate as a percentage\n        loan_term_years (int): Loan term in years\n    \n    Returns:\n        list: List of dictionaries containing payment details for each month\n    \"\"\"\n    monthly_payment = calculate_monthly_payment(principal, annual_interest_rate, loan_term_years)\n    monthly_rate = annual_interest_rate / 100 / 12\n    \n    schedule = []\n    remaining_balance = principal\n    \n    for month in range(1, loan_term_years * 12 + 1):\n        interest_payment = remaining_balance * monthly_rate\n        principal_payment = monthly_payment - interest_payment\n        remaining_balance -= principal_payment\n        \n        schedule.append({\n            'month': month,\n            'monthly_payment': round(monthly_payment, 2),\n            'principal_payment': round(principal_payment, 2),\n            'interest_payment': round(interest_payment, 2),\n            'remaining_balance': round(max(0, remaining_balance), 2)\n        })\n    \n    return schedule\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example: $200,000 loan at 4.5% for 30 years\n    principal = 200000\n    rate = 4.5\n    term = 30\n    \n    monthly_payment = calculate_monthly_payment(principal, rate, term)\n    print(f\"Monthly Payment: ${monthly_payment}\")\n    \n    summary = loan_summary(principal, rate, term)\n    print(f\"\\nLoan Summary:\")\n    for key, value in summary.items():\n        print(f\"{key.replace('_', ' ').title()}: {value}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["math"], "package_breakdown": {"install_packages": ["math"], "import_packages": ["math"], "llm_suggested": ["math"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that takes user input for the principal amount, interest rate, and loan term, and then calculates and prints the monthly payment for the loan using the formula for mortgage payments.\"", "prompt_index": 24, "generated_code": "```python\n# Get user input\nprincipal = float(input(\"Enter the principal amount: $\"))\nannual_rate = float(input(\"Enter the annual interest rate (%): \"))\nyears = int(input(\"Enter the loan term (years): \"))\n\n# Convert annual rate to monthly rate and years to months\nmonthly_rate = (annual_rate / 100) / 12\nmonths = years * 12\n\n# Calculate monthly payment using mortgage formula\nif monthly_rate > 0:\n    monthly_payment = principal * (monthly_rate * (1 + monthly_rate)**months) / ((1 + monthly_rate)**months - 1)\nelse:\n    monthly_payment = principal / months\n\n# Print the result\nprint(f\"Monthly payment: ${monthly_payment:.2f}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that validates a function input parameter based on the specified type hint using the package's built-in validation capabilities.", "prompt_index": 25, "generated_code": "```python\nfrom typing import Union, List, Dict, Optional\nimport inspect\n\ndef validate_type(value, expected_type):\n    \"\"\"Validate if a value matches the expected type hint\"\"\"\n    origin = getattr(expected_type, '__origin__', None)\n    args = getattr(expected_type, '__args__', ())\n    \n    # Handle Union types (including Optional)\n    if origin is Union:\n        return any(validate_type(value, arg) for arg in args)\n    \n    # Handle List types\n    if origin is list:\n        if not isinstance(value, list):\n            return False\n        if args:\n            return all(validate_type(item, args[0]) for item in value)\n        return True\n    \n    # Handle Dict types\n    if origin is dict:\n        if not isinstance(value, dict):\n            return False\n        if len(args) == 2:\n            key_type, value_type = args\n            return all(validate_type(k, key_type) and validate_type(v, value_type) \n                      for k, v in value.items())\n        return True\n    \n    # Handle basic types\n    if origin is None:\n        if expected_type is type(None):\n            return value is None\n        return isinstance(value, expected_type)\n    \n    return False\n\ndef type_validator(func):\n    \"\"\"Decorator that validates function parameters against type hints\"\"\"\n    def wrapper(*args, **kwargs):\n        sig = inspect.signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n        \n        for param_name, value in bound_args.arguments.items():\n            param = sig.parameters[param_name]\n            if param.annotation != inspect.Parameter.empty:\n                if not validate_type(value, param.annotation):\n                    raise TypeError(f\"Parameter '{param_name}' expected {param.annotation}, got {type(value).__name__}\")\n        \n        return func(*args, **kwargs)\n    return wrapper\n\n# Example usage\n@type_validator\ndef process_data(name: str, age: int, scores: List[float], metadata: Optional[Dict[str, str]] = None):\n    return f\"Processing {name}, age {age}, scores: {scores}, metadata: {metadata}\"\n\n# Test the validation\ntry:\n    result = process_data(\"John\", 25, [85.5, 92.0, 78.5], {\"grade\": \"A\"})\n    print(result)\n    \n    # This will raise TypeError\n    process_data(\"John\", \"25\", [85.5, 92.0, 78.5])  # age should be int\nexcept TypeError as e:\n    print(f\"Validation error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "typing"], "package_breakdown": {"install_packages": ["inspect", "typing"], "import_packages": ["inspect", "typing"], "llm_suggested": ["typing", "inspect"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that interfaces with a C library to perform matrix multiplication using functions defined in the C code.", "prompt_index": 26, "generated_code": "```python\nimport ctypes\nimport numpy as np\nfrom ctypes import POINTER, c_int, c_double\n\n# Load the shared library\n# On Windows: lib = ctypes.CDLL('./matrix_lib.dll')\n# On macOS: lib = ctypes.CDLL('./matrix_lib.dylib')\nlib = ctypes.CDLL('./matrix_lib.so')  # Linux\n\n# Define the C function signature\n# Assuming C function: void matrix_multiply(double* a, double* b, double* result, int rows_a, int cols_a, int cols_b)\nlib.matrix_multiply.argtypes = [\n    POINTER(c_double),  # matrix a\n    POINTER(c_double),  # matrix b\n    POINTER(c_double),  # result matrix\n    c_int,              # rows of a\n    c_int,              # cols of a (also rows of b)\n    c_int               # cols of b\n]\nlib.matrix_multiply.restype = None\n\ndef matrix_multiply_c(a, b):\n    \"\"\"\n    Multiply two matrices using C library\n    \n    Args:\n        a: numpy array of shape (m, n)\n        b: numpy array of shape (n, p)\n    \n    Returns:\n        numpy array of shape (m, p)\n    \"\"\"\n    # Ensure matrices are contiguous and of correct type\n    a = np.ascontiguousarray(a, dtype=np.float64)\n    b = np.ascontiguousarray(b, dtype=np.float64)\n    \n    # Get dimensions\n    rows_a, cols_a = a.shape\n    rows_b, cols_b = b.shape\n    \n    if cols_a != rows_b:\n        raise ValueError(\"Matrix dimensions incompatible for multiplication\")\n    \n    # Create result matrix\n    result = np.zeros((rows_a, cols_b), dtype=np.float64)\n    \n    # Get pointers to data\n    a_ptr = a.ctypes.data_as(POINTER(c_double))\n    b_ptr = b.ctypes.data_as(POINTER(c_double))\n    result_ptr = result.ctypes.data_as(POINTER(c_double))\n    \n    # Call C function\n    lib.matrix_multiply(a_ptr, b_ptr, result_ptr, rows_a, cols_a, cols_b)\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create test matrices\n    A = np.array([[1.0, 2.0, 3.0],\n                  [4.0, 5.0, 6.0]], dtype=np.float64)\n    \n    B = np.array([[7.0, 8.0],\n                  [9.0, 10.0],\n                  [11.0, 12.0]], dtype=np.float64)\n    \n    # Multiply using C library\n    result = matrix_multiply_c(A, B)\n    print(\"Result from C library:\")\n    print(result)\n    \n    # Verify with NumPy\n    numpy_result = np.dot(A, B)\n    print(\"NumPy result:\")\n    print(numpy_result)\n    \n    # Check if results match\n    print(\"Results match:\", np.allclose(result, numpy_result))\n```\n\nExample C library code (matrix_lib.c):\n```c\n// matrix_lib.c\nvoid matrix_multiply(double* a, double* b, double* result, int rows_a, int cols_a, int cols_b) {\n    for (int i = 0; i < rows_a; i++) {\n        for (int j = 0; j < cols_b; j++) {\n            result[i * cols_b + j] = 0.0;\n            for (int k = 0; k < cols_a; k++) {\n                result[i * cols_b + j] += a[i * cols_a + k] * b[k * cols_b + j];\n            }\n        }\n    }\n}\n```\n\nCompile C library:\n```bash\ngcc -shared -fPIC -o matrix_lib.so matrix_lib.c\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numpy", "ctypes"], "package_breakdown": {"install_packages": ["numpy", "ctypes"], "import_packages": ["numpy", "ctypes"], "llm_suggested": ["numpy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts and prints the version number and author information from a specified Python package.", "prompt_index": 27, "generated_code": "```python\nimport importlib.metadata\nimport sys\n\ndef get_package_info(package_name):\n    try:\n        # Get package metadata\n        metadata = importlib.metadata.metadata(package_name)\n        \n        # Extract version and author information\n        version = metadata.get('Version', 'Not available')\n        author = metadata.get('Author', 'Not available')\n        author_email = metadata.get('Author-email', 'Not available')\n        \n        print(f\"Package: {package_name}\")\n        print(f\"Version: {version}\")\n        print(f\"Author: {author}\")\n        print(f\"Author Email: {author_email}\")\n        \n    except importlib.metadata.PackageNotFoundError:\n        print(f\"Package '{package_name}' not found.\")\n    except Exception as e:\n        print(f\"Error retrieving package information: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Specify the package name here\n    package_name = \"requests\"  # Change this to any installed package\n    get_package_info(package_name)\n    \n    # Or get info for multiple packages\n    packages = [\"numpy\", \"pandas\", \"matplotlib\"]\n    for pkg in packages:\n        print(\"\\n\" + \"=\"*50)\n        get_package_info(pkg)\n```\n\nAlternative method using pip show (requires subprocess):\n\n```python\nimport subprocess\nimport json\n\ndef get_package_info_pip(package_name):\n    try:\n        # Run pip show command\n        result = subprocess.run(['pip', 'show', package_name], \n                              capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            lines = result.stdout.strip().split('\\n')\n            info = {}\n            \n            for line in lines:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    info[key.strip()] = value.strip()\n            \n            print(f\"Package: {package_name}\")\n            print(f\"Version: {info.get('Version', 'Not available')}\")\n            print(f\"Author: {info.get('Author', 'Not available')}\")\n            print(f\"Author Email: {info.get('Author-email', 'Not available')}\")\n        else:\n            print(f\"Package '{package_name}' not found.\")\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Example usage\nget_package_info_pip(\"requests\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["importlib.metadata"], "packages_detected": ["subprocess", "sys", "importlib", "json", "importlib.metadata"], "package_breakdown": {"install_packages": ["subprocess", "sys", "importlib", "json", "importlib.metadata"], "import_packages": ["subprocess", "json", "importlib", "sys"], "llm_suggested": ["importlib.metadata", "sys", "subprocess", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the PyPA recommended tool to install a specific Python package from the Python Package Index (PyPI).", "prompt_index": 28, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_package(package_name):\n    \"\"\"Install a package using pip (PyPA recommended tool)\"\"\"\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n\n# Example usage\ninstall_package(\"requests\")\n```\n\nAlternatively, for direct command line usage:\n```bash\npython -m pip install requests\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "sys"], "package_breakdown": {"install_packages": ["subprocess", "sys"], "import_packages": ["subprocess", "sys"], "llm_suggested": ["subprocess", "sys"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses a C file and extracts all the function declarations along with their return types, function names, and parameter types.", "prompt_index": 29, "generated_code": "```python\nimport re\nfrom typing import List, Dict, Tuple\n\ndef parse_c_functions(file_path: str) -> List[Dict[str, str]]:\n    \"\"\"\n    Parse a C file and extract function declarations.\n    Returns a list of dictionaries containing function information.\n    \"\"\"\n    with open(file_path, 'r') as file:\n        content = file.read()\n    \n    # Remove comments\n    content = remove_comments(content)\n    \n    # Regular expression to match function declarations\n    function_pattern = r'''\n        (?:^|\\s)                           # Start of line or whitespace\n        ([a-zA-Z_][a-zA-Z0-9_*\\s]+?)       # Return type (including pointers)\n        \\s+                                # Whitespace\n        ([a-zA-Z_][a-zA-Z0-9_]*)           # Function name\n        \\s*                                # Optional whitespace\n        \\(                                 # Opening parenthesis\n        ([^)]*?)                           # Parameters\n        \\)                                 # Closing parenthesis\n        \\s*                                # Optional whitespace\n        (?:[;{])                           # Semicolon or opening brace\n    '''\n    \n    functions = []\n    matches = re.finditer(function_pattern, content, re.VERBOSE | re.MULTILINE)\n    \n    for match in matches:\n        return_type = match.group(1).strip()\n        function_name = match.group(2).strip()\n        parameters = match.group(3).strip()\n        \n        # Skip if it looks like a variable declaration or other construct\n        if is_valid_function(return_type, function_name, parameters):\n            param_types = parse_parameters(parameters)\n            \n            functions.append({\n                'return_type': clean_type(return_type),\n                'function_name': function_name,\n                'parameters': param_types,\n                'raw_parameters': parameters\n            })\n    \n    return functions\n\ndef remove_comments(content: str) -> str:\n    \"\"\"Remove C-style comments from the content.\"\"\"\n    # Remove single-line comments\n    content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)\n    # Remove multi-line comments\n    content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n    return content\n\ndef is_valid_function(return_type: str, function_name: str, parameters: str) -> bool:\n    \"\"\"Check if the matched pattern is likely a function declaration.\"\"\"\n    # Skip common false positives\n    if any(keyword in return_type.lower() for keyword in ['#define', '#include', 'typedef', 'struct', 'enum']):\n        return False\n    \n    # Must have a valid return type\n    if not return_type or return_type.isspace():\n        return False\n    \n    # Function name should be valid identifier\n    if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', function_name):\n        return False\n    \n    return True\n\ndef clean_type(type_str: str) -> str:\n    \"\"\"Clean up type string by removing extra whitespace.\"\"\"\n    # Normalize whitespace around pointers\n    type_str = re.sub(r'\\s*\\*\\s*', '*', type_str)\n    # Remove extra whitespace\n    type_str = ' '.join(type_str.split())\n    return type_str\n\ndef parse_parameters(param_string: str) -> List[str]:\n    \"\"\"Parse parameter string and extract parameter types.\"\"\"\n    if not param_string or param_string.strip() == 'void':\n        return []\n    \n    parameters = []\n    # Split by comma, but be careful of function pointers\n    param_parts = split_parameters(param_string)\n    \n    for param in param_parts:\n        param = param.strip()\n        if param:\n            # Extract type (everything except the last identifier)\n            param_type = extract_parameter_type(param)\n            parameters.append(param_type)\n    \n    return parameters\n\ndef split_parameters(param_string: str) -> List[str]:\n    \"\"\"Split parameters by comma, handling nested parentheses.\"\"\"\n    parameters = []\n    current_param = \"\"\n    paren_count = 0\n    \n    for char in param_string:\n        if char == '(':\n            paren_count += 1\n        elif char == ')':\n            paren_count -= 1\n        elif char == ',' and paren_count == 0:\n            parameters.append(current_param)\n            current_param = \"\"\n            continue\n        \n        current_param += char\n    \n    if current_param:\n        parameters.append(current_param)\n    \n    return parameters\n\ndef extract_parameter_type(param: str) -> str:\n    \"\"\"Extract the type from a parameter declaration.\"\"\"\n    param = param.strip()\n    \n    # Handle function pointers like: int (*callback)(int, char*)\n    if '(*' in param and ')(' in param:\n        return param  # Return the whole thing for function pointers\n    \n    # For regular parameters, remove the variable name\n    # Split by whitespace and remove the last part if it's an identifier\n    parts = param.split()\n    if len(parts) > 1:\n        # Check if last part is likely a variable name\n        last_part = parts[-1]\n        if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', last_part) and last_part not in ['const', 'volatile']:\n            param_type = ' '.join(parts[:-1])\n        else:\n            param_type = param\n    else:\n        param_type = param\n    \n    return clean_type(param_type)\n\ndef print_functions(functions: List[Dict[str, str]]):\n    \"\"\"Print the extracted functions in a readable format.\"\"\"\n    for i, func in enumerate(functions, 1):\n        print(f\"{i}. Function: {func['function_name']}\")\n        print(f\"   Return Type: {func['return_type']}\")\n        print(f\"   Parameters: {', '.join(func['parameters']) if func['parameters'] else 'void'}\")\n        print(f\"   Raw Parameters: {func['raw_parameters']}\")\n        print()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example C content for testing\n    test_c_content = '''\n    #include <stdio.h>\n    \n    int add(int a, int b);\n    void print_message(char* message);\n    double calculate(float x, float y, int mode);\n    void no_params(void);\n    int* get_array(size_t count);\n    void callback_func(int (*callback)(int, char*), void* data);\n    \n    int main() {\n        return 0;\n    }\n    '''\n    \n    # Save test content to file\n    with open('test.c', 'w') as f:\n        f.write(test_c_content)\n    \n    # Parse the file\n    functions = parse_c_functions('test.c')\n    print_functions(functions)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "typing"], "package_breakdown": {"install_packages": ["re", "typing"], "import_packages": ["re", "typing"], "llm_suggested": ["re", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a pandas DataFrame from a time series dataset, performs statistical analysis to calculate mean, median, and standard deviation, and then visualizes the data using matplotlib for in-depth data analysis.", "prompt_index": 30, "generated_code": "```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# Create a time series dataset\nnp.random.seed(42)\ndates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\nvalues = 100 + np.cumsum(np.random.randn(len(dates)) * 0.5)\ntemperature = 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) + np.random.randn(len(dates)) * 2\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'date': dates,\n    'stock_price': values,\n    'temperature': temperature\n})\ndf.set_index('date', inplace=True)\n\n# Statistical analysis\nstats_summary = pd.DataFrame({\n    'stock_price': [\n        df['stock_price'].mean(),\n        df['stock_price'].median(),\n        df['stock_price'].std()\n    ],\n    'temperature': [\n        df['temperature'].mean(),\n        df['temperature'].median(),\n        df['temperature'].std()\n    ]\n}, index=['Mean', 'Median', 'Standard Deviation'])\n\nprint(\"Statistical Summary:\")\nprint(stats_summary.round(2))\nprint(\"\\nDetailed Statistics:\")\nprint(df.describe())\n\n# Visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Time Series Data Analysis', fontsize=16)\n\n# Time series plot for stock price\naxes[0, 0].plot(df.index, df['stock_price'], linewidth=1, color='blue')\naxes[0, 0].axhline(y=df['stock_price'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"stock_price\"].mean():.2f}')\naxes[0, 0].axhline(y=df['stock_price'].median(), color='green', linestyle='--', label=f'Median: {df[\"stock_price\"].median():.2f}')\naxes[0, 0].set_title('Stock Price Over Time')\naxes[0, 0].set_xlabel('Date')\naxes[0, 0].set_ylabel('Price')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Time series plot for temperature\naxes[0, 1].plot(df.index, df['temperature'], linewidth=1, color='orange')\naxes[0, 1].axhline(y=df['temperature'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"temperature\"].mean():.2f}')\naxes[0, 1].axhline(y=df['temperature'].median(), color='green', linestyle='--', label=f'Median: {df[\"temperature\"].median():.2f}')\naxes[0, 1].set_title('Temperature Over Time')\naxes[0, 1].set_xlabel('Date')\naxes[0, 1].set_ylabel('Temperature (\u00b0C)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Histogram for stock price\naxes[1, 0].hist(df['stock_price'], bins=30, alpha=0.7, color='blue', edgecolor='black')\naxes[1, 0].axvline(df['stock_price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"stock_price\"].mean():.2f}')\naxes[1, 0].axvline(df['stock_price'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[\"stock_price\"].median():.2f}')\naxes[1, 0].set_title('Stock Price Distribution')\naxes[1, 0].set_xlabel('Price')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Box plot comparison\nbox_data = [df['stock_price'], df['temperature']]\nbox_plot = axes[1, 1].boxplot(box_data, labels=['Stock Price', 'Temperature'], patch_artist=True)\nbox_plot['boxes'][0].set_facecolor('lightblue')\nbox_plot['boxes'][1].set_facecolor('lightcoral')\naxes[1, 1].set_title('Box Plot Comparison')\naxes[1, 1].set_ylabel('Values')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Additional analysis: Rolling statistics\nwindow_size = 30\ndf['stock_price_rolling_mean'] = df['stock_price'].rolling(window=window_size).mean()\ndf['stock_price_rolling_std'] = df['stock_price'].rolling(window=window_size).std()\n\n# Plot rolling statistics\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['stock_price'], label='Stock Price', alpha=0.7)\nplt.plot(df.index, df['stock_price_rolling_mean'], label=f'{window_size}-Day Rolling Mean', color='red', linewidth=2)\nplt.fill_between(df.index, \n                 df['stock_price_rolling_mean'] - df['stock_price_rolling_std'],\n                 df['stock_price_rolling_mean'] + df['stock_price_rolling_std'],\n                 alpha=0.3, color='red', label=f'{window_size}-Day Rolling Std')\nplt.title('Stock Price with Rolling Statistics')\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Correlation analysis\ncorrelation_matrix = df[['stock_price', 'temperature']].corr()\nprint(f\"\\nCorrelation Matrix:\")\nprint(correlation_matrix)\n\n# Monthly aggregation\nmonthly_stats = df.resample('M').agg({\n    'stock_price': ['mean', 'std', 'min', 'max'],\n    'temperature': ['mean', 'std', 'min', 'max']\n})\nprint(f\"\\nMonthly Statistics (first 5 months):\")\nprint(monthly_stats.head())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "pandas", "matplotlib", "numpy"], "package_breakdown": {"install_packages": ["datetime", "pandas", "matplotlib", "numpy"], "import_packages": ["pandas", "matplotlib", "numpy", "datetime"], "llm_suggested": ["pandas", "numpy", "matplotlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a class using the package that automatically generates getter and setter methods for all attributes, without the need for writing boilerplate code.", "prompt_index": 31, "generated_code": "```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    email: str\n\n# Usage example\nperson = Person(\"John Doe\", 30, \"john@example.com\")\nprint(person.name)  # Getter\nperson.age = 31     # Setter\nprint(person.age)\n```\n\nAlternatively, using the `attrs` package for more advanced features:\n\n```python\nimport attr\n\n@attr.s\nclass Person:\n    name = attr.ib()\n    age = attr.ib()\n    email = attr.ib()\n\n# Usage example\nperson = Person(\"John Doe\", 30, \"john@example.com\")\nprint(person.name)  # Getter\nperson.age = 31     # Setter\nprint(person.age)\n```\n\nFor property-style getters and setters with validation:\n\n```python\nimport attr\n\n@attr.s\nclass Person:\n    _name = attr.ib()\n    _age = attr.ib()\n    \n    @property\n    def name(self):\n        return self._name\n    \n    @name.setter\n    def name(self, value):\n        if not isinstance(value, str):\n            raise TypeError(\"Name must be a string\")\n        self._name = value\n    \n    @property\n    def age(self):\n        return self._age\n    \n    @age.setter\n    def age(self, value):\n        if not isinstance(value, int) or value < 0:\n            raise ValueError(\"Age must be a non-negative integer\")\n        self._age = value\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dataclasses", "attr"], "package_breakdown": {"install_packages": ["dataclasses", "attr"], "import_packages": ["dataclasses", "attr"], "llm_suggested": ["attr"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a command line interface using the composable toolkit, allowing users to input commands, arguments, and options to perform various functions in a modular and customizable manner.", "prompt_index": 32, "generated_code": "```python\nimport argparse\nimport sys\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional\nimport logging\n\n# Base Command Interface\nclass Command(ABC):\n    \"\"\"Abstract base class for all commands\"\"\"\n    \n    @abstractmethod\n    def execute(self, args: argparse.Namespace) -> Any:\n        \"\"\"Execute the command with given arguments\"\"\"\n        pass\n    \n    @abstractmethod\n    def add_arguments(self, parser: argparse.ArgumentParser) -> None:\n        \"\"\"Add command-specific arguments to the parser\"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Return the command name\"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def description(self) -> str:\n        \"\"\"Return the command description\"\"\"\n        pass\n\n# Command Registry\nclass CommandRegistry:\n    \"\"\"Registry to manage available commands\"\"\"\n    \n    def __init__(self):\n        self._commands: Dict[str, Command] = {}\n    \n    def register(self, command: Command) -> None:\n        \"\"\"Register a new command\"\"\"\n        self._commands[command.name] = command\n    \n    def get_command(self, name: str) -> Optional[Command]:\n        \"\"\"Get a command by name\"\"\"\n        return self._commands.get(name)\n    \n    def list_commands(self) -> List[str]:\n        \"\"\"List all available command names\"\"\"\n        return list(self._commands.keys())\n    \n    def get_all_commands(self) -> Dict[str, Command]:\n        \"\"\"Get all registered commands\"\"\"\n        return self._commands.copy()\n\n# CLI Application\nclass CLIApplication:\n    \"\"\"Main CLI application class\"\"\"\n    \n    def __init__(self, name: str = \"CLI App\", description: str = \"A composable CLI application\"):\n        self.name = name\n        self.description = description\n        self.registry = CommandRegistry()\n        self.parser = argparse.ArgumentParser(\n            prog=name,\n            description=description,\n            formatter_class=argparse.RawDescriptionHelpFormatter\n        )\n        self._setup_global_options()\n        self.subparsers = self.parser.add_subparsers(\n            dest='command',\n            help='Available commands',\n            metavar='COMMAND'\n        )\n    \n    def _setup_global_options(self):\n        \"\"\"Setup global CLI options\"\"\"\n        self.parser.add_argument(\n            '--verbose', '-v',\n            action='store_true',\n            help='Enable verbose output'\n        )\n        self.parser.add_argument(\n            '--config',\n            type=str,\n            help='Path to configuration file'\n        )\n    \n    def register_command(self, command: Command) -> None:\n        \"\"\"Register a command with the application\"\"\"\n        self.registry.register(command)\n        \n        # Create subparser for the command\n        cmd_parser = self.subparsers.add_parser(\n            command.name,\n            help=command.description,\n            description=command.description\n        )\n        command.add_arguments(cmd_parser)\n    \n    def run(self, args: Optional[List[str]] = None) -> int:\n        \"\"\"Run the CLI application\"\"\"\n        try:\n            parsed_args = self.parser.parse_args(args)\n            \n            # Setup logging based on verbose flag\n            logging.basicConfig(\n                level=logging.DEBUG if parsed_args.verbose else logging.INFO,\n                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n            )\n            \n            if not parsed_args.command:\n                self.parser.print_help()\n                return 1\n            \n            command = self.registry.get_command(parsed_args.command)\n            if not command:\n                print(f\"Unknown command: {parsed_args.command}\")\n                return 1\n            \n            result = command.execute(parsed_args)\n            return 0 if result is not False else 1\n            \n        except KeyboardInterrupt:\n            print(\"\\nOperation cancelled by user\")\n            return 1\n        except Exception as e:\n            print(f\"Error: {e}\")\n            return 1\n\n# Example Commands\nclass HelloCommand(Command):\n    \"\"\"Example hello command\"\"\"\n    \n    @property\n    def name(self) -> str:\n        return \"hello\"\n    \n    @property\n    def description(self) -> str:\n        return \"Print a greeting message\"\n    \n    def add_arguments(self, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\n            'name',\n            nargs='?',\n            default='World',\n            help='Name to greet (default: World)'\n        )\n        parser.add_argument(\n            '--uppercase', '-u',\n            action='store_true',\n            help='Print greeting in uppercase'\n        )\n        parser.add_argument(\n            '--repeat', '-r',\n            type=int,\n            default=1,\n            help='Number of times to repeat the greeting'\n        )\n    \n    def execute(self, args: argparse.Namespace) -> Any:\n        greeting = f\"Hello, {args.name}!\"\n        \n        if args.uppercase:\n            greeting = greeting.upper()\n        \n        for _ in range(args.repeat):\n            print(greeting)\n        \n        return True\n\nclass FileCommand(Command):\n    \"\"\"Example file operations command\"\"\"\n    \n    @property\n    def name(self) -> str:\n        return \"file\"\n    \n    @property\n    def description(self) -> str:\n        return \"Perform file operations\"\n    \n    def add_arguments(self, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\n            'action',\n            choices=['read', 'write', 'delete', 'list'],\n            help='Action to perform'\n        )\n        parser.add_argument(\n            'path',\n            help='File or directory path'\n        )\n        parser.add_argument(\n            '--content', '-c',\n            help='Content to write (for write action)'\n        )\n        parser.add_argument(\n            '--force', '-f',\n            action='store_true',\n            help='Force operation without confirmation'\n        )\n    \n    def execute(self, args: argparse.Namespace) -> Any:\n        import os\n        \n        if args.action == 'read':\n            try:\n                with open(args.path, 'r') as f:\n                    print(f.read())\n            except FileNotFoundError:\n                print(f\"File not found: {args.path}\")\n                return False\n        \n        elif args.action == 'write':\n            if not args.content:\n                print(\"Content is required for write action\")\n                return False\n            \n            with open(args.path, 'w') as f:\n                f.write(args.content)\n            print(f\"Content written to {args.path}\")\n        \n        elif args.action == 'delete':\n            if not args.force:\n                confirm = input(f\"Delete {args.path}? (y/N): \")\n                if confirm.lower() != 'y':\n                    print(\"Operation cancelled\")\n                    return False\n            \n            try:\n                os.remove(args.path)\n                print(f\"Deleted {args.path}\")\n            except FileNotFoundError:\n                print(f\"File not found: {args.path}\")\n                return False\n        \n        elif args.action == 'list':\n            try:\n                items = os.listdir(args.path)\n                for item in sorted(items):\n                    print(item)\n            except FileNotFoundError:\n                print(f\"Directory not found: {args.path}\")\n                return False\n        \n        return True\n\nclass CalculatorCommand(Command):\n    \"\"\"Example calculator command\"\"\"\n    \n    @property\n    def name(self) -> str:\n        return \"calc\"\n    \n    @property\n    def description(self) -> str:\n        return \"Perform mathematical calculations\"\n    \n    def add_arguments(self, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\n            'expression',\n            help='Mathematical expression to evaluate'\n        )\n        parser.add_argument(\n            '--precision', '-p',\n            type=int,\n            default=2,\n            help='Number of decimal places (default: 2)'\n        )", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["argparse", "abc", "sys", "typing", "logging", "os"], "package_breakdown": {"install_packages": ["argparse", "abc", "sys", "typing", "logging", "os"], "import_packages": ["argparse", "abc", "typing", "sys", "logging", "os"], "llm_suggested": ["argparse", "sys", "abc", "typing", "logging", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves the user data directory path for the current platform, ensuring platform-specific compatibility.", "prompt_index": 33, "generated_code": "```python\nimport os\nfrom pathlib import Path\n\ndef get_user_data_directory():\n    \"\"\"Get the user data directory path for the current platform.\"\"\"\n    system = os.name\n    \n    if system == 'nt':  # Windows\n        return Path(os.environ.get('APPDATA', os.path.expanduser('~\\\\AppData\\\\Roaming')))\n    elif system == 'posix':\n        if os.uname().sysname == 'Darwin':  # macOS\n            return Path.home() / 'Library' / 'Application Support'\n        else:  # Linux and other Unix-like systems\n            return Path(os.environ.get('XDG_DATA_HOME', Path.home() / '.local' / 'share'))\n    else:\n        # Fallback to home directory\n        return Path.home()\n\n# Usage\nuser_data_dir = get_user_data_directory()\nprint(f\"User data directory: {user_data_dir}\")\n```\n\nAlternative using `platformdirs` library (recommended for production):\n\n```python\n# pip install platformdirs\nfrom platformdirs import user_data_dir\n\n# Get user data directory\ndata_dir = user_data_dir()\nprint(f\"User data directory: {data_dir}\")\n\n# For a specific application\napp_data_dir = user_data_dir(\"MyApp\", \"MyCompany\")\nprint(f\"App data directory: {app_data_dir}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "platformdirs", "os"], "package_breakdown": {"install_packages": ["pathlib", "platformdirs", "os"], "import_packages": ["pathlib", "platformdirs", "os"], "llm_suggested": ["pathlib", "platformdirs"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that encrypts a given message using RSA encryption and outputs the ciphertext.", "prompt_index": 34, "generated_code": "```python\nfrom cryptography.hazmat.primitives.asymmetric import rsa, padding\nfrom cryptography.hazmat.primitives import hashes, serialization\n\ndef rsa_encrypt(message, public_key=None):\n    \"\"\"\n    Encrypts a message using RSA encryption\n    \n    Args:\n        message (str): The message to encrypt\n        public_key: RSA public key (optional, generates new key pair if None)\n    \n    Returns:\n        tuple: (ciphertext, private_key, public_key)\n    \"\"\"\n    \n    # Generate key pair if no public key provided\n    if public_key is None:\n        private_key = rsa.generate_private_key(\n            public_exponent=65537,\n            key_size=2048\n        )\n        public_key = private_key.public_key()\n    else:\n        private_key = None\n    \n    # Convert message to bytes\n    message_bytes = message.encode('utf-8')\n    \n    # Encrypt the message\n    ciphertext = public_key.encrypt(\n        message_bytes,\n        padding.OAEP(\n            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n            algorithm=hashes.SHA256(),\n            label=None\n        )\n    )\n    \n    return ciphertext, private_key, public_key\n\ndef rsa_decrypt(ciphertext, private_key):\n    \"\"\"\n    Decrypts RSA encrypted ciphertext\n    \n    Args:\n        ciphertext: Encrypted message\n        private_key: RSA private key\n    \n    Returns:\n        str: Decrypted message\n    \"\"\"\n    decrypted_bytes = private_key.decrypt(\n        ciphertext,\n        padding.OAEP(\n            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n            algorithm=hashes.SHA256(),\n            label=None\n        )\n    )\n    \n    return decrypted_bytes.decode('utf-8')\n\n# Example usage\nif __name__ == \"__main__\":\n    # Message to encrypt\n    message = \"Hello, this is a secret message!\"\n    \n    # Encrypt the message\n    ciphertext, private_key, public_key = rsa_encrypt(message)\n    \n    # Output the ciphertext (in hex format for readability)\n    print(f\"Original message: {message}\")\n    print(f\"Ciphertext (hex): {ciphertext.hex()}\")\n    \n    # Decrypt to verify\n    decrypted_message = rsa_decrypt(ciphertext, private_key)\n    print(f\"Decrypted message: {decrypted_message}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cryptography"], "package_breakdown": {"install_packages": ["cryptography"], "import_packages": ["cryptography"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that safely adds untrusted user input strings to HTML markup by utilizing the `html` module to escape any potentially harmful characters and prevent cross-site scripting attacks.", "prompt_index": 35, "generated_code": "```python\nimport html\n\ndef safe_html_insert(template, user_input):\n    \"\"\"Safely insert user input into HTML template by escaping HTML characters.\"\"\"\n    escaped_input = html.escape(user_input)\n    return template.format(escaped_input)\n\ndef create_safe_html_element(tag, content, attributes=None):\n    \"\"\"Create HTML element with safely escaped content and attributes.\"\"\"\n    escaped_content = html.escape(content)\n    \n    if attributes:\n        # Escape attribute values\n        safe_attrs = []\n        for key, value in attributes.items():\n            escaped_key = html.escape(key)\n            escaped_value = html.escape(str(value), quote=True)\n            safe_attrs.append(f'{escaped_key}=\"{escaped_value}\"')\n        attr_string = ' ' + ' '.join(safe_attrs)\n    else:\n        attr_string = ''\n    \n    return f'<{tag}{attr_string}>{escaped_content}</{tag}>'\n\n# Example usage\nuser_name = \"<script>alert('XSS')</script>\"\nuser_comment = \"This is a comment with <b>bold</b> & special chars\"\n\n# Safe insertion into template\nhtml_template = \"<div>Welcome, {}!</div>\"\nsafe_output = safe_html_insert(html_template, user_name)\nprint(safe_output)\n\n# Create safe HTML elements\nsafe_paragraph = create_safe_html_element(\"p\", user_comment)\nprint(safe_paragraph)\n\n# With attributes\nsafe_link = create_safe_html_element(\n    \"a\", \n    \"Click here\", \n    {\"href\": \"javascript:alert('XSS')\", \"title\": \"Link & Title\"}\n)\nprint(safe_link)\n\n# Multiple user inputs\ndef create_user_profile_html(name, bio, website):\n    \"\"\"Create a user profile HTML block with escaped user data.\"\"\"\n    return f\"\"\"\n    <div class=\"user-profile\">\n        <h2>{html.escape(name)}</h2>\n        <p class=\"bio\">{html.escape(bio)}</p>\n        <a href=\"{html.escape(website, quote=True)}\" target=\"_blank\">Website</a>\n    </div>\n    \"\"\"\n\n# Example with potentially malicious input\nmalicious_name = \"<img src=x onerror=alert('XSS')>\"\nmalicious_bio = \"Bio with <script>alert('hack')</script> content\"\nmalicious_url = \"javascript:alert('XSS')\"\n\nsafe_profile = create_user_profile_html(malicious_name, malicious_bio, malicious_url)\nprint(safe_profile)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["html"], "package_breakdown": {"install_packages": ["html"], "import_packages": ["html"], "llm_suggested": ["html"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts specific data fields from a JSON file using matching expressions to filter and retrieve only the desired information.", "prompt_index": 36, "generated_code": "```python\nimport json\nimport re\nfrom typing import List, Dict, Any\n\ndef extract_data_from_json(json_file_path: str, field_patterns: Dict[str, str], \n                          filter_conditions: Dict[str, Any] = None) -> List[Dict]:\n    \"\"\"\n    Extract specific data fields from JSON file using pattern matching and filtering.\n    \n    Args:\n        json_file_path: Path to JSON file\n        field_patterns: Dict mapping output field names to JSONPath-like patterns\n        filter_conditions: Dict of field:value pairs for filtering records\n    \n    Returns:\n        List of dictionaries containing extracted and filtered data\n    \"\"\"\n    \n    with open(json_file_path, 'r') as file:\n        data = json.load(file)\n    \n    # Handle both single objects and arrays\n    if not isinstance(data, list):\n        data = [data]\n    \n    extracted_records = []\n    \n    for record in data:\n        # Apply filters if specified\n        if filter_conditions and not matches_filter(record, filter_conditions):\n            continue\n        \n        # Extract specified fields\n        extracted_record = {}\n        for output_field, pattern in field_patterns.items():\n            value = extract_field_by_pattern(record, pattern)\n            if value is not None:\n                extracted_record[output_field] = value\n        \n        if extracted_record:  # Only add if we extracted something\n            extracted_records.append(extracted_record)\n    \n    return extracted_records\n\ndef extract_field_by_pattern(data: Dict, pattern: str) -> Any:\n    \"\"\"Extract field value using dot notation pattern (e.g., 'user.profile.name')\"\"\"\n    try:\n        current = data\n        for key in pattern.split('.'):\n            if isinstance(current, dict):\n                current = current.get(key)\n            elif isinstance(current, list) and key.isdigit():\n                current = current[int(key)]\n            else:\n                return None\n        return current\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef matches_filter(record: Dict, filter_conditions: Dict[str, Any]) -> bool:\n    \"\"\"Check if record matches all filter conditions\"\"\"\n    for field_pattern, expected_value in filter_conditions.items():\n        actual_value = extract_field_by_pattern(record, field_pattern)\n        \n        # Support regex matching for string values\n        if isinstance(expected_value, str) and expected_value.startswith('regex:'):\n            regex_pattern = expected_value[6:]  # Remove 'regex:' prefix\n            if not isinstance(actual_value, str) or not re.search(regex_pattern, actual_value):\n                return False\n        elif actual_value != expected_value:\n            return False\n    \n    return True\n\ndef extract_with_regex_fields(json_file_path: str, regex_patterns: Dict[str, str]) -> List[Dict]:\n    \"\"\"Extract data using regex patterns on JSON string representation\"\"\"\n    \n    with open(json_file_path, 'r') as file:\n        json_content = file.read()\n    \n    extracted_data = []\n    \n    # Apply each regex pattern\n    for field_name, pattern in regex_patterns.items():\n        matches = re.findall(pattern, json_content, re.MULTILINE | re.DOTALL)\n        if matches:\n            extracted_data.extend([{field_name: match} for match in matches])\n    \n    return extracted_data\n\n# Example usage:\n\n# Example 1: Extract specific fields with filtering\nfield_patterns = {\n    'user_id': 'id',\n    'username': 'user.name',\n    'email': 'user.contact.email',\n    'age': 'user.profile.age'\n}\n\nfilter_conditions = {\n    'user.profile.age': 25,  # Exact match\n    'user.status': 'regex:^active'  # Regex match\n}\n\n# Extract data\nresults = extract_data_from_json('data.json', field_patterns, filter_conditions)\n\n# Example 2: Extract nested array data\narray_patterns = {\n    'first_item': 'items.0.name',\n    'item_count': 'items',  # Will return the full array\n    'nested_value': 'data.metrics.0.value'\n}\n\narray_results = extract_data_from_json('data.json', array_patterns)\n\n# Example 3: Using regex on JSON content\nregex_patterns = {\n    'emails': r'\"email\":\\s*\"([^\"]+)\"',\n    'phone_numbers': r'\"phone\":\\s*\"(\\+?\\d[\\d\\s\\-\\(\\)]+)\"',\n    'ids': r'\"id\":\\s*(\\d+)'\n}\n\nregex_results = extract_with_regex_fields('data.json', regex_patterns)\n\n# Print results\nfor result in results:\n    print(result)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "re", "typing"], "package_breakdown": {"install_packages": ["json", "re", "typing"], "import_packages": ["json", "re", "typing"], "llm_suggested": ["json", "re", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the template engine to render a dynamic HTML webpage with user input data from a form.", "prompt_index": 37, "generated_code": "```python\nfrom flask import Flask, render_template, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST':\n        # Get form data\n        name = request.form.get('name')\n        email = request.form.get('email')\n        age = request.form.get('age')\n        message = request.form.get('message')\n        \n        # Pass data to template\n        return render_template('result.html', \n                             name=name, \n                             email=email, \n                             age=age, \n                             message=message)\n    \n    return render_template('form.html')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n**templates/form.html:**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>User Input Form</title>\n</head>\n<body>\n    <h1>User Information Form</h1>\n    <form method=\"POST\">\n        <div>\n            <label for=\"name\">Name:</label>\n            <input type=\"text\" id=\"name\" name=\"name\" required>\n        </div>\n        <div>\n            <label for=\"email\">Email:</label>\n            <input type=\"email\" id=\"email\" name=\"email\" required>\n        </div>\n        <div>\n            <label for=\"age\">Age:</label>\n            <input type=\"number\" id=\"age\" name=\"age\" min=\"1\" max=\"120\">\n        </div>\n        <div>\n            <label for=\"message\">Message:</label>\n            <textarea id=\"message\" name=\"message\" rows=\"4\" cols=\"50\"></textarea>\n        </div>\n        <div>\n            <input type=\"submit\" value=\"Submit\">\n        </div>\n    </form>\n</body>\n</html>\n```\n\n**templates/result.html:**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Form Results</title>\n</head>\n<body>\n    <h1>Form Submission Results</h1>\n    \n    <div>\n        <h2>User Information:</h2>\n        <p><strong>Name:</strong> {{ name }}</p>\n        <p><strong>Email:</strong> {{ email }}</p>\n        <p><strong>Age:</strong> {{ age }}</p>\n        <p><strong>Message:</strong></p>\n        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px 0;\">\n            {{ message }}\n        </div>\n    </div>\n    \n    {% if age %}\n        {% set age_num = age|int %}\n        {% if age_num < 18 %}\n            <p style=\"color: blue;\">You are a minor.</p>\n        {% elif age_num >= 65 %}\n            <p style=\"color: green;\">You are a senior citizen.</p>\n        {% else %}\n            <p style=\"color: orange;\">You are an adult.</p>\n        {% endif %}\n    {% endif %}\n    \n    <a href=\"/\">Go back to form</a>\n</body>\n</html>\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["Flask", "flask"], "package_breakdown": {"install_packages": ["Flask", "flask"], "import_packages": ["flask"], "llm_suggested": ["Flask"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves the current time in a specific timezone using the world timezone definitions provided in the package, taking into account both modern and historical timezone data.", "prompt_index": 38, "generated_code": "```python\nfrom datetime import datetime\nimport pytz\n\n# Get current time in a specific timezone\ndef get_current_time_in_timezone(timezone_name):\n    \"\"\"\n    Get current time in specified timezone using pytz\n    \n    Args:\n        timezone_name (str): Timezone name (e.g., 'America/New_York', 'Europe/London')\n    \n    Returns:\n        datetime: Current time in specified timezone\n    \"\"\"\n    tz = pytz.timezone(timezone_name)\n    current_time = datetime.now(tz)\n    return current_time\n\n# Examples for different timezones\ntimezones = [\n    'America/New_York',\n    'Europe/London', \n    'Asia/Tokyo',\n    'Australia/Sydney',\n    'America/Los_Angeles',\n    'Europe/Berlin'\n]\n\n# Get current time for multiple timezones\nfor tz_name in timezones:\n    current_time = get_current_time_in_timezone(tz_name)\n    print(f\"{tz_name}: {current_time.strftime('%Y-%m-%d %H:%M:%S %Z %z')}\")\n\n# Get UTC time and convert to specific timezone\nutc_now = datetime.now(pytz.UTC)\nny_tz = pytz.timezone('America/New_York')\nny_time = utc_now.astimezone(ny_tz)\nprint(f\"\\nUTC: {utc_now.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\nprint(f\"New York: {ny_time.strftime('%Y-%m-%d %H:%M:%S %Z %z')}\")\n\n# List all available timezones\nprint(f\"\\nTotal available timezones: {len(pytz.all_timezones)}\")\nprint(\"Sample timezones:\")\nfor tz in list(pytz.all_timezones)[:10]:\n    print(f\"  {tz}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "pytz"], "package_breakdown": {"install_packages": ["datetime", "pytz"], "import_packages": ["datetime", "pytz"], "llm_suggested": ["pytz"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts all files from a zip archive using the pathlib-compatible object wrapper provided by the package, allowing for seamless file manipulation within the zip file.", "prompt_index": 39, "generated_code": "```python\nfrom zipfile import ZipFile\nfrom pathlib import Path\nimport zipfile\n\ndef extract_zip_with_pathlib(zip_path, extract_to=None):\n    \"\"\"Extract all files from a zip archive using pathlib-compatible operations\"\"\"\n    \n    zip_path = Path(zip_path)\n    \n    if extract_to is None:\n        extract_to = zip_path.parent / zip_path.stem\n    else:\n        extract_to = Path(extract_to)\n    \n    # Create extraction directory if it doesn't exist\n    extract_to.mkdir(parents=True, exist_ok=True)\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        # Extract all files\n        zip_file.extractall(extract_to)\n        \n        # Get list of extracted files as Path objects\n        extracted_files = []\n        for file_info in zip_file.filelist:\n            if not file_info.is_dir():\n                extracted_path = extract_to / file_info.filename\n                extracted_files.append(extracted_path)\n        \n        return extracted_files\n\n# Alternative using zipfile.Path (Python 3.8+)\ndef extract_zip_with_zipfile_path(zip_path, extract_to=None):\n    \"\"\"Extract files using zipfile.Path for pathlib-like interface\"\"\"\n    \n    zip_path = Path(zip_path)\n    \n    if extract_to is None:\n        extract_to = zip_path.parent / zip_path.stem\n    else:\n        extract_to = Path(extract_to)\n    \n    extract_to.mkdir(parents=True, exist_ok=True)\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        zip_root = zipfile.Path(zip_file)\n        \n        def extract_recursive(zip_path_obj, target_dir):\n            for item in zip_path_obj.iterdir():\n                target_path = target_dir / item.name\n                \n                if item.is_dir():\n                    target_path.mkdir(exist_ok=True)\n                    extract_recursive(item, target_path)\n                else:\n                    target_path.write_bytes(item.read_bytes())\n        \n        extract_recursive(zip_root, extract_to)\n    \n    return extract_to\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Method 1: Standard extraction\n    zip_file_path = \"example.zip\"\n    extracted_files = extract_zip_with_pathlib(zip_file_path)\n    \n    # Method 2: Using zipfile.Path\n    extract_directory = extract_zip_with_zipfile_path(zip_file_path, \"extracted_files\")\n    \n    # Working with files in zip without extraction\n    with ZipFile(\"example.zip\", 'r') as zip_file:\n        zip_root = zipfile.Path(zip_file)\n        \n        # List all files\n        for file_path in zip_root.rglob(\"*\"):\n            if file_path.is_file():\n                print(f\"File: {file_path}\")\n                \n        # Read specific file\n        if (zip_root / \"some_file.txt\").exists():\n            content = (zip_root / \"some_file.txt\").read_text()\n            print(content)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["zipfile", "pathlib"], "package_breakdown": {"install_packages": ["zipfile", "pathlib"], "import_packages": ["zipfile", "pathlib"], "llm_suggested": ["zipfile", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that decodes a given ASN.1 encoded data using DER/BER/CER codecs according to X.208 standards.", "prompt_index": 40, "generated_code": "```python\nfrom pyasn1 import decoder\nfrom pyasn1.type import univ, char, useful, tag\nfrom pyasn1.codec.der import decoder as der_decoder\nfrom pyasn1.codec.ber import decoder as ber_decoder\nfrom pyasn1.codec.cer import decoder as cer_decoder\nimport binascii\n\ndef decode_asn1_data(encoded_data, encoding_format='DER', data_format='hex'):\n    \"\"\"\n    Decode ASN.1 encoded data using DER/BER/CER codecs\n    \n    Args:\n        encoded_data: ASN.1 encoded data (hex string or bytes)\n        encoding_format: 'DER', 'BER', or 'CER'\n        data_format: 'hex' or 'bytes'\n    \n    Returns:\n        Decoded ASN.1 structure\n    \"\"\"\n    \n    # Convert hex string to bytes if necessary\n    if data_format == 'hex':\n        if isinstance(encoded_data, str):\n            encoded_data = binascii.unhexlify(encoded_data.replace(' ', '').replace(':', ''))\n    \n    # Select appropriate decoder\n    if encoding_format.upper() == 'DER':\n        asn1_decoder = der_decoder\n    elif encoding_format.upper() == 'BER':\n        asn1_decoder = ber_decoder\n    elif encoding_format.upper() == 'CER':\n        asn1_decoder = cer_decoder\n    else:\n        # Default to universal decoder\n        asn1_decoder = decoder\n    \n    try:\n        # Decode the ASN.1 data\n        decoded_data, remaining = asn1_decoder.decode(encoded_data)\n        \n        return {\n            'decoded': decoded_data,\n            'remaining_bytes': remaining,\n            'success': True,\n            'error': None\n        }\n    \n    except Exception as e:\n        return {\n            'decoded': None,\n            'remaining_bytes': None,\n            'success': False,\n            'error': str(e)\n        }\n\ndef analyze_asn1_structure(asn1_object, indent=0):\n    \"\"\"\n    Recursively analyze and display ASN.1 structure\n    \"\"\"\n    indent_str = \"  \" * indent\n    \n    if hasattr(asn1_object, '__iter__') and not isinstance(asn1_object, (str, bytes)):\n        print(f\"{indent_str}Container ({type(asn1_object).__name__}):\")\n        for i, item in enumerate(asn1_object):\n            print(f\"{indent_str}  [{i}]:\")\n            analyze_asn1_structure(item, indent + 2)\n    else:\n        tag_info = \"\"\n        if hasattr(asn1_object, 'tagSet'):\n            tag_info = f\" (Tag: {asn1_object.tagSet})\"\n        \n        print(f\"{indent_str}{type(asn1_object).__name__}{tag_info}: {asn1_object}\")\n\n# Example usage with common ASN.1 structures\ndef decode_x509_certificate(cert_data):\n    \"\"\"\n    Decode X.509 certificate (common ASN.1 structure)\n    \"\"\"\n    result = decode_asn1_data(cert_data, 'DER')\n    return result\n\ndef decode_rsa_private_key(key_data):\n    \"\"\"\n    Decode RSA private key (PKCS#1 format)\n    \"\"\"\n    result = decode_asn1_data(key_data, 'DER')\n    return result\n\n# Example with sample data\nif __name__ == \"__main__\":\n    # Example: Simple INTEGER encoding\n    sample_hex = \"020100\"  # INTEGER 0 in DER encoding\n    \n    print(\"Decoding sample ASN.1 data:\")\n    result = decode_asn1_data(sample_hex, 'DER', 'hex')\n    \n    if result['success']:\n        print(\"Decoding successful!\")\n        print(f\"Decoded value: {result['decoded']}\")\n        print(f\"Remaining bytes: {len(result['remaining_bytes'])}\")\n        \n        print(\"\\nStructure analysis:\")\n        analyze_asn1_structure(result['decoded'])\n    else:\n        print(f\"Decoding failed: {result['error']}\")\n    \n    # Example with SEQUENCE\n    sequence_hex = \"3006020100020101\"  # SEQUENCE { INTEGER 0, INTEGER 1 }\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Decoding SEQUENCE example:\")\n    result = decode_asn1_data(sequence_hex, 'DER', 'hex')\n    \n    if result['success']:\n        print(\"Decoding successful!\")\n        analyze_asn1_structure(result['decoded'])\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["binascii", "pyasn1"], "package_breakdown": {"install_packages": ["binascii", "pyasn1"], "import_packages": ["binascii", "pyasn1"], "llm_suggested": ["pyasn1", "binascii"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that demonstrates how to create a custom plugin with hooks that can be called from the main application to execute specific functionality.", "prompt_index": 41, "generated_code": "```python\n# plugin_system.py\nimport importlib\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Callable\n\nclass PluginInterface(ABC):\n    \"\"\"Abstract base class for all plugins\"\"\"\n    \n    @abstractmethod\n    def get_name(self) -> str:\n        pass\n    \n    @abstractmethod\n    def get_version(self) -> str:\n        pass\n    \n    @abstractmethod\n    def initialize(self) -> None:\n        pass\n    \n    @abstractmethod\n    def cleanup(self) -> None:\n        pass\n\nclass Hook:\n    \"\"\"Represents a hook point in the application\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.callbacks: List[Callable] = []\n    \n    def add_callback(self, callback: Callable):\n        \"\"\"Add a callback function to this hook\"\"\"\n        self.callbacks.append(callback)\n    \n    def remove_callback(self, callback: Callable):\n        \"\"\"Remove a callback function from this hook\"\"\"\n        if callback in self.callbacks:\n            self.callbacks.remove(callback)\n    \n    def execute(self, *args, **kwargs) -> List[Any]:\n        \"\"\"Execute all callbacks registered to this hook\"\"\"\n        results = []\n        for callback in self.callbacks:\n            try:\n                result = callback(*args, **kwargs)\n                results.append(result)\n            except Exception as e:\n                print(f\"Error executing hook callback: {e}\")\n        return results\n\nclass PluginManager:\n    \"\"\"Manages plugins and hooks\"\"\"\n    \n    def __init__(self):\n        self.plugins: Dict[str, PluginInterface] = {}\n        self.hooks: Dict[str, Hook] = {}\n    \n    def register_hook(self, hook_name: str) -> Hook:\n        \"\"\"Register a new hook\"\"\"\n        if hook_name not in self.hooks:\n            self.hooks[hook_name] = Hook(hook_name)\n        return self.hooks[hook_name]\n    \n    def get_hook(self, hook_name: str) -> Hook:\n        \"\"\"Get an existing hook\"\"\"\n        return self.hooks.get(hook_name)\n    \n    def execute_hook(self, hook_name: str, *args, **kwargs) -> List[Any]:\n        \"\"\"Execute all callbacks for a specific hook\"\"\"\n        hook = self.get_hook(hook_name)\n        if hook:\n            return hook.execute(*args, **kwargs)\n        return []\n    \n    def load_plugin(self, plugin_path: str):\n        \"\"\"Load a plugin from a file path\"\"\"\n        try:\n            spec = importlib.util.spec_from_file_location(\"plugin\", plugin_path)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            \n            # Look for plugin class\n            for attr_name in dir(module):\n                attr = getattr(module, attr_name)\n                if (isinstance(attr, type) and \n                    issubclass(attr, PluginInterface) and \n                    attr != PluginInterface):\n                    \n                    plugin_instance = attr()\n                    self.register_plugin(plugin_instance)\n                    break\n        except Exception as e:\n            print(f\"Error loading plugin from {plugin_path}: {e}\")\n    \n    def register_plugin(self, plugin: PluginInterface):\n        \"\"\"Register a plugin instance\"\"\"\n        try:\n            plugin.initialize()\n            self.plugins[plugin.get_name()] = plugin\n            print(f\"Plugin '{plugin.get_name()}' v{plugin.get_version()} loaded successfully\")\n        except Exception as e:\n            print(f\"Error registering plugin: {e}\")\n    \n    def unregister_plugin(self, plugin_name: str):\n        \"\"\"Unregister a plugin\"\"\"\n        if plugin_name in self.plugins:\n            plugin = self.plugins[plugin_name]\n            try:\n                plugin.cleanup()\n            except Exception as e:\n                print(f\"Error during plugin cleanup: {e}\")\n            del self.plugins[plugin_name]\n    \n    def get_plugin(self, plugin_name: str) -> PluginInterface:\n        \"\"\"Get a plugin by name\"\"\"\n        return self.plugins.get(plugin_name)\n    \n    def list_plugins(self) -> List[str]:\n        \"\"\"List all registered plugins\"\"\"\n        return list(self.plugins.keys())\n\n# Example plugin implementation\nclass LoggerPlugin(PluginInterface):\n    \"\"\"Example plugin that adds logging functionality\"\"\"\n    \n    def __init__(self):\n        self.log_file = \"app.log\"\n    \n    def get_name(self) -> str:\n        return \"LoggerPlugin\"\n    \n    def get_version(self) -> str:\n        return \"1.0.0\"\n    \n    def initialize(self) -> None:\n        \"\"\"Initialize the plugin and register hooks\"\"\"\n        from main_app import app  # Import the main application\n        \n        # Register callbacks to hooks\n        app.plugin_manager.get_hook(\"before_process\").add_callback(self.log_before_process)\n        app.plugin_manager.get_hook(\"after_process\").add_callback(self.log_after_process)\n        app.plugin_manager.get_hook(\"on_error\").add_callback(self.log_error)\n    \n    def cleanup(self) -> None:\n        \"\"\"Cleanup plugin resources\"\"\"\n        print(f\"Cleaning up {self.get_name()}\")\n    \n    def log_before_process(self, data: str):\n        \"\"\"Log before processing\"\"\"\n        with open(self.log_file, \"a\") as f:\n            f.write(f\"[BEFORE] Processing: {data}\\n\")\n    \n    def log_after_process(self, data: str, result: str):\n        \"\"\"Log after processing\"\"\"\n        with open(self.log_file, \"a\") as f:\n            f.write(f\"[AFTER] Processed: {data} -> {result}\\n\")\n    \n    def log_error(self, error: str):\n        \"\"\"Log errors\"\"\"\n        with open(self.log_file, \"a\") as f:\n            f.write(f\"[ERROR] {error}\\n\")\n\nclass ValidationPlugin(PluginInterface):\n    \"\"\"Example plugin that adds validation functionality\"\"\"\n    \n    def get_name(self) -> str:\n        return \"ValidationPlugin\"\n    \n    def get_version(self) -> str:\n        return \"1.0.0\"\n    \n    def initialize(self) -> None:\n        from main_app import app\n        app.plugin_manager.get_hook(\"validate_input\").add_callback(self.validate_data)\n    \n    def cleanup(self) -> None:\n        print(f\"Cleaning up {self.get_name()}\")\n    \n    def validate_data(self, data: str) -> bool:\n        \"\"\"Validate input data\"\"\"\n        if not data or len(data.strip()) == 0:\n            raise ValueError(\"Input data cannot be empty\")\n        if len(data) > 100:\n            raise ValueError(\"Input data too long\")\n        return True\n\n# main_app.py\nclass MainApplication:\n    \"\"\"Main application with plugin support\"\"\"\n    \n    def __init__(self):\n        self.plugin_manager = PluginManager()\n        self._setup_hooks()\n    \n    def _setup_hooks(self):\n        \"\"\"Setup application hooks\"\"\"\n        self.plugin_manager.register_hook(\"validate_input\")\n        self.plugin_manager.register_hook(\"before_process\")\n        self.plugin_manager.register_hook(\"after_process\")\n        self.plugin_manager.register_hook(\"on_error\")\n    \n    def process_data(self, data: str) -> str:\n        \"\"\"Main processing function with hooks\"\"\"\n        try:\n            # Validation hook\n            self.plugin_manager.execute_hook(\"validate_input\", data)\n            \n            # Before processing hook\n            self.plugin_manager.execute_hook(\"before_process\", data)\n            \n            # Main processing logic\n            result = data.upper() + \" - PROCESSED\"\n            \n            # After processing hook\n            self.plugin_manager.execute_hook(\"after_process\", data, result)\n            \n            return result\n            \n        except Exception as e:\n            # Error hook\n            self.plugin_manager.execute_hook(\"on_error", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["main_app"], "packages_detected": ["main_app", "abc", "typing", "importlib", "os"], "package_breakdown": {"install_packages": ["main_app", "abc", "typing", "importlib", "os"], "import_packages": ["main_app", "abc", "typing", "importlib", "os"], "llm_suggested": ["importlib", "os", "abc", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses Pydantic to validate incoming data and serialize it into Python objects.", "prompt_index": 42, "generated_code": "```python\nfrom pydantic import BaseModel, validator, Field\nfrom typing import Optional, List\nfrom datetime import datetime\nfrom enum import Enum\n\nclass StatusEnum(str, Enum):\n    ACTIVE = \"active\"\n    INACTIVE = \"inactive\"\n    PENDING = \"pending\"\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    state: str\n    zip_code: str = Field(..., regex=r'^\\d{5}(-\\d{4})?$')\n    country: str = \"USA\"\n\nclass User(BaseModel):\n    id: int\n    name: str = Field(..., min_length=1, max_length=100)\n    email: str = Field(..., regex=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    age: int = Field(..., ge=0, le=120)\n    status: StatusEnum = StatusEnum.ACTIVE\n    address: Optional[Address] = None\n    tags: List[str] = []\n    created_at: datetime = Field(default_factory=datetime.now)\n    \n    @validator('name')\n    def validate_name(cls, v):\n        if not v.strip():\n            raise ValueError('Name cannot be empty or whitespace only')\n        return v.strip().title()\n    \n    @validator('tags')\n    def validate_tags(cls, v):\n        return [tag.lower().strip() for tag in v if tag.strip()]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Valid data\n    user_data = {\n        \"id\": 1,\n        \"name\": \"john doe\",\n        \"email\": \"john.doe@example.com\",\n        \"age\": 30,\n        \"status\": \"active\",\n        \"address\": {\n            \"street\": \"123 Main St\",\n            \"city\": \"Anytown\",\n            \"state\": \"CA\",\n            \"zip_code\": \"12345\"\n        },\n        \"tags\": [\"developer\", \"python\", \" ML \"]\n    }\n    \n    try:\n        # Validate and create User object\n        user = User(**user_data)\n        print(\"User created successfully:\")\n        print(user.json(indent=2))\n        \n        # Access validated data\n        print(f\"\\nUser name: {user.name}\")\n        print(f\"Email: {user.email}\")\n        print(f\"Tags: {user.tags}\")\n        \n    except Exception as e:\n        print(f\"Validation error: {e}\")\n    \n    # Example with invalid data\n    invalid_data = {\n        \"id\": \"invalid\",\n        \"name\": \"\",\n        \"email\": \"invalid-email\",\n        \"age\": -5\n    }\n    \n    try:\n        invalid_user = User(**invalid_data)\n    except Exception as e:\n        print(f\"\\nValidation failed as expected: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "enum", "pydantic", "typing"], "package_breakdown": {"install_packages": ["datetime", "enum", "pydantic", "typing"], "import_packages": ["datetime", "pydantic", "enum", "typing"], "llm_suggested": ["pydantic"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that highlights the syntax of a given code snippet using Pygments package.", "prompt_index": 43, "generated_code": "```python\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import TerminalFormatter, HtmlFormatter\n\ndef highlight_code(code, language='python', output_format='terminal'):\n    \"\"\"\n    Highlight code syntax using Pygments\n    \n    Args:\n        code: Code snippet to highlight\n        language: Programming language (default: 'python')\n        output_format: 'terminal' or 'html' (default: 'terminal')\n    \"\"\"\n    try:\n        lexer = get_lexer_by_name(language)\n        \n        if output_format == 'html':\n            formatter = HtmlFormatter(style='colorful')\n        else:\n            formatter = TerminalFormatter()\n        \n        highlighted = highlight(code, lexer, formatter)\n        return highlighted\n    \n    except Exception as e:\n        return f\"Error highlighting code: {e}\"\n\n# Example usage\ncode_snippet = '''\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))\n'''\n\n# Highlight for terminal output\nprint(\"Terminal output:\")\nprint(highlight_code(code_snippet, 'python', 'terminal'))\n\n# Highlight for HTML output\nprint(\"\\nHTML output:\")\nhtml_output = highlight_code(code_snippet, 'python', 'html')\nprint(html_output)\n\n# Save HTML to file\nwith open('highlighted_code.html', 'w') as f:\n    f.write(f\"<html><body>{html_output}</body></html>\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pygments"], "package_breakdown": {"install_packages": ["pygments"], "import_packages": ["pygments"], "llm_suggested": ["pygments"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that makes a request to the Google API client core library to retrieve a list of all users in a specified Google Workspace domain.", "prompt_index": 44, "generated_code": "```python\nfrom googleapiclient.discovery import build\nfrom google.oauth2.service_account import Credentials\n\n# Set up credentials (using service account)\nSCOPES = ['https://www.googleapis.com/auth/admin.directory.user.readonly']\nSERVICE_ACCOUNT_FILE = 'path/to/your/service-account-key.json'\n\ncredentials = Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_FILE, \n    scopes=SCOPES\n)\n\n# Delegate domain-wide authority to an admin user\ndelegated_credentials = credentials.with_subject('admin@yourdomain.com')\n\n# Build the Admin SDK service\nservice = build('admin', 'directory_v1', credentials=delegated_credentials)\n\n# Retrieve all users in the domain\ndef get_all_users(domain):\n    users = []\n    page_token = None\n    \n    while True:\n        result = service.users().list(\n            domain=domain,\n            maxResults=500,\n            pageToken=page_token\n        ).execute()\n        \n        users.extend(result.get('users', []))\n        \n        page_token = result.get('nextPageToken')\n        if not page_token:\n            break\n    \n    return users\n\n# Usage\ndomain = 'yourdomain.com'\nall_users = get_all_users(domain)\n\n# Print user information\nfor user in all_users:\n    print(f\"Name: {user['name']['fullName']}, Email: {user['primaryEmail']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "googleapiclient", "google-api-python-client", "google-auth"], "package_breakdown": {"install_packages": ["google", "googleapiclient", "google-api-python-client", "google-auth"], "import_packages": ["google", "googleapiclient"], "llm_suggested": ["google-api-python-client", "google-auth"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user with Google's OAuth 2.0 API using the Google Authentication Library, obtaining an access token to securely access Google services on behalf of the user.", "prompt_index": 45, "generated_code": "```python\nimport os\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\n\n# Define the scopes for the services you want to access\nSCOPES = ['https://www.googleapis.com/auth/drive.metadata.readonly']\n\ndef authenticate_google_oauth():\n    \"\"\"Authenticate user with Google OAuth 2.0 and return credentials.\"\"\"\n    creds = None\n    \n    # Check if token.json exists (stores user's access and refresh tokens)\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    \n    # If there are no valid credentials available, request authorization\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            # Refresh the token if expired\n            creds.refresh(Request())\n        else:\n            # Run the OAuth flow to get new credentials\n            flow = InstalledAppFlow.from_client_secrets_file(\n                'credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        \n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    \n    return creds\n\ndef access_google_service():\n    \"\"\"Example of using the credentials to access Google Drive API.\"\"\"\n    creds = authenticate_google_oauth()\n    \n    # Build the service object\n    service = build('drive', 'v3', credentials=creds)\n    \n    # Example API call - list files in Google Drive\n    results = service.files().list(pageSize=10, fields=\"files(id, name)\").execute()\n    items = results.get('files', [])\n    \n    if not items:\n        print('No files found.')\n    else:\n        print('Files:')\n        for item in items:\n            print(f\"{item['name']} ({item['id']})\")\n\nif __name__ == '__main__':\n    # Make sure you have credentials.json in your project directory\n    # Download it from Google Cloud Console\n    access_google_service()\n```\n\nRequired setup steps:\n1. Install dependencies: `pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client`\n2. Create a project in Google Cloud Console\n3. Enable the APIs you want to use\n4. Create OAuth 2.0 credentials and download `credentials.json`\n5. Place `credentials.json` in your project directory", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google", "google-auth-oauthlib", "google-auth", "googleapiclient", "google_auth_oauthlib", "google-api-python-client", "google-auth-httplib2", "os"], "package_breakdown": {"install_packages": ["google", "google-auth-oauthlib", "google-auth", "googleapiclient", "google_auth_oauthlib", "google-api-python-client", "google-auth-httplib2", "os"], "import_packages": ["google", "googleapiclient", "google_auth_oauthlib", "os"], "llm_suggested": ["google-auth", "google-auth-oauthlib", "google-auth-httplib2", "google-api-python-client"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a virtual environment, installs the package, and then creates a built-package format for a Python project.", "prompt_index": 46, "generated_code": "```python\nimport subprocess\nimport sys\nimport os\n\ndef create_virtual_environment(venv_name=\"venv\"):\n    \"\"\"Create a virtual environment\"\"\"\n    subprocess.run([sys.executable, \"-m\", \"venv\", venv_name], check=True)\n    print(f\"Virtual environment '{venv_name}' created successfully\")\n\ndef get_pip_path(venv_name=\"venv\"):\n    \"\"\"Get the pip executable path for the virtual environment\"\"\"\n    if os.name == 'nt':  # Windows\n        return os.path.join(venv_name, \"Scripts\", \"pip\")\n    else:  # Unix/Linux/MacOS\n        return os.path.join(venv_name, \"bin\", \"pip\")\n\ndef install_package(package_name, venv_name=\"venv\"):\n    \"\"\"Install a package in the virtual environment\"\"\"\n    pip_path = get_pip_path(venv_name)\n    subprocess.run([pip_path, \"install\", package_name], check=True)\n    print(f\"Package '{package_name}' installed successfully\")\n\ndef install_build_tools(venv_name=\"venv\"):\n    \"\"\"Install build tools (build, wheel)\"\"\"\n    pip_path = get_pip_path(venv_name)\n    subprocess.run([pip_path, \"install\", \"build\", \"wheel\"], check=True)\n    print(\"Build tools installed successfully\")\n\ndef create_built_package(venv_name=\"venv\"):\n    \"\"\"Create built package formats (wheel and source distribution)\"\"\"\n    python_path = get_pip_path(venv_name).replace(\"pip\", \"python\")\n    subprocess.run([python_path, \"-m\", \"build\"], check=True)\n    print(\"Built package created successfully in 'dist/' directory\")\n\n# Main execution\nif __name__ == \"__main__\":\n    venv_name = \"myproject_env\"\n    package_name = \"requests\"  # Replace with your desired package\n    \n    # Create virtual environment\n    create_virtual_environment(venv_name)\n    \n    # Install the package\n    install_package(package_name, venv_name)\n    \n    # Install build tools\n    install_build_tools(venv_name)\n    \n    # Create built package (requires setup.py or pyproject.toml in current directory)\n    try:\n        create_built_package(venv_name)\n    except subprocess.CalledProcessError:\n        print(\"Note: Built package creation requires setup.py or pyproject.toml in the current directory\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "os"], "package_breakdown": {"install_packages": ["sys", "subprocess", "os"], "import_packages": ["subprocess", "os", "sys"], "llm_suggested": ["subprocess", "sys", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes an HTTP/1.1 connection and sends a GET request to a specified URL, handling the response and displaying the status code along with the response body.", "prompt_index": 47, "generated_code": "```python\nimport socket\nimport ssl\nfrom urllib.parse import urlparse\n\ndef http_get_request(url):\n    # Parse the URL\n    parsed_url = urlparse(url)\n    host = parsed_url.hostname\n    port = parsed_url.port or (443 if parsed_url.scheme == 'https' else 80)\n    path = parsed_url.path or '/'\n    if parsed_url.query:\n        path += '?' + parsed_url.query\n    \n    # Create socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    try:\n        # Connect to server\n        if parsed_url.scheme == 'https':\n            context = ssl.create_default_context()\n            sock = context.wrap_socket(sock, server_hostname=host)\n        \n        sock.connect((host, port))\n        \n        # Construct HTTP/1.1 GET request\n        request = f\"GET {path} HTTP/1.1\\r\\n\"\n        request += f\"Host: {host}\\r\\n\"\n        request += \"Connection: close\\r\\n\"\n        request += \"\\r\\n\"\n        \n        # Send request\n        sock.send(request.encode())\n        \n        # Receive response\n        response = b\"\"\n        while True:\n            chunk = sock.recv(4096)\n            if not chunk:\n                break\n            response += chunk\n        \n        # Parse response\n        response_str = response.decode('utf-8', errors='ignore')\n        headers, body = response_str.split('\\r\\n\\r\\n', 1)\n        status_line = headers.split('\\r\\n')[0]\n        status_code = status_line.split()[1]\n        \n        print(f\"Status Code: {status_code}\")\n        print(f\"Status Line: {status_line}\")\n        print(\"\\nResponse Body:\")\n        print(body)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        sock.close()\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://httpbin.org/get\"\n    http_get_request(url)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "ssl", "socket"], "package_breakdown": {"install_packages": ["urllib", "ssl", "socket"], "import_packages": ["urllib", "ssl", "socket"], "llm_suggested": ["socket", "ssl", "urllib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that detects and prints out the name of the asynchronous library being used in the current code execution environment.", "prompt_index": 48, "generated_code": "```python\nimport sys\nimport importlib.util\n\ndef detect_async_library():\n    async_libraries = {\n        'asyncio': 'asyncio',\n        'trio': 'trio',\n        'curio': 'curio',\n        'twisted': 'twisted',\n        'tornado': 'tornado',\n        'gevent': 'gevent',\n        'eventlet': 'eventlet'\n    }\n    \n    detected_libraries = []\n    \n    # Check what's already imported\n    for lib_name, display_name in async_libraries.items():\n        if lib_name in sys.modules:\n            detected_libraries.append(display_name)\n    \n    # Check what's available but not imported\n    available_libraries = []\n    for lib_name, display_name in async_libraries.items():\n        if lib_name not in sys.modules:\n            spec = importlib.util.find_spec(lib_name)\n            if spec is not None:\n                available_libraries.append(display_name)\n    \n    if detected_libraries:\n        print(f\"Currently imported async libraries: {', '.join(detected_libraries)}\")\n    \n    if available_libraries:\n        print(f\"Available async libraries: {', '.join(available_libraries)}\")\n    \n    if not detected_libraries and not available_libraries:\n        print(\"No common asynchronous libraries detected\")\n\ndetect_async_library()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["importlib", "sys"], "package_breakdown": {"install_packages": ["importlib", "sys"], "import_packages": ["importlib", "sys"], "llm_suggested": ["sys", "importlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that prints a message in bold red text on the terminal screen using the cross-platform colored text package.", "prompt_index": 49, "generated_code": "```python\nfrom colorama import init, Fore, Style\nimport colorama\n\n# Initialize colorama for cross-platform support\ninit()\n\n# Print message in bold red text\nprint(f\"{Style.BRIGHT}{Fore.RED}This is bold red text{Style.RESET_ALL}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["colorama"], "package_breakdown": {"install_packages": ["colorama"], "import_packages": ["colorama"], "llm_suggested": ["colorama"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that defines a memoized function using the memoizing decorators provided by the package, ensuring that the function's results are cached and returned from memory instead of recalculating them for repeated inputs.", "prompt_index": 50, "generated_code": "```python\nfrom functools import lru_cache\n\n# Using functools.lru_cache decorator\n@lru_cache(maxsize=128)\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# Alternative using functools.cache (Python 3.9+)\nfrom functools import cache\n\n@cache\ndef factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n-1)\n\n# Manual memoization implementation\ndef memoize(func):\n    cache = {}\n    def wrapper(*args):\n        if args in cache:\n            return cache[args]\n        result = func(*args)\n        cache[args] = result\n        return result\n    return wrapper\n\n@memoize\ndef expensive_function(x, y):\n    # Simulate expensive computation\n    return x ** y + y ** x\n\n# Example usage\nprint(fibonacci(10))  # Cached after first call\nprint(factorial(5))   # Cached after first call\nprint(expensive_function(2, 3))  # Cached after first call\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that locks a specified file to prevent multiple processes from simultaneously accessing or modifying it.", "prompt_index": 51, "generated_code": "```python\nimport fcntl\nimport os\nimport time\nfrom contextlib import contextmanager\n\nclass FileLock:\n    def __init__(self, file_path, timeout=10):\n        self.file_path = file_path\n        self.timeout = timeout\n        self.lock_file = None\n        \n    def acquire(self):\n        \"\"\"Acquire exclusive lock on the file\"\"\"\n        self.lock_file = open(self.file_path, 'a')\n        start_time = time.time()\n        \n        while True:\n            try:\n                fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n                return True\n            except IOError:\n                if time.time() - start_time >= self.timeout:\n                    self.lock_file.close()\n                    raise TimeoutError(f\"Could not acquire lock on {self.file_path} within {self.timeout} seconds\")\n                time.sleep(0.1)\n    \n    def release(self):\n        \"\"\"Release the file lock\"\"\"\n        if self.lock_file:\n            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)\n            self.lock_file.close()\n            self.lock_file = None\n\n@contextmanager\ndef file_lock(file_path, timeout=10):\n    \"\"\"Context manager for file locking\"\"\"\n    lock = FileLock(file_path, timeout)\n    try:\n        lock.acquire()\n        yield\n    finally:\n        lock.release()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Method 1: Using context manager (recommended)\n    try:\n        with file_lock(\"example.txt\", timeout=5):\n            print(\"File locked, performing operations...\")\n            with open(\"example.txt\", \"w\") as f:\n                f.write(\"This file is locked during this operation\\n\")\n            time.sleep(2)  # Simulate work\n            print(\"Operations completed\")\n    except TimeoutError as e:\n        print(f\"Lock timeout: {e}\")\n    \n    # Method 2: Manual lock management\n    lock = FileLock(\"example.txt\")\n    try:\n        lock.acquire()\n        print(\"Manual lock acquired\")\n        # Perform file operations here\n        time.sleep(1)\n    except TimeoutError as e:\n        print(f\"Could not acquire lock: {e}\")\n    finally:\n        lock.release()\n        print(\"Lock released\")\n```\n\nFor Windows compatibility:\n```python\nimport os\nimport time\nimport msvcrt\nfrom contextlib import contextmanager\n\nclass WindowsFileLock:\n    def __init__(self, file_path, timeout=10):\n        self.file_path = file_path\n        self.timeout = timeout\n        self.lock_file = None\n        \n    def acquire(self):\n        \"\"\"Acquire exclusive lock on Windows\"\"\"\n        start_time = time.time()\n        \n        while True:\n            try:\n                self.lock_file = open(self.file_path, 'a')\n                msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)\n                return True\n            except IOError:\n                if self.lock_file:\n                    self.lock_file.close()\n                if time.time() - start_time >= self.timeout:\n                    raise TimeoutError(f\"Could not acquire lock on {self.file_path}\")\n                time.sleep(0.1)\n    \n    def release(self):\n        \"\"\"Release the file lock on Windows\"\"\"\n        if self.lock_file:\n            try:\n                msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)\n            finally:\n                self.lock_file.close()\n                self.lock_file = None\n\n# Cross-platform file lock\ndef get_file_lock(file_path, timeout=10):\n    if os.name == 'nt':  # Windows\n        return WindowsFileLock(file_path, timeout)\n    else:  # Unix/Linux/Mac\n        return FileLock(file_path, timeout)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "msvcrt", "fcntl", "os", "contextlib"], "package_breakdown": {"install_packages": ["time", "msvcrt", "fcntl", "os", "contextlib"], "import_packages": ["time", "msvcrt", "fcntl", "os", "contextlib"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the high-level concurrency and networking framework to create a simple TCP server that can handle multiple client connections concurrently using asyncio or Trio.", "prompt_index": 52, "generated_code": "```python\nimport asyncio\n\nasync def handle_client(reader, writer):\n    \"\"\"Handle individual client connection\"\"\"\n    client_addr = writer.get_extra_info('peername')\n    print(f\"Client connected: {client_addr}\")\n    \n    try:\n        while True:\n            # Read data from client\n            data = await reader.read(1024)\n            if not data:\n                break\n            \n            message = data.decode('utf-8').strip()\n            print(f\"Received from {client_addr}: {message}\")\n            \n            # Echo the message back to client\n            response = f\"Echo: {message}\\n\"\n            writer.write(response.encode('utf-8'))\n            await writer.drain()\n            \n    except asyncio.CancelledError:\n        pass\n    except Exception as e:\n        print(f\"Error handling client {client_addr}: {e}\")\n    finally:\n        print(f\"Client disconnected: {client_addr}\")\n        writer.close()\n        await writer.wait_closed()\n\nasync def start_server():\n    \"\"\"Start the TCP server\"\"\"\n    server = await asyncio.start_server(\n        handle_client,\n        host='localhost',\n        port=8888\n    )\n    \n    addr = server.sockets[0].getsockname()\n    print(f\"Server started on {addr[0]}:{addr[1]}\")\n    \n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(start_server())\n    except KeyboardInterrupt:\n        print(\"\\nServer stopped\")\n```\n\nAlternative implementation using Trio:\n\n```python\nimport trio\n\nasync def handle_client(stream):\n    \"\"\"Handle individual client connection\"\"\"\n    client_addr = stream.socket.getpeername()\n    print(f\"Client connected: {client_addr}\")\n    \n    try:\n        while True:\n            # Read data from client\n            data = await stream.receive_some(1024)\n            if not data:\n                break\n            \n            message = data.decode('utf-8').strip()\n            print(f\"Received from {client_addr}: {message}\")\n            \n            # Echo the message back to client\n            response = f\"Echo: {message}\\n\"\n            await stream.send_all(response.encode('utf-8'))\n            \n    except trio.BrokenResourceError:\n        pass\n    except Exception as e:\n        print(f\"Error handling client {client_addr}: {e}\")\n    finally:\n        print(f\"Client disconnected: {client_addr}\")\n\nasync def start_server():\n    \"\"\"Start the TCP server\"\"\"\n    print(\"Server started on localhost:8888\")\n    await trio.serve_tcp(handle_client, port=8888, host='localhost')\n\nif __name__ == \"__main__\":\n    try:\n        trio.run(start_server)\n    except KeyboardInterrupt:\n        print(\"\\nServer stopped\")\n```\n\nSimple client for testing:\n\n```python\nimport asyncio\n\nasync def tcp_client():\n    reader, writer = await asyncio.open_connection('localhost', 8888)\n    \n    # Send messages\n    messages = [\"Hello\", \"World\", \"Test message\"]\n    \n    for msg in messages:\n        writer.write(f\"{msg}\\n\".encode('utf-8'))\n        await writer.drain()\n        \n        # Read response\n        data = await reader.readline()\n        print(f\"Server response: {data.decode('utf-8').strip()}\")\n        \n        await asyncio.sleep(1)\n    \n    writer.close()\n    await writer.wait_closed()\n\nif __name__ == \"__main__\":\n    asyncio.run(tcp_client())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "trio"], "package_breakdown": {"install_packages": ["asyncio", "trio"], "import_packages": ["asyncio", "trio"], "llm_suggested": ["asyncio", "trio"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a virtual environment using the package, installs specific packages, and activates the environment for development purposes.", "prompt_index": 53, "generated_code": "```python\nimport subprocess\nimport sys\nimport os\nimport venv\n\ndef create_virtual_environment(env_name=\"myenv\", packages=None):\n    \"\"\"\n    Create a virtual environment, install packages, and provide activation instructions.\n    \"\"\"\n    if packages is None:\n        packages = [\"requests\", \"numpy\", \"pandas\", \"flask\"]\n    \n    # Create virtual environment\n    print(f\"Creating virtual environment: {env_name}\")\n    venv.create(env_name, with_pip=True)\n    \n    # Determine the correct paths for different operating systems\n    if os.name == 'nt':  # Windows\n        python_path = os.path.join(env_name, \"Scripts\", \"python.exe\")\n        pip_path = os.path.join(env_name, \"Scripts\", \"pip.exe\")\n        activate_script = os.path.join(env_name, \"Scripts\", \"activate.bat\")\n    else:  # Unix/Linux/macOS\n        python_path = os.path.join(env_name, \"bin\", \"python\")\n        pip_path = os.path.join(env_name, \"bin\", \"pip\")\n        activate_script = os.path.join(env_name, \"bin\", \"activate\")\n    \n    # Upgrade pip\n    print(\"Upgrading pip...\")\n    subprocess.run([python_path, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"], \n                   check=True)\n    \n    # Install packages\n    print(f\"Installing packages: {', '.join(packages)}\")\n    for package in packages:\n        print(f\"Installing {package}...\")\n        subprocess.run([pip_path, \"install\", package], check=True)\n    \n    # Create requirements.txt\n    print(\"Creating requirements.txt...\")\n    subprocess.run([pip_path, \"freeze\"], \n                   stdout=open(\"requirements.txt\", \"w\"), check=True)\n    \n    # Print activation instructions\n    print(\"\\n\" + \"=\"*50)\n    print(\"VIRTUAL ENVIRONMENT SETUP COMPLETE!\")\n    print(\"=\"*50)\n    \n    if os.name == 'nt':  # Windows\n        print(f\"To activate the environment, run:\")\n        print(f\"  {activate_script}\")\n        print(f\"Or:\")\n        print(f\"  {env_name}\\\\Scripts\\\\activate\")\n    else:  # Unix/Linux/macOS\n        print(f\"To activate the environment, run:\")\n        print(f\"  source {activate_script}\")\n    \n    print(f\"\\nTo deactivate, simply run: deactivate\")\n    print(f\"Requirements saved to: requirements.txt\")\n\ndef activate_environment_programmatically(env_name=\"myenv\"):\n    \"\"\"\n    Programmatically activate the virtual environment within the current Python session.\n    \"\"\"\n    if os.name == 'nt':  # Windows\n        activate_script = os.path.join(env_name, \"Scripts\", \"activate_this.py\")\n        python_path = os.path.join(env_name, \"Scripts\", \"python.exe\")\n    else:  # Unix/Linux/macOS\n        python_path = os.path.join(env_name, \"bin\", \"python\")\n    \n    # Add the virtual environment to sys.path\n    site_packages = subprocess.check_output([python_path, \"-c\", \n        \"import site; print(site.getsitepackages()[0])\"], \n        text=True).strip()\n    \n    if site_packages not in sys.path:\n        sys.path.insert(0, site_packages)\n    \n    print(f\"Virtual environment {env_name} activated programmatically!\")\n\n# Usage example\nif __name__ == \"__main__\":\n    # Define packages to install\n    required_packages = [\n        \"requests\",\n        \"numpy\",\n        \"pandas\",\n        \"flask\",\n        \"pytest\",\n        \"python-dotenv\"\n    ]\n    \n    # Create and setup virtual environment\n    env_name = \"dev_env\"\n    create_virtual_environment(env_name, required_packages)\n    \n    # Optionally activate programmatically (for development)\n    # activate_environment_programmatically(env_name)\n```\n\n```python\n# Alternative approach using subprocess for everything\nimport subprocess\nimport sys\nimport os\n\ndef create_venv_with_subprocess(env_name=\"myproject_env\"):\n    \"\"\"\n    Alternative method using subprocess for all operations.\n    \"\"\"\n    try:\n        # Create virtual environment\n        print(f\"Creating virtual environment: {env_name}\")\n        subprocess.run([sys.executable, \"-m\", \"venv\", env_name], check=True)\n        \n        # Determine activation command based on OS\n        if os.name == 'nt':  # Windows\n            activate_cmd = f\"{env_name}\\\\Scripts\\\\activate && \"\n            python_cmd = f\"{env_name}\\\\Scripts\\\\python\"\n            pip_cmd = f\"{env_name}\\\\Scripts\\\\pip\"\n        else:  # Unix/Linux/macOS\n            activate_cmd = f\"source {env_name}/bin/activate && \"\n            python_cmd = f\"{env_name}/bin/python\"\n            pip_cmd = f\"{env_name}/bin/pip\"\n        \n        # Install packages\n        packages = [\"django\", \"celery\", \"redis\", \"psycopg2-binary\"]\n        \n        for package in packages:\n            print(f\"Installing {package}...\")\n            subprocess.run([python_cmd, \"-m\", \"pip\", \"install\", package], \n                          check=True)\n        \n        print(\"Setup complete!\")\n        return True\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred: {e}\")\n        return False\n\n# Run the setup\ncreate_venv_with_subprocess(\"my_dev_env\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "sys", "venv", "os"], "package_breakdown": {"install_packages": ["subprocess", "sys", "venv", "os"], "import_packages": ["subprocess", "venv", "os", "sys"], "llm_suggested": ["subprocess", "sys", "os", "venv"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that defines a class with reusable constraint types using the `typing.Annotated` feature to specify constraints for function arguments and return types.", "prompt_index": 54, "generated_code": "```python\nfrom typing import Annotated, Any\nfrom dataclasses import dataclass\n\n# Define constraint types\nPositiveInt = Annotated[int, \"value > 0\"]\nNonEmptyString = Annotated[str, \"len(value) > 0\"]\nEmailString = Annotated[str, \"contains @ and .\"]\nRangeFloat = Annotated[float, \"0.0 <= value <= 100.0\"]\nMinLength = Annotated[list, \"len(value) >= min_length\"]\n\n@dataclass\nclass Validator:\n    \"\"\"Simple validator for annotated constraints\"\"\"\n    \n    @staticmethod\n    def validate_positive_int(value: int) -> bool:\n        return isinstance(value, int) and value > 0\n    \n    @staticmethod\n    def validate_non_empty_string(value: str) -> bool:\n        return isinstance(value, str) and len(value) > 0\n    \n    @staticmethod\n    def validate_email(value: str) -> bool:\n        return isinstance(value, str) and '@' in value and '.' in value\n    \n    @staticmethod\n    def validate_range_float(value: float) -> bool:\n        return isinstance(value, float) and 0.0 <= value <= 100.0\n    \n    @staticmethod\n    def validate_min_length(value: list, min_len: int = 1) -> bool:\n        return isinstance(value, list) and len(value) >= min_len\n\nclass UserService:\n    \"\"\"Example class using annotated constraint types\"\"\"\n    \n    def __init__(self, validator: Validator = None):\n        self.validator = validator or Validator()\n    \n    def create_user(\n        self, \n        user_id: PositiveInt,\n        name: NonEmptyString,\n        email: EmailString,\n        score: RangeFloat\n    ) -> dict[str, Any]:\n        \"\"\"Create a user with validated inputs\"\"\"\n        # Validation logic\n        assert self.validator.validate_positive_int(user_id), \"Invalid user_id\"\n        assert self.validator.validate_non_empty_string(name), \"Invalid name\"\n        assert self.validator.validate_email(email), \"Invalid email\"\n        assert self.validator.validate_range_float(score), \"Invalid score\"\n        \n        return {\n            \"id\": user_id,\n            \"name\": name,\n            \"email\": email,\n            \"score\": score\n        }\n    \n    def get_user_tags(self, user_id: PositiveInt) -> MinLength:\n        \"\"\"Get user tags (returns non-empty list)\"\"\"\n        assert self.validator.validate_positive_int(user_id), \"Invalid user_id\"\n        \n        # Mock return - in practice would fetch from database\n        tags = [\"python\", \"developer\", \"active\"]\n        assert self.validator.validate_min_length(tags), \"Tags list cannot be empty\"\n        return tags\n    \n    def update_score(\n        self, \n        user_id: PositiveInt, \n        new_score: RangeFloat\n    ) -> RangeFloat:\n        \"\"\"Update user score and return the new score\"\"\"\n        assert self.validator.validate_positive_int(user_id), \"Invalid user_id\"\n        assert self.validator.validate_range_float(new_score), \"Invalid score\"\n        \n        # Mock update logic\n        return new_score\n\n# Usage example\nif __name__ == \"__main__\":\n    service = UserService()\n    \n    # Valid usage\n    user = service.create_user(\n        user_id=1,\n        name=\"John Doe\",\n        email=\"john@example.com\",\n        score=85.5\n    )\n    print(f\"Created user: {user}\")\n    \n    # Get tags\n    tags = service.get_user_tags(1)\n    print(f\"User tags: {tags}\")\n    \n    # Update score\n    new_score = service.update_score(1, 92.0)\n    print(f\"Updated score: {new_score}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dataclasses", "typing"], "package_breakdown": {"install_packages": ["dataclasses", "typing"], "import_packages": ["dataclasses", "typing"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the pytest package to test a function that calculates the factorial of a given number and asserts that the result is correct.", "prompt_index": 55, "generated_code": "```python\nimport pytest\n\ndef factorial(n):\n    \"\"\"Calculate the factorial of a given number.\"\"\"\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    if n == 0 or n == 1:\n        return 1\n    result = 1\n    for i in range(2, n + 1):\n        result *= i\n    return result\n\ndef test_factorial_positive_numbers():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(4) == 24\n    assert factorial(5) == 120\n    assert factorial(10) == 3628800\n\ndef test_factorial_negative_numbers():\n    with pytest.raises(ValueError):\n        factorial(-1)\n    with pytest.raises(ValueError):\n        factorial(-5)\n\ndef test_factorial_large_number():\n    assert factorial(12) == 479001600\n\n@pytest.mark.parametrize(\"n, expected\", [\n    (0, 1),\n    (1, 1),\n    (2, 2),\n    (3, 6),\n    (4, 24),\n    (5, 120),\n    (6, 720),\n])\ndef test_factorial_parametrized(n, expected):\n    assert factorial(n) == expected\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pytest"], "package_breakdown": {"install_packages": ["pytest"], "import_packages": ["pytest"], "llm_suggested": ["pytest"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that serializes and deserializes data using the common protobufs provided by Google APIs.", "prompt_index": 56, "generated_code": "```python\n# Install required package: pip install googleapis-common-protos protobuf\n\nfrom google.protobuf import timestamp_pb2\nfrom google.protobuf import duration_pb2\nfrom google.protobuf import struct_pb2\nfrom google.protobuf import any_pb2\nfrom google.protobuf import field_mask_pb2\nfrom google.protobuf.json_format import MessageToJson, Parse\nimport json\nfrom datetime import datetime, timedelta\n\n# Example 1: Working with Timestamp\ndef serialize_timestamp():\n    # Create a timestamp\n    timestamp = timestamp_pb2.Timestamp()\n    timestamp.GetCurrentTime()  # Set to current time\n    \n    # Or set manually\n    timestamp.seconds = int(datetime.now().timestamp())\n    timestamp.nanos = 0\n    \n    # Serialize to binary\n    binary_data = timestamp.SerializeToString()\n    \n    # Serialize to JSON\n    json_data = MessageToJson(timestamp)\n    \n    return binary_data, json_data\n\ndef deserialize_timestamp(binary_data, json_data):\n    # Deserialize from binary\n    timestamp1 = timestamp_pb2.Timestamp()\n    timestamp1.ParseFromString(binary_data)\n    \n    # Deserialize from JSON\n    timestamp2 = Parse(json_data, timestamp_pb2.Timestamp())\n    \n    # Convert to Python datetime\n    dt = timestamp1.ToDatetime()\n    \n    return timestamp1, timestamp2, dt\n\n# Example 2: Working with Duration\ndef serialize_duration():\n    duration = duration_pb2.Duration()\n    duration.seconds = 3600  # 1 hour\n    duration.nanos = 500000000  # 0.5 seconds\n    \n    binary_data = duration.SerializeToString()\n    json_data = MessageToJson(duration)\n    \n    return binary_data, json_data\n\ndef deserialize_duration(binary_data, json_data):\n    duration1 = duration_pb2.Duration()\n    duration1.ParseFromString(binary_data)\n    \n    duration2 = Parse(json_data, duration_pb2.Duration())\n    \n    # Convert to Python timedelta\n    td = duration1.ToTimedelta()\n    \n    return duration1, duration2, td\n\n# Example 3: Working with Struct (dynamic JSON-like data)\ndef serialize_struct():\n    struct = struct_pb2.Struct()\n    \n    # Add various data types\n    struct.fields[\"string_field\"].string_value = \"hello\"\n    struct.fields[\"number_field\"].number_value = 42.5\n    struct.fields[\"bool_field\"].bool_value = True\n    struct.fields[\"null_field\"].null_value = struct_pb2.NullValue.NULL_VALUE\n    \n    # Add nested structure\n    nested = struct.fields[\"nested\"].struct_value\n    nested.fields[\"inner_field\"].string_value = \"nested value\"\n    \n    # Add list\n    list_value = struct.fields[\"list_field\"].list_value\n    list_value.values.add().string_value = \"item1\"\n    list_value.values.add().number_value = 123\n    \n    binary_data = struct.SerializeToString()\n    json_data = MessageToJson(struct)\n    \n    return binary_data, json_data\n\ndef deserialize_struct(binary_data, json_data):\n    struct1 = struct_pb2.Struct()\n    struct1.ParseFromString(binary_data)\n    \n    struct2 = Parse(json_data, struct_pb2.Struct())\n    \n    return struct1, struct2\n\n# Example 4: Working with Any (can contain any protobuf message)\ndef serialize_any():\n    # Create a timestamp to pack into Any\n    timestamp = timestamp_pb2.Timestamp()\n    timestamp.GetCurrentTime()\n    \n    # Pack into Any\n    any_msg = any_pb2.Any()\n    any_msg.Pack(timestamp)\n    \n    binary_data = any_msg.SerializeToString()\n    json_data = MessageToJson(any_msg)\n    \n    return binary_data, json_data\n\ndef deserialize_any(binary_data, json_data):\n    any1 = any_pb2.Any()\n    any1.ParseFromString(binary_data)\n    \n    any2 = Parse(json_data, any_pb2.Any())\n    \n    # Unpack the Any message\n    if any1.Is(timestamp_pb2.Timestamp.DESCRIPTOR):\n        timestamp = timestamp_pb2.Timestamp()\n        any1.Unpack(timestamp)\n        return any1, any2, timestamp\n    \n    return any1, any2, None\n\n# Example 5: Working with FieldMask\ndef serialize_field_mask():\n    field_mask = field_mask_pb2.FieldMask()\n    field_mask.paths.extend([\"name\", \"email\", \"address.street\"])\n    \n    binary_data = field_mask.SerializeToString()\n    json_data = MessageToJson(field_mask)\n    \n    return binary_data, json_data\n\ndef deserialize_field_mask(binary_data, json_data):\n    field_mask1 = field_mask_pb2.FieldMask()\n    field_mask1.ParseFromString(binary_data)\n    \n    field_mask2 = Parse(json_data, field_mask_pb2.FieldMask())\n    \n    return field_mask1, field_mask2\n\n# Utility function to create Struct from Python dict\ndef dict_to_struct(data):\n    struct = struct_pb2.Struct()\n    struct.update(data)\n    return struct\n\n# Utility function to convert Struct to Python dict\ndef struct_to_dict(struct):\n    return dict(struct)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test Timestamp\n    print(\"=== Timestamp ===\")\n    ts_binary, ts_json = serialize_timestamp()\n    ts1, ts2, dt = deserialize_timestamp(ts_binary, ts_json)\n    print(f\"Timestamp as datetime: {dt}\")\n    print(f\"JSON: {ts_json}\")\n    \n    # Test Duration\n    print(\"\\n=== Duration ===\")\n    dur_binary, dur_json = serialize_duration()\n    dur1, dur2, td = deserialize_duration(dur_binary, dur_json)\n    print(f\"Duration as timedelta: {td}\")\n    print(f\"JSON: {dur_json}\")\n    \n    # Test Struct\n    print(\"\\n=== Struct ===\")\n    struct_binary, struct_json = serialize_struct()\n    struct1, struct2 = deserialize_struct(struct_binary, struct_json)\n    print(f\"JSON: {struct_json}\")\n    \n    # Test dict to struct conversion\n    python_dict = {\"name\": \"John\", \"age\": 30, \"active\": True}\n    struct_from_dict = dict_to_struct(python_dict)\n    dict_from_struct = struct_to_dict(struct_from_dict)\n    print(f\"Dict to Struct to Dict: {dict_from_struct}\")\n    \n    # Test Any\n    print(\"\\n=== Any ===\")\n    any_binary, any_json = serialize_any()\n    any1, any2, unpacked = deserialize_any(any_binary, any_json)\n    if unpacked:\n        print(f\"Unpacked timestamp: {unpacked.ToDatetime()}\")\n    print(f\"JSON: {any_json}\")\n    \n    # Test FieldMask\n    print(\"\\n=== FieldMask ===\")\n    fm_binary, fm_json = serialize_field_mask()\n    fm1, fm2 = deserialize_field_mask(fm_binary, fm_json)\n    print(f\"Field paths: {list(fm1.paths)}\")\n    print(f\"JSON: {fm_json}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "datetime", "protobuf", "json", "googleapis-common-protos"], "package_breakdown": {"install_packages": ["google", "datetime", "protobuf", "json", "googleapis-common-protos"], "import_packages": ["google", "json", "datetime"], "llm_suggested": ["protobuf", "googleapis-common-protos"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts Protocol Buffers messages to gRPC messages and vice versa for seamless communication between microservices.", "prompt_index": 57, "generated_code": "```python\nimport grpc\nfrom google.protobuf.message import Message\nfrom google.protobuf import descriptor_pb2\nfrom google.protobuf.json_format import MessageToJson, Parse\nfrom typing import Type, Any, Dict\nimport json\n\nclass ProtobufGrpcConverter:\n    \"\"\"Utility class for converting between Protocol Buffers and gRPC messages\"\"\"\n    \n    @staticmethod\n    def protobuf_to_grpc(protobuf_message: Message) -> bytes:\n        \"\"\"Convert Protocol Buffers message to gRPC-compatible bytes\"\"\"\n        return protobuf_message.SerializeToString()\n    \n    @staticmethod\n    def grpc_to_protobuf(grpc_bytes: bytes, message_type: Type[Message]) -> Message:\n        \"\"\"Convert gRPC bytes back to Protocol Buffers message\"\"\"\n        message = message_type()\n        message.ParseFromString(grpc_bytes)\n        return message\n    \n    @staticmethod\n    def protobuf_to_json(protobuf_message: Message) -> str:\n        \"\"\"Convert Protocol Buffers message to JSON string\"\"\"\n        return MessageToJson(protobuf_message, preserving_proto_field_name=True)\n    \n    @staticmethod\n    def json_to_protobuf(json_str: str, message_type: Type[Message]) -> Message:\n        \"\"\"Convert JSON string to Protocol Buffers message\"\"\"\n        return Parse(json_str, message_type())\n\n# Example gRPC service implementation\nclass MicroserviceClient:\n    \"\"\"Client for communicating with gRPC microservices\"\"\"\n    \n    def __init__(self, server_address: str):\n        self.channel = grpc.insecure_channel(server_address)\n        self.converter = ProtobufGrpcConverter()\n    \n    def send_request(self, stub_class, method_name: str, request_message: Message) -> Any:\n        \"\"\"Send gRPC request and return response\"\"\"\n        try:\n            stub = stub_class(self.channel)\n            method = getattr(stub, method_name)\n            response = method(request_message)\n            return response\n        except grpc.RpcError as e:\n            print(f\"gRPC error: {e.code()}, {e.details()}\")\n            return None\n    \n    def close(self):\n        \"\"\"Close the gRPC channel\"\"\"\n        self.channel.close()\n\n# Example service implementation\nclass MessageProcessor:\n    \"\"\"Process and convert messages between services\"\"\"\n    \n    def __init__(self):\n        self.converter = ProtobufGrpcConverter()\n    \n    def process_incoming_message(self, grpc_bytes: bytes, message_type: Type[Message]) -> Dict:\n        \"\"\"Process incoming gRPC message and convert to dict\"\"\"\n        # Convert gRPC bytes to protobuf\n        protobuf_msg = self.converter.grpc_to_protobuf(grpc_bytes, message_type)\n        \n        # Convert to JSON for processing\n        json_str = self.converter.protobuf_to_json(protobuf_msg)\n        \n        # Return as dictionary\n        return json.loads(json_str)\n    \n    def prepare_outgoing_message(self, data: Dict, message_type: Type[Message]) -> bytes:\n        \"\"\"Prepare outgoing message for gRPC transmission\"\"\"\n        # Convert dict to JSON\n        json_str = json.dumps(data)\n        \n        # Convert JSON to protobuf\n        protobuf_msg = self.converter.json_to_protobuf(json_str, message_type)\n        \n        # Convert to gRPC bytes\n        return self.converter.protobuf_to_grpc(protobuf_msg)\n\n# Decorator for automatic message conversion\ndef grpc_message_converter(request_type: Type[Message], response_type: Type[Message]):\n    \"\"\"Decorator to automatically handle protobuf/gRPC conversions\"\"\"\n    def decorator(func):\n        def wrapper(self, request_bytes: bytes) -> bytes:\n            converter = ProtobufGrpcConverter()\n            \n            # Convert incoming gRPC bytes to protobuf\n            request_message = converter.grpc_to_protobuf(request_bytes, request_type)\n            \n            # Call the original function with protobuf message\n            response_message = func(self, request_message)\n            \n            # Convert response protobuf to gRPC bytes\n            return converter.protobuf_to_grpc(response_message)\n        \n        return wrapper\n    return decorator\n\n# Example usage with a hypothetical service\nclass UserService:\n    \"\"\"Example microservice for user operations\"\"\"\n    \n    def __init__(self):\n        self.processor = MessageProcessor()\n    \n    @grpc_message_converter(UserRequest, UserResponse)\n    def get_user(self, request: UserRequest) -> UserResponse:\n        \"\"\"Get user information\"\"\"\n        # Process the protobuf request\n        user_id = request.user_id\n        \n        # Create response (example)\n        response = UserResponse()\n        response.user_id = user_id\n        response.username = f\"user_{user_id}\"\n        response.email = f\"user_{user_id}@example.com\"\n        \n        return response\n\n# Async version for high-performance scenarios\nimport asyncio\nimport grpc.aio\n\nclass AsyncMicroserviceClient:\n    \"\"\"Async client for gRPC microservices\"\"\"\n    \n    def __init__(self, server_address: str):\n        self.channel = grpc.aio.insecure_channel(server_address)\n        self.converter = ProtobufGrpcConverter()\n    \n    async def send_request_async(self, stub_class, method_name: str, request_message: Message) -> Any:\n        \"\"\"Send async gRPC request\"\"\"\n        try:\n            stub = stub_class(self.channel)\n            method = getattr(stub, method_name)\n            response = await method(request_message)\n            return response\n        except grpc.aio.AioRpcError as e:\n            print(f\"Async gRPC error: {e.code()}, {e.details()}\")\n            return None\n    \n    async def close(self):\n        \"\"\"Close the async gRPC channel\"\"\"\n        await self.channel.close()\n\n# Batch processing utility\nclass BatchMessageProcessor:\n    \"\"\"Process multiple messages in batch\"\"\"\n    \n    def __init__(self):\n        self.converter = ProtobufGrpcConverter()\n    \n    def process_batch(self, messages: list, message_type: Type[Message]) -> list:\n        \"\"\"Process a batch of messages\"\"\"\n        results = []\n        for msg_bytes in messages:\n            try:\n                protobuf_msg = self.converter.grpc_to_protobuf(msg_bytes, message_type)\n                json_str = self.converter.protobuf_to_json(protobuf_msg)\n                results.append(json.loads(json_str))\n            except Exception as e:\n                print(f\"Error processing message: {e}\")\n                results.append(None)\n        return results\n    \n    def prepare_batch(self, data_list: list, message_type: Type[Message]) -> list:\n        \"\"\"Prepare a batch of messages for transmission\"\"\"\n        results = []\n        for data in data_list:\n            try:\n                json_str = json.dumps(data)\n                protobuf_msg = self.converter.json_to_protobuf(json_str, message_type)\n                grpc_bytes = self.converter.protobuf_to_grpc(protobuf_msg)\n                results.append(grpc_bytes)\n            except Exception as e:\n                print(f\"Error preparing message: {e}\")\n                results.append(None)\n        return results\n\n# Example proto message classes (these would be generated from .proto files)\nclass UserRequest(Message):\n    \"\"\"Example user request message\"\"\"\n    pass\n\nclass UserResponse(Message):\n    \"\"\"Example user response message\"\"\"\n    pass\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "grpc", "grpcio", "json", "protobuf", "asyncio", "grpcio-status", "typing"], "package_breakdown": {"install_packages": ["google", "grpc", "grpcio", "json", "protobuf", "asyncio", "grpcio-status", "typing"], "import_packages": ["google", "asyncio", "typing", "json", "grpc"], "llm_suggested": ["grpcio", "protobuf", "grpcio-status"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the next generation HTTP client to make a GET request to a specified URL and returns the response data.", "prompt_index": 58, "generated_code": "```python\nimport httpx\n\ndef make_get_request(url):\n    with httpx.Client() as client:\n        response = client.get(url)\n        response.raise_for_status()\n        return response.json()\n\n# Alternative async version\nimport asyncio\n\nasync def make_async_get_request(url):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        return response.json()\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Synchronous request\n    url = \"https://api.github.com/users/octocat\"\n    data = make_get_request(url)\n    print(data)\n    \n    # Asynchronous request\n    async def main():\n        data = await make_async_get_request(url)\n        print(data)\n    \n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "httpx"], "package_breakdown": {"install_packages": ["asyncio", "httpx"], "import_packages": ["asyncio", "httpx"], "llm_suggested": ["httpx"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves the current time zone information using the IANA time zone data provider and displays the corresponding time zone name, current time, and offset from UTC.", "prompt_index": 59, "generated_code": "```python\nimport datetime\nimport zoneinfo\n\n# Get the local time zone\nlocal_tz = datetime.datetime.now().astimezone().tzinfo\n\n# Get current time in local timezone\ncurrent_time = datetime.datetime.now(local_tz)\n\n# Get timezone name\ntz_name = str(local_tz)\n\n# Calculate UTC offset\nutc_time = datetime.datetime.now(datetime.timezone.utc)\nlocal_time = datetime.datetime.now(local_tz)\noffset = local_time.utcoffset()\n\nprint(f\"Time Zone: {tz_name}\")\nprint(f\"Current Time: {current_time.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\nprint(f\"UTC Offset: {offset}\")\n\n# Alternative method using a specific IANA timezone\n# Example with New York timezone\nny_tz = zoneinfo.ZoneInfo(\"America/New_York\")\nny_time = datetime.datetime.now(ny_tz)\nny_offset = ny_time.utcoffset()\n\nprint(f\"\\nExample - New York:\")\nprint(f\"Time Zone: America/New_York\")\nprint(f\"Current Time: {ny_time.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\nprint(f\"UTC Offset: {ny_offset}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "zoneinfo"], "package_breakdown": {"install_packages": ["datetime", "zoneinfo"], "import_packages": ["datetime", "zoneinfo"], "llm_suggested": ["datetime", "zoneinfo"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that validates a given JSON object against a specified JSON Schema using the functionalities provided by the package.", "prompt_index": 60, "generated_code": "```python\nimport json\nimport jsonschema\nfrom jsonschema import validate, ValidationError\n\ndef validate_json_against_schema(json_data, schema):\n    \"\"\"\n    Validates a JSON object against a specified JSON Schema.\n    \n    Args:\n        json_data: The JSON data to validate (dict or string)\n        schema: The JSON schema to validate against (dict or string)\n    \n    Returns:\n        tuple: (is_valid: bool, error_message: str or None)\n    \"\"\"\n    try:\n        # Convert strings to dict if necessary\n        if isinstance(json_data, str):\n            json_data = json.loads(json_data)\n        if isinstance(schema, str):\n            schema = json.loads(schema)\n        \n        # Validate the JSON data against the schema\n        validate(instance=json_data, schema=schema)\n        return True, None\n        \n    except ValidationError as e:\n        return False, f\"Validation error: {e.message}\"\n    except json.JSONDecodeError as e:\n        return False, f\"JSON decode error: {str(e)}\"\n    except Exception as e:\n        return False, f\"Unexpected error: {str(e)}\"\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a JSON schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\", \"minimum\": 0},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"}\n        },\n        \"required\": [\"name\", \"age\"]\n    }\n    \n    # Valid JSON data\n    valid_data = {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"email\": \"john@example.com\"\n    }\n    \n    # Invalid JSON data\n    invalid_data = {\n        \"name\": \"Jane Doe\",\n        \"age\": -5,  # Invalid: negative age\n        \"email\": \"invalid-email\"  # Invalid: not a proper email format\n    }\n    \n    # Validate valid data\n    is_valid, error = validate_json_against_schema(valid_data, schema)\n    print(f\"Valid data validation: {is_valid}\")\n    if error:\n        print(f\"Error: {error}\")\n    \n    # Validate invalid data\n    is_valid, error = validate_json_against_schema(invalid_data, schema)\n    print(f\"Invalid data validation: {is_valid}\")\n    if error:\n        print(f\"Error: {error}\")\n\n# Alternative approach using Draft7Validator for more control\ndef validate_with_draft7(json_data, schema):\n    \"\"\"\n    Validates JSON using Draft7Validator for more detailed error reporting.\n    \"\"\"\n    try:\n        if isinstance(json_data, str):\n            json_data = json.loads(json_data)\n        if isinstance(schema, str):\n            schema = json.loads(schema)\n        \n        validator = jsonschema.Draft7Validator(schema)\n        errors = list(validator.iter_errors(json_data))\n        \n        if errors:\n            error_messages = [f\"Path: {'.'.join(str(p) for p in error.path)}, Error: {error.message}\" \n                            for error in errors]\n            return False, error_messages\n        \n        return True, None\n        \n    except Exception as e:\n        return False, [f\"Validation failed: {str(e)}\"]\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "jsonschema"], "package_breakdown": {"install_packages": ["json", "jsonschema"], "import_packages": ["json", "jsonschema"], "llm_suggested": ["json", "jsonschema"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that decodes an ASN.1-based protocol message using the modules provided in this package.", "prompt_index": 61, "generated_code": "```python\nfrom pyasn1.codec.der import decoder\nfrom pyasn1.type import univ, char, namedtype, tag\nfrom pyasn1 import error\n\n# Define ASN.1 structure (example: X.509 certificate-like structure)\nclass Name(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('commonName', char.UTF8String()),\n        namedtype.OptionalNamedType('organizationName', char.UTF8String()),\n        namedtype.OptionalNamedType('countryName', char.UTF8String())\n    )\n\nclass Certificate(univ.Sequence):\n    componentType = namedtype.NamedTypes(\n        namedtype.NamedType('version', univ.Integer()),\n        namedtype.NamedType('serialNumber', univ.Integer()),\n        namedtype.NamedType('subject', Name()),\n        namedtype.NamedType('issuer', Name()),\n        namedtype.NamedType('notBefore', char.UTF8String()),\n        namedtype.NamedType('notAfter', char.UTF8String())\n    )\n\ndef decode_asn1_message(encoded_data):\n    \"\"\"\n    Decode ASN.1 encoded message\n    \n    Args:\n        encoded_data: bytes - DER/BER encoded ASN.1 data\n    \n    Returns:\n        Decoded ASN.1 object\n    \"\"\"\n    try:\n        # Decode the message\n        decoded_obj, remainder = decoder.decode(encoded_data, asn1Spec=Certificate())\n        \n        # Extract values\n        version = decoded_obj['version']\n        serial_number = decoded_obj['serialNumber']\n        subject_name = decoded_obj['subject']['commonName']\n        issuer_name = decoded_obj['issuer']['commonName']\n        not_before = decoded_obj['notBefore']\n        not_after = decoded_obj['notAfter']\n        \n        return {\n            'version': int(version),\n            'serialNumber': int(serial_number),\n            'subject': str(subject_name),\n            'issuer': str(issuer_name),\n            'notBefore': str(not_before),\n            'notAfter': str(not_after)\n        }\n        \n    except error.PyAsn1Error as e:\n        print(f\"ASN.1 decoding error: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example encoded data (you would replace this with actual encoded bytes)\n    # This is a mock example - real ASN.1 data would be binary\n    sample_encoded_data = b'\\x30\\x82\\x01\\x00...'  # Replace with actual DER/BER data\n    \n    # Decode the message\n    result = decode_asn1_message(sample_encoded_data)\n    \n    if result:\n        print(\"Decoded ASN.1 message:\")\n        for key, value in result.items():\n            print(f\"{key}: {value}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyasn1"], "package_breakdown": {"install_packages": ["pyasn1"], "import_packages": ["pyasn1"], "llm_suggested": ["pyasn1"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads and parses a config.ini file using the brain-dead simple config-ini parsing package, allowing access to the configuration settings within the file.", "prompt_index": 62, "generated_code": "```python\nimport configparser\n\n# Create a ConfigParser object\nconfig = configparser.ConfigParser()\n\n# Read the config.ini file\nconfig.read('config.ini')\n\n# Access configuration settings\n# Example: Get a value from a specific section\nvalue = config['section_name']['key_name']\n\n# Alternative syntax\nvalue = config.get('section_name', 'key_name')\n\n# Get value with default fallback\nvalue = config.get('section_name', 'key_name', fallback='default_value')\n\n# Get all sections\nsections = config.sections()\n\n# Get all options in a section\noptions = config.options('section_name')\n\n# Get all items in a section as key-value pairs\nitems = config.items('section_name')\n\n# Check if section exists\nif config.has_section('section_name'):\n    print(\"Section exists\")\n\n# Check if option exists in section\nif config.has_option('section_name', 'key_name'):\n    print(\"Option exists\")\n\n# Example usage with error handling\ntry:\n    config.read('config.ini')\n    database_host = config['database']['host']\n    database_port = config.getint('database', 'port')\n    debug_mode = config.getboolean('settings', 'debug')\n    print(f\"Host: {database_host}, Port: {database_port}, Debug: {debug_mode}\")\nexcept FileNotFoundError:\n    print(\"Config file not found\")\nexcept KeyError as e:\n    print(f\"Configuration key not found: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["configparser"], "package_breakdown": {"install_packages": ["configparser"], "import_packages": ["configparser"], "llm_suggested": ["configparser"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sends a GET request to a specified URL using this minimal low-level HTTP client package and prints out the response content received.", "prompt_index": 63, "generated_code": "```python\nimport socket\n\ndef send_get_request(url):\n    # Parse URL\n    if url.startswith('http://'):\n        url = url[7:]\n    elif url.startswith('https://'):\n        raise ValueError(\"HTTPS not supported in this minimal client\")\n    \n    # Split host and path\n    if '/' in url:\n        host, path = url.split('/', 1)\n        path = '/' + path\n    else:\n        host = url\n        path = '/'\n    \n    # Create socket connection\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.connect((host, 80))\n    \n    # Construct HTTP GET request\n    request = f\"GET {path} HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\"\n    \n    # Send request\n    sock.send(request.encode())\n    \n    # Receive response\n    response = b\"\"\n    while True:\n        data = sock.recv(1024)\n        if not data:\n            break\n        response += data\n    \n    sock.close()\n    \n    # Print response content\n    print(response.decode())\n\n# Example usage\nsend_get_request(\"http://httpbin.org/get\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socket"], "package_breakdown": {"install_packages": ["socket"], "import_packages": ["socket"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates an asynchronous HTTP server using the asyncio framework to handle incoming requests and responses.", "prompt_index": 64, "generated_code": "```python\nimport asyncio\nimport json\nfrom urllib.parse import urlparse, parse_qs\n\nclass AsyncHTTPServer:\n    def __init__(self, host='localhost', port=8080):\n        self.host = host\n        self.port = port\n        self.routes = {}\n    \n    def route(self, path, method='GET'):\n        \"\"\"Decorator to register routes\"\"\"\n        def decorator(func):\n            self.routes[(method, path)] = func\n            return func\n        return decorator\n    \n    async def parse_request(self, reader):\n        \"\"\"Parse incoming HTTP request\"\"\"\n        request_line = await reader.readline()\n        if not request_line:\n            return None\n            \n        request_line = request_line.decode().strip()\n        method, path, version = request_line.split()\n        \n        # Parse headers\n        headers = {}\n        while True:\n            line = await reader.readline()\n            if line == b'\\r\\n':\n                break\n            header_line = line.decode().strip()\n            if ':' in header_line:\n                key, value = header_line.split(':', 1)\n                headers[key.strip().lower()] = value.strip()\n        \n        # Parse body if present\n        body = b''\n        content_length = int(headers.get('content-length', 0))\n        if content_length > 0:\n            body = await reader.read(content_length)\n        \n        # Parse URL and query parameters\n        parsed_url = urlparse(path)\n        query_params = parse_qs(parsed_url.query)\n        \n        return {\n            'method': method,\n            'path': parsed_url.path,\n            'query_params': query_params,\n            'headers': headers,\n            'body': body.decode() if body else ''\n        }\n    \n    async def send_response(self, writer, status_code, headers=None, body=''):\n        \"\"\"Send HTTP response\"\"\"\n        status_messages = {\n            200: 'OK',\n            404: 'Not Found',\n            500: 'Internal Server Error'\n        }\n        \n        response = f\"HTTP/1.1 {status_code} {status_messages.get(status_code, 'Unknown')}\\r\\n\"\n        \n        if headers is None:\n            headers = {}\n        \n        headers['Content-Length'] = str(len(body.encode()))\n        headers['Connection'] = 'close'\n        \n        for key, value in headers.items():\n            response += f\"{key}: {value}\\r\\n\"\n        \n        response += \"\\r\\n\"\n        response += body\n        \n        writer.write(response.encode())\n        await writer.drain()\n    \n    async def handle_request(self, request):\n        \"\"\"Handle incoming request and route to appropriate handler\"\"\"\n        method = request['method']\n        path = request['path']\n        \n        # Find matching route\n        handler = self.routes.get((method, path))\n        if handler:\n            try:\n                response = await handler(request)\n                return response\n            except Exception as e:\n                return {\n                    'status': 500,\n                    'body': f'Internal Server Error: {str(e)}'\n                }\n        else:\n            return {\n                'status': 404,\n                'body': 'Not Found'\n            }\n    \n    async def client_connected(self, reader, writer):\n        \"\"\"Handle new client connection\"\"\"\n        try:\n            request = await self.parse_request(reader)\n            if request is None:\n                return\n            \n            response = await self.handle_request(request)\n            \n            status = response.get('status', 200)\n            headers = response.get('headers', {})\n            body = response.get('body', '')\n            \n            await self.send_response(writer, status, headers, body)\n            \n        except Exception as e:\n            await self.send_response(writer, 500, {}, f'Server Error: {str(e)}')\n        finally:\n            writer.close()\n            await writer.wait_closed()\n    \n    async def start(self):\n        \"\"\"Start the HTTP server\"\"\"\n        server = await asyncio.start_server(\n            self.client_connected, \n            self.host, \n            self.port\n        )\n        \n        print(f\"Server running on http://{self.host}:{self.port}\")\n        \n        async with server:\n            await server.serve_forever()\n\n# Example usage\napp = AsyncHTTPServer()\n\n@app.route('/', 'GET')\nasync def home(request):\n    return {\n        'status': 200,\n        'headers': {'Content-Type': 'text/html'},\n        'body': '<h1>Welcome to Async HTTP Server!</h1>'\n    }\n\n@app.route('/api/data', 'GET')\nasync def get_data(request):\n    data = {'message': 'Hello, World!', 'timestamp': '2023-01-01'}\n    return {\n        'status': 200,\n        'headers': {'Content-Type': 'application/json'},\n        'body': json.dumps(data)\n    }\n\n@app.route('/api/data', 'POST')\nasync def post_data(request):\n    try:\n        data = json.loads(request['body'])\n        response_data = {'received': data, 'status': 'success'}\n        return {\n            'status': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps(response_data)\n        }\n    except json.JSONDecodeError:\n        return {\n            'status': 400,\n            'body': 'Invalid JSON'\n        }\n\n@app.route('/echo', 'GET')\nasync def echo(request):\n    query_params = request.get('query_params', {})\n    return {\n        'status': 200,\n        'headers': {'Content-Type': 'application/json'},\n        'body': json.dumps({\n            'method': request['method'],\n            'path': request['path'],\n            'query_params': query_params,\n            'headers': dict(request['headers'])\n        })\n    }\n\n# Run the server\nif __name__ == '__main__':\n    asyncio.run(app.start())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "urllib.parse", "json", "asyncio"], "package_breakdown": {"install_packages": ["urllib", "urllib.parse", "json", "asyncio"], "import_packages": ["urllib", "json", "asyncio"], "llm_suggested": ["asyncio", "json", "urllib.parse"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that encodes a JSON web token containing user authentication information with a specified secret key and expiration time.", "prompt_index": 65, "generated_code": "```python\nimport jwt\nfrom datetime import datetime, timedelta\n\ndef create_jwt_token(user_id, username, secret_key, expiration_hours=24):\n    \"\"\"\n    Create a JWT token with user authentication information\n    \n    Args:\n        user_id: User's unique identifier\n        username: User's username\n        secret_key: Secret key for encoding the token\n        expiration_hours: Token expiration time in hours (default: 24)\n    \n    Returns:\n        Encoded JWT token as string\n    \"\"\"\n    # Set expiration time\n    expiration_time = datetime.utcnow() + timedelta(hours=expiration_hours)\n    \n    # Create payload with user information\n    payload = {\n        'user_id': user_id,\n        'username': username,\n        'exp': expiration_time,\n        'iat': datetime.utcnow(),  # issued at\n        'iss': 'your-app-name'    # issuer\n    }\n    \n    # Encode the token\n    token = jwt.encode(payload, secret_key, algorithm='HS256')\n    \n    return token\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example parameters\n    user_id = 12345\n    username = \"john_doe\"\n    secret_key = \"your-secret-key-here\"\n    \n    # Generate token\n    jwt_token = create_jwt_token(user_id, username, secret_key, expiration_hours=24)\n    print(f\"JWT Token: {jwt_token}\")\n    \n    # Optional: Decode and verify token\n    try:\n        decoded_payload = jwt.decode(jwt_token, secret_key, algorithms=['HS256'])\n        print(f\"Decoded payload: {decoded_payload}\")\n    except jwt.ExpiredSignatureError:\n        print(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        print(\"Invalid token\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "jwt", "PyJWT"], "package_breakdown": {"install_packages": ["datetime", "jwt", "PyJWT"], "import_packages": ["datetime", "jwt"], "llm_suggested": ["PyJWT"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads a TOML file named 'config.toml', extracts the key-value pairs, and stores them in a dictionary for further processing.", "prompt_index": 66, "generated_code": "```python\nimport toml\n\n# Read the TOML file and store key-value pairs in a dictionary\nwith open('config.toml', 'r') as file:\n    config_dict = toml.load(file)\n\n# The config_dict now contains all key-value pairs from the TOML file\nprint(config_dict)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["toml"], "package_breakdown": {"install_packages": ["toml"], "import_packages": ["toml"], "llm_suggested": ["toml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that lists all available AWS services using the Universal Command Line Environment for AWS.", "prompt_index": 67, "generated_code": "```python\nimport subprocess\nimport json\n\ndef list_aws_services():\n    try:\n        # Run AWS CLI command to list all services\n        result = subprocess.run(['aws', 'help'], \n                              capture_output=True, \n                              text=True, \n                              check=True)\n        \n        # Parse the help output to extract service names\n        lines = result.stdout.split('\\n')\n        services = []\n        in_services_section = False\n        \n        for line in lines:\n            if 'AVAILABLE SERVICES' in line:\n                in_services_section = True\n                continue\n            elif in_services_section and line.strip() == '':\n                break\n            elif in_services_section and line.strip().startswith('o '):\n                service = line.strip()[2:].split()[0]\n                services.append(service)\n        \n        return services\n    \n    except subprocess.CalledProcessError as e:\n        print(f\"Error running AWS CLI: {e}\")\n        return []\n\ndef list_services_programmatically():\n    try:\n        # Alternative method using service model\n        result = subprocess.run(['aws', 'configure', 'list-profiles'], \n                              capture_output=True, \n                              text=True)\n        \n        # Get services by checking what's available in CLI\n        result = subprocess.run(['aws', 'help'], \n                              capture_output=True, \n                              text=True)\n        \n        # Extract service names from help output\n        help_text = result.stdout\n        services = []\n        \n        # Look for service names in the help output\n        import re\n        service_pattern = r'^\\s+o\\s+([a-z0-9-]+)'\n        \n        for line in help_text.split('\\n'):\n            match = re.match(service_pattern, line)\n            if match:\n                services.append(match.group(1))\n        \n        return sorted(services)\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"Available AWS Services:\")\n    services = list_services_programmatically()\n    \n    for i, service in enumerate(services, 1):\n        print(f\"{i:2d}. {service}\")\n    \n    print(f\"\\nTotal services found: {len(services)}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "re", "subprocess"], "package_breakdown": {"install_packages": ["json", "re", "subprocess"], "import_packages": ["json", "re", "subprocess"], "llm_suggested": ["subprocess", "json", "re"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a decorator function to log the input arguments and output result of a specified function, and then applies the decorator to a given function.", "prompt_index": 68, "generated_code": "```python\nimport functools\n\ndef log_function(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Log input arguments\n        args_str = ', '.join(map(str, args))\n        kwargs_str = ', '.join(f\"{k}={v}\" for k, v in kwargs.items())\n        all_args = ', '.join(filter(None, [args_str, kwargs_str]))\n        \n        print(f\"Calling {func.__name__}({all_args})\")\n        \n        # Call the original function\n        result = func(*args, **kwargs)\n        \n        # Log output result\n        print(f\"{func.__name__} returned: {result}\")\n        \n        return result\n    return wrapper\n\n# Example function to apply the decorator to\n@log_function\ndef add_numbers(a, b, multiplier=1):\n    return (a + b) * multiplier\n\n@log_function\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\n# Test the decorated functions\nresult1 = add_numbers(5, 3)\nresult2 = add_numbers(10, 20, multiplier=2)\nresult3 = greet(\"Alice\")\nresult4 = greet(\"Bob\", greeting=\"Hi\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that demonstrates how to create and manipulate a multidict data structure, including adding, updating, and retrieving key-value pairs with support for multiple values per key.", "prompt_index": 69, "generated_code": "```python\nfrom collections import defaultdict\n\nclass MultiDict:\n    def __init__(self):\n        self._data = defaultdict(list)\n    \n    def add(self, key, value):\n        \"\"\"Add a value to a key (allows multiple values per key)\"\"\"\n        self._data[key].append(value)\n    \n    def set(self, key, value):\n        \"\"\"Set a single value for a key (replaces all existing values)\"\"\"\n        self._data[key] = [value]\n    \n    def get(self, key, default=None):\n        \"\"\"Get the first value for a key\"\"\"\n        values = self._data.get(key, [])\n        return values[0] if values else default\n    \n    def get_all(self, key):\n        \"\"\"Get all values for a key\"\"\"\n        return self._data.get(key, [])\n    \n    def update(self, key, old_value, new_value):\n        \"\"\"Update a specific value for a key\"\"\"\n        if key in self._data and old_value in self._data[key]:\n            index = self._data[key].index(old_value)\n            self._data[key][index] = new_value\n            return True\n        return False\n    \n    def remove(self, key, value=None):\n        \"\"\"Remove a specific value or all values for a key\"\"\"\n        if key not in self._data:\n            return False\n        \n        if value is None:\n            # Remove all values for the key\n            del self._data[key]\n            return True\n        else:\n            # Remove specific value\n            if value in self._data[key]:\n                self._data[key].remove(value)\n                if not self._data[key]:  # Remove key if no values left\n                    del self._data[key]\n                return True\n        return False\n    \n    def keys(self):\n        \"\"\"Get all keys\"\"\"\n        return list(self._data.keys())\n    \n    def items(self):\n        \"\"\"Get all key-value pairs as tuples\"\"\"\n        items = []\n        for key, values in self._data.items():\n            for value in values:\n                items.append((key, value))\n        return items\n    \n    def __contains__(self, key):\n        \"\"\"Check if key exists\"\"\"\n        return key in self._data\n    \n    def __len__(self):\n        \"\"\"Get total number of key-value pairs\"\"\"\n        return sum(len(values) for values in self._data.values())\n    \n    def __str__(self):\n        return str(dict(self._data))\n\n# Demonstration of MultiDict usage\nif __name__ == \"__main__\":\n    # Create a multidict\n    md = MultiDict()\n    \n    # Adding multiple values to the same key\n    md.add('colors', 'red')\n    md.add('colors', 'blue')\n    md.add('colors', 'green')\n    md.add('numbers', 1)\n    md.add('numbers', 2)\n    \n    print(\"MultiDict contents:\", md)\n    print(\"All colors:\", md.get_all('colors'))\n    print(\"First color:\", md.get('colors'))\n    print(\"All numbers:\", md.get_all('numbers'))\n    \n    # Setting a single value (replaces existing)\n    md.set('colors', 'yellow')\n    print(\"After setting colors to yellow:\", md.get_all('colors'))\n    \n    # Adding more values\n    md.add('colors', 'purple')\n    md.add('colors', 'orange')\n    print(\"After adding more colors:\", md.get_all('colors'))\n    \n    # Updating a specific value\n    md.update('colors', 'yellow', 'gold')\n    print(\"After updating yellow to gold:\", md.get_all('colors'))\n    \n    # Removing a specific value\n    md.remove('colors', 'purple')\n    print(\"After removing purple:\", md.get_all('colors'))\n    \n    # Check if key exists\n    print(\"'colors' in multidict:\", 'colors' in md)\n    print(\"'animals' in multidict:\", 'animals' in md)\n    \n    # Get all keys and items\n    print(\"All keys:\", md.keys())\n    print(\"All items:\", md.items())\n    print(\"Total items:\", len(md))\n    \n    # Remove all values for a key\n    md.remove('numbers')\n    print(\"After removing all numbers:\", md)\n\n# Alternative implementation using built-in collections\nprint(\"\\n\" + \"=\"*50)\nprint(\"Using collections.defaultdict approach:\")\n\n# Simple multidict using defaultdict\nsimple_multidict = defaultdict(list)\n\n# Adding values\nsimple_multidict['tags'].extend(['python', 'programming', 'data'])\nsimple_multidict['tags'].append('structures')\nsimple_multidict['priority'].append('high')\nsimple_multidict['priority'].append('medium')\n\nprint(\"Simple multidict:\", dict(simple_multidict))\nprint(\"Tags:\", simple_multidict['tags'])\nprint(\"Priorities:\", simple_multidict['priority'])\n\n# Using werkzeug-style MultiDict (if available)\ntry:\n    from werkzeug.datastructures import MultiDict as WerkzeugMultiDict\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Using Werkzeug MultiDict:\")\n    \n    wmd = WerkzeugMultiDict([\n        ('name', 'John'),\n        ('name', 'Jane'),\n        ('age', '25'),\n        ('hobby', 'reading'),\n        ('hobby', 'swimming')\n    ])\n    \n    print(\"Werkzeug MultiDict:\", wmd)\n    print(\"First name:\", wmd.get('name'))\n    print(\"All names:\", wmd.getlist('name'))\n    print(\"All hobbies:\", wmd.getlist('hobby'))\n    \nexcept ImportError:\n    print(\"\\nWerkzeug not available. Install with: pip install werkzeug\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections", "werkzeug"], "package_breakdown": {"install_packages": ["collections", "werkzeug"], "import_packages": ["collections", "werkzeug"], "llm_suggested": ["collections", "werkzeug"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "\"Generate Python code that extracts the domain name and path from a given URL using the functions provided in the \"Yet another URL library\".\"", "prompt_index": 70, "generated_code": "```python\nimport yarl\n\ndef extract_domain_and_path(url_string):\n    url = yarl.URL(url_string)\n    domain = url.host\n    path = url.path\n    return domain, path\n\n# Example usage\nurl = \"https://www.example.com/path/to/page\"\ndomain, path = extract_domain_and_path(url)\nprint(f\"Domain: {domain}\")\nprint(f\"Path: {path}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yarl"], "package_breakdown": {"install_packages": ["yarl"], "import_packages": ["yarl"], "llm_suggested": ["yarl"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads data from a Parquet file using Apache Arrow and performs data manipulation operations such as filtering, grouping, and aggregation.", "prompt_index": 71, "generated_code": "```python\nimport pyarrow.parquet as pq\nimport pyarrow.compute as pc\nimport pyarrow as pa\n\n# Read Parquet file\ntable = pq.read_table('data.parquet')\n\n# Convert to pandas for easier manipulation (optional)\ndf = table.to_pandas()\n\n# Method 1: Using PyArrow compute functions\n# Filter data - example: filter rows where 'age' > 25\nfiltered_table = pc.filter(table, pc.greater(table['age'], 25))\n\n# Group by and aggregate using PyArrow\n# Example: group by 'category' and sum 'amount'\ngrouped = table.group_by(['category']).aggregate([\n    ('amount', 'sum'),\n    ('quantity', 'mean'),\n    ('price', 'max')\n])\n\n# Method 2: Using pandas after conversion\n# Filter\nfiltered_df = df[df['age'] > 25]\n\n# Group by and aggregate\ngrouped_df = df.groupby('category').agg({\n    'amount': 'sum',\n    'quantity': 'mean',\n    'price': 'max'\n}).reset_index()\n\n# Multiple filters with logical operations\ncomplex_filter = pc.and_(\n    pc.greater(table['age'], 18),\n    pc.less(table['salary'], 100000)\n)\nfiltered_complex = pc.filter(table, complex_filter)\n\n# Sort data\nsorted_table = pc.sort_indices(table, sort_keys=[('age', 'descending')])\n\n# Select specific columns\nselected_columns = table.select(['name', 'age', 'salary'])\n\n# Add computed column\ntable_with_computed = table.append_column(\n    'age_category', \n    pc.if_else(pc.greater(table['age'], 30), 'Senior', 'Junior')\n)\n\n# Write results back to Parquet\npq.write_table(grouped, 'grouped_results.parquet')\npq.write_table(filtered_table, 'filtered_results.parquet')\n\n# Display results\nprint(\"Original table shape:\", table.shape)\nprint(\"Filtered table shape:\", filtered_table.shape)\nprint(\"Grouped results:\")\nprint(grouped.to_pandas())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyarrow", "pandas"], "package_breakdown": {"install_packages": ["pyarrow", "pandas"], "import_packages": ["pyarrow"], "llm_suggested": ["pyarrow", "pandas"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user using OAuthlib with the Requests library for making HTTP requests.", "prompt_index": 72, "generated_code": "```python\nfrom requests_oauthlib import OAuth2Session\nfrom oauthlib.oauth2 import WebApplicationClient\nimport requests\n\n# OAuth2 configuration\nCLIENT_ID = 'your_client_id'\nCLIENT_SECRET = 'your_client_secret'\nREDIRECT_URI = 'http://localhost:8080/callback'\nAUTHORIZATION_BASE_URL = 'https://provider.com/oauth/authorize'\nTOKEN_URL = 'https://provider.com/oauth/token'\n\ndef authenticate_oauth2():\n    # Create OAuth2 session\n    oauth = OAuth2Session(CLIENT_ID, redirect_uri=REDIRECT_URI)\n    \n    # Step 1: Get authorization URL\n    authorization_url, state = oauth.authorization_url(AUTHORIZATION_BASE_URL)\n    print(f'Please go to {authorization_url} and authorize access.')\n    \n    # Step 2: Get authorization response (callback URL)\n    authorization_response = input('Paste the full redirect URL here: ')\n    \n    # Step 3: Fetch access token\n    token = oauth.fetch_token(\n        TOKEN_URL,\n        authorization_response=authorization_response,\n        client_secret=CLIENT_SECRET\n    )\n    \n    return oauth, token\n\ndef make_authenticated_request(oauth_session, api_url):\n    \"\"\"Make an authenticated API request\"\"\"\n    response = oauth_session.get(api_url)\n    return response.json() if response.status_code == 200 else None\n\n# Usage example\nif __name__ == \"__main__\":\n    # Authenticate user\n    oauth_session, access_token = authenticate_oauth2()\n    \n    # Make authenticated request\n    api_endpoint = 'https://api.provider.com/user'\n    user_data = make_authenticated_request(oauth_session, api_endpoint)\n    \n    if user_data:\n        print(\"User authenticated successfully!\")\n        print(f\"User data: {user_data}\")\n    else:\n        print(\"Authentication failed or API request failed\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["requests_oauthlib"], "packages_detected": ["requests-oauthlib", "oauthlib", "requests_oauthlib", "requests"], "package_breakdown": {"install_packages": ["requests-oauthlib", "oauthlib", "requests_oauthlib", "requests"], "import_packages": ["requests_oauthlib", "oauthlib", "requests"], "llm_suggested": ["requests-oauthlib", "oauthlib", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a progress meter with customizable options such as bar style, speed, and completion message for tracking the progress of a data processing task.", "prompt_index": 73, "generated_code": "```python\nimport time\nimport sys\nfrom typing import Optional, Callable\n\nclass ProgressMeter:\n    def __init__(self, \n                 total: int,\n                 bar_length: int = 50,\n                 bar_style: str = 'standard',\n                 prefix: str = 'Progress',\n                 suffix: str = 'Complete',\n                 fill_char: str = '\u2588',\n                 empty_char: str = '-',\n                 completion_message: str = 'Task completed!',\n                 show_percentage: bool = True,\n                 show_count: bool = True,\n                 show_rate: bool = True):\n        \n        self.total = total\n        self.current = 0\n        self.bar_length = bar_length\n        self.prefix = prefix\n        self.suffix = suffix\n        self.completion_message = completion_message\n        self.show_percentage = show_percentage\n        self.show_count = show_count\n        self.show_rate = show_rate\n        self.start_time = time.time()\n        \n        # Bar style configurations\n        self.styles = {\n            'standard': {'fill': '\u2588', 'empty': '-'},\n            'blocks': {'fill': '\u2593', 'empty': '\u2591'},\n            'arrows': {'fill': '>', 'empty': '.'},\n            'dots': {'fill': '\u25cf', 'empty': '\u25cb'},\n            'hash': {'fill': '#', 'empty': '_'},\n            'custom': {'fill': fill_char, 'empty': empty_char}\n        }\n        \n        if bar_style in self.styles:\n            self.fill_char = self.styles[bar_style]['fill']\n            self.empty_char = self.styles[bar_style]['empty']\n        else:\n            self.fill_char = fill_char\n            self.empty_char = empty_char\n    \n    def update(self, amount: int = 1, message: Optional[str] = None):\n        \"\"\"Update progress by specified amount\"\"\"\n        self.current = min(self.current + amount, self.total)\n        self._display_progress(message)\n        \n        if self.current >= self.total:\n            self._display_completion()\n    \n    def set_progress(self, value: int, message: Optional[str] = None):\n        \"\"\"Set progress to specific value\"\"\"\n        self.current = min(max(value, 0), self.total)\n        self._display_progress(message)\n        \n        if self.current >= self.total:\n            self._display_completion()\n    \n    def _display_progress(self, message: Optional[str] = None):\n        \"\"\"Display the progress bar\"\"\"\n        if self.total == 0:\n            return\n        \n        # Calculate progress\n        progress = self.current / self.total\n        filled_length = int(self.bar_length * progress)\n        \n        # Create progress bar\n        bar = (self.fill_char * filled_length + \n               self.empty_char * (self.bar_length - filled_length))\n        \n        # Build progress string\n        progress_str = f'\\r{self.prefix}: [{bar}]'\n        \n        if self.show_percentage:\n            progress_str += f' {progress * 100:.1f}%'\n        \n        if self.show_count:\n            progress_str += f' ({self.current}/{self.total})'\n        \n        if self.show_rate and self.current > 0:\n            elapsed_time = time.time() - self.start_time\n            rate = self.current / elapsed_time\n            progress_str += f' | {rate:.1f} items/sec'\n            \n            # Estimate time remaining\n            if self.current < self.total:\n                remaining_items = self.total - self.current\n                eta = remaining_items / rate\n                progress_str += f' | ETA: {self._format_time(eta)}'\n        \n        if message:\n            progress_str += f' | {message}'\n        \n        # Write progress (without newline)\n        sys.stdout.write(progress_str)\n        sys.stdout.flush()\n    \n    def _display_completion(self):\n        \"\"\"Display completion message\"\"\"\n        elapsed_time = time.time() - self.start_time\n        print(f'\\n{self.completion_message}')\n        print(f'Total time: {self._format_time(elapsed_time)}')\n        if self.total > 0:\n            avg_rate = self.total / elapsed_time\n            print(f'Average rate: {avg_rate:.1f} items/sec')\n    \n    def _format_time(self, seconds: float) -> str:\n        \"\"\"Format time in human readable format\"\"\"\n        if seconds < 60:\n            return f'{seconds:.1f}s'\n        elif seconds < 3600:\n            minutes, secs = divmod(seconds, 60)\n            return f'{int(minutes)}m {int(secs)}s'\n        else:\n            hours, remainder = divmod(seconds, 3600)\n            minutes, secs = divmod(remainder, 60)\n            return f'{int(hours)}h {int(minutes)}m {int(secs)}s'\n\n# Advanced progress meter with callback support\nclass AdvancedProgressMeter(ProgressMeter):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.callbacks = []\n    \n    def add_callback(self, callback: Callable[[int, int], None]):\n        \"\"\"Add callback function called on each update\"\"\"\n        self.callbacks.append(callback)\n    \n    def update(self, amount: int = 1, message: Optional[str] = None):\n        \"\"\"Update progress and trigger callbacks\"\"\"\n        super().update(amount, message)\n        \n        # Trigger callbacks\n        for callback in self.callbacks:\n            callback(self.current, self.total)\n\n# Example usage and demo functions\ndef simulate_data_processing():\n    \"\"\"Simulate a data processing task with progress tracking\"\"\"\n    total_items = 100\n    \n    # Create progress meter with custom styling\n    progress = ProgressMeter(\n        total=total_items,\n        bar_style='blocks',\n        prefix='Processing Data',\n        completion_message='Data processing completed successfully!',\n        bar_length=40\n    )\n    \n    # Simulate processing\n    for i in range(total_items):\n        # Simulate variable processing time\n        time.sleep(0.05 + (i % 10) * 0.01)\n        \n        # Update progress with custom message\n        if i % 20 == 0:\n            progress.update(1, f\"Processing batch {i//20 + 1}\")\n        else:\n            progress.update(1)\n\ndef simulate_file_download():\n    \"\"\"Simulate file download with different progress styles\"\"\"\n    file_size = 1000\n    \n    progress = ProgressMeter(\n        total=file_size,\n        bar_style='arrows',\n        prefix='Downloading',\n        suffix='Downloaded',\n        completion_message='Download completed!',\n        show_rate=True\n    )\n    \n    downloaded = 0\n    while downloaded < file_size:\n        # Simulate variable download speed\n        chunk_size = min(50 + (downloaded % 100), file_size - downloaded)\n        time.sleep(0.1)\n        downloaded += chunk_size\n        progress.set_progress(downloaded, f\"{downloaded/1000:.1f}KB\")\n\ndef callback_example():\n    \"\"\"Example with callbacks\"\"\"\n    def milestone_callback(current, total):\n        if current % 25 == 0 and current > 0:\n            print(f\"\\n\ud83c\udf89 Milestone reached: {current} items processed!\")\n    \n    progress = AdvancedProgressMeter(\n        total=100,\n        bar_style='standard',\n        completion_message='Task with callbacks completed!'\n    )\n    \n    progress.add_callback(milestone_callback)\n    \n    for i in range(100):\n        time.sleep(0.03)\n        progress.update(1)\n\n# Demo function to showcase different styles\ndef demo_all_styles():\n    \"\"\"Demonstrate different progress bar styles\"\"\"\n    styles = ['standard', 'blocks', 'arrows', 'dots', 'hash']\n    \n    for style in styles:\n        print(f\"\\nDemonstrating '{style}' style:\")", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "sys", "typing"], "package_breakdown": {"install_packages": ["time", "sys", "typing"], "import_packages": ["time", "typing", "sys"], "llm_suggested": ["time", "sys", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a list of asynchronous callbacks using the aiosignal package and registers them for execution.", "prompt_index": 74, "generated_code": "```python\nimport asyncio\nfrom aiosignal import Signal\n\n# Create a signal\nsignal = Signal()\n\n# Define async callback functions\nasync def callback1(sender, **kwargs):\n    print(f\"Callback 1 executed with sender: {sender}, kwargs: {kwargs}\")\n    await asyncio.sleep(0.1)\n\nasync def callback2(sender, **kwargs):\n    print(f\"Callback 2 executed with sender: {sender}, kwargs: {kwargs}\")\n    await asyncio.sleep(0.1)\n\nasync def callback3(sender, **kwargs):\n    print(f\"Callback 3 executed with sender: {sender}, kwargs: {kwargs}\")\n    await asyncio.sleep(0.1)\n\n# Create list of callbacks\ncallbacks = [callback1, callback2, callback3]\n\n# Register callbacks with the signal\nfor callback in callbacks:\n    signal.connect(callback)\n\n# Function to emit the signal\nasync def emit_signal():\n    await signal.send(\"sender_object\", message=\"Hello from signal!\")\n\n# Main execution\nasync def main():\n    print(\"Emitting signal to registered callbacks...\")\n    await emit_signal()\n    print(\"All callbacks executed!\")\n\n# Run the async code\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "aiosignal"], "package_breakdown": {"install_packages": ["asyncio", "aiosignal"], "import_packages": ["asyncio", "aiosignal"], "llm_suggested": ["aiosignal"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a custom class using the package to define a mutable sequence with the ability to add, remove, and modify elements, ensuring it adheres to the collections.abc.MutableSequence interface.", "prompt_index": 75, "generated_code": "```python\nfrom collections.abc import MutableSequence\n\nclass CustomMutableSequence(MutableSequence):\n    def __init__(self, data=None):\n        self._data = list(data) if data is not None else []\n    \n    def __getitem__(self, index):\n        return self._data[index]\n    \n    def __setitem__(self, index, value):\n        self._data[index] = value\n    \n    def __delitem__(self, index):\n        del self._data[index]\n    \n    def __len__(self):\n        return len(self._data)\n    \n    def insert(self, index, value):\n        self._data.insert(index, value)\n    \n    def __repr__(self):\n        return f\"CustomMutableSequence({self._data})\"\n\n# Example usage\nseq = CustomMutableSequence([1, 2, 3])\nseq.append(4)  # Inherited from MutableSequence\nseq[1] = 10    # Modify element\ndel seq[0]     # Remove element\nprint(seq)     # CustomMutableSequence([10, 3, 4])\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections"], "package_breakdown": {"install_packages": ["collections"], "import_packages": ["collections"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that monitors and logs the CPU usage, memory usage, and network activity of the current system, compatible with Python 2.7 syntax.", "prompt_index": 76, "generated_code": "```python\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport psutil\nimport time\nimport logging\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(\n    filename='system_monitor.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\ndef get_cpu_usage():\n    \"\"\"Get CPU usage percentage\"\"\"\n    return psutil.cpu_percent(interval=1)\n\ndef get_memory_usage():\n    \"\"\"Get memory usage information\"\"\"\n    memory = psutil.virtual_memory()\n    return {\n        'total': memory.total,\n        'available': memory.available,\n        'percent': memory.percent,\n        'used': memory.used\n    }\n\ndef get_network_activity():\n    \"\"\"Get network I/O statistics\"\"\"\n    net_io = psutil.net_io_counters()\n    return {\n        'bytes_sent': net_io.bytes_sent,\n        'bytes_recv': net_io.bytes_recv,\n        'packets_sent': net_io.packets_sent,\n        'packets_recv': net_io.packets_recv\n    }\n\ndef bytes_to_mb(bytes_value):\n    \"\"\"Convert bytes to megabytes\"\"\"\n    return bytes_value / (1024.0 * 1024.0)\n\ndef monitor_system():\n    \"\"\"Main monitoring function\"\"\"\n    print(\"System monitoring started. Press Ctrl+C to stop.\")\n    \n    # Store previous network values for calculating rates\n    prev_net = get_network_activity()\n    prev_time = time.time()\n    \n    try:\n        while True:\n            # Get current system stats\n            cpu_percent = get_cpu_usage()\n            memory_info = get_memory_usage()\n            current_net = get_network_activity()\n            current_time = time.time()\n            \n            # Calculate network rates\n            time_delta = current_time - prev_time\n            if time_delta > 0:\n                net_sent_rate = (current_net['bytes_sent'] - prev_net['bytes_sent']) / time_delta\n                net_recv_rate = (current_net['bytes_recv'] - prev_net['bytes_recv']) / time_delta\n            else:\n                net_sent_rate = 0\n                net_recv_rate = 0\n            \n            # Create log message\n            log_message = (\n                \"CPU: {:.1f}% | \"\n                \"Memory: {:.1f}% ({:.1f}MB used / {:.1f}MB total) | \"\n                \"Network: \u2191{:.2f}MB/s \u2193{:.2f}MB/s | \"\n                \"Total: \u2191{:.1f}MB \u2193{:.1f}MB\"\n            ).format(\n                cpu_percent,\n                memory_info['percent'],\n                bytes_to_mb(memory_info['used']),\n                bytes_to_mb(memory_info['total']),\n                bytes_to_mb(net_sent_rate),\n                bytes_to_mb(net_recv_rate),\n                bytes_to_mb(current_net['bytes_sent']),\n                bytes_to_mb(current_net['bytes_recv'])\n            )\n            \n            # Log to file and print to console\n            logging.info(log_message)\n            print(\"{} - {}\".format(datetime.now().strftime('%H:%M:%S'), log_message))\n            \n            # Update previous values\n            prev_net = current_net\n            prev_time = current_time\n            \n            # Wait before next measurement\n            time.sleep(5)\n            \n    except KeyboardInterrupt:\n        print(\"\\nMonitoring stopped by user.\")\n        logging.info(\"System monitoring stopped by user\")\n\nif __name__ == \"__main__\":\n    monitor_system()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["psutil", "datetime", "time", "logging"], "package_breakdown": {"install_packages": ["psutil", "datetime", "time", "logging"], "import_packages": ["psutil", "datetime", "time", "logging"], "llm_suggested": ["psutil"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads a TOML file, preserves the original styling (indentation, spacing, and comments), and writes the data back to a new or existing TOML file while maintaining the same style.", "prompt_index": 77, "generated_code": "```python\nimport tomlkit\n\ndef preserve_toml_style(input_file, output_file):\n    \"\"\"\n    Read a TOML file and write it back while preserving original styling.\n    \"\"\"\n    # Read the TOML file with preserved formatting\n    with open(input_file, 'r', encoding='utf-8') as f:\n        data = tomlkit.load(f)\n    \n    # Write back to output file with preserved style\n    with open(output_file, 'w', encoding='utf-8') as f:\n        tomlkit.dump(data, f)\n\n# Example usage\npreserve_toml_style('input.toml', 'output.toml')\n\n# Alternative: Read, modify data, and write back with preserved style\ndef modify_and_preserve_toml(input_file, output_file):\n    with open(input_file, 'r', encoding='utf-8') as f:\n        data = tomlkit.load(f)\n    \n    # Example modification - add or update a value\n    # data['new_key'] = 'new_value'\n    # data['existing_section']['modified_key'] = 'modified_value'\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        tomlkit.dump(data, f)\n\n# Example with error handling\ndef safe_preserve_toml_style(input_file, output_file):\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            data = tomlkit.load(f)\n        \n        with open(output_file, 'w', encoding='utf-8') as f:\n            tomlkit.dump(data, f)\n            \n        print(f\"Successfully copied {input_file} to {output_file} with preserved styling\")\n        \n    except FileNotFoundError:\n        print(f\"Error: {input_file} not found\")\n    except tomlkit.exceptions.TOMLKitError as e:\n        print(f\"TOML parsing error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tomlkit"], "package_breakdown": {"install_packages": ["tomlkit"], "import_packages": ["tomlkit"], "llm_suggested": ["tomlkit"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to a MySQL database using the Database Abstraction Library and retrieves all records from a specified table, then prints the results to the console.", "prompt_index": 78, "generated_code": "```python\nimport mysql.connector\nfrom mysql.connector import Error\n\ndef connect_and_retrieve_data(host, database, username, password, table_name):\n    try:\n        # Establish connection to MySQL database\n        connection = mysql.connector.connect(\n            host=host,\n            database=database,\n            user=username,\n            password=password\n        )\n        \n        if connection.is_connected():\n            cursor = connection.cursor()\n            \n            # Execute query to retrieve all records from specified table\n            query = f\"SELECT * FROM {table_name}\"\n            cursor.execute(query)\n            \n            # Fetch all records\n            records = cursor.fetchall()\n            \n            # Get column names\n            column_names = [desc[0] for desc in cursor.description]\n            \n            # Print column headers\n            print(\" | \".join(column_names))\n            print(\"-\" * (len(\" | \".join(column_names))))\n            \n            # Print all records\n            for record in records:\n                print(\" | \".join(str(value) for value in record))\n                \n    except Error as e:\n        print(f\"Error connecting to MySQL: {e}\")\n        \n    finally:\n        # Close database connection\n        if connection.is_connected():\n            cursor.close()\n            connection.close()\n\n# Usage example\nconnect_and_retrieve_data(\n    host='localhost',\n    database='your_database_name',\n    username='your_username',\n    password='your_password',\n    table_name='your_table_name'\n)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["mysql", "mysql-connector-python"], "package_breakdown": {"install_packages": ["mysql", "mysql-connector-python"], "import_packages": ["mysql"], "llm_suggested": ["mysql-connector-python"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads key-value pairs from a .env file and sets them as environment variables to be used within a Python script.", "prompt_index": 79, "generated_code": "```python\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access environment variables\ndatabase_url = os.getenv('DATABASE_URL')\napi_key = os.getenv('API_KEY')\ndebug_mode = os.getenv('DEBUG', 'False').lower() == 'true'\n\n# Example usage\nprint(f\"Database URL: {database_url}\")\nprint(f\"API Key: {api_key}\")\nprint(f\"Debug Mode: {debug_mode}\")\n```\n\nYou'll need to install the python-dotenv package:\n```bash\npip install python-dotenv\n```\n\nExample .env file:\n```\nDATABASE_URL=postgresql://localhost/mydb\nAPI_KEY=your_secret_api_key\nDEBUG=True\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dotenv", "python-dotenv", "os"], "package_breakdown": {"install_packages": ["dotenv", "python-dotenv", "os"], "import_packages": ["dotenv", "os"], "llm_suggested": ["python-dotenv"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that signs an OAuth request using this package's thorough and spec-compliant implementation of the request-signing logic.", "prompt_index": 80, "generated_code": "```python\nimport requests\nfrom requests_oauthlib import OAuth1\n\n# OAuth 1.0a signing\noauth = OAuth1(\n    client_key='your_consumer_key',\n    client_secret='your_consumer_secret',\n    resource_owner_key='your_access_token',\n    resource_owner_secret='your_access_token_secret',\n    signature_method='HMAC-SHA1',\n    signature_type='AUTH_HEADER'\n)\n\n# Make signed request\nresponse = requests.get('https://api.example.com/data', auth=oauth)\n\n# For OAuth 2.0 Bearer token\nfrom requests_oauthlib import OAuth2Session\n\ntoken = {\n    'access_token': 'your_access_token',\n    'token_type': 'Bearer'\n}\n\noauth2 = OAuth2Session(token=token)\nresponse = oauth2.get('https://api.example.com/data')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["requests_oauthlib"], "packages_detected": ["requests-oauthlib", "requests_oauthlib", "requests"], "package_breakdown": {"install_packages": ["requests-oauthlib", "requests_oauthlib", "requests"], "import_packages": ["requests_oauthlib", "requests"], "llm_suggested": ["requests", "requests-oauthlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that demonstrates how to use exception groups as defined in PEP 654 to organize and handle multiple exceptions in a single context.", "prompt_index": 81, "generated_code": "```python\n# Basic ExceptionGroup usage\ndef demonstrate_basic_exception_group():\n    \"\"\"Demonstrates creating and raising an ExceptionGroup\"\"\"\n    try:\n        errors = [\n            ValueError(\"Invalid value\"),\n            TypeError(\"Wrong type\"),\n            KeyError(\"Missing key\")\n        ]\n        raise ExceptionGroup(\"Multiple validation errors\", errors)\n    except* ValueError as eg:\n        print(f\"Caught ValueError group: {eg.exceptions}\")\n    except* TypeError as eg:\n        print(f\"Caught TypeError group: {eg.exceptions}\")\n    except* KeyError as eg:\n        print(f\"Caught KeyError group: {eg.exceptions}\")\n\n# Nested ExceptionGroups\ndef demonstrate_nested_exception_groups():\n    \"\"\"Shows how to work with nested ExceptionGroups\"\"\"\n    inner_group = ExceptionGroup(\"Inner errors\", [\n        ValueError(\"Inner value error\"),\n        RuntimeError(\"Inner runtime error\")\n    ])\n    \n    outer_group = ExceptionGroup(\"Outer errors\", [\n        TypeError(\"Outer type error\"),\n        inner_group\n    ])\n    \n    try:\n        raise outer_group\n    except* ValueError as eg:\n        print(f\"Caught nested ValueError: {eg.exceptions}\")\n    except* RuntimeError as eg:\n        print(f\"Caught nested RuntimeError: {eg.exceptions}\")\n    except* TypeError as eg:\n        print(f\"Caught outer TypeError: {eg.exceptions}\")\n\n# Practical example: Concurrent task processing\nimport asyncio\nfrom typing import List, Any\n\nasync def process_item(item: Any) -> str:\n    \"\"\"Simulates processing an item that might fail\"\"\"\n    if item == \"error1\":\n        raise ValueError(f\"Value error processing {item}\")\n    elif item == \"error2\":\n        raise TypeError(f\"Type error processing {item}\")\n    elif item == \"error3\":\n        raise RuntimeError(f\"Runtime error processing {item}\")\n    return f\"Processed {item}\"\n\nasync def process_batch_with_exception_groups(items: List[Any]):\n    \"\"\"Processes items concurrently and collects all errors\"\"\"\n    tasks = [process_item(item) for item in items]\n    results = []\n    errors = []\n    \n    # Gather results, collecting exceptions\n    for i, task in enumerate(asyncio.as_completed(tasks)):\n        try:\n            result = await task\n            results.append(result)\n        except Exception as e:\n            errors.append(e)\n    \n    # If there were errors, raise them as an ExceptionGroup\n    if errors:\n        raise ExceptionGroup(f\"Errors processing batch of {len(items)} items\", errors)\n    \n    return results\n\nasync def demonstrate_concurrent_processing():\n    \"\"\"Shows exception groups in concurrent processing\"\"\"\n    items = [\"item1\", \"error1\", \"item2\", \"error2\", \"item3\", \"error3\"]\n    \n    try:\n        results = await process_batch_with_exception_groups(items)\n        print(f\"All items processed successfully: {results}\")\n    except* ValueError as eg:\n        print(f\"Value errors occurred: {[str(e) for e in eg.exceptions]}\")\n    except* TypeError as eg:\n        print(f\"Type errors occurred: {[str(e) for e in eg.exceptions]}\")\n    except* RuntimeError as eg:\n        print(f\"Runtime errors occurred: {[str(e) for e in eg.exceptions]}\")\n\n# Exception group manipulation\ndef demonstrate_exception_group_methods():\n    \"\"\"Shows various ExceptionGroup methods and properties\"\"\"\n    errors = [\n        ValueError(\"Value error 1\"),\n        ValueError(\"Value error 2\"),\n        TypeError(\"Type error 1\"),\n        RuntimeError(\"Runtime error 1\")\n    ]\n    \n    eg = ExceptionGroup(\"Mixed errors\", errors)\n    \n    print(f\"Message: {eg.message}\")\n    print(f\"Number of exceptions: {len(eg.exceptions)}\")\n    \n    # Split exception group by type\n    value_errors, rest = eg.split(ValueError)\n    if value_errors:\n        print(f\"Value errors: {[str(e) for e in value_errors.exceptions]}\")\n    if rest:\n        print(f\"Other errors: {[str(e) for e in rest.exceptions]}\")\n    \n    # Subgroup selection\n    type_errors = eg.subgroup(TypeError)\n    if type_errors:\n        print(f\"Type errors subgroup: {[str(e) for e in type_errors.exceptions]}\")\n\n# Custom exception group handling\nclass ValidationError(Exception):\n    pass\n\nclass NetworkError(Exception):\n    pass\n\ndef validate_and_process_data(data_items):\n    \"\"\"Example function that validates and processes data, collecting errors\"\"\"\n    validation_errors = []\n    network_errors = []\n    processing_errors = []\n    \n    for item in data_items:\n        try:\n            # Simulate validation\n            if item.get(\"invalid\"):\n                raise ValidationError(f\"Invalid data: {item}\")\n            \n            # Simulate network operation\n            if item.get(\"network_fail\"):\n                raise NetworkError(f\"Network error for: {item}\")\n            \n            # Simulate processing\n            if item.get(\"process_fail\"):\n                raise RuntimeError(f\"Processing error for: {item}\")\n                \n        except ValidationError as e:\n            validation_errors.append(e)\n        except NetworkError as e:\n            network_errors.append(e)\n        except RuntimeError as e:\n            processing_errors.append(e)\n    \n    # Collect all errors into groups\n    all_errors = []\n    if validation_errors:\n        all_errors.append(ExceptionGroup(\"Validation errors\", validation_errors))\n    if network_errors:\n        all_errors.append(ExceptionGroup(\"Network errors\", network_errors))\n    if processing_errors:\n        all_errors.append(ExceptionGroup(\"Processing errors\", processing_errors))\n    \n    if all_errors:\n        raise ExceptionGroup(\"Data processing failed\", all_errors)\n\ndef demonstrate_custom_exception_handling():\n    \"\"\"Shows handling of custom exception groups\"\"\"\n    test_data = [\n        {\"id\": 1, \"valid\": True},\n        {\"id\": 2, \"invalid\": True},\n        {\"id\": 3, \"network_fail\": True},\n        {\"id\": 4, \"process_fail\": True},\n        {\"id\": 5, \"invalid\": True, \"network_fail\": True}\n    ]\n    \n    try:\n        validate_and_process_data(test_data)\n    except* ValidationError as eg:\n        print(f\"Validation failures: {len(eg.exceptions)} items\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* NetworkError as eg:\n        print(f\"Network failures: {len(eg.exceptions)} items\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n    except* RuntimeError as eg:\n        print(f\"Processing failures: {len(eg.exceptions)} items\")\n        for exc in eg.exceptions:\n            print(f\"  - {exc}\")\n\n# Running the demonstrations\nif __name__ == \"__main__\":\n    print(\"=== Basic ExceptionGroup ===\")\n    demonstrate_basic_exception_group()\n    \n    print(\"\\n=== Nested ExceptionGroups ===\")\n    demonstrate_nested_exception_groups()\n    \n    print(\"\\n=== ExceptionGroup Methods ===\")\n    demonstrate_exception_group_methods()\n    \n    print(\"\\n=== Custom Exception Handling ===\")\n    demonstrate_custom_exception_handling()\n    \n    print(\"\\n=== Concurrent Processing ===\")\n    asyncio.run(demonstrate_concurrent_processing())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "typing"], "package_breakdown": {"install_packages": ["asyncio", "typing"], "import_packages": ["asyncio", "typing"], "llm_suggested": ["asyncio", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates and manipulates persistent data structures in Python using Rust's rpds library bindings.", "prompt_index": 82, "generated_code": "```python\n# Install rpds first: pip install rpds-py\n\nfrom rpds import HashTrieMap, HashTrieSet, List, Vector\n\n# Creating persistent data structures\ndef create_persistent_structures():\n    # Persistent HashMap\n    map1 = HashTrieMap({\"a\": 1, \"b\": 2, \"c\": 3})\n    \n    # Persistent Set\n    set1 = HashTrieSet([1, 2, 3, 4, 5])\n    \n    # Persistent List (linked list)\n    list1 = List([1, 2, 3, 4, 5])\n    \n    # Persistent Vector (array-like)\n    vec1 = Vector([1, 2, 3, 4, 5])\n    \n    return map1, set1, list1, vec1\n\n# HashMap operations\ndef hashmap_operations():\n    original = HashTrieMap({\"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\"})\n    \n    # Insert - returns new map, original unchanged\n    updated = original.insert(\"country\", \"USA\")\n    \n    # Remove - returns new map\n    without_age = original.remove(\"age\")\n    \n    # Update existing key\n    older = original.insert(\"age\", 31)\n    \n    # Bulk operations\n    bulk_update = original.update({\"age\": 35, \"job\": \"Engineer\"})\n    \n    print(f\"Original: {original}\")\n    print(f\"Updated: {updated}\")\n    print(f\"Without age: {without_age}\")\n    print(f\"Older: {older}\")\n    print(f\"Bulk update: {bulk_update}\")\n    \n    # Access operations\n    print(f\"Get age: {original.get('age')}\")\n    print(f\"Get with default: {original.get('salary', 0)}\")\n    print(f\"Contains key: {'name' in original}\")\n    \n    return original, updated\n\n# HashSet operations\ndef hashset_operations():\n    set1 = HashTrieSet([1, 2, 3, 4, 5])\n    \n    # Add element - returns new set\n    set2 = set1.add(6)\n    \n    # Remove element\n    set3 = set1.discard(3)\n    \n    # Set operations\n    other_set = HashTrieSet([4, 5, 6, 7, 8])\n    union_set = set1.union(other_set)\n    intersection_set = set1.intersection(other_set)\n    difference_set = set1.difference(other_set)\n    \n    print(f\"Original set: {set1}\")\n    print(f\"Added 6: {set2}\")\n    print(f\"Removed 3: {set3}\")\n    print(f\"Union: {union_set}\")\n    print(f\"Intersection: {intersection_set}\")\n    print(f\"Difference: {difference_set}\")\n    \n    # Membership testing\n    print(f\"Contains 3: {3 in set1}\")\n    print(f\"Length: {len(set1)}\")\n    \n    return set1, set2\n\n# List operations (persistent linked list)\ndef list_operations():\n    original_list = List([1, 2, 3, 4, 5])\n    \n    # Prepend (O(1) operation)\n    prepended = original_list.cons(0)\n    \n    # Get head and tail\n    head = original_list.first\n    tail = original_list.rest\n    \n    # Convert to Python list for iteration\n    python_list = list(original_list)\n    \n    print(f\"Original: {original_list}\")\n    print(f\"Prepended 0: {prepended}\")\n    print(f\"Head: {head}\")\n    print(f\"Tail: {tail}\")\n    print(f\"As Python list: {python_list}\")\n    \n    # Functional operations\n    doubled = List([x * 2 for x in original_list])\n    filtered = List([x for x in original_list if x % 2 == 0])\n    \n    print(f\"Doubled: {doubled}\")\n    print(f\"Even numbers: {filtered}\")\n    \n    return original_list, prepended\n\n# Vector operations (persistent array)\ndef vector_operations():\n    vec = Vector([10, 20, 30, 40, 50])\n    \n    # Append element\n    appended = vec.append(60)\n    \n    # Set element at index\n    updated = vec.set(2, 35)\n    \n    # Access by index\n    element = vec[2]\n    \n    # Slice operations\n    slice_vec = vec[1:4]\n    \n    print(f\"Original vector: {vec}\")\n    print(f\"Appended 60: {appended}\")\n    print(f\"Updated index 2: {updated}\")\n    print(f\"Element at index 2: {element}\")\n    print(f\"Slice [1:4]: {slice_vec}\")\n    \n    # Bulk operations\n    extended = vec.extend([60, 70, 80])\n    print(f\"Extended: {extended}\")\n    \n    return vec, appended\n\n# Demonstrating immutability and structural sharing\ndef demonstrate_immutability():\n    # Original map\n    original = HashTrieMap({\"x\": 1, \"y\": 2, \"z\": 3})\n    \n    # Create multiple versions\n    version1 = original.insert(\"w\", 4)\n    version2 = original.insert(\"x\", 10)\n    version3 = version1.insert(\"v\", 5)\n    \n    print(\"Demonstrating immutability:\")\n    print(f\"Original: {original}\")\n    print(f\"Version 1: {version1}\")\n    print(f\"Version 2: {version2}\")\n    print(f\"Version 3: {version3}\")\n    \n    # All versions coexist without affecting each other\n    return original, version1, version2, version3\n\n# Complex data structure example\ndef complex_example():\n    # Nested persistent structures\n    users = HashTrieMap({\n        \"alice\": HashTrieMap({\n            \"age\": 30,\n            \"skills\": HashTrieSet([\"python\", \"rust\", \"go\"]),\n            \"projects\": List([\"web-app\", \"cli-tool\", \"library\"])\n        }),\n        \"bob\": HashTrieMap({\n            \"age\": 25,\n            \"skills\": HashTrieSet([\"javascript\", \"python\"]),\n            \"projects\": List([\"frontend\", \"api\"])\n        })\n    })\n    \n    # Update nested structure\n    updated_users = users.transform(\n        [\"alice\", \"skills\"], \n        lambda skills: skills.add(\"kubernetes\")\n    )\n    \n    print(\"Original users:\")\n    for name, data in users.items():\n        print(f\"  {name}: {dict(data)}\")\n    \n    print(\"\\nUpdated users:\")\n    for name, data in updated_users.items():\n        print(f\"  {name}: {dict(data)}\")\n    \n    return users, updated_users\n\n# Performance comparison helper\ndef performance_example():\n    import time\n    \n    # Large dataset\n    data = {f\"key_{i}\": i for i in range(10000)}\n    \n    # Python dict operations\n    start = time.time()\n    py_dict = data.copy()\n    for i in range(100):\n        py_dict = {**py_dict, f\"new_key_{i}\": i}\n    py_time = time.time() - start\n    \n    # Persistent map operations\n    start = time.time()\n    persistent_map = HashTrieMap(data)\n    for i in range(100):\n        persistent_map = persistent_map.insert(f\"new_key_{i}\", i)\n    persistent_time = time.time() - start\n    \n    print(f\"Python dict time: {py_time:.4f}s\")\n    print(f\"Persistent map time: {persistent_time:.4f}s\")\n    \n    return py_time, persistent_time\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"=== Creating Persistent Structures ===\")\n    map1, set1, list1, vec1 = create_persistent_structures()\n    \n    print(\"\\n=== HashMap Operations ===\")\n    hashmap_operations()", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "rpds-py", "rpds"], "package_breakdown": {"install_packages": ["time", "rpds-py", "rpds"], "import_packages": ["time", "rpds"], "llm_suggested": ["rpds-py"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the utility library to match file paths against gitignore style patterns to determine if a file should be ignored or not.", "prompt_index": 83, "generated_code": "```python\nimport fnmatch\nimport os\nfrom pathlib import Path\n\nclass GitignorePattern:\n    def __init__(self, gitignore_file='.gitignore'):\n        self.patterns = []\n        self.load_patterns(gitignore_file)\n    \n    def load_patterns(self, gitignore_file):\n        \"\"\"Load patterns from .gitignore file\"\"\"\n        if os.path.exists(gitignore_file):\n            with open(gitignore_file, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith('#'):\n                        self.patterns.append(line)\n    \n    def should_ignore(self, file_path):\n        \"\"\"Check if a file should be ignored based on gitignore patterns\"\"\"\n        path = Path(file_path)\n        path_str = str(path)\n        path_parts = path.parts\n        \n        for pattern in self.patterns:\n            # Handle negation patterns (starting with !)\n            negate = pattern.startswith('!')\n            if negate:\n                pattern = pattern[1:]\n            \n            # Handle directory patterns (ending with /)\n            is_dir_pattern = pattern.endswith('/')\n            if is_dir_pattern:\n                pattern = pattern[:-1]\n            \n            # Check if pattern matches\n            matched = False\n            \n            # Absolute path pattern (starting with /)\n            if pattern.startswith('/'):\n                pattern = pattern[1:]\n                if fnmatch.fnmatch(path_str, pattern) or fnmatch.fnmatch(str(path.name), pattern):\n                    matched = True\n            else:\n                # Relative pattern - check against full path and individual parts\n                if fnmatch.fnmatch(path_str, pattern) or fnmatch.fnmatch(str(path.name), pattern):\n                    matched = True\n                # Check against any part of the path\n                for part in path_parts:\n                    if fnmatch.fnmatch(part, pattern):\n                        matched = True\n                        break\n                # Check if any parent directory matches\n                for i in range(len(path_parts)):\n                    subpath = '/'.join(path_parts[i:])\n                    if fnmatch.fnmatch(subpath, pattern):\n                        matched = True\n                        break\n            \n            if matched:\n                return not negate  # If negated and matched, don't ignore\n        \n        return False\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create gitignore matcher\n    gitignore = GitignorePattern('.gitignore')\n    \n    # Test files\n    test_files = [\n        'file.txt',\n        'node_modules/package.json',\n        'src/main.py',\n        '.env',\n        'dist/build.js',\n        'README.md'\n    ]\n    \n    for file_path in test_files:\n        if gitignore.should_ignore(file_path):\n            print(f\"IGNORE: {file_path}\")\n        else:\n            print(f\"INCLUDE: {file_path}\")\n```\n\nAlternative using the `pathspec` library (more accurate gitignore implementation):\n\n```python\nimport pathspec\n\ndef match_gitignore_patterns(gitignore_file='.gitignore'):\n    \"\"\"Load gitignore patterns and return a pathspec object\"\"\"\n    with open(gitignore_file, 'r') as f:\n        spec = pathspec.PathSpec.from_lines('gitwildmatch', f)\n    return spec\n\ndef should_ignore_file(file_path, spec):\n    \"\"\"Check if file should be ignored\"\"\"\n    return spec.match_file(file_path)\n\n# Usage\nif __name__ == \"__main__\":\n    # Load gitignore patterns\n    spec = match_gitignore_patterns('.gitignore')\n    \n    # Test files\n    test_files = [\n        'file.txt',\n        'node_modules/package.json', \n        'src/main.py',\n        '.env',\n        'dist/build.js'\n    ]\n    \n    for file_path in test_files:\n        if should_ignore_file(file_path, spec):\n            print(f\"IGNORE: {file_path}\")\n        else:\n            print(f\"INCLUDE: {file_path}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "pathspec", "fnmatch", "os"], "package_breakdown": {"install_packages": ["pathlib", "pathspec", "fnmatch", "os"], "import_packages": ["pathlib", "pathspec", "os", "fnmatch"], "llm_suggested": ["fnmatch", "os", "pathlib", "pathspec"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates multiple concurrent tasks within the same process to efficiently execute parallel computations.", "prompt_index": 84, "generated_code": "```python\nimport asyncio\nimport concurrent.futures\nimport threading\nimport time\nfrom multiprocessing import Pool\nimport math\n\n# Method 1: Using asyncio for I/O-bound concurrent tasks\nasync def async_task(task_id, delay):\n    print(f\"Task {task_id} started\")\n    await asyncio.sleep(delay)\n    result = task_id * 2\n    print(f\"Task {task_id} completed with result: {result}\")\n    return result\n\nasync def run_async_tasks():\n    tasks = [async_task(i, 1) for i in range(5)]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Method 2: Using ThreadPoolExecutor for I/O-bound tasks\ndef cpu_bound_task(n):\n    \"\"\"Simulate CPU-intensive work\"\"\"\n    result = sum(i * i for i in range(n))\n    return result\n\ndef run_thread_pool():\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        tasks = [100000, 200000, 300000, 400000, 500000]\n        futures = [executor.submit(cpu_bound_task, task) for task in tasks]\n        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n    return results\n\n# Method 3: Using ProcessPoolExecutor for CPU-bound tasks\ndef heavy_computation(n):\n    \"\"\"CPU-intensive mathematical computation\"\"\"\n    return sum(math.sqrt(i) for i in range(n))\n\ndef run_process_pool():\n    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n        tasks = [1000000, 2000000, 3000000, 4000000]\n        futures = [executor.submit(heavy_computation, task) for task in tasks]\n        results = [future.result() for future in futures]\n    return results\n\n# Method 4: Manual threading approach\nclass WorkerThread(threading.Thread):\n    def __init__(self, task_id, work_function, *args):\n        super().__init__()\n        self.task_id = task_id\n        self.work_function = work_function\n        self.args = args\n        self.result = None\n\n    def run(self):\n        self.result = self.work_function(*self.args)\n        print(f\"Thread {self.task_id} completed\")\n\ndef run_manual_threads():\n    threads = []\n    for i in range(4):\n        thread = WorkerThread(i, cpu_bound_task, 100000 * (i + 1))\n        threads.append(thread)\n        thread.start()\n    \n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n    \n    results = [thread.result for thread in threads]\n    return results\n\n# Method 5: Combined approach with asyncio and ThreadPoolExecutor\nasync def hybrid_concurrent_execution():\n    loop = asyncio.get_event_loop()\n    \n    # Run CPU-bound tasks in thread pool\n    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n        cpu_tasks = [\n            loop.run_in_executor(executor, heavy_computation, 500000),\n            loop.run_in_executor(executor, heavy_computation, 600000),\n            loop.run_in_executor(executor, heavy_computation, 700000)\n        ]\n        \n        # Run I/O-bound async tasks concurrently\n        io_tasks = [async_task(i, 0.5) for i in range(3)]\n        \n        # Execute both types concurrently\n        cpu_results, io_results = await asyncio.gather(\n            asyncio.gather(*cpu_tasks),\n            asyncio.gather(*io_tasks)\n        )\n    \n    return cpu_results, io_results\n\n# Example usage and benchmarking\ndef benchmark_execution():\n    print(\"=== Asyncio Tasks ===\")\n    start = time.time()\n    results = asyncio.run(run_async_tasks())\n    print(f\"Asyncio execution time: {time.time() - start:.2f}s\")\n    print(f\"Results: {results}\\n\")\n    \n    print(\"=== Thread Pool Executor ===\")\n    start = time.time()\n    results = run_thread_pool()\n    print(f\"Thread pool execution time: {time.time() - start:.2f}s\")\n    print(f\"Results count: {len(results)}\\n\")\n    \n    print(\"=== Manual Threading ===\")\n    start = time.time()\n    results = run_manual_threads()\n    print(f\"Manual threading execution time: {time.time() - start:.2f}s\")\n    print(f\"Results: {results}\\n\")\n    \n    print(\"=== Hybrid Execution ===\")\n    start = time.time()\n    cpu_results, io_results = asyncio.run(hybrid_concurrent_execution())\n    print(f\"Hybrid execution time: {time.time() - start:.2f}s\")\n    print(f\"CPU results count: {len(cpu_results)}\")\n    print(f\"I/O results: {io_results}\")\n\nif __name__ == \"__main__\":\n    benchmark_execution()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 2, "hallucinations": ["concurrent", "concurrent.futures"], "packages_detected": ["concurrent", "math", "asyncio", "concurrent.futures", "threading", "time", "multiprocessing"], "package_breakdown": {"install_packages": ["concurrent", "math", "asyncio", "concurrent.futures", "threading", "time", "multiprocessing"], "import_packages": ["concurrent", "math", "asyncio", "threading", "time", "multiprocessing"], "llm_suggested": ["asyncio", "concurrent.futures", "threading", "time", "multiprocessing", "math"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that defines a parsing grammar to extract key information from a structured text file, using the pyparsing module to create rules and methods for efficient parsing and extraction.", "prompt_index": 85, "generated_code": "```python\nfrom pyparsing import *\nimport re\n\nclass StructuredTextParser:\n    def __init__(self):\n        self.setup_grammar()\n    \n    def setup_grammar(self):\n        # Basic elements\n        self.word = Word(alphas + \"_\", alphanums + \"_\")\n        self.number = Combine(Optional(\"-\") + Word(nums) + Optional(\".\" + Word(nums)))\n        self.string_literal = QuotedString('\"', escChar='\\\\') | QuotedString(\"'\", escChar='\\\\')\n        self.identifier = Word(alphas + \"_\", alphanums + \"_\")\n        \n        # Whitespace handling\n        self.ws = White()\n        \n        # Key-value pairs\n        self.key = self.identifier\n        self.value = (self.number | self.string_literal | self.word)\n        self.key_value = Group(self.key + Suppress(\":\") + self.value)\n        \n        # Lists and arrays\n        self.list_item = self.value | self.key_value\n        self.list_content = Optional(self.list_item + ZeroOrMore(Suppress(\",\") + self.list_item))\n        self.array = Group(Suppress(\"[\") + self.list_content + Suppress(\"]\"))\n        \n        # Sections with headers\n        self.section_header = Group(Suppress(\"[\") + self.identifier + Suppress(\"]\"))\n        self.section_content = ZeroOrMore(self.key_value | self.array)\n        self.section = Group(self.section_header + self.section_content)\n        \n        # Comments\n        self.comment = Suppress(\"#\" + restOfLine)\n        \n        # Main document structure\n        self.document = ZeroOrMore(self.section | self.key_value | self.comment)\n        \n        # Set parse actions for data conversion\n        self.number.setParseAction(self._convert_number)\n        self.key_value.setParseAction(self._create_key_value)\n        self.section.setParseAction(self._create_section)\n    \n    def _convert_number(self, tokens):\n        \"\"\"Convert string numbers to appropriate numeric types\"\"\"\n        try:\n            if '.' in tokens[0]:\n                return float(tokens[0])\n            else:\n                return int(tokens[0])\n        except ValueError:\n            return tokens[0]\n    \n    def _create_key_value(self, tokens):\n        \"\"\"Create key-value dictionary from parsed tokens\"\"\"\n        return {tokens[0]: tokens[1]}\n    \n    def _create_section(self, tokens):\n        \"\"\"Create section dictionary with header and content\"\"\"\n        header = tokens[0][0]\n        content = {}\n        for item in tokens[1:]:\n            if isinstance(item, dict):\n                content.update(item)\n            elif isinstance(item, list):\n                content.setdefault('arrays', []).append(item)\n        return {header: content}\n    \n    def parse_file(self, filename):\n        \"\"\"Parse a structured text file and return extracted data\"\"\"\n        try:\n            with open(filename, 'r', encoding='utf-8') as file:\n                content = file.read()\n            return self.parse_text(content)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File {filename} not found\")\n        except Exception as e:\n            raise Exception(f\"Error parsing file: {str(e)}\")\n    \n    def parse_text(self, text):\n        \"\"\"Parse structured text and return extracted data\"\"\"\n        try:\n            result = self.document.parseString(text, parseAll=True)\n            return self._consolidate_results(result)\n        except ParseException as e:\n            raise ParseException(f\"Parse error at line {e.lineno}, col {e.col}: {e.msg}\")\n    \n    def _consolidate_results(self, parsed_data):\n        \"\"\"Consolidate parsed results into a structured dictionary\"\"\"\n        consolidated = {}\n        \n        for item in parsed_data:\n            if isinstance(item, dict):\n                consolidated.update(item)\n        \n        return consolidated\n\n# Extended parser for specific data formats\nclass ConfigFileParser(StructuredTextParser):\n    def __init__(self):\n        super().__init__()\n        self.setup_config_grammar()\n    \n    def setup_config_grammar(self):\n        # Configuration-specific patterns\n        self.boolean = (CaselessKeyword(\"true\") | CaselessKeyword(\"false\")).setParseAction(self._convert_boolean)\n        self.null_value = CaselessKeyword(\"null\").setParseAction(lambda: None)\n        \n        # Enhanced value types\n        self.value = (self.boolean | self.null_value | self.number | self.string_literal | self.word)\n        self.key_value = Group(self.key + Suppress(\"=\") + self.value)\n        \n        # Environment variables\n        self.env_var = Combine(\"${\" + self.identifier + \"}\")\n        self.value = self.env_var | self.value\n        \n        # Multi-line values\n        self.multiline_value = QuotedString('\"\"\"', multiline=True)\n        self.value = self.multiline_value | self.value\n        \n        # Update main grammar\n        self.document = ZeroOrMore(self.section | self.key_value | self.comment)\n    \n    def _convert_boolean(self, tokens):\n        \"\"\"Convert boolean strings to Python boolean values\"\"\"\n        return tokens[0].lower() == 'true'\n\n# Log file parser for structured logs\nclass LogFileParser(StructuredTextParser):\n    def __init__(self):\n        super().__init__()\n        self.setup_log_grammar()\n    \n    def setup_log_grammar(self):\n        # Timestamp patterns\n        self.timestamp = Combine(Word(nums, exact=4) + \"-\" + Word(nums, exact=2) + \"-\" + \n                                Word(nums, exact=2) + \"T\" + Word(nums, exact=2) + \":\" + \n                                Word(nums, exact=2) + \":\" + Word(nums, exact=2))\n        \n        # Log levels\n        self.log_level = oneOf(\"DEBUG INFO WARN ERROR FATAL\")\n        \n        # Log message structure\n        self.log_message = restOfLine\n        self.log_entry = Group(self.timestamp + self.log_level + self.log_message)\n        \n        # Main log document\n        self.document = ZeroOrMore(self.log_entry)\n        \n        # Set parse actions\n        self.log_entry.setParseAction(self._create_log_entry)\n    \n    def _create_log_entry(self, tokens):\n        \"\"\"Create structured log entry dictionary\"\"\"\n        return {\n            'timestamp': tokens[0],\n            'level': tokens[1],\n            'message': tokens[2].strip()\n        }\n\n# Usage example and testing\ndef create_sample_files():\n    \"\"\"Create sample files for testing\"\"\"\n    \n    # Sample configuration file\n    config_content = \"\"\"\n    # Database configuration\n    [database]\n    host: \"localhost\"\n    port: 5432\n    username: \"admin\"\n    password: \"secret123\"\n    ssl_enabled: true\n    \n    [logging]\n    level: \"INFO\"\n    file_path: \"/var/log/app.log\"\n    max_size: 100\n    \n    # Application settings\n    debug_mode: false\n    timeout: 30.5\n    \"\"\"\n    \n    with open('config.txt', 'w') as f:\n        f.write(config_content)\n    \n    # Sample log file\n    log_content = \"\"\"\n    2023-10-15T10:30:45 INFO Application started successfully\n    2023-10-15T10:31:02 DEBUG Database connection established\n    2023-10-15T10:31:15 WARN Cache miss for key: user_123\n    2023-10-15T10:32:01 ERROR Failed to process request: timeout\n    \"\"\"\n    \n    with open('app.log', 'w') as f:\n        f.write(log_content)\n\ndef main():\n    \"\"\"Demonstrate parsing functionality\"\"\"\n    \n    # Create sample files\n    create_sample_files()\n    \n    # Parse configuration file\n    config_parser = ConfigFileParser()\n    try:\n        config_data =", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "pyparsing"], "package_breakdown": {"install_packages": ["re", "pyparsing"], "import_packages": ["re", "pyparsing"], "llm_suggested": ["pyparsing", "re"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a cache for frequently accessed properties in a class, automatically updating and refreshing the cache when properties are modified or accessed.", "prompt_index": 86, "generated_code": "```python\nimport time\nfrom functools import wraps\nfrom typing import Any, Dict, Optional, Callable\nfrom threading import Lock\n\nclass CachedProperty:\n    \"\"\"Descriptor for cached properties with automatic invalidation\"\"\"\n    \n    def __init__(self, func: Callable, ttl: Optional[float] = None):\n        self.func = func\n        self.ttl = ttl  # Time to live in seconds\n        self.name = func.__name__\n        self.lock = Lock()\n        \n    def __get__(self, obj: Any, owner: type = None) -> Any:\n        if obj is None:\n            return self\n            \n        cache = obj._property_cache\n        cache_key = self.name\n        \n        with self.lock:\n            # Check if cached value exists and is valid\n            if cache_key in cache:\n                cached_value, timestamp = cache[cache_key]\n                \n                # Check TTL if specified\n                if self.ttl is None or (time.time() - timestamp) < self.ttl:\n                    return cached_value\n            \n            # Compute new value and cache it\n            value = self.func(obj)\n            cache[cache_key] = (value, time.time())\n            return value\n    \n    def __set__(self, obj: Any, value: Any) -> None:\n        # If property is being set, update cache directly\n        obj._property_cache[self.name] = (value, time.time())\n    \n    def __delete__(self, obj: Any) -> None:\n        # Remove from cache when deleted\n        obj._property_cache.pop(self.name, None)\n\ndef cached_property(ttl: Optional[float] = None):\n    \"\"\"Decorator for creating cached properties\"\"\"\n    def decorator(func: Callable) -> CachedProperty:\n        return CachedProperty(func, ttl)\n    return decorator\n\nclass AutoCache:\n    \"\"\"Base class that provides automatic caching functionality\"\"\"\n    \n    def __init__(self):\n        self._property_cache: Dict[str, tuple] = {}\n        self._cache_lock = Lock()\n        self._dependent_properties: Dict[str, set] = {}\n    \n    def invalidate_cache(self, *property_names: str) -> None:\n        \"\"\"Manually invalidate specific cached properties\"\"\"\n        with self._cache_lock:\n            for prop_name in property_names:\n                self._property_cache.pop(prop_name, None)\n                # Also invalidate dependent properties\n                for dependent in self._dependent_properties.get(prop_name, set()):\n                    self._property_cache.pop(dependent, None)\n    \n    def clear_cache(self) -> None:\n        \"\"\"Clear all cached properties\"\"\"\n        with self._cache_lock:\n            self._property_cache.clear()\n    \n    def add_dependency(self, dependent: str, dependency: str) -> None:\n        \"\"\"Add property dependency for automatic invalidation\"\"\"\n        if dependency not in self._dependent_properties:\n            self._dependent_properties[dependency] = set()\n        self._dependent_properties[dependency].add(dependent)\n    \n    def get_cache_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about cached properties\"\"\"\n        with self._cache_lock:\n            return {\n                'cached_properties': list(self._property_cache.keys()),\n                'cache_size': len(self._property_cache),\n                'dependencies': dict(self._dependent_properties)\n            }\n\n# Example usage class\nclass DataProcessor(AutoCache):\n    def __init__(self, data_source: str):\n        super().__init__()\n        self._data_source = data_source\n        self._raw_data = None\n        \n        # Set up property dependencies\n        self.add_dependency('processed_data', 'raw_data')\n        self.add_dependency('statistics', 'processed_data')\n        self.add_dependency('summary', 'statistics')\n    \n    @property\n    def data_source(self) -> str:\n        return self._data_source\n    \n    @data_source.setter\n    def data_source(self, value: str) -> None:\n        self._data_source = value\n        # Invalidate all dependent caches when source changes\n        self.invalidate_cache('raw_data', 'processed_data', 'statistics', 'summary')\n    \n    @cached_property(ttl=300)  # Cache for 5 minutes\n    def raw_data(self) -> list:\n        \"\"\"Simulate expensive data loading operation\"\"\"\n        print(f\"Loading data from {self._data_source}...\")\n        time.sleep(0.1)  # Simulate network/disk I/O\n        return [i for i in range(1000)]\n    \n    @cached_property(ttl=600)  # Cache for 10 minutes\n    def processed_data(self) -> list:\n        \"\"\"Simulate expensive data processing\"\"\"\n        print(\"Processing raw data...\")\n        time.sleep(0.1)  # Simulate processing time\n        return [x * 2 for x in self.raw_data]\n    \n    @cached_property()  # No TTL - cache until invalidated\n    def statistics(self) -> dict:\n        \"\"\"Calculate statistics from processed data\"\"\"\n        print(\"Calculating statistics...\")\n        data = self.processed_data\n        return {\n            'count': len(data),\n            'sum': sum(data),\n            'avg': sum(data) / len(data) if data else 0,\n            'min': min(data) if data else 0,\n            'max': max(data) if data else 0\n        }\n    \n    @cached_property(ttl=60)  # Cache for 1 minute\n    def summary(self) -> str:\n        \"\"\"Generate summary report\"\"\"\n        print(\"Generating summary...\")\n        stats = self.statistics\n        return f\"Data Summary: {stats['count']} items, avg: {stats['avg']:.2f}\"\n\n# Advanced caching with size limits\nclass LRUCache:\n    \"\"\"Simple LRU cache implementation\"\"\"\n    \n    def __init__(self, max_size: int = 100):\n        self.max_size = max_size\n        self.cache: Dict[str, Any] = {}\n        self.access_order: list = []\n        self.lock = Lock()\n    \n    def get(self, key: str) -> tuple:\n        with self.lock:\n            if key in self.cache:\n                # Move to end (most recently used)\n                self.access_order.remove(key)\n                self.access_order.append(key)\n                return self.cache[key]\n            return None\n    \n    def put(self, key: str, value: tuple) -> None:\n        with self.lock:\n            if key in self.cache:\n                # Update existing\n                self.access_order.remove(key)\n            elif len(self.cache) >= self.max_size:\n                # Remove least recently used\n                lru_key = self.access_order.pop(0)\n                del self.cache[lru_key]\n            \n            self.cache[key] = value\n            self.access_order.append(key)\n    \n    def pop(self, key: str, default=None):\n        with self.lock:\n            if key in self.cache:\n                self.access_order.remove(key)\n                return self.cache.pop(key)\n            return default\n    \n    def clear(self) -> None:\n        with self.lock:\n            self.cache.clear()\n            self.access_order.clear()\n\nclass AdvancedAutoCache(AutoCache):\n    \"\"\"Enhanced cache with LRU eviction and size limits\"\"\"\n    \n    def __init__(self, max_cache_size: int = 100):\n        super().__init__()\n        self._property_cache = LRUCache(max_cache_size)\n    \n    def invalidate_cache(self, *property_names: str) -> None:\n        with self._cache_lock:\n            for prop_name in property_names:\n                self._property_cache.pop(prop_name, None)\n                for dependent in self._dependent_properties.get(prop_name, set()):\n                    self._property_cache.pop(dependent, None)\n\n# Example with method caching\ndef cache_method(ttl: Optional[float] = None, max_size: int = 100):\n    \"\"\"Decorator for caching method results\"\"\"\n    def decorator(func: Callable", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "typing", "threading", "time"], "package_breakdown": {"install_packages": ["functools", "typing", "threading", "time"], "import_packages": ["functools", "threading", "time", "typing"], "llm_suggested": ["functools", "typing", "threading", "time"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that calculates the probability density function and cumulative distribution function for a given distribution using the distribution utilities package.", "prompt_index": 87, "generated_code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef calculate_pdf_cdf(distribution, x_values, **params):\n    \"\"\"\n    Calculate PDF and CDF for a given distribution\n    \n    Args:\n        distribution: scipy.stats distribution object\n        x_values: array of x values\n        **params: distribution parameters\n    \n    Returns:\n        tuple: (pdf_values, cdf_values)\n    \"\"\"\n    pdf_values = distribution.pdf(x_values, **params)\n    cdf_values = distribution.cdf(x_values, **params)\n    \n    return pdf_values, cdf_values\n\n# Example 1: Normal Distribution\nx = np.linspace(-4, 4, 100)\nnormal_pdf, normal_cdf = calculate_pdf_cdf(stats.norm, x, loc=0, scale=1)\n\n# Example 2: Exponential Distribution\nx_exp = np.linspace(0, 5, 100)\nexp_pdf, exp_cdf = calculate_pdf_cdf(stats.expon, x_exp, scale=1)\n\n# Example 3: Beta Distribution\nx_beta = np.linspace(0, 1, 100)\nbeta_pdf, beta_cdf = calculate_pdf_cdf(stats.beta, x_beta, a=2, b=3)\n\n# Plotting function\ndef plot_pdf_cdf(x, pdf, cdf, title):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    ax1.plot(x, pdf, 'b-', linewidth=2)\n    ax1.set_title(f'{title} - PDF')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('Probability Density')\n    ax1.grid(True)\n    \n    ax2.plot(x, cdf, 'r-', linewidth=2)\n    ax2.set_title(f'{title} - CDF')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('Cumulative Probability')\n    ax2.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot examples\nplot_pdf_cdf(x, normal_pdf, normal_cdf, 'Normal Distribution')\nplot_pdf_cdf(x_exp, exp_pdf, exp_cdf, 'Exponential Distribution')\nplot_pdf_cdf(x_beta, beta_pdf, beta_cdf, 'Beta Distribution')\n\n# Calculate specific probabilities\ndef calculate_probabilities(distribution, value, **params):\n    \"\"\"Calculate PDF and CDF at a specific value\"\"\"\n    pdf_at_value = distribution.pdf(value, **params)\n    cdf_at_value = distribution.cdf(value, **params)\n    \n    print(f\"At x = {value}:\")\n    print(f\"PDF = {pdf_at_value:.6f}\")\n    print(f\"CDF = {cdf_at_value:.6f}\")\n    print(f\"P(X > {value}) = {1 - cdf_at_value:.6f}\")\n    \n    return pdf_at_value, cdf_at_value\n\n# Example calculations\nprint(\"Normal Distribution N(0,1) at x=1:\")\ncalculate_probabilities(stats.norm, 1, loc=0, scale=1)\n\nprint(\"\\nExponential Distribution (\u03bb=1) at x=2:\")\ncalculate_probabilities(stats.expon, 2, scale=1)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["scipy", "matplotlib", "numpy"], "package_breakdown": {"install_packages": ["scipy", "matplotlib", "numpy"], "import_packages": ["scipy", "matplotlib", "numpy"], "llm_suggested": ["numpy", "matplotlib", "scipy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that resizes an image file using the Python Imaging Library (Fork) to a specific width and height, maintaining aspect ratio and saving the new image with a specified filename.", "prompt_index": 88, "generated_code": "```python\nfrom PIL import Image\n\ndef resize_image_with_aspect_ratio(input_path, output_path, target_width, target_height):\n    # Open the original image\n    with Image.open(input_path) as img:\n        # Get original dimensions\n        original_width, original_height = img.size\n        \n        # Calculate aspect ratios\n        original_ratio = original_width / original_height\n        target_ratio = target_width / target_height\n        \n        # Determine new dimensions maintaining aspect ratio\n        if original_ratio > target_ratio:\n            # Image is wider, fit to width\n            new_width = target_width\n            new_height = int(target_width / original_ratio)\n        else:\n            # Image is taller, fit to height\n            new_height = target_height\n            new_width = int(target_height * original_ratio)\n        \n        # Resize the image\n        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n        \n        # Save the resized image\n        resized_img.save(output_path)\n        \n        return new_width, new_height\n\n# Example usage\ninput_file = \"original_image.jpg\"\noutput_file = \"resized_image.jpg\"\nwidth = 800\nheight = 600\n\nnew_dimensions = resize_image_with_aspect_ratio(input_file, output_file, width, height)\nprint(f\"Image resized to: {new_dimensions[0]}x{new_dimensions[1]}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["Pillow", "PIL"], "package_breakdown": {"install_packages": ["Pillow", "PIL"], "import_packages": ["PIL"], "llm_suggested": ["Pillow"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that implements the Newton-Raphson method for finding the root of a nonlinear equation in scientific computing.", "prompt_index": 89, "generated_code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef newton_raphson(f, df, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Newton-Raphson method for finding roots of nonlinear equations.\n    \n    Parameters:\n    f: function - the nonlinear function\n    df: function - derivative of f\n    x0: float - initial guess\n    tol: float - tolerance for convergence\n    max_iter: int - maximum number of iterations\n    \n    Returns:\n    root: float - the root found\n    iterations: int - number of iterations used\n    convergence: bool - whether method converged\n    \"\"\"\n    x = x0\n    iterations = 0\n    \n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        \n        # Check if derivative is too small\n        if abs(dfx) < 1e-12:\n            print(f\"Derivative too small at x = {x}\")\n            return x, iterations, False\n        \n        # Newton-Raphson update\n        x_new = x - fx / dfx\n        \n        # Check for convergence\n        if abs(x_new - x) < tol:\n            return x_new, iterations + 1, True\n        \n        x = x_new\n        iterations += 1\n    \n    print(f\"Maximum iterations ({max_iter}) reached\")\n    return x, iterations, False\n\ndef newton_raphson_with_history(f, df, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Newton-Raphson method that returns iteration history for visualization.\n    \"\"\"\n    x = x0\n    history = [x0]\n    \n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        \n        if abs(dfx) < 1e-12:\n            break\n        \n        x_new = x - fx / dfx\n        history.append(x_new)\n        \n        if abs(x_new - x) < tol:\n            break\n        \n        x = x_new\n    \n    return np.array(history)\n\n# Example 1: Simple polynomial\ndef example1():\n    # Find root of f(x) = x^3 - 2x - 5\n    def f(x):\n        return x**3 - 2*x - 5\n    \n    def df(x):\n        return 3*x**2 - 2\n    \n    root, iterations, converged = newton_raphson(f, df, x0=2.0)\n    print(f\"Example 1: Root = {root:.6f}, Iterations = {iterations}, Converged = {converged}\")\n    print(f\"Verification: f({root:.6f}) = {f(root):.2e}\\n\")\n\n# Example 2: Transcendental equation\ndef example2():\n    # Find root of f(x) = e^x - 3x\n    def f(x):\n        return np.exp(x) - 3*x\n    \n    def df(x):\n        return np.exp(x) - 3\n    \n    root, iterations, converged = newton_raphson(f, df, x0=1.0)\n    print(f\"Example 2: Root = {root:.6f}, Iterations = {iterations}, Converged = {converged}\")\n    print(f\"Verification: f({root:.6f}) = {f(root):.2e}\\n\")\n\n# Example 3: Multiple roots with visualization\ndef example3_with_plot():\n    # Find roots of f(x) = x^3 - 6x^2 + 11x - 6 = (x-1)(x-2)(x-3)\n    def f(x):\n        return x**3 - 6*x**2 + 11*x - 6\n    \n    def df(x):\n        return 3*x**2 - 12*x + 11\n    \n    # Find different roots with different initial guesses\n    initial_guesses = [0.5, 1.5, 2.5]\n    roots = []\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Plot function\n    x = np.linspace(-0.5, 3.5, 1000)\n    y = f(x)\n    plt.subplot(2, 1, 1)\n    plt.plot(x, y, 'b-', linewidth=2, label='f(x)')\n    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    plt.grid(True)\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.title('Function: f(x) = x\u00b3 - 6x\u00b2 + 11x - 6')\n    \n    colors = ['red', 'green', 'orange']\n    \n    for i, x0 in enumerate(initial_guesses):\n        history = newton_raphson_with_history(f, df, x0)\n        root = history[-1]\n        roots.append(root)\n        \n        plt.subplot(2, 1, 1)\n        plt.plot(x0, f(x0), 'o', color=colors[i], markersize=8, \n                label=f'Initial guess {i+1}: x\u2080={x0}')\n        plt.plot(root, f(root), 's', color=colors[i], markersize=8,\n                label=f'Root {i+1}: {root:.4f}')\n        \n        # Plot convergence\n        plt.subplot(2, 1, 2)\n        plt.semilogy(range(len(history)), np.abs(f(history)), \n                    'o-', color=colors[i], label=f'x\u2080={x0} \u2192 {root:.4f}')\n    \n    plt.subplot(2, 1, 1)\n    plt.legend()\n    \n    plt.subplot(2, 1, 2)\n    plt.xlabel('Iteration')\n    plt.ylabel('|f(x)|')\n    plt.title('Convergence History')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Example 3: Multiple roots found:\")\n    for i, root in enumerate(roots):\n        print(f\"Root {i+1}: {root:.6f}\")\n\n# Advanced: Newton-Raphson for systems (Jacobian method)\ndef newton_raphson_system(F, J, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Newton-Raphson method for systems of nonlinear equations.\n    \n    Parameters:\n    F: function - returns array of function values\n    J: function - returns Jacobian matrix\n    x0: array - initial guess\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    \n    for i in range(max_iter):\n        Fx = F(x)\n        Jx = J(x)\n        \n        # Solve linear system: J(x) * delta_x = -F(x)\n        try:\n            delta_x = np.linalg.solve(Jx, -Fx)\n        except np.linalg.LinAlgError:\n            print(\"Singular Jacobian matrix\")\n            return x, i, False\n        \n        x_new = x + delta_x\n        \n        if np.linalg.norm(delta_x) < tol:\n            return x_new, i + 1, True\n        \n        x = x_new\n    \n    return x, max_iter, False\n\n# Example: System of equations\ndef system_example():\n    # Solve: x^2 + y^2 = 1, x - y = 0 (intersection of circle and line)\n    def F(vars):\n        x, y = vars\n        return np.array([\n            x**2 + y**2 - 1,  # Circle equation\n            x - y             # Line equation\n        ])\n    \n    def J(vars):\n        x, y = vars\n        return np.array([\n            [2*x, 2*y],  # Partial derivatives of first equation", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "numpy"], "package_breakdown": {"install_packages": ["matplotlib", "numpy"], "import_packages": ["matplotlib", "numpy"], "llm_suggested": ["numpy", "matplotlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves the JSON Schema meta-schemas and vocabularies from the Registry and allows for dynamic validation of JSON data against these schemas.", "prompt_index": 90, "generated_code": "```python\nimport json\nimport requests\nfrom jsonschema import Draft7Validator, Draft202012Validator, RefResolver\nfrom jsonschema.exceptions import ValidationError, SchemaError\nfrom typing import Dict, Any, Optional, List\nimport urllib.parse\n\nclass JSONSchemaRegistry:\n    def __init__(self):\n        self.meta_schemas = {}\n        self.vocabularies = {}\n        self.resolvers = {}\n        self.validators = {}\n        \n        # Common meta-schema URLs\n        self.meta_schema_urls = {\n            'draft-07': 'https://json-schema.org/draft-07/schema',\n            'draft/2019-09': 'https://json-schema.org/draft/2019-09/schema',\n            'draft/2020-12': 'https://json-schema.org/draft/2020-12/schema'\n        }\n        \n        # Common vocabulary URLs for newer drafts\n        self.vocabulary_urls = {\n            'core': 'https://json-schema.org/draft/2020-12/vocab/core',\n            'applicator': 'https://json-schema.org/draft/2020-12/vocab/applicator',\n            'validation': 'https://json-schema.org/draft/2020-12/vocab/validation',\n            'meta-data': 'https://json-schema.org/draft/2020-12/vocab/meta-data',\n            'format-annotation': 'https://json-schema.org/draft/2020-12/vocab/format-annotation',\n            'content': 'https://json-schema.org/draft/2020-12/vocab/content'\n        }\n\n    def fetch_schema(self, url: str) -> Dict[str, Any]:\n        \"\"\"Fetch a JSON schema from a URL.\"\"\"\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            raise Exception(f\"Failed to fetch schema from {url}: {e}\")\n        except json.JSONDecodeError as e:\n            raise Exception(f\"Invalid JSON in schema from {url}: {e}\")\n\n    def load_meta_schemas(self):\n        \"\"\"Load all meta-schemas from the registry.\"\"\"\n        for version, url in self.meta_schema_urls.items():\n            try:\n                schema = self.fetch_schema(url)\n                self.meta_schemas[version] = schema\n                print(f\"Loaded meta-schema: {version}\")\n            except Exception as e:\n                print(f\"Warning: Could not load meta-schema {version}: {e}\")\n\n    def load_vocabularies(self):\n        \"\"\"Load vocabulary schemas.\"\"\"\n        for vocab_name, url in self.vocabulary_urls.items():\n            try:\n                vocab = self.fetch_schema(url)\n                self.vocabularies[vocab_name] = vocab\n                print(f\"Loaded vocabulary: {vocab_name}\")\n            except Exception as e:\n                print(f\"Warning: Could not load vocabulary {vocab_name}: {e}\")\n\n    def create_resolver(self, schema: Dict[str, Any]) -> RefResolver:\n        \"\"\"Create a RefResolver for a schema with loaded meta-schemas and vocabularies.\"\"\"\n        store = {}\n        \n        # Add meta-schemas to the store\n        for version, meta_schema in self.meta_schemas.items():\n            if '$id' in meta_schema:\n                store[meta_schema['$id']] = meta_schema\n        \n        # Add vocabularies to the store\n        for vocab_name, vocab in self.vocabularies.items():\n            if '$id' in vocab:\n                store[vocab['$id']] = vocab\n        \n        # Determine base URI\n        base_uri = schema.get('$id', schema.get('id', ''))\n        \n        return RefResolver(base_uri=base_uri, referrer=schema, store=store)\n\n    def get_validator_class(self, schema: Dict[str, Any]):\n        \"\"\"Determine the appropriate validator class based on schema version.\"\"\"\n        schema_version = schema.get('$schema', '')\n        \n        if 'draft-07' in schema_version:\n            return Draft7Validator\n        elif '2020-12' in schema_version:\n            return Draft202012Validator\n        elif '2019-09' in schema_version:\n            # Use Draft202012Validator as fallback for 2019-09\n            return Draft202012Validator\n        else:\n            # Default to Draft7Validator\n            return Draft7Validator\n\n    def validate_schema(self, schema: Dict[str, Any]) -> bool:\n        \"\"\"Validate that a schema is itself valid according to its meta-schema.\"\"\"\n        try:\n            validator_class = self.get_validator_class(schema)\n            resolver = self.create_resolver(schema)\n            \n            # Create validator with resolver\n            validator = validator_class(schema, resolver=resolver)\n            validator.check_schema(schema)\n            return True\n        except SchemaError as e:\n            print(f\"Schema validation error: {e}\")\n            return False\n        except Exception as e:\n            print(f\"Error validating schema: {e}\")\n            return False\n\n    def validate_data(self, data: Any, schema: Dict[str, Any]) -> tuple[bool, List[str]]:\n        \"\"\"Validate data against a schema.\"\"\"\n        errors = []\n        \n        try:\n            # First validate the schema itself\n            if not self.validate_schema(schema):\n                return False, [\"Invalid schema\"]\n            \n            validator_class = self.get_validator_class(schema)\n            resolver = self.create_resolver(schema)\n            \n            validator = validator_class(schema, resolver=resolver)\n            \n            # Collect all validation errors\n            for error in validator.iter_errors(data):\n                error_path = \" -> \".join(str(p) for p in error.absolute_path)\n                error_msg = f\"Path: {error_path if error_path else 'root'}, Error: {error.message}\"\n                errors.append(error_msg)\n            \n            return len(errors) == 0, errors\n            \n        except Exception as e:\n            return False, [f\"Validation error: {e}\"]\n\n    def load_schema_from_url(self, url: str) -> Dict[str, Any]:\n        \"\"\"Load a schema from a URL and cache it.\"\"\"\n        return self.fetch_schema(url)\n\n    def initialize(self):\n        \"\"\"Initialize the registry by loading meta-schemas and vocabularies.\"\"\"\n        print(\"Initializing JSON Schema Registry...\")\n        self.load_meta_schemas()\n        self.load_vocabularies()\n        print(\"Registry initialization complete.\")\n\n# Example usage\ndef main():\n    # Initialize the registry\n    registry = JSONSchemaRegistry()\n    registry.initialize()\n    \n    # Example schema\n    example_schema = {\n        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n        \"$id\": \"https://example.com/person.schema.json\",\n        \"title\": \"Person\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"firstName\": {\n                \"type\": \"string\",\n                \"description\": \"The person's first name.\"\n            },\n            \"lastName\": {\n                \"type\": \"string\",\n                \"description\": \"The person's last name.\"\n            },\n            \"age\": {\n                \"description\": \"Age in years which must be equal to or greater than zero.\",\n                \"type\": \"integer\",\n                \"minimum\": 0\n            }\n        },\n        \"required\": [\"firstName\", \"lastName\"]\n    }\n    \n    # Example data to validate\n    valid_data = {\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"age\": 30\n    }\n    \n    invalid_data = {\n        \"firstName\": \"Jane\",\n        \"age\": -5  # Missing lastName, invalid age\n    }\n    \n    # Validate the schema itself\n    print(\"\\n--- Schema Validation ---\")\n    is_valid_schema = registry.validate_schema(example_schema)\n    print(f\"Schema is valid: {is_valid_schema}\")\n    \n    # Validate data against schema\n    print(\"\\n--- Data Validation (Valid Data) ---\")\n    is_valid, errors = registry.validate_data(valid_data, example_schema)\n    print(f\"Data is valid: {is_valid}\")\n    if errors:\n        for error in errors:", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "jsonschema", "json", "requests", "typing"], "package_breakdown": {"install_packages": ["urllib", "jsonschema", "json", "requests", "typing"], "import_packages": ["typing", "urllib", "jsonschema", "json", "requests"], "llm_suggested": ["requests", "jsonschema"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that renders a progress bar with a specific percentage completion, a markdown-formatted text, and a syntax-highlighted code snippet to the terminal interface.", "prompt_index": 91, "generated_code": "```python\nimport sys\nimport time\nfrom rich.console import Console\nfrom rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn\nfrom rich.markdown import Markdown\nfrom rich.syntax import Syntax\nfrom rich.panel import Panel\nfrom rich.layout import Layout\n\ndef render_terminal_interface(percentage=75):\n    console = Console()\n    \n    # Clear screen\n    console.clear()\n    \n    # Create layout\n    layout = Layout()\n    layout.split_column(\n        Layout(name=\"progress\", size=3),\n        Layout(name=\"markdown\", size=8),\n        Layout(name=\"code\", size=12)\n    )\n    \n    # Progress bar\n    with Progress(\n        TextColumn(\"[bold blue]Processing...\"),\n        BarColumn(bar_width=40),\n        TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n        console=console,\n        transient=False\n    ) as progress:\n        task = progress.add_task(\"\", total=100)\n        progress.update(task, completed=percentage)\n        \n        # Markdown content\n        markdown_text = \"\"\"\n# Project Status Report\n\n## Current Progress\n- **Data Processing**: \u2705 Complete\n- **Model Training**: \ud83d\udd04 In Progress  \n- **Testing**: \u23f3 Pending\n\n### Key Metrics\n- Accuracy: **92.5%**\n- Performance: *Excellent*\n- Status: `Active`\n        \"\"\"\n        \n        # Code snippet\n        code_snippet = '''\ndef train_model(data, epochs=100):\n    \"\"\"Train machine learning model\"\"\"\n    model = NeuralNetwork(\n        layers=[128, 64, 32, 10],\n        activation='relu',\n        optimizer='adam'\n    )\n    \n    for epoch in range(epochs):\n        loss = model.fit(data)\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n    \n    return model\n        '''\n        \n        # Render components\n        layout[\"progress\"].update(Panel(progress.get_renderable(), title=\"Progress\"))\n        layout[\"markdown\"].update(Panel(Markdown(markdown_text), title=\"Documentation\"))\n        layout[\"code\"].update(Panel(\n            Syntax(code_snippet, \"python\", theme=\"monokai\", line_numbers=True),\n            title=\"Code Snippet\"\n        ))\n        \n        console.print(layout)\n\n# Alternative simpler version without layout\ndef simple_render(percentage=75):\n    console = Console()\n    console.clear()\n    \n    # Progress bar\n    console.print(\"\\n[bold cyan]Progress Status[/bold cyan]\")\n    progress_bar = \"\u2588\" * int(percentage / 2.5) + \"\u2591\" * int((100 - percentage) / 2.5)\n    console.print(f\"[green]{progress_bar}[/green] {percentage}%\\n\")\n    \n    # Markdown\n    markdown = \"\"\"\n# System Status\n## Current Operations\n- Processing: **Active**\n- Memory Usage: *75%*\n- Status: `Running`\n    \"\"\"\n    console.print(Panel(Markdown(markdown), title=\"Status Report\"))\n    \n    # Code\n    code = '''\nimport numpy as np\n\ndef process_data(input_data):\n    result = np.array(input_data)\n    return result.mean()\n    '''\n    \n    console.print(Panel(\n        Syntax(code, \"python\", theme=\"github-dark\", line_numbers=True),\n        title=\"Sample Code\"\n    ))\n\nif __name__ == \"__main__\":\n    # Install required package: pip install rich\n    render_terminal_interface(85)\n    # Or use simple version: simple_render(85)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "rich", "numpy", "sys"], "package_breakdown": {"install_packages": ["time", "rich", "numpy", "sys"], "import_packages": ["rich", "numpy", "time", "sys"], "llm_suggested": ["rich"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a bidirectional communication channel using HTTP/2 for Remote Procedure Calls (RPC) between a client and server, allowing for efficient and low-latency data exchange.", "prompt_index": 92, "generated_code": "```python\n# Server implementation using aiohttp with HTTP/2 support\nimport asyncio\nimport json\nimport logging\nfrom aiohttp import web, WSMsgType\nfrom aiohttp_sse import sse_response\nimport aiohttp_cors\nfrom typing import Dict, Callable, Any\nimport uuid\n\nclass HTTP2RPCServer:\n    def __init__(self, host='localhost', port=8080):\n        self.host = host\n        self.port = port\n        self.app = web.Application()\n        self.rpc_methods: Dict[str, Callable] = {}\n        self.active_connections: Dict[str, Any] = {}\n        self.setup_routes()\n        \n    def setup_routes(self):\n        # CORS setup\n        cors = aiohttp_cors.setup(self.app, defaults={\n            \"*\": aiohttp_cors.ResourceOptions(\n                allow_credentials=True,\n                expose_headers=\"*\",\n                allow_headers=\"*\",\n                allow_methods=\"*\"\n            )\n        })\n        \n        # RPC endpoint\n        rpc_resource = cors.add(self.app.router.add_resource('/rpc'))\n        cors.add(rpc_resource.add_route('POST', self.handle_rpc))\n        \n        # Server-sent events for bidirectional communication\n        sse_resource = cors.add(self.app.router.add_resource('/events/{client_id}'))\n        cors.add(sse_resource.add_route('GET', self.handle_sse))\n        \n        # WebSocket alternative for full duplex\n        ws_resource = cors.add(self.app.router.add_resource('/ws'))\n        cors.add(ws_resource.add_route('GET', self.handle_websocket))\n    \n    def register_method(self, name: str, method: Callable):\n        \"\"\"Register an RPC method\"\"\"\n        self.rpc_methods[name] = method\n    \n    async def handle_rpc(self, request):\n        \"\"\"Handle incoming RPC calls\"\"\"\n        try:\n            data = await request.json()\n            method_name = data.get('method')\n            params = data.get('params', {})\n            request_id = data.get('id', str(uuid.uuid4()))\n            \n            if method_name not in self.rpc_methods:\n                return web.json_response({\n                    'error': f'Method {method_name} not found',\n                    'id': request_id\n                }, status=404)\n            \n            # Execute RPC method\n            result = await self.rpc_methods[method_name](**params)\n            \n            response_data = {\n                'result': result,\n                'id': request_id\n            }\n            \n            return web.json_response(response_data)\n            \n        except Exception as e:\n            return web.json_response({\n                'error': str(e),\n                'id': data.get('id') if 'data' in locals() else None\n            }, status=500)\n    \n    async def handle_sse(self, request):\n        \"\"\"Handle Server-Sent Events for server-to-client communication\"\"\"\n        client_id = request.match_info['client_id']\n        \n        async with sse_response(request) as resp:\n            self.active_connections[client_id] = resp\n            try:\n                # Keep connection alive\n                while not resp.task.done():\n                    await asyncio.sleep(1)\n            finally:\n                self.active_connections.pop(client_id, None)\n        \n        return resp\n    \n    async def handle_websocket(self, request):\n        \"\"\"Handle WebSocket connections for full duplex communication\"\"\"\n        ws = web.WebSocketResponse()\n        await ws.prepare(request)\n        \n        client_id = str(uuid.uuid4())\n        self.active_connections[client_id] = ws\n        \n        try:\n            async for msg in ws:\n                if msg.type == WSMsgType.TEXT:\n                    try:\n                        data = json.loads(msg.data)\n                        method_name = data.get('method')\n                        params = data.get('params', {})\n                        request_id = data.get('id', str(uuid.uuid4()))\n                        \n                        if method_name in self.rpc_methods:\n                            result = await self.rpc_methods[method_name](**params)\n                            await ws.send_str(json.dumps({\n                                'result': result,\n                                'id': request_id\n                            }))\n                        else:\n                            await ws.send_str(json.dumps({\n                                'error': f'Method {method_name} not found',\n                                'id': request_id\n                            }))\n                    except Exception as e:\n                        await ws.send_str(json.dumps({\n                            'error': str(e),\n                            'id': data.get('id') if 'data' in locals() else None\n                        }))\n                elif msg.type == WSMsgType.ERROR:\n                    print(f'WebSocket error: {ws.exception()}')\n        finally:\n            self.active_connections.pop(client_id, None)\n        \n        return ws\n    \n    async def broadcast_to_client(self, client_id: str, data: dict):\n        \"\"\"Send data to a specific client\"\"\"\n        if client_id in self.active_connections:\n            conn = self.active_connections[client_id]\n            if hasattr(conn, 'send_str'):  # WebSocket\n                await conn.send_str(json.dumps(data))\n            else:  # SSE\n                await conn.send(json.dumps(data))\n    \n    async def broadcast_to_all(self, data: dict):\n        \"\"\"Broadcast data to all connected clients\"\"\"\n        for client_id in list(self.active_connections.keys()):\n            try:\n                await self.broadcast_to_client(client_id, data)\n            except Exception as e:\n                print(f\"Error broadcasting to {client_id}: {e}\")\n                self.active_connections.pop(client_id, None)\n    \n    def run(self):\n        \"\"\"Start the server with HTTP/2 support\"\"\"\n        web.run_app(self.app, host=self.host, port=self.port)\n\n\n# Client implementation\nimport aiohttp\nimport asyncio\nimport json\nimport ssl\nfrom typing import Optional, Callable\n\nclass HTTP2RPCClient:\n    def __init__(self, base_url='http://localhost:8080'):\n        self.base_url = base_url\n        self.session: Optional[aiohttp.ClientSession] = None\n        self.client_id = str(uuid.uuid4())\n        self.ws: Optional[aiohttp.ClientWebSocketResponse] = None\n        self.event_handlers: Dict[str, Callable] = {}\n        \n    async def __aenter__(self):\n        # Create HTTP/2 connector\n        connector = aiohttp.TCPConnector(\n            limit=100,\n            limit_per_host=30,\n            enable_cleanup_closed=True\n        )\n        \n        self.session = aiohttp.ClientSession(\n            connector=connector,\n            timeout=aiohttp.ClientTimeout(total=30)\n        )\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.ws and not self.ws.closed:\n            await self.ws.close()\n        if self.session:\n            await self.session.close()\n    \n    async def call_rpc(self, method: str, params: dict = None, timeout: int = 30) -> dict:\n        \"\"\"Make an RPC call to the server\"\"\"\n        if not self.session:\n            raise RuntimeError(\"Client not initialized. Use async with statement.\")\n        \n        payload = {\n            'method': method,\n            'params': params or {},\n            'id': str(uuid.uuid4())\n        }\n        \n        async with self.session.post(\n            f'{self.base_url}/rpc',\n            json=payload,\n            timeout=aiohttp.ClientTimeout(total=timeout)\n        ) as response:\n            result = await response.json()\n            if 'error' in result:\n                raise Exception(f\"RPC Error: {result['error']}\")\n            return result['result']", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 11, "hallucinated_packages": 1, "hallucinations": ["aiohttp_sse"], "packages_detected": ["aiohttp", "aiohttp-cors", "asyncio", "typing", "uuid", "aiohttp_sse", "logging", "ssl", "aiohttp-sse", "json", "aiohttp_cors"], "package_breakdown": {"install_packages": ["aiohttp", "aiohttp-cors", "asyncio", "typing", "uuid", "aiohttp_sse", "logging", "ssl", "aiohttp-sse", "json", "aiohttp_cors"], "import_packages": ["aiohttp", "asyncio", "typing", "uuid", "aiohttp_sse", "logging", "ssl", "json", "aiohttp_cors"], "llm_suggested": ["aiohttp", "aiohttp-sse", "aiohttp-cors", "asyncio", "json", "logging", "typing", "uuid", "ssl"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that inspects the data types of variables at runtime and provides detailed information on their structures and nested attributes.", "prompt_index": 93, "generated_code": "```python\nimport inspect\nimport types\nfrom collections.abc import Iterable\nfrom typing import get_type_hints, get_origin, get_args\n\nclass DataTypeInspector:\n    def __init__(self, max_depth=5, max_items=10):\n        self.max_depth = max_depth\n        self.max_items = max_items\n        self.visited = set()\n    \n    def inspect_variable(self, var, var_name=\"variable\", depth=0):\n        \"\"\"Main inspection method that provides detailed information about a variable\"\"\"\n        if depth > self.max_depth:\n            return f\"{'  ' * depth}[Max depth reached]\"\n        \n        var_id = id(var)\n        if var_id in self.visited and hasattr(var, '__dict__'):\n            return f\"{'  ' * depth}[Circular reference detected]\"\n        \n        if hasattr(var, '__dict__'):\n            self.visited.add(var_id)\n        \n        result = []\n        indent = '  ' * depth\n        \n        # Basic type information\n        var_type = type(var)\n        result.append(f\"{indent}{var_name}: {var_type.__name__}\")\n        result.append(f\"{indent}  Type: {var_type}\")\n        result.append(f\"{indent}  Module: {var_type.__module__}\")\n        result.append(f\"{indent}  MRO: {[cls.__name__ for cls in var_type.__mro__]}\")\n        \n        # Memory information\n        result.append(f\"{indent}  Memory ID: {hex(id(var))}\")\n        result.append(f\"{indent}  Size (bytes): {self._get_size(var)}\")\n        \n        # Value information\n        if self._is_simple_type(var):\n            result.append(f\"{indent}  Value: {repr(var)}\")\n        else:\n            result.append(f\"{indent}  Length: {self._get_length(var)}\")\n        \n        # Type-specific inspection\n        if hasattr(var, '__dict__'):\n            result.extend(self._inspect_object_attributes(var, depth))\n        elif isinstance(var, (list, tuple, set, frozenset)):\n            result.extend(self._inspect_sequence(var, depth))\n        elif isinstance(var, dict):\n            result.extend(self._inspect_dict(var, depth))\n        elif callable(var):\n            result.extend(self._inspect_callable(var, depth))\n        elif isinstance(var, type):\n            result.extend(self._inspect_class(var, depth))\n        \n        return '\\n'.join(result)\n    \n    def _inspect_object_attributes(self, obj, depth):\n        \"\"\"Inspect object attributes and methods\"\"\"\n        result = []\n        indent = '  ' * (depth + 1)\n        \n        # Instance attributes\n        if hasattr(obj, '__dict__'):\n            result.append(f\"{indent}Instance Attributes:\")\n            for attr_name, attr_value in list(obj.__dict__.items())[:self.max_items]:\n                result.append(self.inspect_variable(attr_value, f\"{attr_name}\", depth + 2))\n        \n        # Class attributes and methods\n        result.append(f\"{indent}Class Members:\")\n        members = inspect.getmembers(obj.__class__)\n        for name, value in members[:self.max_items]:\n            if not name.startswith('_'):\n                member_type = 'Method' if callable(value) else 'Attribute'\n                result.append(f\"{indent}  {name}: {member_type} - {type(value).__name__}\")\n        \n        return result\n    \n    def _inspect_sequence(self, seq, depth):\n        \"\"\"Inspect sequence types (list, tuple, set, etc.)\"\"\"\n        result = []\n        indent = '  ' * (depth + 1)\n        result.append(f\"{indent}Elements:\")\n        \n        for i, item in enumerate(list(seq)[:self.max_items]):\n            result.append(self.inspect_variable(item, f\"[{i}]\", depth + 2))\n        \n        if len(seq) > self.max_items:\n            result.append(f\"{indent}  ... and {len(seq) - self.max_items} more items\")\n        \n        return result\n    \n    def _inspect_dict(self, d, depth):\n        \"\"\"Inspect dictionary types\"\"\"\n        result = []\n        indent = '  ' * (depth + 1)\n        result.append(f\"{indent}Key-Value Pairs:\")\n        \n        for i, (key, value) in enumerate(list(d.items())[:self.max_items]):\n            result.append(f\"{indent}  Key: {repr(key)} ({type(key).__name__})\")\n            result.append(self.inspect_variable(value, f\"  Value\", depth + 2))\n        \n        if len(d) > self.max_items:\n            result.append(f\"{indent}  ... and {len(d) - self.max_items} more items\")\n        \n        return result\n    \n    def _inspect_callable(self, func, depth):\n        \"\"\"Inspect callable objects\"\"\"\n        result = []\n        indent = '  ' * (depth + 1)\n        \n        try:\n            sig = inspect.signature(func)\n            result.append(f\"{indent}Signature: {sig}\")\n            \n            # Parameters\n            if sig.parameters:\n                result.append(f\"{indent}Parameters:\")\n                for param_name, param in sig.parameters.items():\n                    result.append(f\"{indent}  {param_name}: {param}\")\n            \n            # Type hints\n            try:\n                hints = get_type_hints(func)\n                if hints:\n                    result.append(f\"{indent}Type Hints: {hints}\")\n            except:\n                pass\n            \n            # Docstring\n            if func.__doc__:\n                result.append(f\"{indent}Docstring: {func.__doc__[:100]}...\")\n                \n        except (ValueError, TypeError):\n            result.append(f\"{indent}[Cannot inspect signature]\")\n        \n        return result\n    \n    def _inspect_class(self, cls, depth):\n        \"\"\"Inspect class types\"\"\"\n        result = []\n        indent = '  ' * (depth + 1)\n        \n        result.append(f\"{indent}Base Classes: {[base.__name__ for base in cls.__bases__]}\")\n        result.append(f\"{indent}Subclasses: {[sub.__name__ for sub in cls.__subclasses__()]}\")\n        \n        # Class methods and attributes\n        result.append(f\"{indent}Class Members:\")\n        for name, value in inspect.getmembers(cls)[:self.max_items]:\n            if not name.startswith('__'):\n                member_type = 'Method' if inspect.ismethod(value) or inspect.isfunction(value) else 'Attribute'\n                result.append(f\"{indent}  {name}: {member_type}\")\n        \n        return result\n    \n    def _is_simple_type(self, var):\n        \"\"\"Check if variable is a simple type that can be displayed directly\"\"\"\n        return isinstance(var, (int, float, str, bool, type(None))) and len(str(var)) < 100\n    \n    def _get_length(self, var):\n        \"\"\"Get length of variable if possible\"\"\"\n        try:\n            return len(var)\n        except (TypeError, AttributeError):\n            return \"N/A\"\n    \n    def _get_size(self, var):\n        \"\"\"Get approximate size of variable\"\"\"\n        try:\n            import sys\n            return sys.getsizeof(var)\n        except:\n            return \"N/A\"\n\n# Usage functions\ndef inspect_variable(var, name=\"variable\", max_depth=3, max_items=5):\n    \"\"\"Convenience function to inspect a single variable\"\"\"\n    inspector = DataTypeInspector(max_depth=max_depth, max_items=max_items)\n    return inspector.inspect_variable(var, name)\n\ndef inspect_all_locals(locals_dict=None, max_depth=2):\n    \"\"\"Inspect all variables in local scope\"\"\"\n    if locals_dict is None:\n        import inspect as insp\n        frame = insp.currentframe().f_back\n        locals_dict = frame.f_locals\n    \n    inspector = DataTypeInsp", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "sys", "typing", "collections", "types"], "package_breakdown": {"install_packages": ["inspect", "sys", "typing", "collections", "types"], "import_packages": ["inspect", "typing", "sys", "collections", "types"], "llm_suggested": ["inspect", "sys", "types", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the advanced features of the python-requests library to send a POST request with custom headers, cookies, and a JSON payload to a specified URL, and then prints the response content.", "prompt_index": 95, "generated_code": "```python\nimport requests\nimport json\n\n# Define the URL\nurl = \"https://httpbin.org/post\"\n\n# Custom headers\nheaders = {\n    'Content-Type': 'application/json',\n    'User-Agent': 'Python-Requests/Advanced-Client',\n    'Authorization': 'Bearer your-token-here',\n    'X-Custom-Header': 'custom-value'\n}\n\n# Custom cookies\ncookies = {\n    'session_id': 'abc123def456',\n    'user_preference': 'dark_mode',\n    'csrf_token': 'xyz789'\n}\n\n# JSON payload\npayload = {\n    'username': 'john_doe',\n    'email': 'john@example.com',\n    'data': {\n        'action': 'create_user',\n        'timestamp': '2024-01-01T12:00:00Z'\n    },\n    'metadata': ['tag1', 'tag2', 'tag3']\n}\n\n# Send POST request with advanced features\nresponse = requests.post(\n    url=url,\n    headers=headers,\n    cookies=cookies,\n    json=payload,  # Automatically sets Content-Type and serializes JSON\n    timeout=30,    # Request timeout\n    verify=True,   # SSL certificate verification\n    allow_redirects=True\n)\n\n# Print response details\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"Response Headers: {dict(response.headers)}\")\nprint(f\"Response Content:\")\nprint(json.dumps(response.json(), indent=2))\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "requests"], "package_breakdown": {"install_packages": ["json", "requests"], "import_packages": ["json", "requests"], "llm_suggested": ["requests", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts text data from a specific website's HTML code using the BeautifulSoup library for screen-scraping purposes.", "prompt_index": 96, "generated_code": "```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_website(url):\n    try:\n        # Send GET request to the website\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Parse HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Extract all text from the page\n        text = soup.get_text(strip=True, separator='\\n')\n        \n        return text\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching the website: {e}\")\n        return None\n\ndef scrape_specific_elements(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Extract specific elements\n        data = {}\n        \n        # Extract title\n        title = soup.find('title')\n        data['title'] = title.get_text() if title else 'No title found'\n        \n        # Extract all headings\n        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n        data['headings'] = [h.get_text(strip=True) for h in headings]\n        \n        # Extract all paragraphs\n        paragraphs = soup.find_all('p')\n        data['paragraphs'] = [p.get_text(strip=True) for p in paragraphs]\n        \n        # Extract all links\n        links = soup.find_all('a', href=True)\n        data['links'] = [{'text': link.get_text(strip=True), 'url': link['href']} for link in links]\n        \n        return data\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching the website: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"https://example.com\"  # Replace with target website\n    \n    # Method 1: Extract all text\n    all_text = scrape_website(url)\n    if all_text:\n        print(\"All text from the website:\")\n        print(all_text[:500])  # Print first 500 characters\n    \n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Method 2: Extract specific elements\n    structured_data = scrape_specific_elements(url)\n    if structured_data:\n        print(\"Title:\", structured_data['title'])\n        print(\"\\nHeadings:\")\n        for heading in structured_data['headings'][:5]:  # First 5 headings\n            print(f\"- {heading}\")\n        \n        print(\"\\nFirst paragraph:\")\n        if structured_data['paragraphs']:\n            print(structured_data['paragraphs'][0])\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bs4", "requests", "beautifulsoup4"], "package_breakdown": {"install_packages": ["bs4", "requests", "beautifulsoup4"], "import_packages": ["bs4", "requests"], "llm_suggested": ["requests", "beautifulsoup4"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts and prints all text within HTML elements that are selected using CSS selectors and parsed with Beautiful Soup.", "prompt_index": 97, "generated_code": "```python\nfrom bs4 import BeautifulSoup\nimport requests\n\n# Example HTML content (you can also load from file or URL)\nhtml_content = \"\"\"\n<html>\n<body>\n    <div class=\"container\">\n        <h1 id=\"title\">Main Title</h1>\n        <p class=\"content\">This is a paragraph.</p>\n        <p class=\"content highlight\">This is highlighted content.</p>\n        <ul>\n            <li class=\"item\">Item 1</li>\n            <li class=\"item\">Item 2</li>\n        </ul>\n    </div>\n</body>\n</html>\n\"\"\"\n\n# Parse HTML with Beautiful Soup\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n# CSS selectors and their descriptions\nselectors = {\n    'h1': 'All h1 elements',\n    '.content': 'Elements with class \"content\"',\n    '#title': 'Element with id \"title\"',\n    'p.content': 'Paragraph elements with class \"content\"',\n    '.container li': 'List items inside container',\n    'p.content.highlight': 'Paragraphs with both \"content\" and \"highlight\" classes'\n}\n\n# Extract and print text for each selector\nfor selector, description in selectors.items():\n    print(f\"\\n{description} ('{selector}'):\")\n    elements = soup.select(selector)\n    \n    if elements:\n        for i, element in enumerate(elements, 1):\n            text = element.get_text(strip=True)\n            print(f\"  {i}. {text}\")\n    else:\n        print(\"  No elements found\")\n\n# Alternative: Extract all text from multiple selectors at once\nprint(\"\\n\" + \"=\"*50)\nprint(\"ALL SELECTED TEXT:\")\nprint(\"=\"*50)\n\nall_selectors = ['h1', '.content', '.item']\nfor selector in all_selectors:\n    elements = soup.select(selector)\n    for element in elements:\n        print(f\"[{selector}] {element.get_text(strip=True)}\")\n```\n\n```python\n# Function to extract text from HTML using CSS selectors\ndef extract_text_by_selectors(html_content, selectors):\n    \"\"\"\n    Extract text from HTML using CSS selectors\n    \n    Args:\n        html_content (str): HTML content as string\n        selectors (list): List of CSS selectors\n    \n    Returns:\n        dict: Dictionary with selectors as keys and extracted text as values\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    results = {}\n    \n    for selector in selectors:\n        elements = soup.select(selector)\n        results[selector] = [elem.get_text(strip=True) for elem in elements]\n    \n    return results\n\n# Example usage\nhtml = \"\"\"\n<div class=\"article\">\n    <h2>Article Title</h2>\n    <p class=\"intro\">Introduction paragraph</p>\n    <p>Regular paragraph</p>\n    <span class=\"author\">By John Doe</span>\n</div>\n\"\"\"\n\nselectors_to_extract = ['h2', '.intro', '.author', 'p']\nextracted_text = extract_text_by_selectors(html, selectors_to_extract)\n\nfor selector, texts in extracted_text.items():\n    print(f\"{selector}: {texts}\")\n```\n\n```python\n# Advanced example with URL scraping\ndef scrape_text_from_url(url, selectors):\n    \"\"\"\n    Scrape text from a webpage using CSS selectors\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        results = {}\n        for selector in selectors:\n            elements = soup.select(selector)\n            results[selector] = []\n            \n            for element in elements:\n                # Get text and clean whitespace\n                text = element.get_text(separator=' ', strip=True)\n                if text:  # Only add non-empty text\n                    results[selector].append(text)\n        \n        return results\n        \n    except requests.RequestException as e:\n        print(f\"Error fetching URL: {e}\")\n        return None\n\n# Example usage (uncomment to use with actual URL)\n# url = \"https://example.com\"\n# selectors = ['h1', 'h2', 'p', '.article-content']\n# scraped_data = scrape_text_from_url(url, selectors)\n# \n# if scraped_data:\n#     for selector, texts in scraped_data.items():\n#         print(f\"\\n{selector}:\")\n#         for i, text in enumerate(texts, 1):\n#             print(f\"  {i}. {text[:100]}...\")  # Truncate long text\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bs4", "requests", "beautifulsoup4"], "package_breakdown": {"install_packages": ["bs4", "requests", "beautifulsoup4"], "import_packages": ["bs4", "requests"], "llm_suggested": ["beautifulsoup4", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads data from an Excel 2010 xlsx file, extracts specific columns, and writes the extracted data to a new Excel 2010 xlsm file.", "prompt_index": 98, "generated_code": "```python\nimport pandas as pd\nfrom openpyxl import Workbook\n\n# Read data from Excel 2010 xlsx file\ndf = pd.read_excel('input_file.xlsx', sheet_name='Sheet1')\n\n# Extract specific columns (replace with your desired column names)\nextracted_data = df[['Column1', 'Column2', 'Column3']]\n\n# Write extracted data to new Excel 2010 xlsm file\nwith pd.ExcelWriter('output_file.xlsm', engine='openpyxl') as writer:\n    extracted_data.to_excel(writer, sheet_name='ExtractedData', index=False)\n```\n\nAlternative approach using openpyxl directly:\n\n```python\nfrom openpyxl import load_workbook, Workbook\n\n# Read data from xlsx file\nsource_wb = load_workbook('input_file.xlsx')\nsource_ws = source_wb.active\n\n# Create new workbook for xlsm file\ntarget_wb = Workbook()\ntarget_ws = target_wb.active\n\n# Define columns to extract (by column letters or indices)\ncolumns_to_extract = ['A', 'C', 'E']  # or [1, 3, 5] for column indices\n\n# Extract and copy data\nfor row_num in range(1, source_ws.max_row + 1):\n    for col_idx, col_letter in enumerate(columns_to_extract, 1):\n        cell_value = source_ws[f'{col_letter}{row_num}'].value\n        target_ws.cell(row=row_num, column=col_idx, value=cell_value)\n\n# Save as xlsm file\ntarget_wb.save('output_file.xlsm')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["openpyxl", "pandas"], "package_breakdown": {"install_packages": ["openpyxl", "pandas"], "import_packages": ["openpyxl", "pandas"], "llm_suggested": ["pandas", "openpyxl"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that implements a Happy Eyeballs algorithm for asyncio, allowing for simultaneous connection attempts to multiple IP addresses for improved network performance and reliability.", "prompt_index": 99, "generated_code": "```python\nimport asyncio\nimport socket\nimport time\nfrom typing import List, Tuple, Optional, Union\n\nclass HappyEyeballsConnector:\n    def __init__(self, delay: float = 0.25, timeout: float = 10.0):\n        \"\"\"\n        Initialize Happy Eyeballs connector.\n        \n        Args:\n            delay: Delay between IPv6 and IPv4 connection attempts (seconds)\n            timeout: Total timeout for all connection attempts (seconds)\n        \"\"\"\n        self.delay = delay\n        self.timeout = timeout\n    \n    async def connect(self, host: str, port: int) -> Tuple[asyncio.StreamReader, asyncio.StreamWriter]:\n        \"\"\"\n        Connect to host:port using Happy Eyeballs algorithm.\n        \n        Args:\n            host: Hostname or IP address\n            port: Port number\n            \n        Returns:\n            Tuple of (reader, writer) for the established connection\n            \n        Raises:\n            ConnectionError: If no connection could be established\n        \"\"\"\n        # Resolve addresses\n        addresses = await self._resolve_addresses(host, port)\n        if not addresses:\n            raise ConnectionError(f\"Could not resolve {host}\")\n        \n        # Separate IPv6 and IPv4 addresses\n        ipv6_addrs = [addr for addr in addresses if addr[0] == socket.AF_INET6]\n        ipv4_addrs = [addr for addr in addresses if addr[0] == socket.AF_INET]\n        \n        # Start connection attempts\n        tasks = []\n        start_time = time.time()\n        \n        try:\n            # Start IPv6 connections immediately\n            for addr in ipv6_addrs:\n                task = asyncio.create_task(self._connect_to_address(addr))\n                tasks.append(task)\n            \n            # Wait for delay before starting IPv4 connections\n            if ipv4_addrs and self.delay > 0:\n                await asyncio.sleep(self.delay)\n            \n            # Start IPv4 connections\n            for addr in ipv4_addrs:\n                # Check if we've exceeded timeout\n                if time.time() - start_time >= self.timeout:\n                    break\n                task = asyncio.create_task(self._connect_to_address(addr))\n                tasks.append(task)\n            \n            # Wait for first successful connection\n            if tasks:\n                done, pending = await asyncio.wait(\n                    tasks, \n                    return_when=asyncio.FIRST_COMPLETED,\n                    timeout=max(0, self.timeout - (time.time() - start_time))\n                )\n                \n                # Cancel remaining tasks\n                for task in pending:\n                    task.cancel()\n                \n                # Check for successful connection\n                for task in done:\n                    try:\n                        result = task.result()\n                        if result:\n                            return result\n                    except Exception:\n                        continue\n            \n            raise ConnectionError(f\"Could not connect to {host}:{port}\")\n            \n        finally:\n            # Ensure all tasks are cancelled\n            for task in tasks:\n                if not task.done():\n                    task.cancel()\n            \n            # Wait for cancellation to complete\n            if tasks:\n                await asyncio.gather(*tasks, return_exceptions=True)\n    \n    async def _resolve_addresses(self, host: str, port: int) -> List[Tuple]:\n        \"\"\"Resolve hostname to list of addresses.\"\"\"\n        try:\n            addr_info = await asyncio.get_event_loop().getaddrinfo(\n                host, port, \n                family=socket.AF_UNSPEC,\n                type=socket.SOCK_STREAM\n            )\n            return addr_info\n        except socket.gaierror:\n            return []\n    \n    async def _connect_to_address(self, addr_info: Tuple) -> Optional[Tuple[asyncio.StreamReader, asyncio.StreamWriter]]:\n        \"\"\"Attempt to connect to a specific address.\"\"\"\n        family, type_, proto, canonname, sockaddr = addr_info\n        \n        try:\n            reader, writer = await asyncio.open_connection(\n                host=sockaddr[0], \n                port=sockaddr[1],\n                family=family\n            )\n            return reader, writer\n        except Exception:\n            return None\n\n\nclass HappyEyeballsPool:\n    \"\"\"Connection pool with Happy Eyeballs support.\"\"\"\n    \n    def __init__(self, max_connections: int = 10, **connector_kwargs):\n        self.max_connections = max_connections\n        self.connector = HappyEyeballsConnector(**connector_kwargs)\n        self._connections = {}\n        self._semaphore = asyncio.Semaphore(max_connections)\n    \n    async def get_connection(self, host: str, port: int) -> Tuple[asyncio.StreamReader, asyncio.StreamWriter]:\n        \"\"\"Get a connection from the pool or create a new one.\"\"\"\n        key = (host, port)\n        \n        async with self._semaphore:\n            if key in self._connections:\n                reader, writer = self._connections[key]\n                if not writer.is_closing():\n                    return reader, writer\n                else:\n                    del self._connections[key]\n            \n            # Create new connection\n            reader, writer = await self.connector.connect(host, port)\n            self._connections[key] = (reader, writer)\n            return reader, writer\n    \n    def close_connection(self, host: str, port: int):\n        \"\"\"Close a specific connection.\"\"\"\n        key = (host, port)\n        if key in self._connections:\n            _, writer = self._connections[key]\n            writer.close()\n            del self._connections[key]\n    \n    async def close_all(self):\n        \"\"\"Close all connections in the pool.\"\"\"\n        for _, writer in self._connections.values():\n            writer.close()\n        \n        # Wait for all connections to close\n        tasks = []\n        for _, writer in self._connections.values():\n            tasks.append(writer.wait_closed())\n        \n        if tasks:\n            await asyncio.gather(*tasks, return_exceptions=True)\n        \n        self._connections.clear()\n\n\n# Example usage and testing\nasync def example_usage():\n    \"\"\"Example of how to use the Happy Eyeballs connector.\"\"\"\n    \n    # Basic connection\n    connector = HappyEyeballsConnector(delay=0.25, timeout=10.0)\n    \n    try:\n        reader, writer = await connector.connect(\"google.com\", 80)\n        \n        # Send HTTP request\n        request = b\"GET / HTTP/1.1\\r\\nHost: google.com\\r\\nConnection: close\\r\\n\\r\\n\"\n        writer.write(request)\n        await writer.drain()\n        \n        # Read response\n        response = await reader.read(1024)\n        print(f\"Response: {response[:100]}...\")\n        \n        writer.close()\n        await writer.wait_closed()\n        \n    except ConnectionError as e:\n        print(f\"Connection failed: {e}\")\n\n\nasync def example_with_pool():\n    \"\"\"Example using connection pool.\"\"\"\n    \n    pool = HappyEyeballsPool(max_connections=5, delay=0.25, timeout=10.0)\n    \n    try:\n        # Multiple requests using the same pool\n        for i in range(3):\n            reader, writer = await pool.get_connection(\"httpbin.org\", 80)\n            \n            request = f\"GET /get?param={i} HTTP/1.1\\r\\nHost: httpbin.org\\r\\nConnection: keep-alive\\r\\n\\r\\n\".encode()\n            writer.write(request)\n            await writer.drain()\n            \n            response = await reader.read(1024)\n            print(f\"Request {i}: {len(response)} bytes received\")\n    \n    finally:\n        await pool.close_all()\n\n\n# Performance testing function\nasync def benchmark_connections(hosts: List[str], port: int = 80, concurrent: int = 5):\n    \"\"\"Benchmark Happy Eyeballs against regular connections.\"\"\"\n    \n    async def regular_connect(host: str, port: int):\n        start = time.time()\n        try:\n            reader, writer = await asyncio.open_connection(host, port)\n            writer.close()\n            await writer.wait_closed()\n            return time.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "typing", "socket", "time"], "package_breakdown": {"install_packages": ["asyncio", "typing", "socket", "time"], "import_packages": ["socket", "asyncio", "time", "typing"], "llm_suggested": ["asyncio", "socket", "time", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads an XML file using the lxml.xmlfile implementation from the standard library and extracts specific elements or attributes from the file for further processing.", "prompt_index": 101, "generated_code": "```python\nfrom lxml import etree\nfrom lxml.etree import xmlfile\nimport io\n\n# Method 1: Using lxml.etree to parse and extract elements/attributes\ndef extract_with_etree(xml_file_path):\n    # Parse the XML file\n    tree = etree.parse(xml_file_path)\n    root = tree.getroot()\n    \n    # Extract specific elements by tag name\n    titles = root.xpath('//title')\n    for title in titles:\n        print(f\"Title: {title.text}\")\n    \n    # Extract elements with specific attributes\n    items_with_id = root.xpath('//item[@id]')\n    for item in items_with_id:\n        print(f\"Item ID: {item.get('id')}, Text: {item.text}\")\n    \n    # Extract specific attribute values\n    all_ids = root.xpath('//@id')\n    print(f\"All IDs: {all_ids}\")\n    \n    return root\n\n# Method 2: Using xmlfile for incremental parsing (memory efficient for large files)\ndef extract_with_xmlfile(xml_file_path):\n    extracted_data = []\n    \n    with open(xml_file_path, 'rb') as f:\n        with xmlfile(f) as xf:\n            # Parse incrementally\n            for event, elem in etree.iterparse(f, events=('start', 'end')):\n                if event == 'end':\n                    # Extract specific elements\n                    if elem.tag == 'title':\n                        extracted_data.append({\n                            'type': 'title',\n                            'text': elem.text,\n                            'attributes': elem.attrib\n                        })\n                    \n                    # Extract elements with specific attributes\n                    elif elem.tag == 'item' and 'id' in elem.attrib:\n                        extracted_data.append({\n                            'type': 'item',\n                            'id': elem.get('id'),\n                            'text': elem.text,\n                            'attributes': elem.attrib\n                        })\n                    \n                    # Clear element to free memory\n                    elem.clear()\n    \n    return extracted_data\n\n# Method 3: Create and write XML using xmlfile\ndef create_sample_xml(output_path):\n    with xmlfile(output_path, encoding='utf-8') as xf:\n        with xf.element('root'):\n            with xf.element('book', id='1', category='fiction'):\n                xf.write('title', 'Sample Book Title')\n                xf.write('author', 'John Doe')\n                with xf.element('metadata'):\n                    xf.write('year', '2023')\n                    xf.write('pages', '300')\n            \n            with xf.element('book', id='2', category='science'):\n                xf.write('title', 'Python Programming')\n                xf.write('author', 'Jane Smith')\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create sample XML file\n    sample_file = 'sample.xml'\n    create_sample_xml(sample_file)\n    \n    print(\"Method 1 - Using etree:\")\n    root = extract_with_etree(sample_file)\n    \n    print(\"\\nMethod 2 - Using xmlfile incremental parsing:\")\n    data = extract_with_xmlfile(sample_file)\n    for item in data:\n        print(f\"Extracted: {item}\")\n    \n    # Advanced extraction examples\n    print(\"\\nAdvanced extractions:\")\n    tree = etree.parse(sample_file)\n    \n    # Extract books by category\n    fiction_books = tree.xpath('//book[@category=\"fiction\"]')\n    print(f\"Fiction books: {len(fiction_books)}\")\n    \n    # Extract all text content from specific elements\n    all_titles = tree.xpath('//title/text()')\n    print(f\"All titles: {all_titles}\")\n    \n    # Extract elements with complex conditions\n    books_with_year = tree.xpath('//book[metadata/year]')\n    for book in books_with_year:\n        title = book.find('title').text\n        year = book.find('metadata/year').text\n        print(f\"Book: {title}, Year: {year}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "lxml"], "package_breakdown": {"install_packages": ["io", "lxml"], "import_packages": ["io", "lxml"], "llm_suggested": ["lxml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the additional routines provided by the package to perform advanced operations on iterables, such as filtering, mapping, and reducing, in a more efficient and concise manner than the standard itertools module.", "prompt_index": 102, "generated_code": "```python\nfrom itertools import compress, accumulate, chain, groupby, islice, takewhile, dropwhile\nfrom functools import reduce\nfrom operator import mul, add\nimport heapq\n\n# Advanced filtering operations\ndef advanced_filter_operations():\n    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    # Multi-condition filtering with compress\n    mask = [x % 2 == 0 and x > 4 for x in data]\n    filtered = list(compress(data, mask))\n    \n    # Partition data into two groups based on condition\n    def partition(predicate, iterable):\n        true_items, false_items = [], []\n        for item in iterable:\n            (true_items if predicate(item) else false_items).append(item)\n        return true_items, false_items\n    \n    evens, odds = partition(lambda x: x % 2 == 0, data)\n    \n    # Take while condition is true, drop while condition is true\n    ascending = takewhile(lambda x: x < 6, data)\n    after_drop = dropwhile(lambda x: x < 5, data)\n    \n    return filtered, evens, odds, list(ascending), list(after_drop)\n\n# Advanced mapping operations\ndef advanced_mapping_operations():\n    data = [1, 2, 3, 4, 5]\n    \n    # Cumulative operations with accumulate\n    cumsum = list(accumulate(data))  # Cumulative sum\n    cumprod = list(accumulate(data, mul))  # Cumulative product\n    cummax = list(accumulate(data, max))  # Running maximum\n    \n    # Multi-level mapping with nested comprehensions\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    flattened = list(chain.from_iterable(matrix))\n    \n    # Sliding window mapping\n    def sliding_window(iterable, n):\n        iterators = [iter(iterable)] * n\n        return zip(*iterators)\n    \n    windows = list(sliding_window(data, 3))\n    window_sums = [sum(window) for window in windows]\n    \n    return cumsum, cumprod, cummax, flattened, windows, window_sums\n\n# Advanced reducing operations\ndef advanced_reducing_operations():\n    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    # Multiple reductions in one pass\n    def multi_reduce(iterable):\n        total = count = maximum = minimum = 0\n        first = True\n        \n        for item in iterable:\n            total += item\n            count += 1\n            if first:\n                maximum = minimum = item\n                first = False\n            else:\n                maximum = max(maximum, item)\n                minimum = min(minimum, item)\n        \n        return total, count, maximum, minimum, total / count if count > 0 else 0\n    \n    sum_val, count_val, max_val, min_val, avg_val = multi_reduce(data)\n    \n    # Group-wise reductions\n    grouped_data = [('A', 1), ('A', 2), ('B', 3), ('B', 4), ('C', 5)]\n    grouped = groupby(grouped_data, key=lambda x: x[0])\n    group_sums = {k: sum(v[1] for v in group) for k, group in grouped}\n    \n    # Parallel reduction using heapq for top-k\n    top_3 = heapq.nlargest(3, data)\n    bottom_3 = heapq.nsmallest(3, data)\n    \n    return sum_val, max_val, min_val, avg_val, group_sums, top_3, bottom_3\n\n# Chaining operations for complex data processing\ndef chained_operations():\n    data = range(1, 101)\n    \n    # Complex pipeline: filter -> map -> reduce\n    result = reduce(add, \n                   map(lambda x: x**2, \n                       filter(lambda x: x % 3 == 0 and x % 5 != 0, data)))\n    \n    # Batch processing with islice\n    def process_in_batches(iterable, batch_size):\n        iterator = iter(iterable)\n        while True:\n            batch = list(islice(iterator, batch_size))\n            if not batch:\n                break\n            yield sum(batch)  # Process each batch\n    \n    batch_sums = list(process_in_batches(data, 10))\n    \n    return result, batch_sums\n\n# Memory-efficient operations with generators\ndef memory_efficient_operations():\n    # Generator-based filtering and mapping\n    def efficient_pipeline(data):\n        # Chain multiple operations without creating intermediate lists\n        filtered = (x for x in data if x % 2 == 0)\n        mapped = (x**2 for x in filtered)\n        accumulated = accumulate(mapped, add)\n        return accumulated\n    \n    data = range(1, 1000)\n    pipeline = efficient_pipeline(data)\n    \n    # Take only first 10 results to demonstrate efficiency\n    first_10 = list(islice(pipeline, 10))\n    \n    # Infinite sequence processing\n    def fibonacci():\n        a, b = 0, 1\n        while True:\n            yield a\n            a, b = b, a + b\n    \n    fib_under_100 = list(takewhile(lambda x: x < 100, fibonacci()))\n    \n    return first_10, fib_under_100\n\n# Advanced grouping and aggregation\ndef advanced_grouping():\n    transactions = [\n        ('2023-01', 'A', 100),\n        ('2023-01', 'B', 150),\n        ('2023-01', 'A', 200),\n        ('2023-02', 'A', 300),\n        ('2023-02', 'B', 250),\n        ('2023-02', 'C', 400)\n    ]\n    \n    # Multi-level grouping\n    from collections import defaultdict\n    \n    grouped = defaultdict(lambda: defaultdict(list))\n    for month, category, amount in transactions:\n        grouped[month][category].append(amount)\n    \n    # Aggregate by month and category\n    monthly_totals = {\n        month: {\n            category: sum(amounts) \n            for category, amounts in categories.items()\n        }\n        for month, categories in grouped.items()\n    }\n    \n    # Top categories by total amount\n    category_totals = defaultdict(int)\n    for month, category, amount in transactions:\n        category_totals[category] += amount\n    \n    top_categories = sorted(category_totals.items(), key=lambda x: x[1], reverse=True)\n    \n    return monthly_totals, top_categories\n\n# Execute all operations\nif __name__ == \"__main__\":\n    print(\"Advanced Filter Operations:\", advanced_filter_operations())\n    print(\"Advanced Mapping Operations:\", advanced_mapping_operations())\n    print(\"Advanced Reducing Operations:\", advanced_reducing_operations())\n    print(\"Chained Operations:\", chained_operations())\n    print(\"Memory Efficient Operations:\", memory_efficient_operations())\n    print(\"Advanced Grouping:\", advanced_grouping())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "itertools", "operator", "heapq", "collections"], "package_breakdown": {"install_packages": ["functools", "itertools", "operator", "heapq", "collections"], "import_packages": ["functools", "itertools", "operator", "heapq", "collections"], "llm_suggested": ["itertools", "functools", "operator", "heapq", "collections"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts reStructuredText documents into HTML, LaTeX, XML, or plaintext formats using the Docutils library.", "prompt_index": 103, "generated_code": "```python\nimport docutils.core\nimport docutils.writers.html4css1\nimport docutils.writers.latex2e\nimport docutils.writers.xml\nimport docutils.writers.plaintext\nfrom pathlib import Path\n\ndef convert_rst_to_html(rst_content, output_file=None):\n    \"\"\"Convert reStructuredText to HTML\"\"\"\n    html_output = docutils.core.publish_string(\n        rst_content,\n        writer_name='html'\n    ).decode('utf-8')\n    \n    if output_file:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(html_output)\n    \n    return html_output\n\ndef convert_rst_to_latex(rst_content, output_file=None):\n    \"\"\"Convert reStructuredText to LaTeX\"\"\"\n    latex_output = docutils.core.publish_string(\n        rst_content,\n        writer_name='latex'\n    ).decode('utf-8')\n    \n    if output_file:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(latex_output)\n    \n    return latex_output\n\ndef convert_rst_to_xml(rst_content, output_file=None):\n    \"\"\"Convert reStructuredText to XML\"\"\"\n    xml_output = docutils.core.publish_string(\n        rst_content,\n        writer_name='xml'\n    ).decode('utf-8')\n    \n    if output_file:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(xml_output)\n    \n    return xml_output\n\ndef convert_rst_to_plaintext(rst_content, output_file=None):\n    \"\"\"Convert reStructuredText to plaintext\"\"\"\n    # For plaintext, we need to use publish_parts\n    parts = docutils.core.publish_parts(\n        rst_content,\n        writer_name='html'\n    )\n    \n    # Extract just the body content and strip HTML tags\n    import re\n    plaintext_output = re.sub(r'<[^>]+>', '', parts['body'])\n    plaintext_output = plaintext_output.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')\n    \n    if output_file:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(plaintext_output)\n    \n    return plaintext_output\n\ndef convert_rst_file(input_file, output_format, output_file=None):\n    \"\"\"Convert RST file to specified format\"\"\"\n    with open(input_file, 'r', encoding='utf-8') as f:\n        rst_content = f.read()\n    \n    format_map = {\n        'html': convert_rst_to_html,\n        'latex': convert_rst_to_latex,\n        'xml': convert_rst_to_xml,\n        'plaintext': convert_rst_to_plaintext,\n        'txt': convert_rst_to_plaintext\n    }\n    \n    if output_format.lower() not in format_map:\n        raise ValueError(f\"Unsupported format: {output_format}\")\n    \n    converter = format_map[output_format.lower()]\n    \n    if not output_file:\n        input_path = Path(input_file)\n        output_file = input_path.with_suffix(f'.{output_format}')\n    \n    return converter(rst_content, output_file)\n\ndef batch_convert_rst(input_dir, output_format, output_dir=None):\n    \"\"\"Convert all RST files in a directory\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir) if output_dir else input_path\n    \n    rst_files = list(input_path.glob('*.rst'))\n    \n    for rst_file in rst_files:\n        output_file = output_path / f\"{rst_file.stem}.{output_format}\"\n        convert_rst_file(rst_file, output_format, output_file)\n        print(f\"Converted {rst_file} -> {output_file}\")\n\n# Advanced conversion with custom settings\ndef convert_rst_with_settings(rst_content, output_format, settings=None):\n    \"\"\"Convert RST with custom docutils settings\"\"\"\n    if settings is None:\n        settings = {}\n    \n    # Default settings\n    default_settings = {\n        'input_encoding': 'utf-8',\n        'output_encoding': 'utf-8',\n        'doctitle_xform': True,\n        'initial_header_level': 1,\n    }\n    \n    # HTML-specific settings\n    if output_format == 'html':\n        default_settings.update({\n            'embed_stylesheet': False,\n            'stylesheet_path': None,\n            'link_stylesheet': True,\n        })\n    \n    # LaTeX-specific settings\n    elif output_format == 'latex':\n        default_settings.update({\n            'documentclass': 'article',\n            'use_latex_toc': True,\n        })\n    \n    final_settings = {**default_settings, **settings}\n    \n    output = docutils.core.publish_string(\n        rst_content,\n        writer_name=output_format,\n        settings_overrides=final_settings\n    )\n    \n    return output.decode('utf-8')\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample RST content\n    sample_rst = \"\"\"\nMy Document\n===========\n\nThis is a sample reStructuredText document.\n\nSection 1\n---------\n\nHere's some **bold text** and *italic text*.\n\n- Item 1\n- Item 2\n- Item 3\n\nCode example::\n\n    def hello():\n        print(\"Hello, World!\")\n\n.. note::\n   This is a note.\n\n`Link to Python <https://python.org>`_\n\"\"\"\n\n    # Convert to different formats\n    html_result = convert_rst_to_html(sample_rst, \"output.html\")\n    latex_result = convert_rst_to_latex(sample_rst, \"output.tex\")\n    xml_result = convert_rst_to_xml(sample_rst, \"output.xml\")\n    plaintext_result = convert_rst_to_plaintext(sample_rst, \"output.txt\")\n    \n    print(\"Conversions completed!\")\n    \n    # Convert a file\n    # convert_rst_file(\"document.rst\", \"html\", \"document.html\")\n    \n    # Batch convert all RST files in a directory\n    # batch_convert_rst(\"docs/\", \"html\", \"output/\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "docutils", "re"], "package_breakdown": {"install_packages": ["pathlib", "docutils", "re"], "import_packages": ["docutils", "pathlib", "re"], "llm_suggested": ["docutils", "pathlib", "re"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts a Markdown string into HTML using the markdown-it library, ensuring accurate and precise parsing of Markdown syntax.", "prompt_index": 104, "generated_code": "```python\nfrom markdown_it import MarkdownIt\n\ndef markdown_to_html(markdown_text):\n    \"\"\"Convert Markdown string to HTML using markdown-it library.\"\"\"\n    md = MarkdownIt()\n    html = md.render(markdown_text)\n    return html\n\n# Example usage\nmarkdown_content = \"\"\"\n# Heading 1\n## Heading 2\n\nThis is a **bold** text and this is *italic* text.\n\n- List item 1\n- List item 2\n- List item 3\n\n1. Numbered item 1\n2. Numbered item 2\n\n[Link text](https://example.com)\n\n`inline code`\n\n```python\n# Code block\ndef hello():\n    print(\"Hello, World!\")\n```\n\n> This is a blockquote\n\n| Column 1 | Column 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n\"\"\"\n\n# Convert to HTML\nhtml_output = markdown_to_html(markdown_content)\nprint(html_output)\n```\n\nFor more advanced features and plugins:\n\n```python\nfrom markdown_it import MarkdownIt\nfrom markdown_it.extensions.table import table\nfrom markdown_it.extensions.strikethrough import strikethrough\n\ndef advanced_markdown_to_html(markdown_text):\n    \"\"\"Convert Markdown to HTML with additional features enabled.\"\"\"\n    md = (MarkdownIt('commonmark', {'breaks': True, 'linkify': True})\n          .enable(['table', 'strikethrough'])\n          .disable(['image']))\n    \n    html = md.render(markdown_text)\n    return html\n\n# Example with advanced features\nadvanced_markdown = \"\"\"\n# Advanced Example\n\n~~strikethrough text~~\n\nAuto-link: https://example.com\n\n| Feature | Status |\n|---------|--------|\n| Tables  | \u2713      |\n| Links   | \u2713      |\n\"\"\"\n\nhtml_result = advanced_markdown_to_html(advanced_markdown)\nprint(html_result)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["markdown_it"], "packages_detected": ["markdown-it-py", "markdown_it"], "package_breakdown": {"install_packages": ["markdown-it-py", "markdown_it"], "import_packages": ["markdown_it"], "llm_suggested": ["markdown-it-py"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts all URLs from a Markdown file and prints them out as a list.", "prompt_index": 105, "generated_code": "```python\nimport re\n\ndef extract_urls_from_markdown(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n    \n    # Pattern for markdown links: [text](url)\n    markdown_links = re.findall(r'\\[.*?\\]\\((.*?)\\)', content)\n    \n    # Pattern for plain URLs\n    plain_urls = re.findall(r'https?://[^\\s\\)]+', content)\n    \n    # Combine and remove duplicates\n    all_urls = list(set(markdown_links + plain_urls))\n    \n    return all_urls\n\n# Usage\nfile_path = 'your_markdown_file.md'\nurls = extract_urls_from_markdown(file_path)\n\nfor url in urls:\n    print(url)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re"], "package_breakdown": {"install_packages": ["re"], "import_packages": ["re"], "llm_suggested": ["re"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses a given string containing an ISO 8601 formatted date, time, or duration and returns it in a structured format for further processing or display.", "prompt_index": 106, "generated_code": "```python\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Union, Optional\n\ndef parse_iso8601(iso_string: str) -> Dict[str, Union[datetime, timedelta, Dict]]:\n    \"\"\"\n    Parse ISO 8601 formatted date, time, or duration string.\n    \n    Returns a dictionary with parsed components and type information.\n    \"\"\"\n    iso_string = iso_string.strip()\n    \n    # Check if it's a duration (starts with P)\n    if iso_string.startswith('P'):\n        return parse_duration(iso_string)\n    \n    # Check if it's a date/time\n    try:\n        # Try parsing as datetime with various ISO formats\n        dt = parse_datetime(iso_string)\n        return {\n            'type': 'datetime',\n            'parsed': dt,\n            'components': {\n                'year': dt.year,\n                'month': dt.month,\n                'day': dt.day,\n                'hour': dt.hour,\n                'minute': dt.minute,\n                'second': dt.second,\n                'microsecond': dt.microsecond,\n                'timezone': dt.tzinfo\n            }\n        }\n    except ValueError:\n        raise ValueError(f\"Invalid ISO 8601 format: {iso_string}\")\n\ndef parse_datetime(iso_string: str) -> datetime:\n    \"\"\"Parse ISO 8601 date/time formats.\"\"\"\n    # Common ISO 8601 formats\n    formats = [\n        '%Y-%m-%dT%H:%M:%S.%fZ',      # 2023-12-25T14:30:45.123456Z\n        '%Y-%m-%dT%H:%M:%SZ',         # 2023-12-25T14:30:45Z\n        '%Y-%m-%dT%H:%M:%S.%f',       # 2023-12-25T14:30:45.123456\n        '%Y-%m-%dT%H:%M:%S',          # 2023-12-25T14:30:45\n        '%Y-%m-%dT%H:%M',             # 2023-12-25T14:30\n        '%Y-%m-%d',                   # 2023-12-25\n        '%Y-%m',                      # 2023-12\n        '%Y',                         # 2023\n    ]\n    \n    # Handle timezone offsets (+/-HH:MM)\n    tz_pattern = r'([+-]\\d{2}):?(\\d{2})$'\n    tz_match = re.search(tz_pattern, iso_string)\n    \n    if tz_match:\n        # Remove timezone for parsing, handle separately\n        base_string = re.sub(tz_pattern, '', iso_string)\n        tz_hours = int(tz_match.group(1))\n        tz_minutes = int(tz_match.group(2))\n        \n        # Parse base datetime\n        dt = None\n        for fmt in formats:\n            try:\n                dt = datetime.strptime(base_string, fmt)\n                break\n            except ValueError:\n                continue\n        \n        if dt is None:\n            raise ValueError(\"Could not parse datetime part\")\n        \n        # Apply timezone offset\n        offset = timedelta(hours=abs(tz_hours), minutes=tz_minutes)\n        if tz_hours < 0:\n            offset = -offset\n        \n        # Create timezone-aware datetime\n        from datetime import timezone\n        tz = timezone(offset)\n        dt = dt.replace(tzinfo=tz)\n        return dt\n    \n    # Try parsing without timezone\n    for fmt in formats:\n        try:\n            return datetime.strptime(iso_string, fmt)\n        except ValueError:\n            continue\n    \n    raise ValueError(\"Could not parse datetime\")\n\ndef parse_duration(duration_string: str) -> Dict[str, Union[timedelta, Dict]]:\n    \"\"\"Parse ISO 8601 duration format (P[n]Y[n]M[n]DT[n]H[n]M[n]S).\"\"\"\n    pattern = r'P(?:(\\d+)Y)?(?:(\\d+)M)?(?:(\\d+)D)?(?:T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+(?:\\.\\d+)?)S)?)?'\n    match = re.match(pattern, duration_string)\n    \n    if not match:\n        raise ValueError(f\"Invalid ISO 8601 duration format: {duration_string}\")\n    \n    years, months, days, hours, minutes, seconds = match.groups()\n    \n    # Convert to integers/floats, defaulting to 0\n    years = int(years) if years else 0\n    months = int(months) if months else 0\n    days = int(days) if days else 0\n    hours = int(hours) if hours else 0\n    minutes = int(minutes) if minutes else 0\n    seconds = float(seconds) if seconds else 0\n    \n    # Create timedelta (note: timedelta doesn't support years/months directly)\n    total_days = days + (years * 365) + (months * 30)  # Approximate\n    td = timedelta(\n        days=total_days,\n        hours=hours,\n        minutes=minutes,\n        seconds=seconds\n    )\n    \n    return {\n        'type': 'duration',\n        'parsed': td,\n        'components': {\n            'years': years,\n            'months': months,\n            'days': days,\n            'hours': hours,\n            'minutes': minutes,\n            'seconds': seconds,\n            'total_seconds': td.total_seconds()\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test cases\n    test_strings = [\n        \"2023-12-25T14:30:45.123456Z\",\n        \"2023-12-25T14:30:45+02:00\",\n        \"2023-12-25\",\n        \"P1Y2M3DT4H5M6S\",\n        \"PT2H30M\",\n        \"P7D\"\n    ]\n    \n    for test_string in test_strings:\n        try:\n            result = parse_iso8601(test_string)\n            print(f\"Input: {test_string}\")\n            print(f\"Type: {result['type']}\")\n            print(f\"Parsed: {result['parsed']}\")\n            print(f\"Components: {result['components']}\")\n            print(\"-\" * 50)\n        except ValueError as e:\n            print(f\"Error parsing '{test_string}': {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "re", "typing"], "package_breakdown": {"install_packages": ["datetime", "re", "typing"], "import_packages": ["datetime", "re", "typing"], "llm_suggested": ["re", "datetime", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses an XML file using the ElementTree API in combination with libxml2/libxslt to extract specific data and manipulate the XML structure.", "prompt_index": 107, "generated_code": "```python\nimport xml.etree.ElementTree as ET\nfrom lxml import etree, html\nimport libxml2\nimport libxslt\n\n# Method 1: Using ElementTree for basic XML parsing\ndef parse_with_elementtree(xml_file):\n    \"\"\"Parse XML file using ElementTree and extract specific data\"\"\"\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    \n    # Extract specific data\n    extracted_data = []\n    for element in root.findall('.//book'):  # Find all book elements\n        book_data = {\n            'title': element.find('title').text if element.find('title') is not None else '',\n            'author': element.find('author').text if element.find('author') is not None else '',\n            'price': element.find('price').text if element.find('price') is not None else ''\n        }\n        extracted_data.append(book_data)\n    \n    # Manipulate XML structure - add new element\n    new_book = ET.SubElement(root, 'book')\n    new_book.set('id', 'new_book')\n    title = ET.SubElement(new_book, 'title')\n    title.text = 'New Book Title'\n    \n    # Save modified XML\n    tree.write('modified_books.xml', encoding='utf-8', xml_declaration=True)\n    return extracted_data\n\n# Method 2: Using libxml2 for more advanced parsing\ndef parse_with_libxml2(xml_file):\n    \"\"\"Parse XML file using libxml2 and extract data\"\"\"\n    doc = libxml2.parseFile(xml_file)\n    \n    # Extract specific data using XPath\n    books = doc.xpathEval('//book')\n    extracted_data = []\n    \n    for book in books:\n        title_nodes = book.xpathEval('./title')\n        author_nodes = book.xpathEval('./author')\n        price_nodes = book.xpathEval('./price')\n        \n        book_data = {\n            'title': title_nodes[0].content if title_nodes else '',\n            'author': author_nodes[0].content if author_nodes else '',\n            'price': price_nodes[0].content if price_nodes else ''\n        }\n        extracted_data.append(book_data)\n    \n    # Manipulate XML structure\n    root = doc.getRootElement()\n    new_book = root.newChild(None, 'book', None)\n    new_book.setProp('id', 'libxml2_book')\n    new_book.newChild(None, 'title', 'Book added via libxml2')\n    \n    # Save modified XML\n    doc.saveFile('modified_libxml2.xml')\n    doc.freeDoc()\n    \n    return extracted_data\n\n# Method 3: Using lxml (which uses libxml2/libxslt under the hood)\ndef parse_with_lxml(xml_file):\n    \"\"\"Parse XML file using lxml and apply XSLT transformation\"\"\"\n    # Parse XML\n    with open(xml_file, 'r') as f:\n        doc = etree.parse(f)\n    \n    # Extract specific data using XPath\n    books = doc.xpath('//book')\n    extracted_data = []\n    \n    for book in books:\n        book_data = {\n            'title': book.xpath('./title/text()')[0] if book.xpath('./title/text()') else '',\n            'author': book.xpath('./author/text()')[0] if book.xpath('./author/text()') else '',\n            'price': book.xpath('./price/text()')[0] if book.xpath('./price/text()') else ''\n        }\n        extracted_data.append(book_data)\n    \n    # Manipulate XML structure\n    root = doc.getroot()\n    new_book = etree.SubElement(root, 'book')\n    new_book.set('id', 'lxml_book')\n    title = etree.SubElement(new_book, 'title')\n    title.text = 'Book added via lxml'\n    \n    # Apply XSLT transformation\n    xslt_doc = etree.parse('transform.xsl')\n    transform = etree.XSLT(xslt_doc)\n    result = transform(doc)\n    \n    # Save results\n    with open('modified_lxml.xml', 'wb') as f:\n        f.write(etree.tostring(doc, pretty_print=True, encoding='utf-8', xml_declaration=True))\n    \n    with open('transformed.html', 'wb') as f:\n        f.write(etree.tostring(result, pretty_print=True, encoding='utf-8'))\n    \n    return extracted_data\n\n# Method 4: Combined approach with XSLT processing\ndef advanced_xml_processing(xml_file, xsl_file):\n    \"\"\"Advanced XML processing combining libxml2 and libxslt\"\"\"\n    # Parse XML and XSL documents\n    xml_doc = libxml2.parseFile(xml_file)\n    xsl_doc = libxml2.parseFile(xsl_file)\n    \n    # Create XSLT stylesheet\n    style = libxslt.parseStylesheetDoc(xsl_doc)\n    \n    # Apply transformation\n    result = style.applyStylesheet(xml_doc, None)\n    \n    # Extract data from original document\n    books = xml_doc.xpathEval('//book[@category=\"fiction\"]')\n    fiction_books = []\n    \n    for book in books:\n        title = book.xpathEval('./title/text()')[0] if book.xpathEval('./title/text()') else 'Unknown'\n        author = book.xpathEval('./author/text()')[0] if book.xpathEval('./author/text()') else 'Unknown'\n        fiction_books.append({'title': title, 'author': author})\n    \n    # Save transformed result\n    style.saveResultToFilename('transformed_output.xml', result, 0)\n    \n    # Cleanup\n    style.freeStylesheet()\n    xml_doc.freeDoc()\n    result.freeDoc()\n    \n    return fiction_books\n\n# Sample XSLT stylesheet content (save as transform.xsl)\nxslt_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n    <xsl:template match=\"/\">\n        <html>\n            <body>\n                <h2>Book Catalog</h2>\n                <table border=\"1\">\n                    <tr>\n                        <th>Title</th>\n                        <th>Author</th>\n                        <th>Price</th>\n                    </tr>\n                    <xsl:for-each select=\"//book\">\n                        <tr>\n                            <td><xsl:value-of select=\"title\"/></td>\n                            <td><xsl:value-of select=\"author\"/></td>\n                            <td><xsl:value-of select=\"price\"/></td>\n                        </tr>\n                    </xsl:for-each>\n                </table>\n            </body>\n        </html>\n    </xsl:template>\n</xsl:stylesheet>\"\"\"\n\n# Create sample XSLT file\nwith open('transform.xsl', 'w') as f:\n    f.write(xslt_content)\n\n# Sample XML content for testing\nsample_xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<bookstore>\n    <book id=\"1\" category=\"fiction\">\n        <title>Great Gatsby</title>\n        <author>F. Scott Fitzgerald</author>\n        <price>12.99</price>\n    </book>\n    <book id=\"2\" category=\"fiction\">\n        <title>1984</title>\n        <author>George Orwell</author>\n        <price>13.99</price>\n    </book>\n    <book id=\"3\" category=\"web\">\n        <title>Learning XML</title>\n        <author>Erik T. Ray</author>\n        <price>39.95</price>\n    </book>\n</bookstore>\"\"\"\n\n# Create sample XML file\nwith", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 3, "hallucinations": ["xml", "libxml2", "libxslt"], "packages_detected": ["xml", "libxml2", "lxml", "libxslt"], "package_breakdown": {"install_packages": ["xml", "libxml2", "lxml", "libxslt"], "import_packages": ["xml", "libxslt", "libxml2", "lxml"], "llm_suggested": ["lxml", "libxml2", "libxslt"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uploads a local file named 'example.txt' to a specific bucket in Google Cloud Storage using the Google Cloud Storage API client library.", "prompt_index": 108, "generated_code": "```python\nfrom google.cloud import storage\n\n# Initialize the client\nclient = storage.Client()\n\n# Specify bucket name\nbucket_name = 'your-bucket-name'\nbucket = client.bucket(bucket_name)\n\n# Specify the local file and destination blob name\nlocal_file_path = 'example.txt'\ndestination_blob_name = 'example.txt'\n\n# Create a blob object and upload the file\nblob = bucket.blob(destination_blob_name)\nblob.upload_from_filename(local_file_path)\n\nprint(f\"File {local_file_path} uploaded to {destination_blob_name}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-storage"], "package_breakdown": {"install_packages": ["google", "google-cloud-storage"], "import_packages": ["google"], "llm_suggested": ["google-cloud-storage"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses OpenTelemetry Semantic Conventions to instrument a Flask application for distributed tracing with automatic context propagation and correlation of traces across services.", "prompt_index": 109, "generated_code": "```python\nfrom flask import Flask, request, jsonify\nimport requests\nimport time\nimport logging\n\nfrom opentelemetry import trace, baggage, context\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.resource import ResourceAttributes\nfrom opentelemetry.semconv.trace import SpanAttributes\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.logging import LoggingInstrumentor\nfrom opentelemetry.propagate import inject, extract\n\n# Configure OpenTelemetry\ndef configure_opentelemetry(service_name: str, service_version: str = \"1.0.0\"):\n    # Create resource with semantic conventions\n    resource = Resource.create({\n        ResourceAttributes.SERVICE_NAME: service_name,\n        ResourceAttributes.SERVICE_VERSION: service_version,\n        ResourceAttributes.SERVICE_INSTANCE_ID: f\"{service_name}-instance-1\",\n        ResourceAttributes.DEPLOYMENT_ENVIRONMENT: \"development\"\n    })\n    \n    # Set up tracer provider\n    trace.set_tracer_provider(TracerProvider(resource=resource))\n    tracer_provider = trace.get_tracer_provider()\n    \n    # Configure Jaeger exporter\n    jaeger_exporter = JaegerExporter(\n        agent_host_name=\"localhost\",\n        agent_port=6831,\n    )\n    \n    # Add span processor\n    span_processor = BatchSpanProcessor(jaeger_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    \n    # Auto-instrument Flask and requests\n    FlaskInstrumentor().instrument()\n    RequestsInstrumentor().instrument()\n    LoggingInstrumentor().instrument(set_logging_format=True)\n    \n    return trace.get_tracer(__name__)\n\n# Initialize Flask app\napp = Flask(__name__)\ntracer = configure_opentelemetry(\"user-service\")\n\n# Custom middleware for additional context propagation\n@app.before_request\ndef before_request():\n    # Extract context from incoming request headers\n    ctx = extract(request.headers)\n    context.attach(ctx)\n    \n    # Add custom baggage\n    ctx = baggage.set_baggage(\"user.id\", request.headers.get(\"X-User-ID\", \"anonymous\"), ctx)\n    context.attach(ctx)\n\n@app.after_request\ndef after_request(response):\n    # Inject context into response headers for downstream services\n    headers = {}\n    inject(headers)\n    for key, value in headers.items():\n        response.headers[key] = value\n    return response\n\n# Database simulation with custom instrumentation\ndef get_user_from_db(user_id: str):\n    with tracer.start_as_current_span(\"database.query\") as span:\n        # Add semantic convention attributes\n        span.set_attributes({\n            SpanAttributes.DB_SYSTEM: \"postgresql\",\n            SpanAttributes.DB_NAME: \"users\",\n            SpanAttributes.DB_OPERATION: \"SELECT\",\n            SpanAttributes.DB_SQL_TABLE: \"users\",\n            \"db.query\": f\"SELECT * FROM users WHERE id = '{user_id}'\"\n        })\n        \n        # Simulate database delay\n        time.sleep(0.1)\n        \n        # Add custom attributes\n        span.set_attribute(\"user.id\", user_id)\n        span.set_attribute(\"db.rows_affected\", 1)\n        \n        return {\"id\": user_id, \"name\": f\"User {user_id}\", \"email\": f\"user{user_id}@example.com\"}\n\n# External service call with context propagation\ndef call_external_service(user_id: str):\n    with tracer.start_as_current_span(\"external.service.call\") as span:\n        # Add HTTP semantic convention attributes\n        span.set_attributes({\n            SpanAttributes.HTTP_METHOD: \"GET\",\n            SpanAttributes.HTTP_URL: f\"http://profile-service/users/{user_id}/profile\",\n            SpanAttributes.HTTP_SCHEME: \"http\",\n            SpanAttributes.HTTP_HOST: \"profile-service\",\n            SpanAttributes.HTTP_TARGET: f\"/users/{user_id}/profile\",\n            SpanAttributes.NET_PEER_NAME: \"profile-service\",\n            SpanAttributes.NET_PEER_PORT: 8080\n        })\n        \n        # Prepare headers for context propagation\n        headers = {}\n        inject(headers)\n        \n        try:\n            # Simulate external service call\n            response = requests.get(\n                f\"http://localhost:8081/users/{user_id}/profile\",\n                headers=headers,\n                timeout=5\n            )\n            \n            # Add response attributes\n            span.set_attributes({\n                SpanAttributes.HTTP_STATUS_CODE: response.status_code,\n                SpanAttributes.HTTP_RESPONSE_SIZE: len(response.content)\n            })\n            \n            if response.status_code >= 400:\n                span.set_status(trace.Status(trace.StatusCode.ERROR, f\"HTTP {response.status_code}\"))\n            \n            return response.json() if response.status_code == 200 else None\n            \n        except requests.exceptions.RequestException as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return None\n\n# Routes with semantic convention attributes\n@app.route(\"/health\")\ndef health_check():\n    with tracer.start_as_current_span(\"health.check\") as span:\n        span.set_attributes({\n            SpanAttributes.HTTP_ROUTE: \"/health\",\n            \"health.status\": \"ok\"\n        })\n        return {\"status\": \"healthy\", \"service\": \"user-service\"}\n\n@app.route(\"/users/<user_id>\")\ndef get_user(user_id: str):\n    with tracer.start_as_current_span(\"get_user\") as span:\n        # Add semantic convention attributes\n        span.set_attributes({\n            SpanAttributes.HTTP_ROUTE: \"/users/<user_id>\",\n            \"user.id\": user_id,\n            \"operation.type\": \"read\"\n        })\n        \n        # Add baggage from context\n        current_user = baggage.get_baggage(\"user.id\")\n        if current_user:\n            span.set_attribute(\"request.user.id\", current_user)\n        \n        try:\n            # Get user from database\n            user_data = get_user_from_db(user_id)\n            \n            # Get additional profile data from external service\n            profile_data = call_external_service(user_id)\n            \n            # Combine data\n            result = {\n                \"user\": user_data,\n                \"profile\": profile_data,\n                \"correlation_id\": span.get_span_context().trace_id\n            }\n            \n            span.set_attributes({\n                \"response.user.found\": True,\n                \"response.profile.found\": profile_data is not None\n            })\n            \n            return jsonify(result)\n            \n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return jsonify({\"error\": \"Internal server error\"}), 500\n\n@app.route(\"/users\", methods=[\"POST\"])\ndef create_user():\n    with tracer.start_as_current_span(\"create_user\") as span:\n        span.set_attributes({\n            SpanAttributes.HTTP_ROUTE: \"/users\",\n            SpanAttributes.HTTP_METHOD: \"POST\",\n            \"operation.type\": \"create\"\n        })\n        \n        try:\n            user_data = request.get_json()\n            \n            # Validate input\n            if not user_data or \"name\" not in user_data:\n                span.set_status(trace.Status(trace.StatusCode.ERROR, \"Invalid input\"))\n                return jsonify({\"error\": \"Name is required\"}), 400\n            \n            # Simulate user creation\n            with tracer.start_as_current_span(\"database.insert\") as", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 12, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-logging", "opentelemetry-instrumentation-flask", "time", "logging", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests", "opentelemetry-semantic-conventions"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-logging", "opentelemetry-instrumentation-flask", "time", "logging", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests", "opentelemetry-semantic-conventions"], "import_packages": ["opentelemetry", "flask", "time", "logging", "requests"], "llm_suggested": ["flask", "requests", "opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-logging", "opentelemetry-semantic-conventions"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a WSGI web application using the features and utilities provided by the comprehensive WSGI web application library.", "prompt_index": 110, "generated_code": "```python\nfrom wsgiref.simple_server import make_server\nfrom wsgiref.util import request_uri\nfrom urllib.parse import parse_qs\nimport json\n\nclass WSGIApp:\n    def __init__(self):\n        self.routes = {}\n    \n    def route(self, path, methods=['GET']):\n        def decorator(func):\n            self.routes[path] = {'handler': func, 'methods': methods}\n            return func\n        return decorator\n    \n    def __call__(self, environ, start_response):\n        path = environ.get('PATH_INFO', '/')\n        method = environ.get('REQUEST_METHOD', 'GET')\n        \n        # Route matching\n        if path in self.routes:\n            route_info = self.routes[path]\n            if method in route_info['methods']:\n                try:\n                    response_data = route_info['handler'](environ)\n                    status = '200 OK'\n                    headers = [('Content-Type', 'application/json')]\n                    start_response(status, headers)\n                    return [json.dumps(response_data).encode('utf-8')]\n                except Exception as e:\n                    status = '500 Internal Server Error'\n                    headers = [('Content-Type', 'text/plain')]\n                    start_response(status, headers)\n                    return [str(e).encode('utf-8')]\n            else:\n                status = '405 Method Not Allowed'\n                headers = [('Content-Type', 'text/plain')]\n                start_response(status, headers)\n                return [b'Method Not Allowed']\n        \n        # 404 Not Found\n        status = '404 Not Found'\n        headers = [('Content-Type', 'text/plain')]\n        start_response(status, headers)\n        return [b'Not Found']\n\n# Create application instance\napp = WSGIApp()\n\n@app.route('/')\ndef home(environ):\n    return {\n        'message': 'Welcome to WSGI App',\n        'uri': request_uri(environ),\n        'method': environ.get('REQUEST_METHOD')\n    }\n\n@app.route('/users', methods=['GET', 'POST'])\ndef users(environ):\n    method = environ.get('REQUEST_METHOD')\n    if method == 'GET':\n        query_params = parse_qs(environ.get('QUERY_STRING', ''))\n        return {\n            'users': ['alice', 'bob', 'charlie'],\n            'query_params': query_params\n        }\n    elif method == 'POST':\n        try:\n            content_length = int(environ.get('CONTENT_LENGTH', 0))\n            if content_length > 0:\n                post_data = environ['wsgi.input'].read(content_length).decode('utf-8')\n                return {\n                    'message': 'User created',\n                    'data': post_data\n                }\n        except ValueError:\n            pass\n        return {'message': 'User created (no data)'}\n\n@app.route('/info')\ndef server_info(environ):\n    return {\n        'server_name': environ.get('SERVER_NAME'),\n        'server_port': environ.get('SERVER_PORT'),\n        'remote_addr': environ.get('REMOTE_ADDR'),\n        'user_agent': environ.get('HTTP_USER_AGENT'),\n        'content_type': environ.get('CONTENT_TYPE'),\n        'request_uri': request_uri(environ)\n    }\n\n# Run the server\nif __name__ == '__main__':\n    server = make_server('localhost', 8000, app)\n    print(\"WSGI server running on http://localhost:8000\")\n    print(\"Available endpoints:\")\n    print(\"  GET  /\")\n    print(\"  GET  /info\")\n    print(\"  GET  /users\")\n    print(\"  POST /users\")\n    server.serve_forever()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "json", "wsgiref"], "package_breakdown": {"install_packages": ["urllib", "json", "wsgiref"], "import_packages": ["urllib", "wsgiref", "json"], "llm_suggested": ["wsgiref", "urllib", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the decorators provided by this package to add logging functionality to a function, printing the function name, its arguments, and return value every time the function is called.", "prompt_index": 111, "generated_code": "```python\nimport functools\n\ndef log_function_calls(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Print function name and arguments\n        args_str = ', '.join([repr(arg) for arg in args])\n        kwargs_str = ', '.join([f\"{k}={repr(v)}\" for k, v in kwargs.items()])\n        all_args = ', '.join(filter(None, [args_str, kwargs_str]))\n        \n        print(f\"Calling {func.__name__}({all_args})\")\n        \n        # Call the original function\n        result = func(*args, **kwargs)\n        \n        # Print return value\n        print(f\"{func.__name__} returned: {repr(result)}\")\n        \n        return result\n    return wrapper\n\n# Example usage\n@log_function_calls\ndef add_numbers(a, b):\n    return a + b\n\n@log_function_calls\ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n\n# Test the decorated functions\nadd_numbers(3, 5)\ngreet(\"Alice\")\ngreet(\"Bob\", greeting=\"Hi\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that serializes a Python dictionary containing various data types into a MessagePack format for efficient and compact storage or transmission.", "prompt_index": 112, "generated_code": "```python\nimport msgpack\n\n# Sample dictionary with various data types\ndata = {\n    'string': 'Hello World',\n    'integer': 42,\n    'float': 3.14159,\n    'boolean': True,\n    'null_value': None,\n    'list': [1, 2, 3, 'mixed', True],\n    'nested_dict': {\n        'nested_string': 'nested value',\n        'nested_number': 100\n    },\n    'bytes': b'binary data',\n    'tuple': (1, 2, 3),  # Will be converted to list\n    'unicode': 'caf\u00e9 \u00f1o\u00f1o \u4e2d\u6587'\n}\n\n# Serialize to MessagePack format\npacked_data = msgpack.packb(data)\n\nprint(f\"Original data size (approximate): {len(str(data))} characters\")\nprint(f\"Packed data size: {len(packed_data)} bytes\")\nprint(f\"Packed data (hex): {packed_data.hex()}\")\n\n# Deserialize to verify\nunpacked_data = msgpack.unpackb(packed_data, raw=False)\nprint(f\"\\nDeserialized data matches original: {data == unpacked_data}\")\n\n# Save to file\nwith open('data.msgpack', 'wb') as f:\n    msgpack.pack(data, f)\n\n# Load from file\nwith open('data.msgpack', 'rb') as f:\n    loaded_data = msgpack.unpack(f, raw=False)\n    \nprint(f\"File data matches original: {data == loaded_data}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["msgpack"], "package_breakdown": {"install_packages": ["msgpack"], "import_packages": ["msgpack"], "llm_suggested": ["msgpack"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that defines custom type annotations using the type system extensions provided by the mypy type checker for ensuring type safety and consistency in your program.", "prompt_index": 113, "generated_code": "```python\nfrom typing import (\n    TypeVar, Generic, Protocol, Union, Optional, List, Dict, \n    Callable, Any, ClassVar, Final, Literal, TypedDict, NewType\n)\nfrom typing_extensions import TypeAlias, Self, override\nfrom abc import abstractmethod\n\n# NewType for creating distinct types\nUserId = NewType('UserId', int)\nEmail = NewType('Email', str)\n\n# Type variables\nT = TypeVar('T')\nK = TypeVar('K')\nV = TypeVar('V')\nNumberType = TypeVar('NumberType', int, float)\n\n# Type aliases\nJSONValue: TypeAlias = Union[str, int, float, bool, None, Dict[str, Any], List[Any]]\nCoordinates: TypeAlias = tuple[float, float]\n\n# Literal types\nStatus = Literal['pending', 'approved', 'rejected']\nLogLevel = Literal['DEBUG', 'INFO', 'WARNING', 'ERROR']\n\n# TypedDict for structured dictionaries\nclass UserData(TypedDict):\n    id: UserId\n    name: str\n    email: Email\n    age: Optional[int]\n    is_active: bool\n\nclass UserDataPartial(TypedDict, total=False):\n    name: str\n    email: Email\n    age: int\n    is_active: bool\n\n# Protocol for structural typing\nclass Drawable(Protocol):\n    def draw(self) -> None: ...\n    \n    @property\n    def area(self) -> float: ...\n\nclass Comparable(Protocol):\n    def __lt__(self, other: Self) -> bool: ...\n\n# Generic classes\nclass Stack(Generic[T]):\n    def __init__(self) -> None:\n        self._items: List[T] = []\n    \n    def push(self, item: T) -> None:\n        self._items.append(item)\n    \n    def pop(self) -> T:\n        if not self._items:\n            raise IndexError(\"Stack is empty\")\n        return self._items.pop()\n    \n    def peek(self) -> Optional[T]:\n        return self._items[-1] if self._items else None\n\nclass Cache(Generic[K, V]):\n    def __init__(self, max_size: int = 100) -> None:\n        self._data: Dict[K, V] = {}\n        self._max_size: Final[int] = max_size\n    \n    def get(self, key: K) -> Optional[V]:\n        return self._data.get(key)\n    \n    def set(self, key: K, value: V) -> None:\n        if len(self._data) >= self._max_size:\n            # Remove oldest item (simplified)\n            self._data.pop(next(iter(self._data)))\n        self._data[key] = value\n\n# Abstract base class with type annotations\nclass Shape(Protocol):\n    @abstractmethod\n    def area(self) -> float: ...\n    \n    @abstractmethod\n    def perimeter(self) -> float: ...\n\nclass Rectangle:\n    def __init__(self, width: float, height: float) -> None:\n        self.width: float = width\n        self.height: float = height\n    \n    def area(self) -> float:\n        return self.width * self.height\n    \n    def perimeter(self) -> float:\n        return 2 * (self.width + self.height)\n\nclass Circle:\n    PI: ClassVar[float] = 3.14159\n    \n    def __init__(self, radius: float) -> None:\n        self.radius: float = radius\n    \n    def area(self) -> float:\n        return self.PI * self.radius ** 2\n    \n    def perimeter(self) -> float:\n        return 2 * self.PI * self.radius\n\n# Function with advanced type annotations\ndef process_shapes(shapes: List[Drawable]) -> Dict[str, float]:\n    \"\"\"Process a list of drawable shapes and return their total area.\"\"\"\n    total_area = 0.0\n    shape_count = len(shapes)\n    \n    for shape in shapes:\n        shape.draw()\n        total_area += shape.area\n    \n    return {\n        'total_area': total_area,\n        'average_area': total_area / shape_count if shape_count > 0 else 0.0\n    }\n\n# Higher-order function types\nProcessorFunc = Callable[[T], T]\nFilterFunc = Callable[[T], bool]\nReducerFunc = Callable[[T, T], T]\n\ndef apply_processors(\n    items: List[T], \n    processors: List[ProcessorFunc[T]]\n) -> List[T]:\n    \"\"\"Apply a series of processors to a list of items.\"\"\"\n    result = items.copy()\n    for processor in processors:\n        result = [processor(item) for item in result]\n    return result\n\ndef filter_and_reduce(\n    items: List[NumberType], \n    filter_func: FilterFunc[NumberType], \n    reduce_func: ReducerFunc[NumberType],\n    initial: NumberType\n) -> NumberType:\n    \"\"\"Filter items and reduce them to a single value.\"\"\"\n    filtered_items = [item for item in items if filter_func(item)]\n    result = initial\n    for item in filtered_items:\n        result = reduce_func(result, item)\n    return result\n\n# Class with Self type\nclass Builder:\n    def __init__(self) -> None:\n        self._config: Dict[str, Any] = {}\n    \n    def set_option(self, key: str, value: Any) -> Self:\n        self._config[key] = value\n        return self\n    \n    def build(self) -> Dict[str, Any]:\n        return self._config.copy()\n\n# Overload example\nfrom typing import overload\n\nclass DataProcessor:\n    @overload\n    def process(self, data: str) -> str: ...\n    \n    @overload\n    def process(self, data: int) -> int: ...\n    \n    @overload\n    def process(self, data: List[T]) -> List[T]: ...\n    \n    def process(self, data: Union[str, int, List[Any]]) -> Union[str, int, List[Any]]:\n        if isinstance(data, str):\n            return data.upper()\n        elif isinstance(data, int):\n            return data * 2\n        elif isinstance(data, list):\n            return data[::-1]\n        else:\n            raise TypeError(f\"Unsupported type: {type(data)}\")\n\n# Usage examples\ndef example_usage() -> None:\n    # Using NewTypes\n    user_id: UserId = UserId(123)\n    email: Email = Email(\"user@example.com\")\n    \n    # Using TypedDict\n    user: UserData = {\n        'id': user_id,\n        'name': 'John Doe',\n        'email': email,\n        'age': 30,\n        'is_active': True\n    }\n    \n    # Using generic classes\n    int_stack: Stack[int] = Stack()\n    int_stack.push(1)\n    int_stack.push(2)\n    \n    str_cache: Cache[str, UserData] = Cache()\n    str_cache.set(\"user123\", user)\n    \n    # Using protocols\n    shapes: List[Drawable] = [Rectangle(5, 10), Circle(3)]\n    results = process_shapes(shapes)\n    \n    # Using builder pattern with Self\n    config = (Builder()\n              .set_option(\"debug\", True)\n              .set_option(\"timeout\", 30)\n              .build())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["typing_extensions"], "packages_detected": ["typing_extensions", "abc", "typing"], "package_breakdown": {"install_packages": ["typing_extensions", "abc", "typing"], "import_packages": ["abc", "typing_extensions", "typing"], "llm_suggested": ["typing", "typing_extensions", "abc"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates an instance of the Pickler class from the package, extends the standard pickle.Pickler functionality by adding custom serialization methods, and then pickles an object using the extended Pickler instance.", "prompt_index": 114, "generated_code": "```python\nimport pickle\nimport io\n\nclass CustomPickler(pickle.Pickler):\n    def __init__(self, file, protocol=None):\n        super().__init__(file, protocol)\n        \n    def save_custom_object(self, obj):\n        \"\"\"Custom serialization method for objects with 'custom_data' attribute\"\"\"\n        if hasattr(obj, 'custom_data'):\n            # Custom serialization logic\n            self.save_reduce(\n                self._rebuild_custom_object,\n                (obj.custom_data, obj.__class__.__name__),\n                obj=obj\n            )\n        else:\n            # Fall back to default pickling\n            pickle.Pickler.save_global(self, obj)\n    \n    @staticmethod\n    def _rebuild_custom_object(custom_data, class_name):\n        \"\"\"Helper function to rebuild custom objects\"\"\"\n        class CustomObject:\n            def __init__(self, data):\n                self.custom_data = data\n        return CustomObject(custom_data)\n    \n    def reducer_override(self, obj):\n        \"\"\"Override method to handle custom object types\"\"\"\n        if hasattr(obj, 'custom_data'):\n            return self._rebuild_custom_object, (obj.custom_data, obj.__class__.__name__)\n        return NotImplemented\n\n# Create a custom object to pickle\nclass MyCustomObject:\n    def __init__(self, data):\n        self.custom_data = data\n        self.regular_attr = \"standard attribute\"\n\n# Create an instance of the object\nmy_obj = MyCustomObject({\"key\": \"value\", \"number\": 42})\n\n# Create a bytes buffer to store pickled data\nbuffer = io.BytesIO()\n\n# Create an instance of the extended Pickler\ncustom_pickler = CustomPickler(buffer, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Pickle the object using the extended Pickler\ncustom_pickler.dump(my_obj)\n\n# Get the pickled data\npickled_data = buffer.getvalue()\n\n# Demonstrate unpickling\nbuffer.seek(0)\nunpickled_obj = pickle.load(buffer)\n\nprint(f\"Original object custom_data: {my_obj.custom_data}\")\nprint(f\"Unpickled object custom_data: {unpickled_obj.custom_data}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "pickle"], "package_breakdown": {"install_packages": ["io", "pickle"], "import_packages": ["io", "pickle"], "llm_suggested": ["pickle", "io"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that serializes and deserializes data using protocol buffers in a visually appealing and pythonic manner.", "prompt_index": 115, "generated_code": "```python\n# person.proto (Protocol Buffer definition)\n\"\"\"\nsyntax = \"proto3\";\n\nmessage Person {\n    string name = 1;\n    int32 age = 2;\n    string email = 3;\n    repeated string phone_numbers = 4;\n    \n    enum PhoneType {\n        MOBILE = 0;\n        HOME = 1;\n        WORK = 2;\n    }\n    \n    message PhoneNumber {\n        string number = 1;\n        PhoneType type = 2;\n    }\n    \n    repeated PhoneNumber phones = 5;\n}\n\nmessage AddressBook {\n    repeated Person people = 1;\n}\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport json\nfrom pathlib import Path\n\n# Simulated protobuf classes (in real usage, these would be generated)\nclass Person:\n    def __init__(self):\n        self.name = \"\"\n        self.age = 0\n        self.email = \"\"\n        self.phone_numbers = []\n        self.phones = []\n    \n    def SerializeToString(self) -> bytes:\n        data = {\n            'name': self.name,\n            'age': self.age,\n            'email': self.email,\n            'phone_numbers': self.phone_numbers,\n            'phones': [(p.number, p.type) for p in self.phones]\n        }\n        return json.dumps(data).encode('utf-8')\n    \n    def ParseFromString(self, data: bytes):\n        parsed = json.loads(data.decode('utf-8'))\n        self.name = parsed['name']\n        self.age = parsed['age']\n        self.email = parsed['email']\n        self.phone_numbers = parsed['phone_numbers']\n        self.phones = [PhoneNumber(num, ptype) for num, ptype in parsed['phones']]\n\nclass PhoneNumber:\n    def __init__(self, number=\"\", phone_type=0):\n        self.number = number\n        self.type = phone_type\n\nclass AddressBook:\n    def __init__(self):\n        self.people = []\n    \n    def SerializeToString(self) -> bytes:\n        data = [person.SerializeToString().decode('utf-8') for person in self.people]\n        return json.dumps(data).encode('utf-8')\n    \n    def ParseFromString(self, data: bytes):\n        parsed = json.loads(data.decode('utf-8'))\n        self.people = []\n        for person_data in parsed:\n            person = Person()\n            person.ParseFromString(person_data.encode('utf-8'))\n            self.people.append(person)\n\n# Pythonic Protocol Buffer Manager\n@dataclass\nclass ProtoBufManager:\n    \"\"\"Elegant Protocol Buffer serialization manager\"\"\"\n    \n    @staticmethod\n    def create_person(name: str, age: int, email: str, \n                     phone_numbers: Optional[List[str]] = None,\n                     phones: Optional[List[tuple]] = None) -> Person:\n        \"\"\"Create a Person with fluent interface\"\"\"\n        person = Person()\n        person.name = name\n        person.age = age\n        person.email = email\n        person.phone_numbers = phone_numbers or []\n        \n        if phones:\n            person.phones = [PhoneNumber(num, ptype) for num, ptype in phones]\n        \n        return person\n    \n    @staticmethod\n    def serialize_to_file(message, filepath: Path) -> bool:\n        \"\"\"Serialize protobuf message to file with error handling\"\"\"\n        try:\n            with open(filepath, 'wb') as f:\n                f.write(message.SerializeToString())\n            return True\n        except Exception as e:\n            print(f\"\u274c Serialization failed: {e}\")\n            return False\n    \n    @staticmethod\n    def deserialize_from_file(message_class, filepath: Path):\n        \"\"\"Deserialize protobuf message from file\"\"\"\n        try:\n            with open(filepath, 'rb') as f:\n                message = message_class()\n                message.ParseFromString(f.read())\n                return message\n        except Exception as e:\n            print(f\"\u274c Deserialization failed: {e}\")\n            return None\n    \n    @staticmethod\n    def pretty_print_person(person: Person) -> None:\n        \"\"\"Beautiful console output for Person\"\"\"\n        print(\"\ud83d\udc64 Person Details:\")\n        print(f\"   \ud83d\udcdb Name: {person.name}\")\n        print(f\"   \ud83c\udf82 Age: {person.age}\")\n        print(f\"   \ud83d\udce7 Email: {person.email}\")\n        \n        if person.phone_numbers:\n            print(f\"   \ud83d\udcf1 Phone Numbers: {', '.join(person.phone_numbers)}\")\n        \n        if person.phones:\n            print(\"   \ud83d\udcde Structured Phones:\")\n            for phone in person.phones:\n                type_emoji = \"\ud83d\udcf1\" if phone.type == 0 else \"\ud83c\udfe0\" if phone.type == 1 else \"\ud83d\udcbc\"\n                print(f\"      {type_emoji} {phone.number}\")\n        print(\"\u2500\" * 40)\n\n# Demo Usage\ndef demonstrate_protobuf_serialization():\n    \"\"\"Comprehensive demonstration of Protocol Buffer usage\"\"\"\n    \n    print(\"\ud83d\ude80 Protocol Buffer Serialization Demo\\n\")\n    \n    # Create sample data with fluent style\n    people_data = [\n        (\"Alice Johnson\", 28, \"alice@example.com\", \n         [\"555-1234\", \"555-5678\"], [(\"555-9999\", 0), (\"555-0000\", 1)]),\n        (\"Bob Smith\", 35, \"bob@example.com\", \n         [\"555-2468\"], [(\"555-1357\", 2)]),\n        (\"Carol Davis\", 42, \"carol@example.com\", \n         [], [(\"555-8642\", 0), (\"555-9753\", 1), (\"555-1111\", 2)])\n    ]\n    \n    # Create address book with elegant list comprehension\n    address_book = AddressBook()\n    address_book.people = [\n        ProtoBufManager.create_person(name, age, email, phones, structured_phones)\n        for name, age, email, phones, structured_phones in people_data\n    ]\n    \n    # Serialize to file\n    output_file = Path(\"address_book.pb\")\n    print(\"\ud83d\udcbe Serializing address book...\")\n    \n    if ProtoBufManager.serialize_to_file(address_book, output_file):\n        print(f\"\u2705 Successfully serialized to {output_file}\")\n        print(f\"\ud83d\udcca File size: {output_file.stat().st_size} bytes\\n\")\n    \n    # Deserialize from file\n    print(\"\ud83d\udcc2 Deserializing address book...\")\n    loaded_book = ProtoBufManager.deserialize_from_file(AddressBook, output_file)\n    \n    if loaded_book:\n        print(\"\u2705 Successfully deserialized!\\n\")\n        print(f\"\ud83d\udc65 Found {len(loaded_book.people)} people in address book:\\n\")\n        \n        # Display all people with beautiful formatting\n        for person in loaded_book.people:\n            ProtoBufManager.pretty_print_person(person)\n    \n    # Individual person serialization example\n    print(\"\\n\ud83d\udd04 Individual Person Serialization Example:\")\n    single_person = ProtoBufManager.create_person(\n        \"David Wilson\", 29, \"david@example.com\",\n        phone_numbers=[\"555-7777\"],\n        phones=[(\"555-8888\", 0), (\"555-9999\", 2)]\n    )\n    \n    # Serialize individual person\n    person_bytes = single_person.SerializeToString()\n    print(f\"\ud83d\udce6 Serialized person size: {len(person_bytes)} bytes\")\n    \n    # Deserialize individual person\n    restored_person = Person()\n    restored_person.ParseFromString(person_bytes)\n    \n    print(\"\ud83d\udd04 Restored person:\")\n    ProtoBufManager.pretty_print_person(restored_person)\n    \n    # Cleanup\n    if output_file.exists():\n        output_file.unlink()\n        print(\"\ud83e\uddf9 Cleaned up temporary files\")\n\n# Context manager for protobuf operations\nclass", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "typing", "dataclasses", "json"], "package_breakdown": {"install_packages": ["pathlib", "typing", "dataclasses", "json"], "import_packages": ["json", "pathlib", "dataclasses", "typing"], "llm_suggested": ["dataclasses", "typing", "json", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves a list of classifiers available on PyPI (pypi.org) using the canonical source for classifiers package.", "prompt_index": 116, "generated_code": "```python\nimport requests\n\ndef get_pypi_classifiers():\n    \"\"\"Retrieve the list of classifiers available on PyPI from the canonical source.\"\"\"\n    url = \"https://pypi.org/classifiers/\"\n    \n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        \n        # Parse the HTML to extract classifiers\n        from bs4 import BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find all classifier entries\n        classifiers = []\n        for li in soup.find_all('li'):\n            classifier_text = li.get_text().strip()\n            if classifier_text and '::' in classifier_text:\n                classifiers.append(classifier_text)\n        \n        return classifiers\n    \n    except requests.RequestException as e:\n        print(f\"Error fetching classifiers: {e}\")\n        return []\n\n# Alternative method using the trove-classifiers package (recommended)\ndef get_pypi_classifiers_canonical():\n    \"\"\"Get classifiers using the canonical trove-classifiers package.\"\"\"\n    try:\n        from trove_classifiers import classifiers\n        return list(classifiers)\n    except ImportError:\n        print(\"trove-classifiers package not installed. Install with: pip install trove-classifiers\")\n        return []\n\n# Usage\nif __name__ == \"__main__\":\n    # Method 1: Using trove-classifiers package (recommended)\n    classifiers = get_pypi_classifiers_canonical()\n    \n    # Method 2: Fallback to web scraping if package not available\n    if not classifiers:\n        classifiers = get_pypi_classifiers()\n    \n    print(f\"Found {len(classifiers)} classifiers:\")\n    for classifier in classifiers[:10]:  # Show first 10\n        print(f\"  {classifier}\")\n```\n\nTo use the canonical source, first install the trove-classifiers package:\n```bash\npip install trove-classifiers\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["trove_classifiers"], "packages_detected": ["trove-classifiers", "trove_classifiers", "beautifulsoup4", "bs4", "requests"], "package_breakdown": {"install_packages": ["trove-classifiers", "trove_classifiers", "beautifulsoup4", "bs4", "requests"], "import_packages": ["bs4", "trove_classifiers", "requests"], "llm_suggested": ["requests", "beautifulsoup4", "trove-classifiers"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that scans the system environment to detect any active shell processes and provides a detailed report on their surrounding context and activities.", "prompt_index": 117, "generated_code": "```python\nimport psutil\nimport os\nimport time\nfrom datetime import datetime\nfrom collections import defaultdict\n\ndef get_shell_processes():\n    \"\"\"Detect active shell processes and gather detailed information\"\"\"\n    \n    # Common shell executables\n    shell_names = {\n        'bash', 'sh', 'zsh', 'fish', 'csh', 'tcsh', 'ksh', \n        'dash', 'ash', 'powershell', 'pwsh', 'cmd', 'command'\n    }\n    \n    shell_processes = []\n    \n    for proc in psutil.process_iter(['pid', 'name', 'exe', 'cmdline', 'create_time', \n                                   'ppid', 'username', 'cwd', 'status', 'cpu_percent',\n                                   'memory_info', 'connections', 'environ']):\n        try:\n            proc_info = proc.info\n            proc_name = proc_info['name'].lower() if proc_info['name'] else ''\n            \n            # Check if process is a shell\n            if any(shell in proc_name for shell in shell_names):\n                # Get additional process details\n                process = psutil.Process(proc_info['pid'])\n                \n                # Get child processes\n                children = []\n                try:\n                    for child in process.children(recursive=True):\n                        children.append({\n                            'pid': child.pid,\n                            'name': child.name(),\n                            'cmdline': ' '.join(child.cmdline()) if child.cmdline() else 'N/A'\n                        })\n                except (psutil.NoSuchProcess, psutil.AccessDenied):\n                    pass\n                \n                # Get network connections\n                connections = []\n                try:\n                    for conn in process.connections():\n                        connections.append({\n                            'local_addr': f\"{conn.laddr.ip}:{conn.laddr.port}\" if conn.laddr else 'N/A',\n                            'remote_addr': f\"{conn.raddr.ip}:{conn.raddr.port}\" if conn.raddr else 'N/A',\n                            'status': conn.status\n                        })\n                except (psutil.NoSuchProcess, psutil.AccessDenied):\n                    pass\n                \n                # Get environment variables (limited for security)\n                env_vars = {}\n                try:\n                    env = proc_info.get('environ', {})\n                    if env:\n                        # Only include common, non-sensitive environment variables\n                        safe_vars = ['PATH', 'HOME', 'USER', 'SHELL', 'TERM', 'PWD', 'LANG']\n                        env_vars = {k: v for k, v in env.items() if k in safe_vars}\n                except:\n                    pass\n                \n                shell_info = {\n                    'pid': proc_info['pid'],\n                    'name': proc_info['name'],\n                    'executable': proc_info['exe'],\n                    'command_line': ' '.join(proc_info['cmdline']) if proc_info['cmdline'] else 'N/A',\n                    'parent_pid': proc_info['ppid'],\n                    'username': proc_info['username'],\n                    'working_directory': proc_info['cwd'],\n                    'status': proc_info['status'],\n                    'create_time': datetime.fromtimestamp(proc_info['create_time']).strftime('%Y-%m-%d %H:%M:%S'),\n                    'cpu_percent': proc_info['cpu_percent'],\n                    'memory_mb': round(proc_info['memory_info'].rss / 1024 / 1024, 2) if proc_info['memory_info'] else 0,\n                    'child_processes': children,\n                    'network_connections': connections,\n                    'environment_vars': env_vars\n                }\n                \n                shell_processes.append(shell_info)\n                \n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            continue\n    \n    return shell_processes\n\ndef analyze_shell_activity(shell_processes):\n    \"\"\"Analyze shell processes for patterns and activities\"\"\"\n    \n    analysis = {\n        'total_shells': len(shell_processes),\n        'shell_types': defaultdict(int),\n        'users': set(),\n        'suspicious_activity': [],\n        'network_activity': [],\n        'resource_usage': {'high_cpu': [], 'high_memory': []}\n    }\n    \n    for shell in shell_processes:\n        # Count shell types\n        shell_type = shell['name'].lower()\n        analysis['shell_types'][shell_type] += 1\n        \n        # Track users\n        analysis['users'].add(shell['username'])\n        \n        # Check for suspicious activity\n        if len(shell['child_processes']) > 10:\n            analysis['suspicious_activity'].append({\n                'pid': shell['pid'],\n                'issue': 'High number of child processes',\n                'count': len(shell['child_processes'])\n            })\n        \n        # Check for network connections\n        if shell['network_connections']:\n            analysis['network_activity'].append({\n                'pid': shell['pid'],\n                'connections': shell['network_connections']\n            })\n        \n        # Check resource usage\n        if shell['cpu_percent'] > 50:\n            analysis['resource_usage']['high_cpu'].append({\n                'pid': shell['pid'],\n                'cpu_percent': shell['cpu_percent']\n            })\n        \n        if shell['memory_mb'] > 100:\n            analysis['resource_usage']['high_memory'].append({\n                'pid': shell['pid'],\n                'memory_mb': shell['memory_mb']\n            })\n    \n    return analysis\n\ndef generate_report():\n    \"\"\"Generate comprehensive shell process report\"\"\"\n    \n    print(\"=\"*80)\n    print(\"SHELL PROCESS DETECTION AND ANALYSIS REPORT\")\n    print(\"=\"*80)\n    print(f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print()\n    \n    # Get shell processes\n    shell_processes = get_shell_processes()\n    \n    if not shell_processes:\n        print(\"No active shell processes detected.\")\n        return\n    \n    # Analyze processes\n    analysis = analyze_shell_activity(shell_processes)\n    \n    # Summary\n    print(\"SUMMARY\")\n    print(\"-\" * 40)\n    print(f\"Total shell processes found: {analysis['total_shells']}\")\n    print(f\"Unique users running shells: {len(analysis['users'])}\")\n    print(f\"Shell types detected: {dict(analysis['shell_types'])}\")\n    print()\n    \n    # Detailed process information\n    print(\"DETAILED SHELL PROCESS INFORMATION\")\n    print(\"-\" * 40)\n    \n    for i, shell in enumerate(shell_processes, 1):\n        print(f\"\\n[{i}] Shell Process Details:\")\n        print(f\"    PID: {shell['pid']}\")\n        print(f\"    Name: {shell['name']}\")\n        print(f\"    User: {shell['username']}\")\n        print(f\"    Command: {shell['command_line']}\")\n        print(f\"    Working Directory: {shell['working_directory']}\")\n        print(f\"    Status: {shell['status']}\")\n        print(f\"    Created: {shell['create_time']}\")\n        print(f\"    CPU Usage: {shell['cpu_percent']}%\")\n        print(f\"    Memory Usage: {shell['memory_mb']} MB\")\n        print(f\"    Parent PID: {shell['parent_pid']}\")\n        \n        if shell['child_processes']:\n            print(f\"    Child Processes ({len(shell['child_processes'])}):\")\n            for child in shell['child_processes'][:5]:  # Show first 5\n                print(f\"        - {child['name']} (PID: {child['pid']})\")\n            if len(shell['child_processes']) > 5:\n                print(f\"        ... and {len(shell['child_processes']) - 5} more\")\n        \n        if shell['network_connections']:\n            print(f\"    Network Connections:\")\n            for conn in shell['network_connections']:\n                print(f", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["psutil", "datetime", "time", "collections", "os"], "package_breakdown": {"install_packages": ["psutil", "datetime", "time", "collections", "os"], "import_packages": ["datetime", "psutil", "time", "collections", "os"], "llm_suggested": ["psutil"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to a WebSocket server using low level API options and sends a message with custom headers and payload.", "prompt_index": 118, "generated_code": "```python\nimport asyncio\nimport websockets\nimport json\n\nasync def connect_websocket_with_custom_headers():\n    # Custom headers to send with the WebSocket handshake\n    custom_headers = {\n        'Authorization': 'Bearer your-token-here',\n        'User-Agent': 'Custom-WebSocket-Client/1.0',\n        'X-Custom-Header': 'custom-value'\n    }\n    \n    # WebSocket connection URI\n    uri = \"wss://echo.websocket.org\"\n    \n    try:\n        # Connect with custom headers and low-level options\n        async with websockets.connect(\n            uri,\n            extra_headers=custom_headers,\n            compression=None,  # Disable compression\n            ping_interval=20,  # Ping every 20 seconds\n            ping_timeout=10,   # Wait 10 seconds for pong\n            close_timeout=10,  # Wait 10 seconds when closing\n            max_size=2**20,    # Max message size (1MB)\n            max_queue=32       # Max queued messages\n        ) as websocket:\n            \n            print(f\"Connected to {uri}\")\n            \n            # Custom payload\n            payload = {\n                \"type\": \"message\",\n                \"data\": {\n                    \"text\": \"Hello WebSocket!\",\n                    \"timestamp\": \"2024-01-01T00:00:00Z\",\n                    \"user_id\": 12345\n                },\n                \"metadata\": {\n                    \"version\": \"1.0\",\n                    \"client\": \"python-client\"\n                }\n            }\n            \n            # Send JSON message\n            await websocket.send(json.dumps(payload))\n            print(\"Message sent:\", payload)\n            \n            # Wait for response\n            response = await websocket.recv()\n            print(\"Received response:\", response)\n            \n            # Send binary message\n            binary_payload = b\"Binary message with custom headers\"\n            await websocket.send(binary_payload)\n            print(\"Binary message sent\")\n            \n            # Wait for binary response\n            binary_response = await websocket.recv()\n            print(\"Received binary response:\", binary_response)\n            \n    except websockets.exceptions.ConnectionClosedError as e:\n        print(f\"Connection closed: {e}\")\n    except websockets.exceptions.InvalidURI as e:\n        print(f\"Invalid URI: {e}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Run the WebSocket client\nasyncio.run(connect_websocket_with_custom_headers())\n```\n\nFor more advanced low-level control:\n\n```python\nimport asyncio\nimport websockets\nimport ssl\nfrom websockets.client import WebSocketClientProtocol\n\nclass CustomWebSocketClient(WebSocketClientProtocol):\n    async def process_request(self, path, request_headers):\n        # Custom request processing\n        request_headers['X-Custom-Processing'] = 'enabled'\n        return path, request_headers\n    \n    async def process_response(self, status, headers):\n        # Custom response processing\n        print(f\"Response status: {status}\")\n        print(f\"Response headers: {dict(headers)}\")\n        return status, headers\n\nasync def advanced_websocket_connection():\n    # SSL context for secure connections\n    ssl_context = ssl.create_default_context()\n    ssl_context.check_hostname = False\n    ssl_context.verify_mode = ssl.CERT_NONE\n    \n    # Advanced connection parameters\n    uri = \"wss://echo.websocket.org\"\n    \n    # Custom headers\n    headers = {\n        'Authorization': 'Bearer advanced-token',\n        'X-API-Key': 'your-api-key',\n        'X-Client-Version': '2.0.0'\n    }\n    \n    try:\n        # Connect with custom protocol class and advanced options\n        async with websockets.connect(\n            uri,\n            extra_headers=headers,\n            ssl=ssl_context,\n            create_protocol=CustomWebSocketClient,\n            subprotocols=['chat', 'superchat'],  # WebSocket subprotocols\n            origin='https://yourapp.com',        # Origin header\n            extensions=['permessage-deflate'],   # WebSocket extensions\n            compression='deflate',               # Compression algorithm\n            ping_interval=30,\n            ping_timeout=15,\n            close_timeout=15,\n            max_size=10**6,     # 1MB max message size\n            max_queue=128,      # Message queue size\n            read_limit=2**16,   # Read buffer limit\n            write_limit=2**16   # Write buffer limit\n        ) as websocket:\n            \n            print(f\"Advanced connection established to {uri}\")\n            print(f\"Selected subprotocol: {websocket.subprotocol}\")\n            print(f\"Extensions: {websocket.extensions}\")\n            \n            # Send message with custom payload\n            message = {\n                \"command\": \"subscribe\",\n                \"channel\": \"notifications\",\n                \"auth\": {\n                    \"token\": \"user-specific-token\",\n                    \"session_id\": \"abc123\"\n                },\n                \"parameters\": {\n                    \"format\": \"json\",\n                    \"compression\": True\n                }\n            }\n            \n            await websocket.send(json.dumps(message))\n            print(\"Subscription message sent\")\n            \n            # Listen for messages\n            async for message in websocket:\n                print(f\"Received: {message}\")\n                break  # Exit after first message for demo\n                \n    except Exception as e:\n        print(f\"Advanced connection error: {e}\")\n\n# Run advanced WebSocket client\nasyncio.run(advanced_websocket_connection())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["websockets", "asyncio", "ssl", "json"], "package_breakdown": {"install_packages": ["websockets", "asyncio", "ssl", "json"], "import_packages": ["json", "ssl", "asyncio", "websockets"], "llm_suggested": ["asyncio", "websockets", "json", "ssl"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that calculates and displays the code coverage percentage of a Python script using the package for code coverage measurement.", "prompt_index": 119, "generated_code": "```python\nimport coverage\nimport sys\nimport os\n\ndef calculate_coverage(script_path):\n    \"\"\"Calculate and display code coverage for a Python script\"\"\"\n    \n    # Create a coverage object\n    cov = coverage.Coverage()\n    \n    # Start coverage measurement\n    cov.start()\n    \n    try:\n        # Execute the script\n        with open(script_path, 'r') as f:\n            script_code = f.read()\n        \n        # Execute the script in the current namespace\n        exec(script_code, {'__name__': '__main__'})\n        \n    except Exception as e:\n        print(f\"Error executing script: {e}\")\n        return None\n    \n    finally:\n        # Stop coverage measurement\n        cov.stop()\n    \n    # Save coverage data\n    cov.save()\n    \n    # Get coverage report\n    total_coverage = cov.report(show_missing=True)\n    \n    print(f\"\\nCode Coverage Summary:\")\n    print(f\"Overall Coverage: {total_coverage:.2f}%\")\n    \n    return total_coverage\n\ndef calculate_coverage_with_html_report(script_path, html_dir=\"htmlcov\"):\n    \"\"\"Calculate coverage and generate HTML report\"\"\"\n    \n    cov = coverage.Coverage()\n    cov.start()\n    \n    try:\n        with open(script_path, 'r') as f:\n            script_code = f.read()\n        exec(script_code, {'__name__': '__main__'})\n    except Exception as e:\n        print(f\"Error executing script: {e}\")\n        return None\n    finally:\n        cov.stop()\n    \n    cov.save()\n    \n    # Generate console report\n    total_coverage = cov.report(show_missing=True)\n    \n    # Generate HTML report\n    cov.html_report(directory=html_dir)\n    \n    print(f\"\\nCode Coverage: {total_coverage:.2f}%\")\n    print(f\"HTML report generated in '{html_dir}' directory\")\n    \n    return total_coverage\n\ndef calculate_coverage_for_tests(source_dir, test_file):\n    \"\"\"Calculate coverage for running tests against source code\"\"\"\n    \n    cov = coverage.Coverage(source=[source_dir])\n    cov.start()\n    \n    try:\n        # Run the test file\n        exec(open(test_file).read())\n    except Exception as e:\n        print(f\"Error running tests: {e}\")\n        return None\n    finally:\n        cov.stop()\n    \n    cov.save()\n    \n    # Generate report\n    total_coverage = cov.report(show_missing=True)\n    \n    print(f\"\\nTest Coverage: {total_coverage:.2f}%\")\n    \n    return total_coverage\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1: Basic coverage calculation\n    script_to_test = \"example_script.py\"\n    \n    if len(sys.argv) > 1:\n        script_to_test = sys.argv[1]\n    \n    if os.path.exists(script_to_test):\n        print(f\"Calculating coverage for: {script_to_test}\")\n        coverage_percentage = calculate_coverage(script_to_test)\n        \n        if coverage_percentage is not None:\n            print(f\"Final Coverage: {coverage_percentage:.2f}%\")\n    else:\n        print(f\"Script '{script_to_test}' not found\")\n        \n    # Example 2: Coverage with HTML report\n    # calculate_coverage_with_html_report(\"your_script.py\")\n```\n\nTo use this code, you'll also need to install the coverage package:\n```bash\npip install coverage\n```\n\nThe code provides three functions:\n1. `calculate_coverage()` - Basic coverage calculation with console output\n2. `calculate_coverage_with_html_report()` - Coverage with HTML report generation\n3. `calculate_coverage_for_tests()` - Coverage calculation for test files against source code\n\nRun with: `python coverage_calculator.py your_script.py`", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["coverage", "os", "sys"], "package_breakdown": {"install_packages": ["coverage", "os", "sys"], "import_packages": ["coverage", "os", "sys"], "llm_suggested": ["coverage"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a ZeroMQ socket, binds it to a specific port for publishing messages, and sends a series of JSON-encoded data packets to a specified endpoint using ZeroMQ bindings.", "prompt_index": 120, "generated_code": "```python\nimport zmq\nimport json\nimport time\n\n# Create ZeroMQ context\ncontext = zmq.Context()\n\n# Create publisher socket\nsocket = context.socket(zmq.PUB)\n\n# Bind to specific port\nport = 5555\nsocket.bind(f\"tcp://*:{port}\")\n\n# Allow time for connections to establish\ntime.sleep(1)\n\n# Sample data packets to send\ndata_packets = [\n    {\"id\": 1, \"message\": \"Hello World\", \"timestamp\": \"2024-01-01T10:00:00\"},\n    {\"id\": 2, \"message\": \"Data packet 2\", \"value\": 42.5},\n    {\"id\": 3, \"message\": \"Status update\", \"status\": \"active\", \"count\": 100},\n    {\"id\": 4, \"message\": \"Final packet\", \"data\": [1, 2, 3, 4, 5]}\n]\n\n# Send JSON-encoded data packets\ntry:\n    for packet in data_packets:\n        # Encode data as JSON\n        json_data = json.dumps(packet)\n        \n        # Send the message\n        socket.send_string(json_data)\n        print(f\"Sent: {json_data}\")\n        \n        # Small delay between messages\n        time.sleep(0.5)\n        \nfinally:\n    # Clean up\n    socket.close()\n    context.term()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyzmq", "json", "zmq", "time"], "package_breakdown": {"install_packages": ["pyzmq", "json", "zmq", "time"], "import_packages": ["json", "zmq", "time"], "llm_suggested": ["pyzmq"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that encrypts a message using the NaCl library's cryptography functions and then decrypts the encrypted message to recover the original plaintext.", "prompt_index": 121, "generated_code": "```python\nimport nacl.secret\nimport nacl.utils\n\n# Generate a random key\nkey = nacl.utils.random(nacl.secret.SecretBox.KEY_SIZE)\n\n# Create a SecretBox instance\nbox = nacl.secret.SecretBox(key)\n\n# Original message\nmessage = b\"Hello, this is a secret message!\"\n\n# Encrypt the message\nencrypted = box.encrypt(message)\n\nprint(f\"Original message: {message}\")\nprint(f\"Encrypted message: {encrypted}\")\n\n# Decrypt the message\ndecrypted = box.decrypt(encrypted)\n\nprint(f\"Decrypted message: {decrypted}\")\nprint(f\"Messages match: {message == decrypted}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["nacl", "PyNaCl"], "package_breakdown": {"install_packages": ["nacl", "PyNaCl"], "import_packages": ["nacl"], "llm_suggested": ["PyNaCl"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retries calling a specific function until it succeeds, with a maximum number of attempts and a delay between each attempt.", "prompt_index": 122, "generated_code": "```python\nimport time\nfrom typing import Callable, Any, Optional\n\ndef retry_function(func: Callable, max_attempts: int = 3, delay: float = 1.0, \n                  *args, **kwargs) -> Any:\n    \"\"\"\n    Retry calling a function until it succeeds or max attempts reached.\n    \n    Args:\n        func: Function to retry\n        max_attempts: Maximum number of attempts\n        delay: Delay in seconds between attempts\n        *args, **kwargs: Arguments to pass to the function\n    \n    Returns:\n        Result of successful function call\n    \n    Raises:\n        Exception: Last exception if all attempts fail\n    \"\"\"\n    last_exception = None\n    \n    for attempt in range(1, max_attempts + 1):\n        try:\n            result = func(*args, **kwargs)\n            return result\n        except Exception as e:\n            last_exception = e\n            if attempt < max_attempts:\n                print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds...\")\n                time.sleep(delay)\n            else:\n                print(f\"Attempt {attempt} failed: {e}. No more attempts.\")\n    \n    raise last_exception\n\n# Alternative implementation with exponential backoff\ndef retry_with_backoff(func: Callable, max_attempts: int = 3, \n                      initial_delay: float = 1.0, backoff_factor: float = 2.0,\n                      *args, **kwargs) -> Any:\n    \"\"\"\n    Retry with exponential backoff.\n    \"\"\"\n    last_exception = None\n    delay = initial_delay\n    \n    for attempt in range(1, max_attempts + 1):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n            if attempt < max_attempts:\n                print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                delay *= backoff_factor\n            else:\n                print(f\"Attempt {attempt} failed: {e}. No more attempts.\")\n    \n    raise last_exception\n\n# Decorator version\ndef retry(max_attempts: int = 3, delay: float = 1.0):\n    \"\"\"\n    Decorator to retry function calls.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return retry_function(func, max_attempts, delay, *args, **kwargs)\n        return wrapper\n    return decorator\n\n# Example usage:\n# @retry(max_attempts=5, delay=2.0)\n# def unreliable_function():\n#     # Your function that might fail\n#     pass\n\n# Or direct call:\n# result = retry_function(your_function, max_attempts=5, delay=2.0, arg1, arg2, kwarg1=value)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "typing"], "package_breakdown": {"install_packages": ["time", "typing"], "import_packages": ["time", "typing"], "llm_suggested": ["time", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the Poetry PEP 517 Build Backend package to programmatically build a Python project with its dependencies and generate a distributable package in a specific format, such as a wheel or a source distribution.", "prompt_index": 123, "generated_code": "```python\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\ndef build_project_with_poetry(project_path, output_dir=None, format_type=\"both\"):\n    \"\"\"\n    Build a Python project using Poetry PEP 517 backend\n    \n    Args:\n        project_path (str): Path to the project containing pyproject.toml\n        output_dir (str): Output directory for built packages (optional)\n        format_type (str): \"wheel\", \"sdist\", or \"both\"\n    \"\"\"\n    project_path = Path(project_path).resolve()\n    \n    if not (project_path / \"pyproject.toml\").exists():\n        raise FileNotFoundError(f\"pyproject.toml not found in {project_path}\")\n    \n    # Set output directory\n    if output_dir is None:\n        output_dir = project_path / \"dist\"\n    else:\n        output_dir = Path(output_dir)\n    \n    output_dir.mkdir(exist_ok=True)\n    \n    # Change to project directory\n    original_cwd = Path.cwd()\n    try:\n        import os\n        os.chdir(project_path)\n        \n        # Build using poetry\n        if format_type in [\"wheel\", \"both\"]:\n            print(\"Building wheel...\")\n            subprocess.run([sys.executable, \"-m\", \"poetry\", \"build\", \"-f\", \"wheel\"], \n                         check=True)\n        \n        if format_type in [\"sdist\", \"both\"]:\n            print(\"Building source distribution...\")\n            subprocess.run([sys.executable, \"-m\", \"poetry\", \"build\", \"-f\", \"sdist\"], \n                         check=True)\n        \n        # Move built packages to output directory if different\n        dist_dir = project_path / \"dist\"\n        if output_dir != dist_dir and dist_dir.exists():\n            for file in dist_dir.glob(\"*\"):\n                shutil.move(str(file), str(output_dir / file.name))\n                \n        print(f\"Packages built successfully in {output_dir}\")\n        \n    finally:\n        os.chdir(original_cwd)\n\n# Alternative approach using build backend directly\ndef build_with_pep517_backend(project_path, output_dir=None):\n    \"\"\"\n    Build using PEP 517 build backend directly\n    \"\"\"\n    try:\n        import build\n        from pathlib import Path\n        \n        project_path = Path(project_path).resolve()\n        \n        if output_dir is None:\n            output_dir = project_path / \"dist\"\n        else:\n            output_dir = Path(output_dir)\n        \n        output_dir.mkdir(exist_ok=True)\n        \n        # Build wheel\n        print(\"Building wheel with PEP 517 backend...\")\n        build.ProjectBuilder(project_path).build(\"wheel\", str(output_dir))\n        \n        # Build source distribution\n        print(\"Building source distribution with PEP 517 backend...\")\n        build.ProjectBuilder(project_path).build(\"sdist\", str(output_dir))\n        \n        print(f\"Packages built successfully in {output_dir}\")\n        \n    except ImportError:\n        print(\"Please install the 'build' package: pip install build\")\n\n# Using subprocess with build module\ndef build_with_build_module(project_path, output_dir=None, format_type=\"both\"):\n    \"\"\"\n    Build using the 'build' module via subprocess\n    \"\"\"\n    project_path = Path(project_path).resolve()\n    \n    if output_dir is None:\n        output_dir = project_path / \"dist\"\n    else:\n        output_dir = Path(output_dir)\n    \n    cmd = [sys.executable, \"-m\", \"build\"]\n    \n    if format_type == \"wheel\":\n        cmd.extend([\"--wheel\"])\n    elif format_type == \"sdist\":\n        cmd.extend([\"--sdist\"])\n    # \"both\" is default behavior\n    \n    cmd.extend([\"--outdir\", str(output_dir), str(project_path)])\n    \n    print(f\"Running: {' '.join(cmd)}\")\n    subprocess.run(cmd, check=True)\n    \n    print(f\"Packages built successfully in {output_dir}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example project structure setup\n    def create_example_project():\n        \"\"\"Create a minimal example project with Poetry configuration\"\"\"\n        project_dir = Path(\"example_project\")\n        project_dir.mkdir(exist_ok=True)\n        \n        # Create pyproject.toml with Poetry backend\n        pyproject_content = \"\"\"[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"example-package\"\nversion = \"0.1.0\"\ndescription = \"An example package\"\nauthors = [\"Your Name <you@example.com>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\nrequests = \"^2.28.0\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.0.0\"\n\"\"\"\n        \n        with open(project_dir / \"pyproject.toml\", \"w\") as f:\n            f.write(pyproject_content)\n        \n        # Create package structure\n        package_dir = project_dir / \"example_package\"\n        package_dir.mkdir(exist_ok=True)\n        \n        with open(package_dir / \"__init__.py\", \"w\") as f:\n            f.write('__version__ = \"0.1.0\"\\n')\n        \n        with open(package_dir / \"main.py\", \"w\") as f:\n            f.write(\"\"\"\ndef hello():\n    return \"Hello from example package!\"\n\nif __name__ == \"__main__\":\n    print(hello())\n\"\"\")\n        \n        return project_dir\n    \n    # Create example and build\n    try:\n        example_dir = create_example_project()\n        print(f\"Created example project in {example_dir}\")\n        \n        # Method 1: Using Poetry directly\n        build_project_with_poetry(example_dir, format_type=\"both\")\n        \n        # Method 2: Using build module (alternative)\n        # build_with_build_module(example_dir, format_type=\"both\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "shutil", "subprocess", "build", "sys", "tempfile", "os"], "package_breakdown": {"install_packages": ["pathlib", "shutil", "subprocess", "build", "sys", "tempfile", "os"], "import_packages": ["pathlib", "shutil", "subprocess", "build", "tempfile", "sys", "os"], "llm_suggested": ["subprocess", "pathlib", "tempfile", "shutil", "build"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the provided package to create a context manager that sets a timeout for executing asynchronous functions within an asyncio program.", "prompt_index": 124, "generated_code": "```python\nimport asyncio\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def timeout_context(seconds):\n    \"\"\"Context manager that sets a timeout for async operations.\"\"\"\n    try:\n        async with asyncio.timeout(seconds):\n            yield\n    except asyncio.TimeoutError:\n        raise asyncio.TimeoutError(f\"Operation timed out after {seconds} seconds\")\n\n# Alternative implementation using asyncio.wait_for\n@asynccontextmanager\nasync def timeout_context_alt(seconds):\n    \"\"\"Alternative context manager using wait_for approach.\"\"\"\n    task = None\n    try:\n        yield\n    except Exception as e:\n        raise e\n\n# Usage examples:\nasync def slow_operation():\n    await asyncio.sleep(3)\n    return \"Completed\"\n\nasync def main():\n    # Using the timeout context manager\n    try:\n        async with timeout_context(2):\n            result = await slow_operation()\n            print(result)\n    except asyncio.TimeoutError as e:\n        print(f\"Caught timeout: {e}\")\n    \n    # Multiple operations within timeout\n    try:\n        async with timeout_context(5):\n            await asyncio.sleep(1)\n            print(\"First operation done\")\n            await asyncio.sleep(1)\n            print(\"Second operation done\")\n    except asyncio.TimeoutError:\n        print(\"Operations timed out\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "contextlib"], "package_breakdown": {"install_packages": ["asyncio", "contextlib"], "import_packages": ["asyncio", "contextlib"], "llm_suggested": ["asyncio"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a Sorted List, populates it with integers in descending order, and then prints the list after adding a new integer at a specific index.", "prompt_index": 125, "generated_code": "```python\nfrom sortedcontainers import SortedList\n\n# Create a SortedList and populate with integers in descending order\nsorted_list = SortedList([10, 8, 6, 4, 2], key=lambda x: -x)\n\nprint(\"Original sorted list:\", sorted_list)\n\n# Add a new integer (it will maintain sorted order)\nsorted_list.add(7)\n\nprint(\"After adding 7:\", sorted_list)\n\n# To insert at a specific index (breaks sorting), use insert\nsorted_list.insert(2, 1)\n\nprint(\"After inserting 1 at index 2:\", sorted_list)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sortedcontainers"], "package_breakdown": {"install_packages": ["sortedcontainers"], "import_packages": ["sortedcontainers"], "llm_suggested": ["sortedcontainers"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that decorates a specific function named 'old_function' with the @deprecated decorator to mark it as deprecated and provide a custom warning message when the function is called.", "prompt_index": 127, "generated_code": "```python\nimport warnings\nimport functools\n\ndef deprecated(message=\"This function is deprecated\"):\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated. {message}\",\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@deprecated(\"Use new_function() instead\")\ndef old_function():\n    return \"This is the old function\"\n\n# Example usage\nold_function()  # This will show the deprecation warning\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["warnings", "functools"], "package_breakdown": {"install_packages": ["warnings", "functools"], "import_packages": ["warnings", "functools"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that instruments a Python application to collect distributed traces and metrics using the OpenTelemetry Python API.", "prompt_index": 128, "generated_code": "```python\n# requirements.txt dependencies:\n# opentelemetry-api\n# opentelemetry-sdk\n# opentelemetry-auto-instrumentation\n# opentelemetry-exporter-jaeger-thrift\n# opentelemetry-exporter-prometheus\n# opentelemetry-instrumentation-requests\n# opentelemetry-instrumentation-flask\n\nimport time\nimport requests\nfrom flask import Flask, jsonify\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.exporter.prometheus import PrometheusMetricReader\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.sdk.resources import Resource\n\n# Configure resource information\nresource = Resource.create({\n    \"service.name\": \"my-python-app\",\n    \"service.version\": \"1.0.0\",\n    \"deployment.environment\": \"production\"\n})\n\n# Configure tracing\ntrace.set_tracer_provider(TracerProvider(resource=resource))\ntracer_provider = trace.get_tracer_provider()\n\n# Configure Jaeger exporter for traces\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\n\n# Add span processor\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntracer_provider.add_span_processor(span_processor)\n\n# Configure metrics\nprometheus_reader = PrometheusMetricReader()\nmetrics.set_meter_provider(MeterProvider(\n    resource=resource,\n    metric_readers=[prometheus_reader]\n))\n\n# Get tracer and meter\ntracer = trace.get_tracer(__name__)\nmeter = metrics.get_meter(__name__)\n\n# Create custom metrics\nrequest_counter = meter.create_counter(\n    name=\"http_requests_total\",\n    description=\"Total number of HTTP requests\",\n    unit=\"1\"\n)\n\nrequest_duration = meter.create_histogram(\n    name=\"http_request_duration_seconds\",\n    description=\"HTTP request duration in seconds\",\n    unit=\"s\"\n)\n\n# Auto-instrument common libraries\nRequestsInstrumentor().instrument()\n\n# Flask application\napp = Flask(__name__)\nFlaskInstrumentor().instrument_app(app)\n\n@app.route(\"/\")\ndef hello():\n    with tracer.start_as_current_span(\"hello_endpoint\") as span:\n        span.set_attribute(\"endpoint\", \"/\")\n        span.set_attribute(\"method\", \"GET\")\n        \n        # Record metrics\n        request_counter.add(1, {\"endpoint\": \"/\", \"method\": \"GET\"})\n        \n        start_time = time.time()\n        \n        # Simulate some work\n        time.sleep(0.1)\n        \n        duration = time.time() - start_time\n        request_duration.record(duration, {\"endpoint\": \"/\", \"method\": \"GET\"})\n        \n        return jsonify({\"message\": \"Hello, World!\"})\n\n@app.route(\"/external\")\ndef external_call():\n    with tracer.start_as_current_span(\"external_call\") as span:\n        span.set_attribute(\"endpoint\", \"/external\")\n        \n        request_counter.add(1, {\"endpoint\": \"/external\", \"method\": \"GET\"})\n        start_time = time.time()\n        \n        # Make external HTTP request (auto-instrumented)\n        response = requests.get(\"https://httpbin.org/json\")\n        \n        span.set_attribute(\"external.status_code\", response.status_code)\n        span.set_attribute(\"external.url\", \"https://httpbin.org/json\")\n        \n        duration = time.time() - start_time\n        request_duration.record(duration, {\"endpoint\": \"/external\", \"method\": \"GET\"})\n        \n        return jsonify(response.json())\n\n@app.route(\"/error\")\ndef error_endpoint():\n    with tracer.start_as_current_span(\"error_endpoint\") as span:\n        span.set_attribute(\"endpoint\", \"/error\")\n        \n        request_counter.add(1, {\"endpoint\": \"/error\", \"method\": \"GET\", \"status\": \"error\"})\n        \n        try:\n            # Simulate an error\n            raise ValueError(\"Simulated error\")\n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return jsonify({\"error\": str(e)}), 500\n\n# Custom business logic with manual instrumentation\ndef process_data(data):\n    with tracer.start_as_current_span(\"process_data\") as span:\n        span.set_attribute(\"data.size\", len(data))\n        \n        # Simulate processing\n        processed_items = 0\n        for item in data:\n            with tracer.start_as_current_span(\"process_item\") as item_span:\n                item_span.set_attribute(\"item.id\", item.get(\"id\", \"unknown\"))\n                time.sleep(0.01)  # Simulate work\n                processed_items += 1\n        \n        span.set_attribute(\"processed.count\", processed_items)\n        return processed_items\n\n@app.route(\"/process\")\ndef process_endpoint():\n    with tracer.start_as_current_span(\"process_endpoint\") as span:\n        data = [{\"id\": i} for i in range(10)]\n        result = process_data(data)\n        \n        # Record custom metric\n        processing_counter = meter.create_counter(\n            name=\"items_processed_total\",\n            description=\"Total number of items processed\"\n        )\n        processing_counter.add(result)\n        \n        return jsonify({\"processed\": result})\n\n# Metrics endpoint for Prometheus scraping\n@app.route(\"/metrics\")\ndef metrics_endpoint():\n    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}\n\nif __name__ == \"__main__\":\n    print(\"Starting instrumented Flask application...\")\n    print(\"Traces will be sent to Jaeger at localhost:6831\")\n    print(\"Metrics available at http://localhost:5000/metrics\")\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n```\n\n```python\n# Alternative configuration for OTLP exporters (OpenTelemetry Protocol)\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\n\n# OTLP trace exporter configuration\notlp_trace_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4317\",\n    headers={\"api-key\": \"your-api-key\"}\n)\n\n# OTLP metrics exporter configuration  \notlp_metrics_reader = PeriodicExportingMetricReader(\n    OTLPMetricExporter(\n        endpoint=\"http://localhost:4317\",\n        headers={\"api-key\": \"your-api-key\"}\n    ),\n    export_interval_millis=5000\n)\n```\n\n```python\n# Environment variable configuration (recommended for production)\nimport os\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter\n\n# Configure based on environment variables\ndef configure_telemetry():\n    service_name = os.getenv(\"OTEL_SERVICE_NAME\", \"my-python-app\")\n    service_version = os.getenv(\"OTEL_SERVICE_VERSION\", \"1.0.0\")\n    environment = os.getenv(\"OTEL_ENVIRONMENT\", \"development\")\n    \n    resource = Resource.create({\n        \"service.name\": service_name,\n        \"service.version\": service_version,\n        \"deployment.environment\": environment\n    })\n    \n    # Use console exporter in development\n    if environment == \"development\":\n        exporter = ConsoleSpanExporter", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 14, "hallucinated_packages": 2, "hallucinations": ["opentelemetry", "prometheus_client"], "packages_detected": ["opentelemetry-exporter-otlp", "opentelemetry", "opentelemetry-sdk", "time", "prometheus-client", "opentelemetry-exporter-prometheus", "opentelemetry-instrumentation-requests", "requests", "opentelemetry-api", "prometheus_client", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "os"], "package_breakdown": {"install_packages": ["opentelemetry-exporter-otlp", "opentelemetry", "opentelemetry-sdk", "time", "prometheus-client", "opentelemetry-exporter-prometheus", "opentelemetry-instrumentation-requests", "requests", "opentelemetry-api", "prometheus_client", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "os"], "import_packages": ["prometheus_client", "opentelemetry", "flask", "time", "requests", "os"], "llm_suggested": ["time", "requests", "flask", "opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-exporter-prometheus", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry-exporter-otlp", "prometheus-client"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the Standard Health Checking Service for gRPC to continuously monitor the health status of a gRPC server and handle any necessary health check requests.", "prompt_index": 129, "generated_code": "```python\nimport asyncio\nimport grpc\nfrom grpc_health.v1 import health_pb2\nfrom grpc_health.v1 import health_pb2_grpc\nfrom grpc_health.v1.health import HealthServicer\nimport logging\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Health Check Server Implementation\nclass HealthCheckServer:\n    def __init__(self, port=50051):\n        self.port = port\n        self.server = None\n        self.health_servicer = HealthServicer()\n        \n    async def start_server(self):\n        \"\"\"Start the gRPC server with health checking service\"\"\"\n        self.server = grpc.aio.server(ThreadPoolExecutor(max_workers=10))\n        \n        # Add health servicer to server\n        health_pb2_grpc.add_HealthServicer_to_server(\n            self.health_servicer, self.server\n        )\n        \n        listen_addr = f'[::]:{self.port}'\n        self.server.add_insecure_port(listen_addr)\n        \n        # Set initial health status\n        self.health_servicer.set(\"\", health_pb2.HealthCheckResponse.SERVING)\n        self.health_servicer.set(\"MyService\", health_pb2.HealthCheckResponse.SERVING)\n        \n        await self.server.start()\n        logger.info(f\"Health check server started on port {self.port}\")\n        \n        try:\n            await self.server.wait_for_termination()\n        except KeyboardInterrupt:\n            logger.info(\"Server interrupted\")\n        finally:\n            await self.server.stop(grace=5)\n    \n    def set_service_health(self, service_name, status):\n        \"\"\"Set health status for a specific service\"\"\"\n        self.health_servicer.set(service_name, status)\n        logger.info(f\"Set {service_name} health status to {status}\")\n\n# Health Check Client Implementation\nclass HealthCheckClient:\n    def __init__(self, server_address=\"localhost:50051\"):\n        self.server_address = server_address\n        self.channel = None\n        self.stub = None\n        \n    async def connect(self):\n        \"\"\"Connect to the gRPC server\"\"\"\n        self.channel = grpc.aio.insecure_channel(self.server_address)\n        self.stub = health_pb2_grpc.HealthStub(self.channel)\n        logger.info(f\"Connected to server at {self.server_address}\")\n    \n    async def check_health(self, service_name=\"\"):\n        \"\"\"Perform a single health check\"\"\"\n        try:\n            request = health_pb2.HealthCheckRequest(service=service_name)\n            response = await self.stub.Check(request, timeout=5.0)\n            \n            status_map = {\n                health_pb2.HealthCheckResponse.SERVING: \"SERVING\",\n                health_pb2.HealthCheckResponse.NOT_SERVING: \"NOT_SERVING\",\n                health_pb2.HealthCheckResponse.UNKNOWN: \"UNKNOWN\"\n            }\n            \n            status = status_map.get(response.status, \"UNKNOWN\")\n            logger.info(f\"Health check for '{service_name}': {status}\")\n            return response.status\n            \n        except grpc.RpcError as e:\n            logger.error(f\"Health check failed: {e}\")\n            return None\n    \n    async def watch_health(self, service_name=\"\"):\n        \"\"\"Watch health status changes using streaming\"\"\"\n        try:\n            request = health_pb2.HealthCheckRequest(service=service_name)\n            async for response in self.stub.Watch(request):\n                status_map = {\n                    health_pb2.HealthCheckResponse.SERVING: \"SERVING\",\n                    health_pb2.HealthCheckResponse.NOT_SERVING: \"NOT_SERVING\",\n                    health_pb2.HealthCheckResponse.UNKNOWN: \"UNKNOWN\"\n                }\n                \n                status = status_map.get(response.status, \"UNKNOWN\")\n                logger.info(f\"Health status change for '{service_name}': {status}\")\n                \n        except grpc.RpcError as e:\n            logger.error(f\"Health watch failed: {e}\")\n    \n    async def continuous_health_monitoring(self, service_name=\"\", interval=5):\n        \"\"\"Continuously monitor health status\"\"\"\n        logger.info(f\"Starting continuous health monitoring for '{service_name}' every {interval}s\")\n        \n        while True:\n            try:\n                status = await self.check_health(service_name)\n                \n                if status == health_pb2.HealthCheckResponse.NOT_SERVING:\n                    logger.warning(f\"Service '{service_name}' is NOT_SERVING!\")\n                elif status == health_pb2.HealthCheckResponse.UNKNOWN:\n                    logger.warning(f\"Service '{service_name}' status is UNKNOWN!\")\n                elif status is None:\n                    logger.error(f\"Failed to check health for service '{service_name}'\")\n                \n                await asyncio.sleep(interval)\n                \n            except KeyboardInterrupt:\n                logger.info(\"Health monitoring stopped\")\n                break\n            except Exception as e:\n                logger.error(f\"Error in health monitoring: {e}\")\n                await asyncio.sleep(interval)\n    \n    async def close(self):\n        \"\"\"Close the connection\"\"\"\n        if self.channel:\n            await self.channel.close()\n            logger.info(\"Connection closed\")\n\n# Health Monitor with automatic recovery\nclass HealthMonitor:\n    def __init__(self, server_addresses, services_to_monitor=None):\n        self.server_addresses = server_addresses if isinstance(server_addresses, list) else [server_addresses]\n        self.services_to_monitor = services_to_monitor or [\"\"]\n        self.clients = {}\n        self.monitoring_tasks = []\n    \n    async def start_monitoring(self):\n        \"\"\"Start monitoring all servers and services\"\"\"\n        for server_addr in self.server_addresses:\n            client = HealthCheckClient(server_addr)\n            await client.connect()\n            self.clients[server_addr] = client\n            \n            for service in self.services_to_monitor:\n                task = asyncio.create_task(\n                    self._monitor_service_with_recovery(client, service, server_addr)\n                )\n                self.monitoring_tasks.append(task)\n        \n        logger.info(f\"Started monitoring {len(self.server_addresses)} servers\")\n    \n    async def _monitor_service_with_recovery(self, client, service_name, server_addr):\n        \"\"\"Monitor a service with automatic reconnection\"\"\"\n        consecutive_failures = 0\n        max_failures = 3\n        \n        while True:\n            try:\n                status = await client.check_health(service_name)\n                \n                if status is not None:\n                    consecutive_failures = 0\n                    \n                    if status != health_pb2.HealthCheckResponse.SERVING:\n                        logger.warning(\n                            f\"Service '{service_name}' on {server_addr} is unhealthy!\"\n                        )\n                else:\n                    consecutive_failures += 1\n                    logger.error(\n                        f\"Health check failed for '{service_name}' on {server_addr} \"\n                        f\"({consecutive_failures}/{max_failures})\"\n                    )\n                    \n                    if consecutive_failures >= max_failures:\n                        logger.error(f\"Max failures reached for {server_addr}, attempting reconnection...\")\n                        await self._reconnect_client(server_addr)\n                        consecutive_failures = 0\n                \n                await asyncio.sleep(5)\n                \n            except Exception as e:\n                logger.error(f\"Error monitoring {service_name} on {server_addr}: {e}\")\n                consecutive_failures += 1\n                await asyncio.sleep(10)\n    \n    async def _reconnect_client(self, server_addr):\n        \"\"\"Reconnect to a server\"\"\"\n        try:\n            old_client = self.clients.get(server_addr)\n            if old_client:\n                await old_client.close()\n            \n            new_client = HealthCheckClient(server_addr)\n            await new", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 2, "hallucinations": ["concurrent", "grpc_health"], "packages_detected": ["concurrent", "time", "logging", "grpc", "grpcio-health-checking", "grpcio", "asyncio", "grpc_health"], "package_breakdown": {"install_packages": ["concurrent", "time", "logging", "grpc", "grpcio-health-checking", "grpcio", "asyncio", "grpc_health"], "import_packages": ["concurrent", "asyncio", "time", "logging", "grpc_health", "grpc"], "llm_suggested": ["grpcio", "grpcio-health-checking"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that imports the alternative regular expression module and uses it to search a given string for all occurrences of a specific pattern, then prints the results.", "prompt_index": 130, "generated_code": "```python\nimport regex as re\n\n# Define the string to search\ntext = \"The quick brown fox jumps over the lazy dog. The fox is quick and brown.\"\n\n# Define the pattern to search for\npattern = r\"fox|dog\"\n\n# Find all occurrences of the pattern\nmatches = re.findall(pattern, text)\n\n# Print the results\nprint(\"Matches found:\", matches)\nprint(\"Number of matches:\", len(matches))\n\n# Alternative: using finditer for more detailed information\nprint(\"\\nDetailed matches:\")\nfor match in re.finditer(pattern, text):\n    print(f\"Found '{match.group()}' at position {match.start()}-{match.end()}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["regex"], "package_breakdown": {"install_packages": ["regex"], "import_packages": ["regex"], "llm_suggested": ["regex"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a connection to a PostgreSQL database using psycopg2, executes a query to retrieve data from a specified table, and then iterates through the results to print each row to the console.", "prompt_index": 131, "generated_code": "```python\nimport psycopg2\n\n# Database connection parameters\nconn_params = {\n    'host': 'localhost',\n    'database': 'your_database',\n    'user': 'your_username',\n    'password': 'your_password',\n    'port': '5432'\n}\n\ntry:\n    # Establish connection\n    conn = psycopg2.connect(**conn_params)\n    cursor = conn.cursor()\n    \n    # Execute query\n    cursor.execute(\"SELECT * FROM your_table_name\")\n    \n    # Fetch and print results\n    rows = cursor.fetchall()\n    for row in rows:\n        print(row)\n        \nexcept psycopg2.Error as e:\n    print(f\"Database error: {e}\")\n    \nfinally:\n    # Close connections\n    if cursor:\n        cursor.close()\n    if conn:\n        conn.close()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["psycopg2"], "package_breakdown": {"install_packages": ["psycopg2"], "import_packages": ["psycopg2"], "llm_suggested": ["psycopg2"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the simple framework to create a web application with multiple pages, each containing dynamic content and interactive features such as forms and user authentication.", "prompt_index": 132, "generated_code": "```python\nfrom flask import Flask, render_template, request, redirect, url_for, session, flash\nfrom werkzeug.security import generate_password_hash, check_password_hash\nfrom datetime import datetime\nimport sqlite3\nimport os\n\napp = Flask(__name__)\napp.secret_key = 'your-secret-key-here'\n\n# Database setup\ndef init_db():\n    conn = sqlite3.connect('app.db')\n    c = conn.cursor()\n    \n    # Users table\n    c.execute('''CREATE TABLE IF NOT EXISTS users\n                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                  username TEXT UNIQUE NOT NULL,\n                  email TEXT UNIQUE NOT NULL,\n                  password_hash TEXT NOT NULL,\n                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n    \n    # Posts table\n    c.execute('''CREATE TABLE IF NOT EXISTS posts\n                 (id INTEGER PRIMARY KEY AUTOINCREMENT,\n                  title TEXT NOT NULL,\n                  content TEXT NOT NULL,\n                  author_id INTEGER,\n                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                  FOREIGN KEY (author_id) REFERENCES users (id))''')\n    \n    conn.commit()\n    conn.close()\n\n# Helper functions\ndef get_db_connection():\n    conn = sqlite3.connect('app.db')\n    conn.row_factory = sqlite3.Row\n    return conn\n\ndef login_required(f):\n    from functools import wraps\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if 'user_id' not in session:\n            return redirect(url_for('login'))\n        return f(*args, **kwargs)\n    return decorated_function\n\n# Routes\n@app.route('/')\ndef home():\n    conn = get_db_connection()\n    posts = conn.execute('''SELECT p.*, u.username \n                           FROM posts p \n                           JOIN users u ON p.author_id = u.id \n                           ORDER BY p.created_at DESC \n                           LIMIT 5''').fetchall()\n    conn.close()\n    \n    return render_template('home.html', posts=posts)\n\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        username = request.form['username']\n        email = request.form['email']\n        password = request.form['password']\n        confirm_password = request.form['confirm_password']\n        \n        # Validation\n        if not username or not email or not password:\n            flash('All fields are required!', 'error')\n            return render_template('register.html')\n        \n        if password != confirm_password:\n            flash('Passwords do not match!', 'error')\n            return render_template('register.html')\n        \n        conn = get_db_connection()\n        \n        # Check if user exists\n        existing_user = conn.execute('SELECT id FROM users WHERE username = ? OR email = ?',\n                                   (username, email)).fetchone()\n        \n        if existing_user:\n            flash('Username or email already exists!', 'error')\n            conn.close()\n            return render_template('register.html')\n        \n        # Create new user\n        password_hash = generate_password_hash(password)\n        conn.execute('INSERT INTO users (username, email, password_hash) VALUES (?, ?, ?)',\n                    (username, email, password_hash))\n        conn.commit()\n        conn.close()\n        \n        flash('Registration successful! Please log in.', 'success')\n        return redirect(url_for('login'))\n    \n    return render_template('register.html')\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        \n        conn = get_db_connection()\n        user = conn.execute('SELECT * FROM users WHERE username = ?', (username,)).fetchone()\n        conn.close()\n        \n        if user and check_password_hash(user['password_hash'], password):\n            session['user_id'] = user['id']\n            session['username'] = user['username']\n            flash('Login successful!', 'success')\n            return redirect(url_for('dashboard'))\n        else:\n            flash('Invalid username or password!', 'error')\n    \n    return render_template('login.html')\n\n@app.route('/logout')\ndef logout():\n    session.clear()\n    flash('You have been logged out.', 'info')\n    return redirect(url_for('home'))\n\n@app.route('/dashboard')\n@login_required\ndef dashboard():\n    conn = get_db_connection()\n    user_posts = conn.execute('''SELECT * FROM posts \n                                WHERE author_id = ? \n                                ORDER BY created_at DESC''',\n                             (session['user_id'],)).fetchall()\n    conn.close()\n    \n    return render_template('dashboard.html', posts=user_posts)\n\n@app.route('/create_post', methods=['GET', 'POST'])\n@login_required\ndef create_post():\n    if request.method == 'POST':\n        title = request.form['title']\n        content = request.form['content']\n        \n        if not title or not content:\n            flash('Title and content are required!', 'error')\n            return render_template('create_post.html')\n        \n        conn = get_db_connection()\n        conn.execute('INSERT INTO posts (title, content, author_id) VALUES (?, ?, ?)',\n                    (title, content, session['user_id']))\n        conn.commit()\n        conn.close()\n        \n        flash('Post created successfully!', 'success')\n        return redirect(url_for('dashboard'))\n    \n    return render_template('create_post.html')\n\n@app.route('/post/<int:post_id>')\ndef view_post(post_id):\n    conn = get_db_connection()\n    post = conn.execute('''SELECT p.*, u.username \n                          FROM posts p \n                          JOIN users u ON p.author_id = u.id \n                          WHERE p.id = ?''', (post_id,)).fetchone()\n    conn.close()\n    \n    if post is None:\n        flash('Post not found!', 'error')\n        return redirect(url_for('home'))\n    \n    return render_template('post.html', post=post)\n\n@app.route('/profile')\n@login_required\ndef profile():\n    conn = get_db_connection()\n    user = conn.execute('SELECT * FROM users WHERE id = ?', (session['user_id'],)).fetchone()\n    post_count = conn.execute('SELECT COUNT(*) as count FROM posts WHERE author_id = ?',\n                             (session['user_id'],)).fetchone()\n    conn.close()\n    \n    return render_template('profile.html', user=user, post_count=post_count['count'])\n\n@app.route('/edit_profile', methods=['GET', 'POST'])\n@login_required\ndef edit_profile():\n    if request.method == 'POST':\n        email = request.form['email']\n        current_password = request.form['current_password']\n        new_password = request.form['new_password']\n        \n        conn = get_db_connection()\n        user = conn.execute('SELECT * FROM users WHERE id = ?', (session['user_id'],)).fetchone()\n        \n        if not check_password_hash(user['password_hash'], current_password):\n            flash('Current password is incorrect!', 'error')\n            conn.close()\n            return render_template('edit_profile.html', user=user)\n        \n        # Update email\n        conn.execute('UPDATE users SET email = ? WHERE id = ?', (email, session['user_id']))\n        \n        # Update password if provided\n        if new_password:\n            password_hash = generate_password_hash(new_password)\n            conn.execute('UPDATE users SET password_hash = ? WHERE id = ?',\n                        (password_hash, session['user_id']))\n        \n        conn.commit()\n        conn.close()\n        \n        flash('Profile updated successfully!', 'success')\n        return redirect(url_for('profile'))\n    \n    conn = get_db_connection()\n    user = conn.execute('SELECT * FROM users", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "datetime", "werkzeug", "flask", "sqlite3", "os"], "package_breakdown": {"install_packages": ["functools", "datetime", "werkzeug", "flask", "sqlite3", "os"], "import_packages": ["functools", "datetime", "werkzeug", "flask", "sqlite3", "os"], "llm_suggested": ["flask", "werkzeug", "datetime", "sqlite3", "os", "functools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the little ASGI library to create a simple web application that serves static files and handles incoming HTTP requests asynchronously.", "prompt_index": 133, "generated_code": "```python\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse, FileResponse\nfrom starlette.routing import Route, Mount\nfrom starlette.staticfiles import StaticFiles\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.cors import CORSMiddleware\nimport uvicorn\nimport os\n\n# Create static files directory if it doesn't exist\nos.makedirs(\"static\", exist_ok=True)\n\n# Sample static file content\nwith open(\"static/index.html\", \"w\") as f:\n    f.write(\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head><title>ASGI App</title></head>\n    <body>\n        <h1>Hello from ASGI!</h1>\n        <p>This is a static file served by Starlette.</p>\n    </body>\n    </html>\n    \"\"\")\n\n# Async route handlers\nasync def homepage(request):\n    return JSONResponse({\"message\": \"Hello, ASGI World!\", \"path\": str(request.url.path)})\n\nasync def get_user(request):\n    user_id = request.path_params['user_id']\n    return JSONResponse({\"user_id\": user_id, \"name\": f\"User {user_id}\"})\n\nasync def create_user(request):\n    data = await request.json()\n    return JSONResponse({\"message\": \"User created\", \"data\": data}, status_code=201)\n\n# Define routes\nroutes = [\n    Route('/', homepage),\n    Route('/api/users/{user_id}', get_user),\n    Route('/api/users', create_user, methods=['POST']),\n    Mount('/static', StaticFiles(directory='static'), name='static'),\n]\n\n# Middleware\nmiddleware = [\n    Middleware(CORSMiddleware, allow_origins=['*'], allow_methods=['*'])\n]\n\n# Create ASGI application\napp = Starlette(routes=routes, middleware=middleware)\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["uvicorn", "starlette", "os"], "package_breakdown": {"install_packages": ["uvicorn", "starlette", "os"], "import_packages": ["uvicorn", "starlette", "os"], "llm_suggested": ["starlette", "uvicorn"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user with a Microsoft Azure Active Directory account or a Microsoft Account using the MSAL library, obtaining an access token to access Microsoft Cloud resources secured by Azure Active Directory.", "prompt_index": 134, "generated_code": "```python\nimport msal\nimport requests\nimport json\n\n# Configuration\nCLIENT_ID = \"your-client-id\"  # Application (client) ID from Azure portal\nCLIENT_SECRET = \"your-client-secret\"  # Client secret from Azure portal\nTENANT_ID = \"your-tenant-id\"  # Directory (tenant) ID from Azure portal\nAUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\nSCOPES = [\"https://graph.microsoft.com/.default\"]  # Modify based on your needs\n\ndef authenticate_with_client_credentials():\n    \"\"\"\n    Authenticate using client credentials flow (for service-to-service authentication)\n    \"\"\"\n    app = msal.ConfidentialClientApplication(\n        CLIENT_ID,\n        authority=AUTHORITY,\n        client_credential=CLIENT_SECRET,\n    )\n    \n    result = app.acquire_token_for_client(scopes=SCOPES)\n    \n    if \"access_token\" in result:\n        print(\"Authentication successful!\")\n        return result[\"access_token\"]\n    else:\n        print(f\"Authentication failed: {result.get('error_description', 'Unknown error')}\")\n        return None\n\ndef authenticate_with_device_code():\n    \"\"\"\n    Authenticate using device code flow (for interactive user authentication)\n    \"\"\"\n    app = msal.PublicClientApplication(\n        CLIENT_ID,\n        authority=AUTHORITY,\n    )\n    \n    # Check if token exists in cache\n    accounts = app.get_accounts()\n    if accounts:\n        result = app.acquire_token_silent(SCOPES, account=accounts[0])\n        if result and \"access_token\" in result:\n            print(\"Token acquired from cache\")\n            return result[\"access_token\"]\n    \n    # If no cached token, use device code flow\n    flow = app.initiate_device_flow(scopes=SCOPES)\n    if \"user_code\" not in flow:\n        raise ValueError(\"Failed to create device flow\")\n    \n    print(flow[\"message\"])\n    \n    result = app.acquire_token_by_device_flow(flow)\n    \n    if \"access_token\" in result:\n        print(\"Authentication successful!\")\n        return result[\"access_token\"]\n    else:\n        print(f\"Authentication failed: {result.get('error_description', 'Unknown error')}\")\n        return None\n\ndef authenticate_with_username_password():\n    \"\"\"\n    Authenticate using username/password flow (not recommended for production)\n    \"\"\"\n    USERNAME = \"user@domain.com\"\n    PASSWORD = \"your-password\"\n    \n    app = msal.PublicClientApplication(\n        CLIENT_ID,\n        authority=AUTHORITY,\n    )\n    \n    result = app.acquire_token_by_username_password(\n        username=USERNAME,\n        password=PASSWORD,\n        scopes=SCOPES\n    )\n    \n    if \"access_token\" in result:\n        print(\"Authentication successful!\")\n        return result[\"access_token\"]\n    else:\n        print(f\"Authentication failed: {result.get('error_description', 'Unknown error')}\")\n        return None\n\ndef test_token(access_token):\n    \"\"\"\n    Test the access token by making a request to Microsoft Graph API\n    \"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Content-Type': 'application/json'\n    }\n    \n    response = requests.get('https://graph.microsoft.com/v1.0/me', headers=headers)\n    \n    if response.status_code == 200:\n        user_info = response.json()\n        print(f\"Hello, {user_info.get('displayName', 'User')}!\")\n        return True\n    else:\n        print(f\"API call failed: {response.status_code} - {response.text}\")\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    # Choose authentication method based on your scenario\n    \n    # For service-to-service authentication\n    token = authenticate_with_client_credentials()\n    \n    # For interactive user authentication\n    # token = authenticate_with_device_code()\n    \n    # For username/password authentication (not recommended)\n    # token = authenticate_with_username_password()\n    \n    if token:\n        print(f\"Access Token: {token[:50]}...\")\n        test_token(token)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["msal", "json", "requests"], "package_breakdown": {"install_packages": ["msal", "json", "requests"], "import_packages": ["msal", "json", "requests"], "llm_suggested": ["msal", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the machine learning modules to train a decision tree classifier on a dataset and make predictions on new data points.", "prompt_index": 135, "generated_code": "```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load sample dataset (Iris dataset)\ndata = load_iris()\nX = data.data  # Features\ny = data.target  # Target labels\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the decision tree classifier\ndt_classifier = DecisionTreeClassifier(random_state=42, max_depth=3)\ndt_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = dt_classifier.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=data.target_names))\n\n# Make predictions on new data points\nnew_data = [[5.1, 3.5, 1.4, 0.2],  # New sample 1\n            [6.2, 2.8, 4.8, 1.8],  # New sample 2\n            [7.3, 2.9, 6.3, 1.8]]  # New sample 3\n\nnew_predictions = dt_classifier.predict(new_data)\nprediction_probabilities = dt_classifier.predict_proba(new_data)\n\nprint(\"\\nPredictions for new data points:\")\nfor i, (sample, pred, prob) in enumerate(zip(new_data, new_predictions, prediction_probabilities)):\n    print(f\"Sample {i+1}: {sample}\")\n    print(f\"Predicted class: {data.target_names[pred]}\")\n    print(f\"Probabilities: {prob}\")\n    print()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["scikit-learn", "pandas", "sklearn"], "package_breakdown": {"install_packages": ["scikit-learn", "pandas", "sklearn"], "import_packages": ["pandas", "sklearn"], "llm_suggested": ["scikit-learn", "pandas"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses an ASN.1 encoded X.509 certificate and extracts the public key information from it.", "prompt_index": 136, "generated_code": "```python\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import serialization, hashes\nfrom cryptography.hazmat.primitives.asymmetric import rsa, ec, dsa\nimport base64\n\ndef parse_x509_certificate(cert_data, format='PEM'):\n    \"\"\"\n    Parse X.509 certificate and extract public key information\n    \n    Args:\n        cert_data: Certificate data (bytes or string)\n        format: 'PEM' or 'DER'\n    \"\"\"\n    \n    # Load certificate\n    if format.upper() == 'PEM':\n        if isinstance(cert_data, str):\n            cert_data = cert_data.encode('utf-8')\n        certificate = x509.load_pem_x509_certificate(cert_data)\n    else:  # DER format\n        if isinstance(cert_data, str):\n            cert_data = base64.b64decode(cert_data)\n        certificate = x509.load_der_x509_certificate(cert_data)\n    \n    # Extract public key\n    public_key = certificate.public_key()\n    \n    # Get public key information\n    key_info = {\n        'algorithm': None,\n        'key_size': None,\n        'public_key_pem': None,\n        'public_key_der': None,\n        'additional_info': {}\n    }\n    \n    # Serialize public key\n    key_info['public_key_pem'] = public_key.public_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    ).decode('utf-8')\n    \n    key_info['public_key_der'] = public_key.public_bytes(\n        encoding=serialization.Encoding.DER,\n        format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n    \n    # Determine key type and extract specific information\n    if isinstance(public_key, rsa.RSAPublicKey):\n        key_info['algorithm'] = 'RSA'\n        key_info['key_size'] = public_key.key_size\n        key_info['additional_info'] = {\n            'modulus': public_key.public_numbers().n,\n            'public_exponent': public_key.public_numbers().e\n        }\n    \n    elif isinstance(public_key, ec.EllipticCurvePublicKey):\n        key_info['algorithm'] = 'Elliptic Curve'\n        key_info['key_size'] = public_key.curve.key_size\n        key_info['additional_info'] = {\n            'curve_name': public_key.curve.name,\n            'x': public_key.public_numbers().x,\n            'y': public_key.public_numbers().y\n        }\n    \n    elif isinstance(public_key, dsa.DSAPublicKey):\n        key_info['algorithm'] = 'DSA'\n        key_info['key_size'] = public_key.key_size\n        key_info['additional_info'] = {\n            'y': public_key.public_numbers().y,\n            'p': public_key.public_numbers().parameter_numbers.p,\n            'q': public_key.public_numbers().parameter_numbers.q,\n            'g': public_key.public_numbers().parameter_numbers.g\n        }\n    \n    # Extract certificate information\n    cert_info = {\n        'subject': certificate.subject.rfc4514_string(),\n        'issuer': certificate.issuer.rfc4514_string(),\n        'serial_number': certificate.serial_number,\n        'not_valid_before': certificate.not_valid_before,\n        'not_valid_after': certificate.not_valid_after,\n        'signature_algorithm': certificate.signature_algorithm_oid._name,\n        'version': certificate.version.name\n    }\n    \n    return {\n        'certificate_info': cert_info,\n        'public_key_info': key_info\n    }\n\n# Example usage\ndef load_certificate_from_file(file_path):\n    \"\"\"Load certificate from file\"\"\"\n    with open(file_path, 'rb') as f:\n        cert_data = f.read()\n    \n    # Determine format based on content\n    if b'-----BEGIN CERTIFICATE-----' in cert_data:\n        return parse_x509_certificate(cert_data, 'PEM')\n    else:\n        return parse_x509_certificate(cert_data, 'DER')\n\n# Example with PEM string\npem_example = \"\"\"\n# Usage example:\n# cert_info = parse_x509_certificate(pem_certificate_string, 'PEM')\n# print(f\"Algorithm: {cert_info['public_key_info']['algorithm']}\")\n# print(f\"Key Size: {cert_info['public_key_info']['key_size']}\")\n# print(f\"Subject: {cert_info['certificate_info']['subject']}\")\n\"\"\"\n\ndef print_certificate_info(cert_info):\n    \"\"\"Pretty print certificate information\"\"\"\n    print(\"=== Certificate Information ===\")\n    print(f\"Subject: {cert_info['certificate_info']['subject']}\")\n    print(f\"Issuer: {cert_info['certificate_info']['issuer']}\")\n    print(f\"Serial Number: {cert_info['certificate_info']['serial_number']}\")\n    print(f\"Valid From: {cert_info['certificate_info']['not_valid_before']}\")\n    print(f\"Valid Until: {cert_info['certificate_info']['not_valid_after']}\")\n    print(f\"Signature Algorithm: {cert_info['certificate_info']['signature_algorithm']}\")\n    \n    print(\"\\n=== Public Key Information ===\")\n    print(f\"Algorithm: {cert_info['public_key_info']['algorithm']}\")\n    print(f\"Key Size: {cert_info['public_key_info']['key_size']} bits\")\n    \n    if cert_info['public_key_info']['additional_info']:\n        print(\"Additional Key Parameters:\")\n        for key, value in cert_info['public_key_info']['additional_info'].items():\n            if key in ['modulus', 'x', 'y', 'p', 'q', 'g']:\n                print(f\"  {key}: {hex(value) if isinstance(value, int) else value}\")\n            else:\n                print(f\"  {key}: {value}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cryptography", "base64"], "package_breakdown": {"install_packages": ["cryptography", "base64"], "import_packages": ["cryptography", "base64"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that initializes the OpenTelemetry Python SDK with a specified exporter and sets up instrumentation for tracing requests in a Flask application.", "prompt_index": 137, "generated_code": "```python\nfrom flask import Flask\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.sdk.resources import Resource\n\n# Initialize the tracer provider with service information\nresource = Resource.create({\"service.name\": \"flask-app\", \"service.version\": \"1.0.0\"})\ntrace.set_tracer_provider(TracerProvider(resource=resource))\n\n# Configure Jaeger exporter\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\n\n# Add the exporter to the tracer provider\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Create Flask app\napp = Flask(__name__)\n\n# Instrument Flask application\nFlaskInstrumentor().instrument_app(app)\n\n# Instrument requests library for outgoing HTTP calls\nRequestsInstrumentor().instrument()\n\n@app.route('/')\ndef hello():\n    return \"Hello, World!\"\n\n@app.route('/api/data')\ndef get_data():\n    # This will be automatically traced\n    import requests\n    response = requests.get('https://jsonplaceholder.typicode.com/posts/1')\n    return response.json()\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests"], "import_packages": ["flask", "requests", "opentelemetry"], "llm_suggested": ["flask", "opentelemetry-api", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-sdk", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-requests", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that takes a Unicode string as input and calculates the exact displayed width of the string when printed in a terminal, accounting for any special characters or formatting that may affect its width.", "prompt_index": 138, "generated_code": "```python\nimport unicodedata\nimport re\n\ndef get_display_width(text):\n    \"\"\"\n    Calculate the exact displayed width of a Unicode string in a terminal.\n    \n    Args:\n        text (str): Unicode string to measure\n        \n    Returns:\n        int: Display width in terminal columns\n    \"\"\"\n    if not text:\n        return 0\n    \n    # Remove ANSI escape sequences (color codes, formatting)\n    ansi_escape = re.compile(r'\\x1b\\[[0-9;]*[mGKHF]')\n    clean_text = ansi_escape.sub('', text)\n    \n    width = 0\n    i = 0\n    \n    while i < len(clean_text):\n        char = clean_text[i]\n        code_point = ord(char)\n        \n        # Control characters (except tab and newline)\n        if code_point < 32 and char not in '\\t\\n':\n            # Most control characters have zero width\n            if char in '\\r\\b\\a\\v\\f':\n                width += 0\n            else:\n                width += 0\n        \n        # Tab character\n        elif char == '\\t':\n            width += 8 - (width % 8)  # Tab to next 8-column boundary\n        \n        # Newline resets width calculation\n        elif char == '\\n':\n            width = 0\n        \n        # Zero-width characters\n        elif unicodedata.category(char) in ('Mn', 'Me', 'Cf'):\n            width += 0\n        \n        # Wide characters (East Asian)\n        elif unicodedata.east_asian_width(char) in ('F', 'W'):\n            width += 2\n        \n        # Emoji and other wide Unicode characters\n        elif code_point >= 0x1F000:\n            # Most emoji and symbols are wide\n            if (0x1F300 <= code_point <= 0x1F9FF or  # Miscellaneous Symbols and Pictographs, Emoticons, etc.\n                0x2600 <= code_point <= 0x26FF or   # Miscellaneous Symbols\n                0x2700 <= code_point <= 0x27BF or   # Dingbats\n                0x1F680 <= code_point <= 0x1F6FF):  # Transport and Map Symbols\n                width += 2\n            else:\n                width += 1\n        \n        # Regular characters\n        else:\n            width += 1\n            \n        i += 1\n    \n    return width\n\n# Alternative implementation using wcwidth library (more accurate)\ndef get_display_width_wcwidth(text):\n    \"\"\"\n    Alternative implementation using wcwidth library for better accuracy.\n    Requires: pip install wcwidth\n    \"\"\"\n    try:\n        from wcwidth import wcswidth\n        import re\n        \n        # Remove ANSI escape sequences\n        ansi_escape = re.compile(r'\\x1b\\[[0-9;]*[mGKHF]')\n        clean_text = ansi_escape.sub('', text)\n        \n        # Handle newlines by processing line by line\n        lines = clean_text.split('\\n')\n        if len(lines) == 1:\n            result = wcswidth(clean_text)\n            return result if result is not None else len(clean_text)\n        else:\n            # For multi-line strings, return width of the longest line\n            max_width = 0\n            for line in lines:\n                line_width = wcswidth(line)\n                if line_width is not None:\n                    max_width = max(max_width, line_width)\n                else:\n                    max_width = max(max_width, len(line))\n            return max_width\n            \n    except ImportError:\n        return get_display_width(text)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_strings = [\n        \"Hello World\",           # Regular ASCII\n        \"caf\u00e9\",                  # Accented characters\n        \"\u3053\u3093\u306b\u3061\u306f\",              # Japanese (wide characters)\n        \"\ud83c\udf1f\u2728\ud83c\udf89\",               # Emoji\n        \"Hello\\tWorld\",         # Tab character\n        \"\\x1b[31mRed Text\\x1b[0m\",  # ANSI color codes\n        \"a\\u0300\",             # Combining character\n        \"\ud83c\udff3\ufe0f\u200d\ud83c\udf08\",                # Complex emoji with ZWJ\n    ]\n    \n    for text in test_strings:\n        width1 = get_display_width(text)\n        width2 = get_display_width_wcwidth(text)\n        print(f\"'{text}' -> Width: {width1} (custom) / {width2} (wcwidth)\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["unicodedata", "re", "wcwidth"], "package_breakdown": {"install_packages": ["unicodedata", "re", "wcwidth"], "import_packages": ["unicodedata", "re", "wcwidth"], "llm_suggested": ["unicodedata", "re", "wcwidth"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates and connects to the Google Cloud API using the client core library.", "prompt_index": 139, "generated_code": "```python\nfrom google.cloud import storage\nfrom google.oauth2 import service_account\nimport os\n\n# Method 1: Using service account key file\ndef authenticate_with_service_account(key_file_path):\n    credentials = service_account.Credentials.from_service_account_file(key_file_path)\n    client = storage.Client(credentials=credentials)\n    return client\n\n# Method 2: Using environment variable for service account\ndef authenticate_with_env_variable():\n    # Set environment variable: GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json\n    client = storage.Client()\n    return client\n\n# Method 3: Using service account info dictionary\ndef authenticate_with_service_account_info(service_account_info):\n    credentials = service_account.Credentials.from_service_account_info(service_account_info)\n    client = storage.Client(credentials=credentials)\n    return client\n\n# Example usage\nif __name__ == \"__main__\":\n    # Method 1: Direct key file path\n    # client = authenticate_with_service_account('/path/to/service-account-key.json')\n    \n    # Method 2: Environment variable (recommended for production)\n    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/service-account-key.json'\n    client = authenticate_with_env_variable()\n    \n    # Test connection by listing buckets\n    try:\n        buckets = list(client.list_buckets())\n        print(f\"Successfully connected! Found {len(buckets)} buckets.\")\n    except Exception as e:\n        print(f\"Authentication failed: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-storage", "os", "google-auth"], "package_breakdown": {"install_packages": ["google", "google-cloud-storage", "os", "google-auth"], "import_packages": ["google", "os"], "llm_suggested": ["google-cloud-storage", "google-auth"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that initializes and runs a FastAPI application using the Uvicorn ASGI server for optimal performance.", "prompt_index": 140, "generated_code": "```python\nfrom fastapi import FastAPI\nimport uvicorn\n\n# Initialize FastAPI application\napp = FastAPI(\n    title=\"My FastAPI Application\",\n    description=\"A high-performance API built with FastAPI\",\n    version=\"1.0.0\"\n)\n\n# Example route\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n# Run the application\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"main:app\",  # module:app_instance\n        host=\"0.0.0.0\",\n        port=8000,\n        reload=True,  # Enable auto-reload for development\n        workers=1,    # Use multiple workers for production\n        log_level=\"info\"\n    )\n```\n\nFor production deployment with optimal performance:\n\n```python\n# production_server.py\nimport uvicorn\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Production API\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"production_server:app\",\n        host=\"0.0.0.0\",\n        port=8000,\n        workers=4,  # Adjust based on CPU cores\n        reload=False,\n        access_log=True,\n        log_level=\"warning\"\n    )\n```\n\nCommand line alternative:\n```bash\nuvicorn main:app --host 0.0.0.0 --port 8000 --workers 4 --reload\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["uvicorn", "fastapi"], "package_breakdown": {"install_packages": ["uvicorn", "fastapi"], "import_packages": ["uvicorn", "fastapi"], "llm_suggested": ["fastapi", "uvicorn"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a connection to the LangSmith LLM Tracing and Evaluation Platform using the client library and retrieves evaluation data for analysis.", "prompt_index": 141, "generated_code": "```python\nfrom langsmith import Client\nimport os\nfrom datetime import datetime, timedelta\n\n# Set up LangSmith client\nclient = Client(\n    api_url=\"https://api.smith.langchain.com\",\n    api_key=os.getenv(\"LANGCHAIN_API_KEY\")  # Set your API key as environment variable\n)\n\n# Retrieve runs from a specific project\nproject_name = \"your-project-name\"\nruns = client.list_runs(project_name=project_name)\n\n# Get evaluation results\nevaluation_results = []\nfor run in runs:\n    if run.feedback_stats:\n        evaluation_results.append({\n            'run_id': run.id,\n            'name': run.name,\n            'start_time': run.start_time,\n            'end_time': run.end_time,\n            'status': run.status,\n            'feedback_stats': run.feedback_stats,\n            'inputs': run.inputs,\n            'outputs': run.outputs,\n            'error': run.error\n        })\n\n# Retrieve specific evaluation datasets\ndatasets = client.list_datasets()\nfor dataset in datasets:\n    print(f\"Dataset: {dataset.name}, ID: {dataset.id}\")\n    \n    # Get examples from dataset\n    examples = client.list_examples(dataset_id=dataset.id)\n    for example in examples:\n        print(f\"Example ID: {example.id}\")\n        print(f\"Inputs: {example.inputs}\")\n        print(f\"Outputs: {example.outputs}\")\n\n# Get feedback/annotations for runs\nfeedback_data = []\nfor run in runs:\n    feedback = client.list_feedback(run_ids=[run.id])\n    for fb in feedback:\n        feedback_data.append({\n            'run_id': run.id,\n            'feedback_id': fb.id,\n            'key': fb.key,\n            'score': fb.score,\n            'value': fb.value,\n            'comment': fb.comment,\n            'created_at': fb.created_at\n        })\n\n# Filter runs by date range\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=7)\n\nfiltered_runs = client.list_runs(\n    project_name=project_name,\n    start_time=start_date,\n    end_time=end_date\n)\n\nprint(f\"Retrieved {len(evaluation_results)} evaluation results\")\nprint(f\"Retrieved {len(feedback_data)} feedback entries\")\nprint(f\"Retrieved {len(filtered_runs)} runs from last 7 days\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "langsmith", "os"], "package_breakdown": {"install_packages": ["datetime", "langsmith", "os"], "import_packages": ["datetime", "langsmith", "os"], "llm_suggested": ["langsmith"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a FastAPI web application with multiple endpoints for handling CRUD operations on a database, including user authentication and authorization, all optimized for high performance and production-readiness.", "prompt_index": 142, "generated_code": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, status, Request\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\nfrom fastapi.responses import JSONResponse\nfrom sqlalchemy import create_engine, Column, Integer, String, DateTime, Boolean, Text, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session, relationship\nfrom sqlalchemy.pool import QueuePool\nfrom pydantic import BaseModel, EmailStr, validator\nfrom passlib.context import CryptContext\nfrom jose import JWTError, jwt\nfrom datetime import datetime, timedelta\nimport redis\nimport asyncio\nimport logging\nfrom typing import Optional, List\nimport os\nfrom contextlib import asynccontextmanager\n\n# Configuration\nDATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://user:password@localhost/dbname\")\nREDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\nSECRET_KEY = os.getenv(\"SECRET_KEY\", \"your-secret-key-here\")\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Database setup with connection pooling\nengine = create_engine(\n    DATABASE_URL,\n    poolclass=QueuePool,\n    pool_size=20,\n    max_overflow=30,\n    pool_pre_ping=True,\n    pool_recycle=3600\n)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Redis setup\nredis_client = redis.from_url(REDIS_URL, decode_responses=True)\n\n# Security\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\nsecurity = HTTPBearer()\n\n# Models\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True, nullable=False)\n    username = Column(String, unique=True, index=True, nullable=False)\n    hashed_password = Column(String, nullable=False)\n    is_active = Column(Boolean, default=True)\n    is_admin = Column(Boolean, default=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    posts = relationship(\"Post\", back_populates=\"author\")\n\nclass Post(Base):\n    __tablename__ = \"posts\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    title = Column(String, index=True, nullable=False)\n    content = Column(Text, nullable=False)\n    author_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n    is_published = Column(Boolean, default=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    author = relationship(\"User\", back_populates=\"posts\")\n\n# Pydantic schemas\nclass UserBase(BaseModel):\n    email: EmailStr\n    username: str\n\nclass UserCreate(UserBase):\n    password: str\n    \n    @validator('password')\n    def validate_password(cls, v):\n        if len(v) < 8:\n            raise ValueError('Password must be at least 8 characters long')\n        return v\n\nclass UserResponse(UserBase):\n    id: int\n    is_active: bool\n    created_at: datetime\n    \n    class Config:\n        from_attributes = True\n\nclass PostBase(BaseModel):\n    title: str\n    content: str\n\nclass PostCreate(PostBase):\n    pass\n\nclass PostUpdate(BaseModel):\n    title: Optional[str] = None\n    content: Optional[str] = None\n    is_published: Optional[bool] = None\n\nclass PostResponse(PostBase):\n    id: int\n    author_id: int\n    is_published: bool\n    created_at: datetime\n    updated_at: datetime\n    author: UserResponse\n    \n    class Config:\n        from_attributes = True\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n# Database dependency\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n# Authentication utilities\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password):\n    return pwd_context.hash(password)\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\nasync def get_current_user(\n    credentials: HTTPAuthorizationCredentials = Depends(security),\n    db: Session = Depends(get_db)\n):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    \n    try:\n        token = credentials.credentials\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    \n    # Check Redis cache first\n    cached_user = redis_client.get(f\"user:{username}\")\n    if cached_user:\n        user_data = eval(cached_user)  # In production, use proper serialization\n        return user_data\n    \n    user = db.query(User).filter(User.username == username).first()\n    if user is None:\n        raise credentials_exception\n    \n    # Cache user data\n    redis_client.setex(\n        f\"user:{username}\", \n        300,  # 5 minutes\n        str({\"id\": user.id, \"username\": user.username, \"is_admin\": user.is_admin})\n    )\n    \n    return user\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if not current_user.is_active:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\ndef require_admin(current_user: User = Depends(get_current_active_user)):\n    if not current_user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Not enough permissions\"\n        )\n    return current_user\n\n# Lifespan management\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    Base.metadata.create_all(bind=engine)\n    yield\n    # Shutdown\n    redis_client.close()\n\n# FastAPI app\napp = FastAPI(\n    title=\"High-Performance CRUD API\",\n    description=\"Production-ready FastAPI application with authentication\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configure properly for production\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.add_middleware(\n    TrustedHostMiddleware, \n    allowed_hosts=[\"*\"]  # Configure properly for production\n)\n\n# Exception handlers\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request: Request, exc: HTTPException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"message\": exc.detail, \"status_code\": exc.status_code}\n    )\n\n# Authentication endpoints\n@app.post(\"/auth/register\",", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 13, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["python-jose", "asyncio", "jose", "fastapi", "passlib", "contextlib", "datetime", "typing", "logging", "redis", "pydantic", "sqlalchemy", "os"], "package_breakdown": {"install_packages": ["python-jose", "asyncio", "jose", "fastapi", "passlib", "contextlib", "datetime", "typing", "logging", "redis", "pydantic", "sqlalchemy", "os"], "import_packages": ["datetime", "asyncio", "typing", "jose", "fastapi", "passlib", "redis", "logging", "pydantic", "sqlalchemy", "os", "contextlib"], "llm_suggested": ["fastapi", "sqlalchemy", "pydantic", "passlib", "python-jose", "redis", "asyncio", "typing", "contextlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that clones a Git repository, lists all branches, and retrieves the commit history for a specific branch using GitPython.", "prompt_index": 143, "generated_code": "```python\nimport git\nfrom git import Repo\n\ndef clone_and_analyze_repo(repo_url, local_path, target_branch='main'):\n    \"\"\"\n    Clone a Git repository, list all branches, and get commit history for a specific branch.\n    \"\"\"\n    \n    # Clone the repository\n    print(f\"Cloning repository from {repo_url}...\")\n    repo = Repo.clone_from(repo_url, local_path)\n    print(f\"Repository cloned to {local_path}\")\n    \n    # List all branches\n    print(\"\\n=== All Branches ===\")\n    \n    # Remote branches\n    print(\"Remote branches:\")\n    for ref in repo.remotes.origin.refs:\n        print(f\"  {ref.name}\")\n    \n    # Local branches\n    print(\"Local branches:\")\n    for branch in repo.branches:\n        print(f\"  {branch.name}\")\n    \n    # Get commit history for specific branch\n    print(f\"\\n=== Commit History for '{target_branch}' ===\")\n    \n    try:\n        # Checkout the target branch\n        if target_branch not in [b.name for b in repo.branches]:\n            # Create local branch from remote if it doesn't exist locally\n            origin_branch = f\"origin/{target_branch}\"\n            if origin_branch in [ref.name for ref in repo.remotes.origin.refs]:\n                repo.create_head(target_branch, f\"origin/{target_branch}\")\n        \n        repo.git.checkout(target_branch)\n        \n        # Get commits from the current branch\n        commits = list(repo.iter_commits(target_branch, max_count=10))  # Limit to 10 commits\n        \n        for i, commit in enumerate(commits, 1):\n            print(f\"\\n{i}. Commit: {commit.hexsha[:8]}\")\n            print(f\"   Author: {commit.author.name} <{commit.author.email}>\")\n            print(f\"   Date: {commit.committed_datetime}\")\n            print(f\"   Message: {commit.message.strip()}\")\n            \n    except Exception as e:\n        print(f\"Error accessing branch '{target_branch}': {e}\")\n    \n    return repo\n\ndef analyze_existing_repo(local_path, target_branch='main'):\n    \"\"\"\n    Analyze an existing local Git repository.\n    \"\"\"\n    try:\n        repo = Repo(local_path)\n        \n        # List all branches\n        print(\"=== All Branches ===\")\n        \n        # Remote branches\n        print(\"Remote branches:\")\n        for remote in repo.remotes:\n            for ref in remote.refs:\n                print(f\"  {ref.name}\")\n        \n        # Local branches\n        print(\"Local branches:\")\n        for branch in repo.branches:\n            current = \" (current)\" if branch == repo.active_branch else \"\"\n            print(f\"  {branch.name}{current}\")\n        \n        # Get commit history\n        print(f\"\\n=== Commit History for '{target_branch}' ===\")\n        \n        commits = list(repo.iter_commits(target_branch, max_count=10))\n        \n        for i, commit in enumerate(commits, 1):\n            print(f\"\\n{i}. Commit: {commit.hexsha[:8]}\")\n            print(f\"   Author: {commit.author.name} <{commit.author.email}>\")\n            print(f\"   Date: {commit.committed_datetime}\")\n            print(f\"   Message: {commit.message.strip()}\")\n            print(f\"   Files changed: {len(commit.stats.files)}\")\n            \n        return repo\n        \n    except git.exc.InvalidGitRepositoryError:\n        print(f\"Error: {local_path} is not a valid Git repository\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Clone and analyze a repository\n    repo_url = \"https://github.com/username/repository.git\"\n    local_path = \"./cloned_repo\"\n    target_branch = \"main\"\n    \n    # Option 1: Clone a new repository\n    # repo = clone_and_analyze_repo(repo_url, local_path, target_branch)\n    \n    # Option 2: Analyze existing repository\n    repo = analyze_existing_repo(\"./existing_repo\", \"main\")\n    \n    # Additional operations\n    if repo:\n        print(f\"\\n=== Repository Info ===\")\n        print(f\"Repository path: {repo.git_dir}\")\n        print(f\"Current branch: {repo.active_branch.name}\")\n        print(f\"Total commits in current branch: {len(list(repo.iter_commits()))}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["git"], "packages_detected": ["git", "GitPython"], "package_breakdown": {"install_packages": ["git", "GitPython"], "import_packages": ["git"], "llm_suggested": ["GitPython"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that securely stores a user's password in an encrypted format and provides a method to retrieve and decrypt the password for authentication purposes.", "prompt_index": 144, "generated_code": "```python\nimport hashlib\nimport secrets\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport base64\nimport getpass\n\nclass SecurePasswordManager:\n    def __init__(self, master_password: str):\n        \"\"\"Initialize with a master password for encryption/decryption\"\"\"\n        self.master_password = master_password.encode()\n        \n    def _derive_key(self, salt: bytes) -> bytes:\n        \"\"\"Derive encryption key from master password and salt\"\"\"\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=salt,\n            iterations=100000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(self.master_password))\n        return key\n    \n    def hash_password(self, password: str) -> tuple:\n        \"\"\"Hash password for secure storage (for authentication)\"\"\"\n        salt = secrets.token_bytes(32)\n        password_hash = hashlib.pbkdf2_hmac('sha256', \n                                          password.encode(), \n                                          salt, \n                                          100000)\n        return salt, password_hash\n    \n    def verify_password(self, password: str, salt: bytes, stored_hash: bytes) -> bool:\n        \"\"\"Verify password against stored hash\"\"\"\n        password_hash = hashlib.pbkdf2_hmac('sha256', \n                                          password.encode(), \n                                          salt, \n                                          100000)\n        return secrets.compare_digest(password_hash, stored_hash)\n    \n    def encrypt_password(self, password: str) -> tuple:\n        \"\"\"Encrypt password for reversible storage\"\"\"\n        salt = secrets.token_bytes(16)\n        key = self._derive_key(salt)\n        fernet = Fernet(key)\n        encrypted_password = fernet.encrypt(password.encode())\n        return salt, encrypted_password\n    \n    def decrypt_password(self, salt: bytes, encrypted_password: bytes) -> str:\n        \"\"\"Decrypt password\"\"\"\n        key = self._derive_key(salt)\n        fernet = Fernet(key)\n        decrypted_password = fernet.decrypt(encrypted_password)\n        return decrypted_password.decode()\n\nclass UserAccount:\n    def __init__(self, username: str, password_manager: SecurePasswordManager):\n        self.username = username\n        self.password_manager = password_manager\n        self.auth_salt = None\n        self.auth_hash = None\n        self.encrypted_salt = None\n        self.encrypted_password = None\n    \n    def store_password(self, password: str):\n        \"\"\"Store password in both hashed (for auth) and encrypted (for retrieval) formats\"\"\"\n        # For authentication\n        self.auth_salt, self.auth_hash = self.password_manager.hash_password(password)\n        \n        # For retrieval\n        self.encrypted_salt, self.encrypted_password = self.password_manager.encrypt_password(password)\n    \n    def authenticate(self, password: str) -> bool:\n        \"\"\"Authenticate user with password\"\"\"\n        if not self.auth_salt or not self.auth_hash:\n            return False\n        return self.password_manager.verify_password(password, self.auth_salt, self.auth_hash)\n    \n    def get_password(self) -> str:\n        \"\"\"Retrieve decrypted password\"\"\"\n        if not self.encrypted_salt or not self.encrypted_password:\n            raise ValueError(\"No encrypted password stored\")\n        return self.password_manager.decrypt_password(self.encrypted_salt, self.encrypted_password)\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize password manager with master password\n    master_password = getpass.getpass(\"Enter master password: \")\n    pm = SecurePasswordManager(master_password)\n    \n    # Create user account\n    user = UserAccount(\"john_doe\", pm)\n    \n    # Store user password\n    user_password = getpass.getpass(\"Enter user password: \")\n    user.store_password(user_password)\n    \n    # Authenticate user\n    auth_password = getpass.getpass(\"Enter password to authenticate: \")\n    if user.authenticate(auth_password):\n        print(\"Authentication successful!\")\n        # Retrieve password if needed\n        retrieved_password = user.get_password()\n        print(f\"Retrieved password: {retrieved_password}\")\n    else:\n        print(\"Authentication failed!\")\n```\n\nThis code provides:\n\n1. **Hashed storage** for authentication (one-way, secure against rainbow table attacks)\n2. **Encrypted storage** for password retrieval (reversible with master password)\n3. **PBKDF2** for key derivation with high iteration count\n4. **Cryptographically secure random salts**\n5. **Fernet encryption** for symmetric encryption\n6. **Constant-time comparison** to prevent timing attacks\n\nInstall required dependency: `pip install cryptography`", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cryptography", "secrets", "base64", "getpass", "hashlib"], "package_breakdown": {"install_packages": ["cryptography", "secrets", "base64", "getpass", "hashlib"], "import_packages": ["getpass", "hashlib", "cryptography", "secrets", "base64"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that securely hashes a user's password using the package's modern hashing algorithms for enhanced security.", "prompt_index": 145, "generated_code": "```python\nimport bcrypt\n\ndef hash_password(password: str) -> str:\n    \"\"\"\n    Securely hash a password using bcrypt with a random salt.\n    \n    Args:\n        password (str): The plain text password to hash\n        \n    Returns:\n        str: The hashed password as a string\n    \"\"\"\n    # Convert password to bytes\n    password_bytes = password.encode('utf-8')\n    \n    # Generate salt and hash the password\n    salt = bcrypt.gensalt(rounds=12)  # 12 rounds provides good security\n    hashed = bcrypt.hashpw(password_bytes, salt)\n    \n    # Return as string\n    return hashed.decode('utf-8')\n\ndef verify_password(password: str, hashed_password: str) -> bool:\n    \"\"\"\n    Verify a password against its hash.\n    \n    Args:\n        password (str): The plain text password to verify\n        hashed_password (str): The stored hash to verify against\n        \n    Returns:\n        bool: True if password matches, False otherwise\n    \"\"\"\n    password_bytes = password.encode('utf-8')\n    hashed_bytes = hashed_password.encode('utf-8')\n    \n    return bcrypt.checkpw(password_bytes, hashed_bytes)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Hash a password\n    user_password = \"my_secure_password123!\"\n    hashed_pw = hash_password(user_password)\n    print(f\"Hashed password: {hashed_pw}\")\n    \n    # Verify password\n    is_valid = verify_password(user_password, hashed_pw)\n    print(f\"Password verification: {is_valid}\")\n    \n    # Test with wrong password\n    wrong_password = \"wrong_password\"\n    is_valid_wrong = verify_password(wrong_password, hashed_pw)\n    print(f\"Wrong password verification: {is_valid_wrong}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bcrypt"], "package_breakdown": {"install_packages": ["bcrypt"], "import_packages": ["bcrypt"], "llm_suggested": ["bcrypt"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that runs a subprocess in a pseudo terminal, executes a specified command within the subprocess, and captures the output from the pseudo terminal for further processing.", "prompt_index": 146, "generated_code": "```python\nimport pty\nimport os\nimport select\nimport subprocess\nimport time\n\ndef run_command_in_pty(command, timeout=30):\n    \"\"\"\n    Run a command in a pseudo terminal and capture its output.\n    \n    Args:\n        command (str or list): Command to execute\n        timeout (int): Timeout in seconds\n    \n    Returns:\n        str: Captured output from the pseudo terminal\n    \"\"\"\n    # Create a pseudo terminal\n    master_fd, slave_fd = pty.openpty()\n    \n    try:\n        # Start the subprocess with the slave end of the pty\n        if isinstance(command, str):\n            process = subprocess.Popen(\n                command,\n                shell=True,\n                stdin=slave_fd,\n                stdout=slave_fd,\n                stderr=slave_fd,\n                preexec_fn=os.setsid\n            )\n        else:\n            process = subprocess.Popen(\n                command,\n                stdin=slave_fd,\n                stdout=slave_fd,\n                stderr=slave_fd,\n                preexec_fn=os.setsid\n            )\n        \n        # Close the slave end in the parent process\n        os.close(slave_fd)\n        \n        output = \"\"\n        start_time = time.time()\n        \n        while True:\n            # Check if process has terminated\n            if process.poll() is not None:\n                break\n            \n            # Check for timeout\n            if time.time() - start_time > timeout:\n                process.terminate()\n                break\n            \n            # Use select to check if data is available to read\n            ready, _, _ = select.select([master_fd], [], [], 0.1)\n            \n            if ready:\n                try:\n                    # Read available data\n                    data = os.read(master_fd, 1024)\n                    if data:\n                        output += data.decode('utf-8', errors='ignore')\n                    else:\n                        break\n                except OSError:\n                    break\n        \n        # Wait for process to complete\n        process.wait()\n        \n        return output\n        \n    finally:\n        # Clean up file descriptors\n        try:\n            os.close(master_fd)\n        except OSError:\n            pass\n        try:\n            os.close(slave_fd)\n        except OSError:\n            pass\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1: Simple command\n    output = run_command_in_pty(\"ls -la\")\n    print(\"Output from ls -la:\")\n    print(output)\n    \n    # Example 2: Interactive command that benefits from PTY\n    output = run_command_in_pty(\"python3 -c 'print(\\\"Hello from PTY!\\\")'\")\n    print(\"\\nOutput from Python command:\")\n    print(output)\n    \n    # Example 3: Command as list\n    output = run_command_in_pty([\"echo\", \"Hello World\"])\n    print(\"\\nOutput from echo:\")\n    print(output)\n```\n\nAlternative implementation using pexpect library (requires `pip install pexpect`):\n\n```python\nimport pexpect\nimport sys\n\ndef run_command_with_pexpect(command, timeout=30):\n    \"\"\"\n    Run a command using pexpect for easier PTY handling.\n    \n    Args:\n        command (str): Command to execute\n        timeout (int): Timeout in seconds\n    \n    Returns:\n        str: Captured output\n    \"\"\"\n    try:\n        # Spawn the command\n        child = pexpect.spawn(command, timeout=timeout)\n        \n        # Read all output\n        child.expect(pexpect.EOF)\n        output = child.before.decode('utf-8', errors='ignore')\n        \n        child.close()\n        return output\n        \n    except pexpect.TIMEOUT:\n        return \"Command timed out\"\n    except pexpect.EOF:\n        return child.before.decode('utf-8', errors='ignore') if child.before else \"\"\n\n# Example usage with pexpect\nif __name__ == \"__main__\":\n    output = run_command_with_pexpect(\"ls -la\")\n    print(\"Pexpect output:\")\n    print(output)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "pexpect", "sys", "time", "pty", "os", "select"], "package_breakdown": {"install_packages": ["subprocess", "pexpect", "sys", "time", "pty", "os", "select"], "import_packages": ["subprocess", "pexpect", "sys", "time", "pty", "os", "select"], "llm_suggested": ["pty", "os", "select", "subprocess", "time", "pexpect"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses Pexpect to automate the interaction with a console application, such as logging in, executing commands, and capturing output.", "prompt_index": 147, "generated_code": "```python\nimport pexpect\nimport sys\n\ndef automate_ssh_session(hostname, username, password):\n    \"\"\"Automate SSH login and command execution\"\"\"\n    try:\n        # Spawn SSH connection\n        child = pexpect.spawn(f'ssh {username}@{hostname}')\n        child.timeout = 30\n        \n        # Handle different possible prompts\n        index = child.expect(['password:', 'Are you sure you want to continue connecting'])\n        \n        if index == 1:  # First time connecting, accept host key\n            child.sendline('yes')\n            child.expect('password:')\n        \n        # Send password\n        child.sendline(password)\n        \n        # Wait for shell prompt\n        child.expect(['$', '#', '> '])\n        \n        # Execute commands and capture output\n        commands = [\n            'whoami',\n            'pwd',\n            'ls -la',\n            'df -h',\n            'uptime'\n        ]\n        \n        results = {}\n        for cmd in commands:\n            child.sendline(cmd)\n            child.expect(['$', '#', '> '])\n            output = child.before.decode('utf-8')\n            results[cmd] = output.strip()\n            print(f\"Command: {cmd}\")\n            print(f\"Output: {output}\")\n            print(\"-\" * 50)\n        \n        # Logout\n        child.sendline('exit')\n        child.close()\n        \n        return results\n        \n    except pexpect.exceptions.TIMEOUT:\n        print(\"Connection timed out\")\n        return None\n    except pexpect.exceptions.EOF:\n        print(\"Connection closed unexpectedly\")\n        return None\n\ndef automate_ftp_session(hostname, username, password):\n    \"\"\"Automate FTP session\"\"\"\n    try:\n        # Spawn FTP connection\n        child = pexpect.spawn(f'ftp {hostname}')\n        child.timeout = 30\n        \n        # Login\n        child.expect('Name.*:')\n        child.sendline(username)\n        child.expect('Password:')\n        child.sendline(password)\n        child.expect('ftp>')\n        \n        # Execute FTP commands\n        ftp_commands = [\n            'pwd',\n            'ls',\n            'binary',\n            'passive'\n        ]\n        \n        for cmd in ftp_commands:\n            child.sendline(cmd)\n            child.expect('ftp>')\n            output = child.before.decode('utf-8')\n            print(f\"FTP Command: {cmd}\")\n            print(f\"Output: {output}\")\n            print(\"-\" * 30)\n        \n        # Quit FTP\n        child.sendline('quit')\n        child.close()\n        \n    except pexpect.exceptions.TIMEOUT:\n        print(\"FTP connection timed out\")\n    except pexpect.exceptions.EOF:\n        print(\"FTP connection closed unexpectedly\")\n\ndef automate_database_console():\n    \"\"\"Automate MySQL console interaction\"\"\"\n    try:\n        # Spawn MySQL client\n        child = pexpect.spawn('mysql -u root -p')\n        child.timeout = 30\n        \n        # Enter password\n        child.expect('Enter password:')\n        child.sendline('your_password')\n        child.expect('mysql>')\n        \n        # Execute SQL commands\n        sql_commands = [\n            'SHOW DATABASES;',\n            'USE information_schema;',\n            'SHOW TABLES LIMIT 5;',\n            'SELECT VERSION();'\n        ]\n        \n        for sql in sql_commands:\n            child.sendline(sql)\n            child.expect('mysql>')\n            output = child.before.decode('utf-8')\n            print(f\"SQL: {sql}\")\n            print(f\"Result: {output}\")\n            print(\"-\" * 40)\n        \n        # Exit MySQL\n        child.sendline('EXIT;')\n        child.close()\n        \n    except pexpect.exceptions.TIMEOUT:\n        print(\"MySQL connection timed out\")\n    except pexpect.exceptions.EOF:\n        print(\"MySQL connection closed unexpectedly\")\n\ndef interactive_console_automation():\n    \"\"\"Generic console application automation with interactive prompts\"\"\"\n    try:\n        # Spawn a custom application (example: Python interpreter)\n        child = pexpect.spawn('python3')\n        child.timeout = 10\n        \n        # Wait for Python prompt\n        child.expect('>>>')\n        \n        # Send Python commands\n        python_commands = [\n            'import os',\n            'print(\"Hello from automated Python!\")',\n            'x = 5 + 3',\n            'print(f\"Result: {x}\")',\n            'import datetime',\n            'print(datetime.datetime.now())'\n        ]\n        \n        for cmd in python_commands:\n            child.sendline(cmd)\n            child.expect('>>>')\n            output = child.before.decode('utf-8')\n            print(f\"Python: {cmd}\")\n            print(f\"Output: {output}\")\n            print(\"-\" * 30)\n        \n        # Exit Python\n        child.sendline('exit()')\n        child.close()\n        \n    except pexpect.exceptions.TIMEOUT:\n        print(\"Python session timed out\")\n    except pexpect.exceptions.EOF:\n        print(\"Python session ended\")\n\ndef advanced_expect_patterns():\n    \"\"\"Demonstrate advanced pexpect pattern matching\"\"\"\n    try:\n        child = pexpect.spawn('bash')\n        child.timeout = 10\n        \n        # Wait for bash prompt (multiple possible patterns)\n        child.expect([r'\\$\\s*$', r'#\\s*$', r'>\\s*$'])\n        \n        # Send command with complex output\n        child.sendline('echo \"Starting process...\" && sleep 2 && echo \"Process completed\"')\n        \n        # Use multiple expect patterns to handle different outputs\n        patterns = [\n            'Starting process...',\n            'Process completed',\n            pexpect.TIMEOUT\n        ]\n        \n        while True:\n            try:\n                index = child.expect(patterns, timeout=5)\n                if index == 0:\n                    print(\"Process started\")\n                elif index == 1:\n                    print(\"Process completed\")\n                    break\n                elif index == 2:\n                    print(\"Timeout occurred\")\n                    break\n            except pexpect.EOF:\n                break\n        \n        child.close()\n        \n    except Exception as e:\n        print(f\"Error in advanced patterns: {e}\")\n\ndef log_session_to_file():\n    \"\"\"Log entire session to file\"\"\"\n    try:\n        # Create log file\n        with open('session_log.txt', 'wb') as logfile:\n            child = pexpect.spawn('bash')\n            child.logfile = logfile  # Log all output to file\n            child.timeout = 10\n            \n            child.expect([r'\\$', r'#'])\n            \n            # Execute commands\n            commands = ['date', 'whoami', 'pwd', 'ls']\n            for cmd in commands:\n                child.sendline(cmd)\n                child.expect([r'\\$', r'#'])\n            \n            child.sendline('exit')\n            child.close()\n        \n        print(\"Session logged to session_log.txt\")\n        \n    except Exception as e:\n        print(f\"Logging error: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"=== SSH Automation Example ===\")\n    # automate_ssh_session('example.com', 'username', 'password')\n    \n    print(\"\\n=== Interactive Console Automation ===\")\n    interactive_console_automation()\n    \n    print(\"\\n=== Advanced Pattern Matching ===\")\n    advanced_expect_patterns()\n    \n    print(\"\\n=== Session Logging ===\")\n    log_session_to_file()\n    \n    print(\"\\n=== FTP Automation Example ===\")\n    # automate_ftp_session('ftp.example.com', 'username', 'password')\n    \n    print(\"\\n=== Database Console Example ===\")\n    # automate_database_console()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pexpect", "sys"], "package_breakdown": {"install_packages": ["pexpect", "sys"], "import_packages": ["pexpect", "sys"], "llm_suggested": ["pexpect", "sys"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user with their Microsoft Azure account using the Identity Library, retrieves an access token, and makes a request to a protected Azure resource.", "prompt_index": 148, "generated_code": "```python\nfrom azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\nfrom azure.core.exceptions import ClientAuthenticationError\nimport requests\nimport json\n\ndef authenticate_and_access_resource():\n    try:\n        # Option 1: Use DefaultAzureCredential (tries multiple auth methods)\n        credential = DefaultAzureCredential()\n        \n        # Option 2: Use InteractiveBrowserCredential for explicit browser login\n        # credential = InteractiveBrowserCredential()\n        \n        # Define the scope for the resource you want to access\n        # For Azure Resource Manager API\n        scope = \"https://management.azure.com/.default\"\n        \n        # Get access token\n        token = credential.get_token(scope)\n        access_token = token.token\n        \n        print(\"Authentication successful!\")\n        print(f\"Token expires on: {token.expires_on}\")\n        \n        # Make request to protected Azure resource\n        # Example: List all subscriptions\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        # Azure Resource Manager endpoint\n        url = \"https://management.azure.com/subscriptions?api-version=2020-01-01\"\n        \n        response = requests.get(url, headers=headers)\n        \n        if response.status_code == 200:\n            subscriptions = response.json()\n            print(\"Successfully accessed Azure resource!\")\n            print(f\"Found {len(subscriptions.get('value', []))} subscriptions:\")\n            \n            for sub in subscriptions.get('value', []):\n                print(f\"- {sub.get('displayName')} ({sub.get('subscriptionId')})\")\n        else:\n            print(f\"Failed to access resource. Status code: {response.status_code}\")\n            print(f\"Response: {response.text}\")\n            \n    except ClientAuthenticationError as e:\n        print(f\"Authentication failed: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Alternative example for accessing Microsoft Graph API\ndef access_graph_api():\n    try:\n        credential = DefaultAzureCredential()\n        \n        # Microsoft Graph scope\n        scope = \"https://graph.microsoft.com/.default\"\n        \n        token = credential.get_token(scope)\n        access_token = token.token\n        \n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        # Get user profile information\n        url = \"https://graph.microsoft.com/v1.0/me\"\n        \n        response = requests.get(url, headers=headers)\n        \n        if response.status_code == 200:\n            user_info = response.json()\n            print(\"User Profile:\")\n            print(f\"Name: {user_info.get('displayName')}\")\n            print(f\"Email: {user_info.get('mail') or user_info.get('userPrincipalName')}\")\n        else:\n            print(f\"Failed to get user info. Status code: {response.status_code}\")\n            \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    authenticate_and_access_resource()\n    # access_graph_api()\n```\n\n```bash\n# Install required packages\npip install azure-identity requests\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure-identity", "azure", "json", "requests"], "package_breakdown": {"install_packages": ["azure-identity", "azure", "json", "requests"], "import_packages": ["json", "requests", "azure"], "llm_suggested": ["azure-identity", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that downloads a pre-trained model from the Hugging Face model hub using the client library and publishes your own custom dataset to the hub.", "prompt_index": 149, "generated_code": "```python\nfrom huggingface_hub import hf_hub_download, HfApi, login\nfrom datasets import Dataset, DatasetDict\nimport pandas as pd\nimport os\n\n# Login to Hugging Face (requires token)\nlogin(token=\"your_hf_token_here\")\n\n# Download a pre-trained model\nmodel_file = hf_hub_download(\n    repo_id=\"bert-base-uncased\",\n    filename=\"config.json\",\n    cache_dir=\"./models\"\n)\n\n# Download model weights\nmodel_weights = hf_hub_download(\n    repo_id=\"bert-base-uncased\",\n    filename=\"pytorch_model.bin\",\n    cache_dir=\"./models\"\n)\n\n# Create a custom dataset\ndata = {\n    'text': ['This is sample text 1', 'This is sample text 2', 'This is sample text 3'],\n    'label': [0, 1, 0]\n}\n\n# Convert to Dataset\ndataset = Dataset.from_dict(data)\n\n# Create train/test split\ndataset_dict = DatasetDict({\n    'train': dataset.select(range(2)),\n    'test': dataset.select(range(2, 3))\n})\n\n# Push dataset to hub\ndataset_dict.push_to_hub(\n    repo_id=\"your-username/your-custom-dataset\",\n    private=False\n)\n\n# Alternative: Upload dataset using HfApi\napi = HfApi()\n\n# Create repository\napi.create_repo(\n    repo_id=\"your-username/another-custom-dataset\",\n    repo_type=\"dataset\",\n    private=False\n)\n\n# Save dataset locally first\ndataset_dict.save_to_disk(\"./local_dataset\")\n\n# Upload folder to hub\napi.upload_folder(\n    folder_path=\"./local_dataset\",\n    repo_id=\"your-username/another-custom-dataset\",\n    repo_type=\"dataset\"\n)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["huggingface_hub"], "packages_detected": ["pandas", "datasets", "huggingface_hub", "os"], "package_breakdown": {"install_packages": ["pandas", "datasets", "huggingface_hub", "os"], "import_packages": ["datasets", "pandas", "huggingface_hub", "os"], "llm_suggested": ["huggingface_hub", "datasets", "pandas", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a connection to a Snowflake database, retrieves data from a specific table, and performs basic data manipulation operations such as filtering, sorting, and aggregating the data.", "prompt_index": 150, "generated_code": "```python\nimport snowflake.connector\nimport pandas as pd\nfrom snowflake.connector.pandas_tools import write_pandas\n\n# Establish connection to Snowflake\nconn = snowflake.connector.connect(\n    user='your_username',\n    password='your_password',\n    account='your_account',\n    warehouse='your_warehouse',\n    database='your_database',\n    schema='your_schema'\n)\n\n# Create cursor\ncursor = conn.cursor()\n\n# Retrieve data from a specific table\nquery = \"SELECT * FROM your_table_name\"\ncursor.execute(query)\n\n# Fetch results into pandas DataFrame\ndf = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n\n# Basic data manipulation operations\n\n# 1. Filtering - example: filter rows where column 'status' equals 'active'\nfiltered_df = df[df['status'] == 'active']\n\n# 2. Sorting - example: sort by 'date' column in descending order\nsorted_df = df.sort_values('date', ascending=False)\n\n# 3. Aggregating - example: group by 'category' and sum 'amount'\naggregated_df = df.groupby('category')['amount'].sum().reset_index()\n\n# Alternative: Perform operations directly in Snowflake using SQL\n\n# Filtering with SQL\nfilter_query = \"\"\"\nSELECT * FROM your_table_name \nWHERE status = 'active'\n\"\"\"\ncursor.execute(filter_query)\nfiltered_results = cursor.fetchall()\n\n# Sorting with SQL\nsort_query = \"\"\"\nSELECT * FROM your_table_name \nORDER BY date DESC\n\"\"\"\ncursor.execute(sort_query)\nsorted_results = cursor.fetchall()\n\n# Aggregating with SQL\nagg_query = \"\"\"\nSELECT category, SUM(amount) as total_amount\nFROM your_table_name\nGROUP BY category\n\"\"\"\ncursor.execute(agg_query)\nagg_results = cursor.fetchall()\n\n# Display results\nprint(\"Filtered Data:\")\nprint(filtered_df.head())\n\nprint(\"\\nSorted Data:\")\nprint(sorted_df.head())\n\nprint(\"\\nAggregated Data:\")\nprint(aggregated_df)\n\n# Close connections\ncursor.close()\nconn.close()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["snowflake-connector-python", "pandas", "snowflake"], "package_breakdown": {"install_packages": ["snowflake-connector-python", "pandas", "snowflake"], "import_packages": ["pandas", "snowflake"], "llm_suggested": ["snowflake-connector-python", "pandas"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates with the Yandex Cloud API using the SDK and retrieves information about all available cloud resources in a specific region.", "prompt_index": 151, "generated_code": "```python\nimport yandexcloud\nfrom yandex.cloud.resourcemanager.v1.cloud_service_pb2 import ListCloudsRequest\nfrom yandex.cloud.resourcemanager.v1.cloud_service_pb2_grpc import CloudServiceStub\nfrom yandex.cloud.resourcemanager.v1.folder_service_pb2 import ListFoldersRequest\nfrom yandex.cloud.resourcemanager.v1.folder_service_pb2_grpc import FolderServiceStub\nfrom yandex.cloud.compute.v1.instance_service_pb2 import ListInstancesRequest\nfrom yandex.cloud.compute.v1.instance_service_pb2_grpc import InstanceServiceStub\nfrom yandex.cloud.vpc.v1.network_service_pb2 import ListNetworksRequest\nfrom yandex.cloud.vpc.v1.network_service_pb2_grpc import NetworkServiceStub\nfrom yandex.cloud.vpc.v1.subnet_service_pb2 import ListSubnetsRequest\nfrom yandex.cloud.vpc.v1.subnet_service_pb2_grpc import SubnetServiceStub\n\n# Authentication using OAuth token or service account key\n# Option 1: Using OAuth token\noauth_token = \"your_oauth_token_here\"\nsdk = yandexcloud.SDK(token=oauth_token)\n\n# Option 2: Using service account key file\n# sdk = yandexcloud.SDK(service_account_key={\n#     \"id\": \"your_key_id\",\n#     \"service_account_id\": \"your_service_account_id\", \n#     \"private_key\": \"your_private_key\"\n# })\n\ndef get_all_cloud_resources(region_id=\"ru-central1\"):\n    \"\"\"Retrieve all cloud resources in a specific region\"\"\"\n    \n    # Get all clouds\n    cloud_service = sdk.client(CloudServiceStub)\n    clouds_response = cloud_service.List(ListCloudsRequest())\n    \n    print(f\"=== CLOUDS ===\")\n    for cloud in clouds_response.clouds:\n        print(f\"Cloud ID: {cloud.id}, Name: {cloud.name}\")\n        \n        # Get folders for each cloud\n        folder_service = sdk.client(FolderServiceStub)\n        folders_response = folder_service.List(ListFoldersRequest(cloud_id=cloud.id))\n        \n        print(f\"\\n=== FOLDERS IN CLOUD {cloud.name} ===\")\n        for folder in folders_response.folders:\n            print(f\"Folder ID: {folder.id}, Name: {folder.name}\")\n            \n            # Get compute instances\n            instance_service = sdk.client(InstanceServiceStub)\n            instances_response = instance_service.List(ListInstancesRequest(folder_id=folder.id))\n            \n            print(f\"\\n--- COMPUTE INSTANCES IN FOLDER {folder.name} ---\")\n            for instance in instances_response.instances:\n                if instance.zone_id.startswith(region_id):\n                    print(f\"Instance ID: {instance.id}\")\n                    print(f\"Name: {instance.name}\")\n                    print(f\"Zone: {instance.zone_id}\")\n                    print(f\"Status: {instance.status}\")\n                    print(f\"Platform: {instance.platform_id}\")\n                    print(\"---\")\n            \n            # Get VPC networks\n            network_service = sdk.client(NetworkServiceStub)\n            networks_response = network_service.List(ListNetworksRequest(folder_id=folder.id))\n            \n            print(f\"\\n--- VPC NETWORKS IN FOLDER {folder.name} ---\")\n            for network in networks_response.networks:\n                print(f\"Network ID: {network.id}\")\n                print(f\"Name: {network.name}\")\n                print(f\"Description: {network.description}\")\n                print(\"---\")\n                \n                # Get subnets for each network\n                subnet_service = sdk.client(SubnetServiceStub)\n                subnets_response = subnet_service.List(ListSubnetsRequest(folder_id=folder.id))\n                \n                print(f\"--- SUBNETS IN NETWORK {network.name} ---\")\n                for subnet in subnets_response.subnets:\n                    if subnet.network_id == network.id and subnet.zone_id.startswith(region_id):\n                        print(f\"Subnet ID: {subnet.id}\")\n                        print(f\"Name: {subnet.name}\")\n                        print(f\"Zone: {subnet.zone_id}\")\n                        print(f\"CIDR: {subnet.v4_cidr_blocks}\")\n                        print(\"---\")\n            \n            print(\"\\n\" + \"=\"*50 + \"\\n\")\n\nif __name__ == \"__main__\":\n    # Specify the region (default is ru-central1)\n    region = \"ru-central1\"  # Change this to your desired region\n    get_all_cloud_resources(region)\n```\n\nYou'll need to install the Yandex Cloud SDK first:\n```bash\npip install yandexcloud\n```\n\nReplace `\"your_oauth_token_here\"` with your actual OAuth token, or use the service account key authentication method by uncommenting and filling in the appropriate credentials.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yandexcloud", "yandex"], "package_breakdown": {"install_packages": ["yandexcloud", "yandex"], "import_packages": ["yandexcloud", "yandex"], "llm_suggested": ["yandexcloud"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that automatically detects the encoding of a given text file using the Universal Encoding Detector in Python 3.", "prompt_index": 152, "generated_code": "```python\nimport chardet\n\ndef detect_file_encoding(file_path):\n    \"\"\"\n    Detect the encoding of a text file using chardet.\n    \n    Args:\n        file_path (str): Path to the text file\n    \n    Returns:\n        dict: Dictionary containing encoding info\n    \"\"\"\n    with open(file_path, 'rb') as file:\n        raw_data = file.read()\n        result = chardet.detect(raw_data)\n    \n    return result\n\ndef detect_encoding_with_confidence(file_path, min_confidence=0.7):\n    \"\"\"\n    Detect encoding and return only if confidence is above threshold.\n    \n    Args:\n        file_path (str): Path to the text file\n        min_confidence (float): Minimum confidence threshold\n    \n    Returns:\n        str or None: Detected encoding or None if confidence is too low\n    \"\"\"\n    result = detect_file_encoding(file_path)\n    \n    if result['confidence'] >= min_confidence:\n        return result['encoding']\n    else:\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    file_path = \"example.txt\"\n    \n    # Basic detection\n    encoding_info = detect_file_encoding(file_path)\n    print(f\"Detected encoding: {encoding_info['encoding']}\")\n    print(f\"Confidence: {encoding_info['confidence']:.2f}\")\n    print(f\"Language: {encoding_info['language']}\")\n    \n    # Detection with confidence threshold\n    encoding = detect_encoding_with_confidence(file_path)\n    if encoding:\n        print(f\"Reliable encoding detected: {encoding}\")\n    else:\n        print(\"Encoding detection not reliable enough\")\n    \n    # Read file with detected encoding\n    try:\n        with open(file_path, 'r', encoding=encoding_info['encoding']) as file:\n            content = file.read()\n            print(\"File successfully read with detected encoding\")\n    except UnicodeDecodeError:\n        print(\"Error reading file with detected encoding\")\n```\n\nInstall the required package:\n```bash\npip install chardet\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["chardet"], "package_breakdown": {"install_packages": ["chardet"], "import_packages": ["chardet"], "llm_suggested": ["chardet"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates and manages a sliding window memory map using the pure Python implementation provided in the package, allowing for efficient memory management and manipulation within a specified window size.", "prompt_index": 153, "generated_code": "```python\nimport mmap\nimport os\nfrom typing import Optional, Union\n\nclass SlidingWindowMemoryMap:\n    def __init__(self, filename: str, window_size: int, file_size: Optional[int] = None):\n        \"\"\"\n        Initialize sliding window memory map\n        \n        Args:\n            filename: Path to the file to map\n            window_size: Size of the sliding window in bytes\n            file_size: Total file size (if creating new file)\n        \"\"\"\n        self.filename = filename\n        self.window_size = window_size\n        self.window_start = 0\n        self.file_size = file_size or os.path.getsize(filename)\n        \n        # Open file and create memory map\n        self.file = open(filename, 'r+b' if os.path.exists(filename) else 'w+b')\n        if file_size and not os.path.exists(filename):\n            self.file.write(b'\\x00' * file_size)\n            self.file.flush()\n        \n        self.mmap_obj = None\n        self._create_window()\n    \n    def _create_window(self):\n        \"\"\"Create memory map window at current position\"\"\"\n        if self.mmap_obj:\n            self.mmap_obj.close()\n        \n        # Calculate actual window size (don't exceed file boundaries)\n        actual_window_size = min(self.window_size, self.file_size - self.window_start)\n        \n        if actual_window_size > 0:\n            self.mmap_obj = mmap.mmap(\n                self.file.fileno(),\n                actual_window_size,\n                offset=self.window_start,\n                access=mmap.ACCESS_WRITE\n            )\n        else:\n            self.mmap_obj = None\n    \n    def slide_to(self, position: int):\n        \"\"\"Slide window to specific file position\"\"\"\n        if position < 0 or position >= self.file_size:\n            raise ValueError(f\"Position {position} out of bounds [0, {self.file_size})\")\n        \n        self.window_start = position\n        self._create_window()\n    \n    def slide_forward(self, offset: int = None):\n        \"\"\"Slide window forward by offset (default: window_size)\"\"\"\n        if offset is None:\n            offset = self.window_size\n        \n        new_position = min(self.window_start + offset, self.file_size - 1)\n        self.slide_to(new_position)\n    \n    def slide_backward(self, offset: int = None):\n        \"\"\"Slide window backward by offset (default: window_size)\"\"\"\n        if offset is None:\n            offset = self.window_size\n        \n        new_position = max(self.window_start - offset, 0)\n        self.slide_to(new_position)\n    \n    def read(self, size: int = -1, offset: int = 0) -> bytes:\n        \"\"\"Read data from current window\"\"\"\n        if not self.mmap_obj:\n            return b''\n        \n        if size == -1:\n            size = len(self.mmap_obj) - offset\n        \n        return self.mmap_obj[offset:offset + size]\n    \n    def write(self, data: bytes, offset: int = 0):\n        \"\"\"Write data to current window\"\"\"\n        if not self.mmap_obj:\n            raise RuntimeError(\"No active memory map window\")\n        \n        if offset + len(data) > len(self.mmap_obj):\n            raise ValueError(\"Write would exceed window boundaries\")\n        \n        self.mmap_obj[offset:offset + len(data)] = data\n        self.mmap_obj.flush()\n    \n    def search(self, pattern: bytes) -> list:\n        \"\"\"Search for pattern in current window, return list of positions\"\"\"\n        if not self.mmap_obj:\n            return []\n        \n        positions = []\n        data = self.mmap_obj[:]\n        start = 0\n        \n        while True:\n            pos = data.find(pattern, start)\n            if pos == -1:\n                break\n            positions.append(pos)\n            start = pos + 1\n        \n        return positions\n    \n    def get_window_info(self) -> dict:\n        \"\"\"Get information about current window\"\"\"\n        return {\n            'window_start': self.window_start,\n            'window_size': len(self.mmap_obj) if self.mmap_obj else 0,\n            'file_size': self.file_size,\n            'window_end': self.window_start + (len(self.mmap_obj) if self.mmap_obj else 0)\n        }\n    \n    def copy_to_buffer(self, buffer: bytearray, buffer_offset: int = 0, window_offset: int = 0, size: int = -1):\n        \"\"\"Copy data from window to external buffer\"\"\"\n        if not self.mmap_obj:\n            return 0\n        \n        if size == -1:\n            size = len(self.mmap_obj) - window_offset\n        \n        actual_size = min(size, len(self.mmap_obj) - window_offset, len(buffer) - buffer_offset)\n        buffer[buffer_offset:buffer_offset + actual_size] = self.mmap_obj[window_offset:window_offset + actual_size]\n        \n        return actual_size\n    \n    def flush(self):\n        \"\"\"Flush changes to disk\"\"\"\n        if self.mmap_obj:\n            self.mmap_obj.flush()\n    \n    def close(self):\n        \"\"\"Close memory map and file\"\"\"\n        if self.mmap_obj:\n            self.mmap_obj.close()\n        if self.file:\n            self.file.close()\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\n# Usage example and utility functions\ndef create_test_file(filename: str, size: int, pattern: bytes = b'A'):\n    \"\"\"Create a test file with repeating pattern\"\"\"\n    with open(filename, 'wb') as f:\n        for i in range(0, size, len(pattern)):\n            remaining = min(len(pattern), size - i)\n            f.write(pattern[:remaining])\n\n\ndef demonstrate_sliding_window():\n    \"\"\"Demonstrate sliding window memory map functionality\"\"\"\n    filename = \"test_data.bin\"\n    file_size = 1024 * 1024  # 1MB\n    window_size = 4096  # 4KB window\n    \n    # Create test file\n    create_test_file(filename, file_size, b'ABCDEFGHIJKLMNOP')\n    \n    try:\n        with SlidingWindowMemoryMap(filename, window_size) as mm:\n            # Read from initial position\n            data = mm.read(100)\n            print(f\"Initial data: {data[:50]}...\")\n            \n            # Write some data\n            mm.write(b'MODIFIED_DATA_HERE', 0)\n            \n            # Slide window forward\n            mm.slide_forward(2048)\n            print(f\"Window info after sliding: {mm.get_window_info()}\")\n            \n            # Search for pattern\n            positions = mm.search(b'ABCD')\n            print(f\"Found pattern at positions: {positions}\")\n            \n            # Copy to buffer\n            buffer = bytearray(1000)\n            copied = mm.copy_to_buffer(buffer, 0, 0, 500)\n            print(f\"Copied {copied} bytes to buffer\")\n            \n    finally:\n        # Clean up\n        if os.path.exists(filename):\n            os.remove(filename)\n\n\n# Advanced sliding window with caching\nclass CachedSlidingWindowMemoryMap(SlidingWindowMemoryMap):\n    def __init__(self, filename: str, window_size: int, cache_size: int = 3, file_size: Optional[int] = None):\n        self.cache_size = cache_size\n        self.cache = {}  # position -> mmap_data\n        self.cache_order = []  # LRU order\n        super().__init__(filename, window_size, file_size)\n    \n    def _create_window(self):\n        \"\"\"Create window with caching", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["mmap"], "packages_detected": ["mmap", "os", "typing"], "package_breakdown": {"install_packages": ["mmap", "os", "typing"], "import_packages": ["os", "mmap", "typing"], "llm_suggested": ["mmap", "os", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that takes a tabular dataset and formats it in a visually appealing way with aligned columns, headers, and borders for easy readability.", "prompt_index": 154, "generated_code": "```python\ndef format_table(data, headers=None, title=None):\n    \"\"\"\n    Format tabular data with aligned columns, headers, and borders.\n    \n    Args:\n        data: List of lists/tuples representing rows of data\n        headers: List of column headers (optional)\n        title: Table title (optional)\n    \"\"\"\n    if not data:\n        return \"No data to display\"\n    \n    # Convert all data to strings\n    str_data = [[str(cell) for cell in row] for row in data]\n    \n    # Add headers if provided\n    if headers:\n        str_headers = [str(h) for h in headers]\n        all_rows = [str_headers] + str_data\n    else:\n        all_rows = str_data\n    \n    # Calculate column widths\n    num_cols = len(all_rows[0])\n    col_widths = []\n    \n    for col in range(num_cols):\n        max_width = max(len(row[col]) for row in all_rows)\n        col_widths.append(max_width)\n    \n    # Create formatting strings\n    row_format = \"\u2502 \" + \" \u2502 \".join(f\"{{:<{w}}}\" for w in col_widths) + \" \u2502\"\n    separator = \"\u251c\" + \"\u253c\".join(\"\u2500\" * (w + 2) for w in col_widths) + \"\u2524\"\n    top_border = \"\u250c\" + \"\u252c\".join(\"\u2500\" * (w + 2) for w in col_widths) + \"\u2510\"\n    bottom_border = \"\u2514\" + \"\u2534\".join(\"\u2500\" * (w + 2) for w in col_widths) + \"\u2518\"\n    \n    # Build the table\n    result = []\n    \n    # Add title if provided\n    if title:\n        table_width = sum(col_widths) + (num_cols - 1) * 3 + 4\n        title_line = f\"\u250c{'\u2500' * (table_width - 2)}\u2510\"\n        title_content = f\"\u2502{title:^{table_width - 2}}\u2502\"\n        title_bottom = f\"\u251c{'\u252c'.join('\u2500' * (w + 2) for w in col_widths)}\u2524\"\n        result.extend([title_line, title_content, title_bottom])\n    else:\n        result.append(top_border)\n    \n    # Add headers if provided\n    if headers:\n        result.append(row_format.format(*str_headers))\n        result.append(separator)\n    \n    # Add data rows\n    for i, row in enumerate(str_data):\n        result.append(row_format.format(*row))\n        if i < len(str_data) - 1:  # Add separator between data rows (optional)\n            pass  # Remove this pass and uncomment next line for row separators\n            # result.append(separator)\n    \n    result.append(bottom_border)\n    \n    return \"\\n\".join(result)\n\n# Example usage with sample data\nif __name__ == \"__main__\":\n    # Sample dataset\n    sample_data = [\n        [\"John Doe\", 28, \"Engineer\", 75000],\n        [\"Jane Smith\", 32, \"Manager\", 85000],\n        [\"Bob Johnson\", 45, \"Director\", 120000],\n        [\"Alice Brown\", 29, \"Developer\", 70000]\n    ]\n    \n    headers = [\"Name\", \"Age\", \"Position\", \"Salary\"]\n    title = \"Employee Information\"\n    \n    # Format and print the table\n    formatted_table = format_table(sample_data, headers, title)\n    print(formatted_table)\n    \n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Alternative simple version without title\n    simple_table = format_table(sample_data, headers)\n    print(simple_table)\n```\n\n```python\n# Alternative compact version using tabulate library (requires: pip install tabulate)\nfrom tabulate import tabulate\n\ndef quick_format_table(data, headers=None, title=None):\n    \"\"\"Simple table formatting using tabulate library\"\"\"\n    if title:\n        print(f\"\\n{title}\")\n        print(\"=\" * len(title))\n    \n    return tabulate(data, headers=headers, tablefmt=\"grid\")\n\n# Example usage\nsample_data = [\n    [\"Product A\", 150, 12.99],\n    [\"Product B\", 75, 24.50],\n    [\"Product C\", 200, 8.99]\n]\n\nheaders = [\"Product\", \"Quantity\", \"Price\"]\nprint(quick_format_table(sample_data, headers, \"Inventory Report\"))\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tabulate"], "package_breakdown": {"install_packages": ["tabulate"], "import_packages": ["tabulate"], "llm_suggested": ["tabulate"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a directed graph with nodes representing cities and edges representing roads, with attributes such as distance and traffic flow, and calculates the shortest path between two specified cities using Dijkstra's algorithm.", "prompt_index": 155, "generated_code": "```python\nimport heapq\nfrom collections import defaultdict\n\nclass DirectedGraph:\n    def __init__(self):\n        self.nodes = set()\n        self.edges = defaultdict(list)\n    \n    def add_node(self, city):\n        \"\"\"Add a city node to the graph\"\"\"\n        self.nodes.add(city)\n    \n    def add_edge(self, from_city, to_city, distance, traffic_flow):\n        \"\"\"Add a directed edge (road) between cities with distance and traffic flow\"\"\"\n        self.nodes.add(from_city)\n        self.nodes.add(to_city)\n        self.edges[from_city].append({\n            'destination': to_city,\n            'distance': distance,\n            'traffic_flow': traffic_flow\n        })\n    \n    def dijkstra_shortest_path(self, start_city, end_city):\n        \"\"\"Find shortest path using Dijkstra's algorithm based on distance\"\"\"\n        if start_city not in self.nodes or end_city not in self.nodes:\n            return None, float('inf')\n        \n        # Initialize distances and previous nodes\n        distances = {city: float('inf') for city in self.nodes}\n        previous = {city: None for city in self.nodes}\n        distances[start_city] = 0\n        \n        # Priority queue: (distance, city)\n        pq = [(0, start_city)]\n        visited = set()\n        \n        while pq:\n            current_distance, current_city = heapq.heappop(pq)\n            \n            if current_city in visited:\n                continue\n            \n            visited.add(current_city)\n            \n            if current_city == end_city:\n                break\n            \n            # Check all neighbors\n            for edge in self.edges[current_city]:\n                neighbor = edge['destination']\n                distance = edge['distance']\n                \n                if neighbor not in visited:\n                    new_distance = current_distance + distance\n                    \n                    if new_distance < distances[neighbor]:\n                        distances[neighbor] = new_distance\n                        previous[neighbor] = current_city\n                        heapq.heappush(pq, (new_distance, neighbor))\n        \n        # Reconstruct path\n        path = []\n        current = end_city\n        while current is not None:\n            path.append(current)\n            current = previous[current]\n        \n        if path[-1] != start_city:\n            return None, float('inf')  # No path found\n        \n        path.reverse()\n        return path, distances[end_city]\n    \n    def get_path_info(self, path):\n        \"\"\"Get detailed information about a path including traffic flow\"\"\"\n        if not path or len(path) < 2:\n            return []\n        \n        path_info = []\n        for i in range(len(path) - 1):\n            from_city = path[i]\n            to_city = path[i + 1]\n            \n            # Find the edge between these cities\n            for edge in self.edges[from_city]:\n                if edge['destination'] == to_city:\n                    path_info.append({\n                        'from': from_city,\n                        'to': to_city,\n                        'distance': edge['distance'],\n                        'traffic_flow': edge['traffic_flow']\n                    })\n                    break\n        \n        return path_info\n\n# Example usage\ndef create_sample_graph():\n    graph = DirectedGraph()\n    \n    # Add cities and roads with distance and traffic flow\n    graph.add_edge(\"New York\", \"Philadelphia\", 95, 8500)\n    graph.add_edge(\"New York\", \"Boston\", 215, 6200)\n    graph.add_edge(\"Philadelphia\", \"Washington DC\", 140, 7800)\n    graph.add_edge(\"Philadelphia\", \"Baltimore\", 100, 5400)\n    graph.add_edge(\"Boston\", \"Hartford\", 100, 3200)\n    graph.add_edge(\"Hartford\", \"New York\", 110, 4100)\n    graph.add_edge(\"Baltimore\", \"Washington DC\", 40, 9200)\n    graph.add_edge(\"Washington DC\", \"Richmond\", 110, 4800)\n    graph.add_edge(\"Richmond\", \"Norfolk\", 95, 2900)\n    \n    return graph\n\n# Create and test the graph\nif __name__ == \"__main__\":\n    graph = create_sample_graph()\n    \n    # Find shortest path\n    start = \"New York\"\n    end = \"Washington DC\"\n    \n    path, total_distance = graph.dijkstra_shortest_path(start, end)\n    \n    if path:\n        print(f\"Shortest path from {start} to {end}:\")\n        print(f\"Route: {' -> '.join(path)}\")\n        print(f\"Total distance: {total_distance} miles\")\n        \n        # Get detailed path information\n        path_details = graph.get_path_info(path)\n        print(\"\\nDetailed route information:\")\n        for detail in path_details:\n            print(f\"{detail['from']} -> {detail['to']}: \"\n                  f\"{detail['distance']} miles, \"\n                  f\"Traffic flow: {detail['traffic_flow']} vehicles/day\")\n    else:\n        print(f\"No path found from {start} to {end}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections", "heapq"], "package_breakdown": {"install_packages": ["collections", "heapq"], "import_packages": ["collections", "heapq"], "llm_suggested": ["heapq", "collections"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a class named 'Person' with attributes for 'name', 'age', and 'gender', along with methods to set and get each attribute, and a method to print out the person's information.", "prompt_index": 156, "generated_code": "```python\nclass Person:\n    def __init__(self, name=\"\", age=0, gender=\"\"):\n        self.name = name\n        self.age = age\n        self.gender = gender\n    \n    # Name methods\n    def set_name(self, name):\n        self.name = name\n    \n    def get_name(self):\n        return self.name\n    \n    # Age methods\n    def set_age(self, age):\n        self.age = age\n    \n    def get_age(self):\n        return self.age\n    \n    # Gender methods\n    def set_gender(self, gender):\n        self.gender = gender\n    \n    def get_gender(self):\n        return self.gender\n    \n    # Print information method\n    def print_info(self):\n        print(f\"Name: {self.name}\")\n        print(f\"Age: {self.age}\")\n        print(f\"Gender: {self.gender}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that dynamically manages the number of threads in a thread pool based on the current workload to optimize resource usage and prevent overloading the system.", "prompt_index": 157, "generated_code": "```python\nimport threading\nimport time\nimport queue\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom collections import deque\nimport psutil\nimport os\n\nclass AdaptiveThreadPool:\n    def __init__(self, min_threads=2, max_threads=None, cpu_threshold=80, \n                 memory_threshold=85, adjustment_interval=5):\n        self.min_threads = min_threads\n        self.max_threads = max_threads or min(32, (os.cpu_count() or 1) + 4)\n        self.cpu_threshold = cpu_threshold\n        self.memory_threshold = memory_threshold\n        self.adjustment_interval = adjustment_interval\n        \n        self.task_queue = queue.Queue()\n        self.completed_tasks = deque(maxlen=100)\n        self.task_times = deque(maxlen=50)\n        \n        self.executor = ThreadPoolExecutor(max_workers=self.min_threads)\n        self.current_threads = self.min_threads\n        \n        self.monitor_thread = threading.Thread(target=self._monitor_system, daemon=True)\n        self.running = True\n        self.monitor_thread.start()\n        \n        self.lock = threading.Lock()\n        \n    def submit(self, fn, *args, **kwargs):\n        \"\"\"Submit a task to the thread pool\"\"\"\n        start_time = time.time()\n        future = self.executor.submit(self._wrapped_task, fn, start_time, *args, **kwargs)\n        return future\n    \n    def _wrapped_task(self, fn, start_time, *args, **kwargs):\n        \"\"\"Wrapper to track task execution time\"\"\"\n        try:\n            result = fn(*args, **kwargs)\n            end_time = time.time()\n            execution_time = end_time - start_time\n            \n            with self.lock:\n                self.task_times.append(execution_time)\n                self.completed_tasks.append(time.time())\n            \n            return result\n        except Exception as e:\n            with self.lock:\n                self.completed_tasks.append(time.time())\n            raise e\n    \n    def _get_system_metrics(self):\n        \"\"\"Get current system CPU and memory usage\"\"\"\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory_percent = psutil.virtual_memory().percent\n        return cpu_percent, memory_percent\n    \n    def _get_workload_metrics(self):\n        \"\"\"Calculate workload-based metrics\"\"\"\n        with self.lock:\n            # Queue size\n            queue_size = self.task_queue.qsize()\n            \n            # Task completion rate (tasks per second)\n            current_time = time.time()\n            recent_completions = [t for t in self.completed_tasks \n                                if current_time - t <= self.adjustment_interval]\n            completion_rate = len(recent_completions) / self.adjustment_interval\n            \n            # Average task execution time\n            avg_task_time = sum(self.task_times) / len(self.task_times) if self.task_times else 0\n            \n            return queue_size, completion_rate, avg_task_time\n    \n    def _calculate_optimal_threads(self):\n        \"\"\"Calculate optimal number of threads based on system and workload metrics\"\"\"\n        cpu_percent, memory_percent = self._get_system_metrics()\n        queue_size, completion_rate, avg_task_time = self._get_workload_metrics()\n        \n        # Start with current thread count\n        optimal_threads = self.current_threads\n        \n        # System resource-based adjustment\n        if cpu_percent > self.cpu_threshold or memory_percent > self.memory_threshold:\n            # System under stress, reduce threads\n            optimal_threads = max(self.min_threads, int(optimal_threads * 0.8))\n        elif cpu_percent < self.cpu_threshold * 0.6 and memory_percent < self.memory_threshold * 0.6:\n            # System has capacity, consider increasing threads\n            \n            # Workload-based adjustment\n            if queue_size > self.current_threads * 2:  # Queue backlog\n                optimal_threads = min(self.max_threads, optimal_threads + 2)\n            elif completion_rate > 0 and avg_task_time > 2:  # Long-running tasks\n                optimal_threads = min(self.max_threads, optimal_threads + 1)\n            elif queue_size == 0 and completion_rate < 1:  # Low workload\n                optimal_threads = max(self.min_threads, optimal_threads - 1)\n        \n        return optimal_threads\n    \n    def _adjust_thread_pool(self, new_size):\n        \"\"\"Adjust the thread pool size\"\"\"\n        if new_size != self.current_threads:\n            print(f\"Adjusting thread pool: {self.current_threads} -> {new_size}\")\n            \n            # Create new executor with adjusted size\n            old_executor = self.executor\n            self.executor = ThreadPoolExecutor(max_workers=new_size)\n            self.current_threads = new_size\n            \n            # Shutdown old executor gracefully\n            old_executor.shutdown(wait=False)\n    \n    def _monitor_system(self):\n        \"\"\"Background thread to monitor system and adjust thread pool\"\"\"\n        while self.running:\n            try:\n                optimal_threads = self._calculate_optimal_threads()\n                self._adjust_thread_pool(optimal_threads)\n                \n                # Log current status\n                cpu_percent, memory_percent = self._get_system_metrics()\n                queue_size, completion_rate, avg_task_time = self._get_workload_metrics()\n                \n                print(f\"Status - Threads: {self.current_threads}, CPU: {cpu_percent:.1f}%, \"\n                      f\"Memory: {memory_percent:.1f}%, Queue: {queue_size}, \"\n                      f\"Completion Rate: {completion_rate:.2f}/s, Avg Task Time: {avg_task_time:.2f}s\")\n                \n                time.sleep(self.adjustment_interval)\n                \n            except Exception as e:\n                print(f\"Error in monitor thread: {e}\")\n                time.sleep(self.adjustment_interval)\n    \n    def get_stats(self):\n        \"\"\"Get current pool statistics\"\"\"\n        cpu_percent, memory_percent = self._get_system_metrics()\n        queue_size, completion_rate, avg_task_time = self._get_workload_metrics()\n        \n        return {\n            'current_threads': self.current_threads,\n            'cpu_percent': cpu_percent,\n            'memory_percent': memory_percent,\n            'queue_size': queue_size,\n            'completion_rate': completion_rate,\n            'avg_task_time': avg_task_time\n        }\n    \n    def shutdown(self):\n        \"\"\"Shutdown the thread pool\"\"\"\n        self.running = False\n        self.executor.shutdown(wait=True)\n        if self.monitor_thread.is_alive():\n            self.monitor_thread.join()\n\n# Example usage and testing\ndef example_task(duration, task_id):\n    \"\"\"Example task that simulates work\"\"\"\n    time.sleep(duration)\n    return f\"Task {task_id} completed in {duration}s\"\n\ndef cpu_intensive_task(iterations, task_id):\n    \"\"\"CPU intensive task for testing\"\"\"\n    result = 0\n    for i in range(iterations):\n        result += i ** 2\n    return f\"CPU Task {task_id} completed with result: {result}\"\n\n# Demo usage\nif __name__ == \"__main__\":\n    # Create adaptive thread pool\n    pool = AdaptiveThreadPool(\n        min_threads=2,\n        max_threads=8,\n        cpu_threshold=70,\n        memory_threshold=80,\n        adjustment_interval=3\n    )\n    \n    try:\n        # Submit various types of tasks\n        futures = []\n        \n        # Light workload\n        for i in range(5):\n            future = pool.submit(example_task, 1, i)\n            futures.append(future)\n        \n        time.sleep(10)\n        \n        # Heavy workload\n        for i in range(20):\n            if i % 2 == 0:\n                future = pool.submit(example_task, 3, f\"heavy_{i}\")\n            else:\n                future =", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 1, "hallucinations": ["concurrent"], "packages_detected": ["concurrent", "queue", "psutil", "threading", "time", "collections", "os"], "package_breakdown": {"install_packages": ["concurrent", "queue", "psutil", "threading", "time", "collections", "os"], "import_packages": ["concurrent", "queue", "psutil", "threading", "time", "collections", "os"], "llm_suggested": ["threading", "queue", "psutil"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that validates a JSON document against a specified JSON schema using the fastest implementation available in the Python ecosystem.", "prompt_index": 158, "generated_code": "```python\nimport jsonschema\nfrom jsonschema import validate, ValidationError\nimport json\n\ndef validate_json_fast(json_data, schema):\n    \"\"\"\n    Validates JSON data against a schema using the fastest available implementation.\n    \n    Args:\n        json_data: JSON data (dict/list) or JSON string\n        schema: JSON schema (dict) or JSON schema string\n    \n    Returns:\n        bool: True if valid, False if invalid\n    \n    Raises:\n        ValidationError: If validation fails and you want detailed error info\n    \"\"\"\n    try:\n        # Parse JSON strings if needed\n        if isinstance(json_data, str):\n            json_data = json.loads(json_data)\n        if isinstance(schema, str):\n            schema = json.loads(schema)\n        \n        # Create validator with fastest available implementation\n        validator_class = jsonschema.validators.validator_for(schema)\n        validator = validator_class(schema)\n        \n        # Validate\n        validator.validate(json_data)\n        return True\n        \n    except ValidationError as e:\n        print(f\"Validation error: {e.message}\")\n        return False\n    except json.JSONDecodeError as e:\n        print(f\"JSON parsing error: {e}\")\n        return False\n\ndef validate_json_with_errors(json_data, schema):\n    \"\"\"\n    Validates JSON and returns detailed error information.\n    \"\"\"\n    try:\n        if isinstance(json_data, str):\n            json_data = json.loads(json_data)\n        if isinstance(schema, str):\n            schema = json.loads(schema)\n            \n        validate(instance=json_data, schema=schema)\n        return True, None\n        \n    except ValidationError as e:\n        return False, {\n            'message': e.message,\n            'path': list(e.path),\n            'schema_path': list(e.schema_path),\n            'validator': e.validator,\n            'validator_value': e.validator_value\n        }\n    except json.JSONDecodeError as e:\n        return False, {'message': f'JSON parsing error: {e}'}\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\", \"minimum\": 0},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"}\n        },\n        \"required\": [\"name\", \"age\"]\n    }\n    \n    # Valid JSON\n    valid_json = {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"email\": \"john@example.com\"\n    }\n    \n    # Invalid JSON\n    invalid_json = {\n        \"name\": \"John Doe\",\n        \"age\": -5,  # Invalid: negative age\n        \"email\": \"invalid-email\"  # Invalid format\n    }\n    \n    # Test validation\n    print(\"Valid JSON:\", validate_json_fast(valid_json, schema))\n    print(\"Invalid JSON:\", validate_json_fast(invalid_json, schema))\n    \n    # Get detailed errors\n    is_valid, errors = validate_json_with_errors(invalid_json, schema)\n    if not is_valid:\n        print(\"Detailed errors:\", errors)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "jsonschema"], "package_breakdown": {"install_packages": ["json", "jsonschema"], "import_packages": ["json", "jsonschema"], "llm_suggested": ["jsonschema", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates and serializes OpenTelemetry traces and metrics into Protocol Buffers format using the OpenTelemetry Python Proto package.", "prompt_index": 159, "generated_code": "```python\nimport time\nfrom opentelemetry.proto.trace.v1 import trace_pb2\nfrom opentelemetry.proto.metrics.v1 import metrics_pb2\nfrom opentelemetry.proto.common.v1 import common_pb2\nfrom opentelemetry.proto.resource.v1 import resource_pb2\nfrom google.protobuf.timestamp_pb2 import Timestamp\n\ndef create_trace_data():\n    # Create resource\n    resource = resource_pb2.Resource()\n    resource.attributes.append(common_pb2.KeyValue(\n        key=\"service.name\",\n        value=common_pb2.AnyValue(string_value=\"my-service\")\n    ))\n    resource.attributes.append(common_pb2.KeyValue(\n        key=\"service.version\",\n        value=common_pb2.AnyValue(string_value=\"1.0.0\")\n    ))\n\n    # Create span\n    span = trace_pb2.Span()\n    span.trace_id = b'\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10'\n    span.span_id = b'\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08'\n    span.name = \"example-operation\"\n    span.kind = trace_pb2.Span.SPAN_KIND_SERVER\n    span.status.code = trace_pb2.Status.STATUS_CODE_OK\n    \n    # Set timestamps\n    start_time = Timestamp()\n    start_time.GetCurrentTime()\n    span.start_time_unix_nano = int(start_time.seconds * 1e9 + start_time.nanos)\n    \n    end_time = Timestamp()\n    end_time.seconds = start_time.seconds + 1\n    span.end_time_unix_nano = int(end_time.seconds * 1e9 + end_time.nanos)\n\n    # Add span attributes\n    span.attributes.append(common_pb2.KeyValue(\n        key=\"http.method\",\n        value=common_pb2.AnyValue(string_value=\"GET\")\n    ))\n    span.attributes.append(common_pb2.KeyValue(\n        key=\"http.status_code\",\n        value=common_pb2.AnyValue(int_value=200)\n    ))\n\n    # Create scope spans\n    scope_spans = trace_pb2.ScopeSpans()\n    scope_spans.scope.name = \"my-tracer\"\n    scope_spans.scope.version = \"1.0.0\"\n    scope_spans.spans.append(span)\n\n    # Create resource spans\n    resource_spans = trace_pb2.ResourceSpans()\n    resource_spans.resource.CopyFrom(resource)\n    resource_spans.scope_spans.append(scope_spans)\n\n    # Create traces data\n    traces_data = trace_pb2.TracesData()\n    traces_data.resource_spans.append(resource_spans)\n\n    return traces_data\n\ndef create_metrics_data():\n    # Create resource\n    resource = resource_pb2.Resource()\n    resource.attributes.append(common_pb2.KeyValue(\n        key=\"service.name\",\n        value=common_pb2.AnyValue(string_value=\"my-service\")\n    ))\n\n    # Create gauge metric\n    gauge = metrics_pb2.Gauge()\n    data_point = metrics_pb2.NumberDataPoint()\n    data_point.as_double = 42.5\n    \n    timestamp = Timestamp()\n    timestamp.GetCurrentTime()\n    data_point.time_unix_nano = int(timestamp.seconds * 1e9 + timestamp.nanos)\n    data_point.start_time_unix_nano = data_point.time_unix_nano - int(1e9)  # 1 second ago\n    \n    data_point.attributes.append(common_pb2.KeyValue(\n        key=\"environment\",\n        value=common_pb2.AnyValue(string_value=\"production\")\n    ))\n    \n    gauge.data_points.append(data_point)\n\n    # Create metric\n    metric = metrics_pb2.Metric()\n    metric.name = \"cpu_usage\"\n    metric.description = \"CPU usage percentage\"\n    metric.unit = \"%\"\n    metric.gauge.CopyFrom(gauge)\n\n    # Create counter metric\n    counter = metrics_pb2.Sum()\n    counter.aggregation_temporality = metrics_pb2.AGGREGATION_TEMPORALITY_CUMULATIVE\n    counter.is_monotonic = True\n    \n    counter_data_point = metrics_pb2.NumberDataPoint()\n    counter_data_point.as_int = 1000\n    counter_data_point.time_unix_nano = int(timestamp.seconds * 1e9 + timestamp.nanos)\n    counter_data_point.start_time_unix_nano = counter_data_point.time_unix_nano - int(60e9)  # 60 seconds ago\n    \n    counter.data_points.append(counter_data_point)\n\n    counter_metric = metrics_pb2.Metric()\n    counter_metric.name = \"requests_total\"\n    counter_metric.description = \"Total number of requests\"\n    counter_metric.unit = \"1\"\n    counter_metric.sum.CopyFrom(counter)\n\n    # Create scope metrics\n    scope_metrics = metrics_pb2.ScopeMetrics()\n    scope_metrics.scope.name = \"my-meter\"\n    scope_metrics.scope.version = \"1.0.0\"\n    scope_metrics.metrics.append(metric)\n    scope_metrics.metrics.append(counter_metric)\n\n    # Create resource metrics\n    resource_metrics = metrics_pb2.ResourceMetrics()\n    resource_metrics.resource.CopyFrom(resource)\n    resource_metrics.scope_metrics.append(scope_metrics)\n\n    # Create metrics data\n    metrics_data = metrics_pb2.MetricsData()\n    metrics_data.resource_metrics.append(resource_metrics)\n\n    return metrics_data\n\ndef serialize_data():\n    # Create and serialize traces\n    traces = create_trace_data()\n    serialized_traces = traces.SerializeToString()\n    \n    # Create and serialize metrics\n    metrics = create_metrics_data()\n    serialized_metrics = metrics.SerializeToString()\n    \n    return serialized_traces, serialized_metrics\n\ndef deserialize_and_print(serialized_traces, serialized_metrics):\n    # Deserialize traces\n    traces = trace_pb2.TracesData()\n    traces.ParseFromString(serialized_traces)\n    print(\"Deserialized Traces:\")\n    print(f\"Service Name: {traces.resource_spans[0].resource.attributes[0].value.string_value}\")\n    print(f\"Span Name: {traces.resource_spans[0].scope_spans[0].spans[0].name}\")\n    \n    # Deserialize metrics\n    metrics = metrics_pb2.MetricsData()\n    metrics.ParseFromString(serialized_metrics)\n    print(\"\\nDeserialized Metrics:\")\n    print(f\"Service Name: {metrics.resource_metrics[0].resource.attributes[0].value.string_value}\")\n    print(f\"Metric Name: {metrics.resource_metrics[0].scope_metrics[0].metrics[0].name}\")\n    print(f\"Metric Value: {metrics.resource_metrics[0].scope_metrics[0].metrics[0].gauge.data_points[0].as_double}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    serialized_traces, serialized_metrics = serialize_data()\n    \n    print(f\"Serialized traces size: {len(serialized_traces)} bytes\")\n    print(f\"Serialized metrics size: {len(serialized_metrics)} bytes\")\n    \n    # Save to files\n    with open(\"traces.pb\", \"wb\") as f:\n        f.write(serialized_traces)\n    \n    with open(\"metrics.pb\", \"wb\") as f:\n        f.write(serialized_metrics)\n    \n    # Demonstrate deserialization\n    deserialize_and_print(serialized_traces, serialized_metrics)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["google", "opentelemetry-proto", "time", "opentelemetry", "protobuf"], "package_breakdown": {"install_packages": ["google", "opentelemetry-proto", "time", "opentelemetry", "protobuf"], "import_packages": ["google", "time", "opentelemetry"], "llm_suggested": ["opentelemetry-proto", "protobuf"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads and executes build backend hooks specified in a pyproject.toml file for automated build processes and project management.", "prompt_index": 160, "generated_code": "```python\nimport os\nimport sys\nimport subprocess\nimport importlib.util\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional\nimport toml\n\nclass BuildBackendHookExecutor:\n    def __init__(self, project_root: str = \".\"):\n        self.project_root = Path(project_root).resolve()\n        self.pyproject_path = self.project_root / \"pyproject.toml\"\n        self.config = self._load_pyproject_config()\n        self.build_backend = self._get_build_backend()\n        \n    def _load_pyproject_config(self) -> Dict[str, Any]:\n        \"\"\"Load pyproject.toml configuration\"\"\"\n        if not self.pyproject_path.exists():\n            raise FileNotFoundError(f\"pyproject.toml not found in {self.project_root}\")\n        \n        with open(self.pyproject_path, 'r', encoding='utf-8') as f:\n            return toml.load(f)\n    \n    def _get_build_backend(self) -> str:\n        \"\"\"Extract build backend from pyproject.toml\"\"\"\n        build_system = self.config.get('build-system', {})\n        backend = build_system.get('build-backend', 'setuptools.build_meta')\n        return backend\n    \n    def _install_build_requirements(self):\n        \"\"\"Install build requirements if specified\"\"\"\n        build_system = self.config.get('build-system', {})\n        requires = build_system.get('requires', [])\n        \n        if requires:\n            cmd = [sys.executable, '-m', 'pip', 'install'] + requires\n            subprocess.run(cmd, check=True, cwd=self.project_root)\n    \n    def _import_backend_module(self):\n        \"\"\"Dynamically import the build backend module\"\"\"\n        try:\n            module_path = self.build_backend.split('.')\n            module_name = '.'.join(module_path[:-1]) if len(module_path) > 1 else module_path[0]\n            \n            # Try to import the module\n            backend_module = importlib.import_module(module_name)\n            \n            # Get the specific backend object if specified\n            if len(module_path) > 1:\n                for attr in module_path[1:]:\n                    backend_module = getattr(backend_module, attr)\n            \n            return backend_module\n        except ImportError as e:\n            raise ImportError(f\"Could not import build backend {self.build_backend}: {e}\")\n    \n    def get_requires_for_build_wheel(self, config_settings: Optional[Dict] = None) -> List[str]:\n        \"\"\"Get requirements for building wheel\"\"\"\n        backend = self._import_backend_module()\n        if hasattr(backend, 'get_requires_for_build_wheel'):\n            return backend.get_requires_for_build_wheel(config_settings or {})\n        return []\n    \n    def get_requires_for_build_sdist(self, config_settings: Optional[Dict] = None) -> List[str]:\n        \"\"\"Get requirements for building source distribution\"\"\"\n        backend = self._import_backend_module()\n        if hasattr(backend, 'get_requires_for_build_sdist'):\n            return backend.get_requires_for_build_sdist(config_settings or {})\n        return []\n    \n    def prepare_metadata_for_build_wheel(self, metadata_directory: str, \n                                       config_settings: Optional[Dict] = None) -> str:\n        \"\"\"Prepare metadata for wheel build\"\"\"\n        backend = self._import_backend_module()\n        if hasattr(backend, 'prepare_metadata_for_build_wheel'):\n            return backend.prepare_metadata_for_build_wheel(\n                metadata_directory, config_settings or {}\n            )\n        return \"\"\n    \n    def build_wheel(self, wheel_directory: str, config_settings: Optional[Dict] = None,\n                   metadata_directory: Optional[str] = None) -> str:\n        \"\"\"Build wheel\"\"\"\n        backend = self._import_backend_module()\n        if hasattr(backend, 'build_wheel'):\n            return backend.build_wheel(\n                wheel_directory, config_settings or {}, metadata_directory\n            )\n        raise NotImplementedError(\"Backend does not support wheel building\")\n    \n    def build_sdist(self, sdist_directory: str, config_settings: Optional[Dict] = None) -> str:\n        \"\"\"Build source distribution\"\"\"\n        backend = self._import_backend_module()\n        if hasattr(backend, 'build_sdist'):\n            return backend.build_sdist(sdist_directory, config_settings or {})\n        raise NotImplementedError(\"Backend does not support sdist building\")\n    \n    def execute_build_hooks(self, build_type: str = \"wheel\", \n                          output_dir: Optional[str] = None,\n                          config_settings: Optional[Dict] = None) -> str:\n        \"\"\"Execute complete build process with hooks\"\"\"\n        if output_dir is None:\n            output_dir = str(self.project_root / \"dist\")\n        \n        Path(output_dir).mkdir(exist_ok=True, parents=True)\n        \n        # Install build requirements\n        self._install_build_requirements()\n        \n        # Execute pre-build hooks if defined\n        self._execute_custom_hooks(\"pre-build\")\n        \n        try:\n            if build_type.lower() == \"wheel\":\n                # Get wheel build requirements\n                wheel_reqs = self.get_requires_for_build_wheel(config_settings)\n                if wheel_reqs:\n                    cmd = [sys.executable, '-m', 'pip', 'install'] + wheel_reqs\n                    subprocess.run(cmd, check=True, cwd=self.project_root)\n                \n                # Build wheel\n                result = self.build_wheel(output_dir, config_settings)\n                \n            elif build_type.lower() == \"sdist\":\n                # Get sdist build requirements\n                sdist_reqs = self.get_requires_for_build_sdist(config_settings)\n                if sdist_reqs:\n                    cmd = [sys.executable, '-m', 'pip', 'install'] + sdist_reqs\n                    subprocess.run(cmd, check=True, cwd=self.project_root)\n                \n                # Build sdist\n                result = self.build_sdist(output_dir, config_settings)\n                \n            else:\n                raise ValueError(f\"Unsupported build type: {build_type}\")\n            \n            # Execute post-build hooks if defined\n            self._execute_custom_hooks(\"post-build\", {\"build_result\": result})\n            \n            return result\n            \n        except Exception as e:\n            # Execute error hooks if defined\n            self._execute_custom_hooks(\"build-error\", {\"error\": str(e)})\n            raise\n    \n    def _execute_custom_hooks(self, hook_type: str, context: Optional[Dict] = None):\n        \"\"\"Execute custom hooks defined in pyproject.toml\"\"\"\n        tool_config = self.config.get('tool', {})\n        hooks_config = tool_config.get('build-hooks', {})\n        \n        if hook_type in hooks_config:\n            hook_config = hooks_config[hook_type]\n            \n            # Execute shell commands\n            if 'commands' in hook_config:\n                for cmd in hook_config['commands']:\n                    if isinstance(cmd, str):\n                        subprocess.run(cmd, shell=True, check=True, cwd=self.project_root)\n                    elif isinstance(cmd, list):\n                        subprocess.run(cmd, check=True, cwd=self.project_root)\n            \n            # Execute Python scripts\n            if 'scripts' in hook_config:\n                for script_path in hook_config['scripts']:\n                    script_file = self.project_root / script_path\n                    if script_file.exists():\n                        spec = importlib.util.spec_from_file_location(\"hook_script\", script_file)\n                        module = importlib.util.module_from_spec(spec)\n                        \n                        # Pass context to the script\n                        if context:\n                            for key, value in context.items():\n                                setattr(module, key, value)\n                        \n                        spec.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 9, "hallucinated_packages": 1, "hallucinations": ["importlib.util"], "packages_detected": ["pathlib", "subprocess", "tempfile", "typing", "sys", "importlib", "toml", "importlib.util", "os"], "package_breakdown": {"install_packages": ["pathlib", "subprocess", "tempfile", "typing", "sys", "importlib", "toml", "importlib.util", "os"], "import_packages": ["pathlib", "subprocess", "typing", "tempfile", "sys", "importlib", "toml", "os"], "llm_suggested": ["os", "sys", "subprocess", "importlib.util", "tempfile", "pathlib", "typing", "toml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves a specific commit object from a Git repository using the Git Object Database package and prints out its details.", "prompt_index": 161, "generated_code": "```python\nimport git\nfrom git.objects import Commit\n\n# Open the Git repository\nrepo = git.Repo('.')  # Current directory, or specify path to repo\n\n# Get a specific commit by SHA hash\ncommit_sha = 'HEAD'  # or use specific SHA like 'a1b2c3d4e5f6...'\ncommit = repo.commit(commit_sha)\n\n# Print commit details\nprint(f\"Commit SHA: {commit.hexsha}\")\nprint(f\"Author: {commit.author.name} <{commit.author.email}>\")\nprint(f\"Date: {commit.authored_datetime}\")\nprint(f\"Committer: {commit.committer.name} <{commit.committer.email}>\")\nprint(f\"Commit Date: {commit.committed_datetime}\")\nprint(f\"Message: {commit.message.strip()}\")\nprint(f\"Parents: {[parent.hexsha[:8] for parent in commit.parents]}\")\nprint(f\"Tree: {commit.tree.hexsha}\")\n\n# Print modified files\nprint(\"\\nModified files:\")\nfor item in commit.stats.files:\n    stats = commit.stats.files[item]\n    print(f\"  {item}: +{stats['insertions']} -{stats['deletions']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["git"], "packages_detected": ["git", "GitPython"], "package_breakdown": {"install_packages": ["git", "GitPython"], "import_packages": ["git"], "llm_suggested": ["GitPython"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that securely serializes and deserializes sensitive data to be passed between trusted and untrusted environments, ensuring data integrity and preventing potential security vulnerabilities.", "prompt_index": 162, "generated_code": "```python\nimport json\nimport hmac\nimport hashlib\nimport secrets\nimport base64\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nfrom typing import Any, Dict, Optional\nimport time\n\nclass SecureSerializer:\n    def __init__(self, password: str, salt: Optional[bytes] = None):\n        \"\"\"\n        Initialize secure serializer with password-based encryption\n        \n        Args:\n            password: Master password for encryption\n            salt: Optional salt (will generate if not provided)\n        \"\"\"\n        self.salt = salt or secrets.token_bytes(32)\n        self.key = self._derive_key(password.encode())\n        self.fernet = Fernet(self.key)\n        self.hmac_key = secrets.token_bytes(32)\n    \n    def _derive_key(self, password: bytes) -> bytes:\n        \"\"\"Derive encryption key from password using PBKDF2\"\"\"\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=self.salt,\n            iterations=100000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(password))\n        return key\n    \n    def _generate_hmac(self, data: bytes) -> bytes:\n        \"\"\"Generate HMAC for data integrity verification\"\"\"\n        return hmac.new(self.hmac_key, data, hashlib.sha256).digest()\n    \n    def _verify_hmac(self, data: bytes, signature: bytes) -> bool:\n        \"\"\"Verify HMAC signature\"\"\"\n        expected = self._generate_hmac(data)\n        return hmac.compare_digest(expected, signature)\n    \n    def serialize(self, data: Any, ttl_seconds: Optional[int] = None) -> str:\n        \"\"\"\n        Securely serialize data with encryption and integrity protection\n        \n        Args:\n            data: Data to serialize\n            ttl_seconds: Optional time-to-live in seconds\n            \n        Returns:\n            Base64 encoded secure package\n        \"\"\"\n        # Create payload with timestamp and optional TTL\n        payload = {\n            'data': data,\n            'timestamp': int(time.time()),\n            'ttl': ttl_seconds\n        }\n        \n        # Serialize to JSON\n        json_data = json.dumps(payload, separators=(',', ':')).encode('utf-8')\n        \n        # Encrypt the data\n        encrypted_data = self.fernet.encrypt(json_data)\n        \n        # Generate HMAC for integrity\n        signature = self._generate_hmac(encrypted_data)\n        \n        # Create secure package\n        secure_package = {\n            'salt': base64.b64encode(self.salt).decode('ascii'),\n            'data': base64.b64encode(encrypted_data).decode('ascii'),\n            'signature': base64.b64encode(signature).decode('ascii'),\n            'hmac_key': base64.b64encode(self.hmac_key).decode('ascii')\n        }\n        \n        # Encode final package\n        return base64.b64encode(\n            json.dumps(secure_package).encode('utf-8')\n        ).decode('ascii')\n    \n    def deserialize(self, serialized_data: str, password: str) -> Any:\n        \"\"\"\n        Securely deserialize and verify data\n        \n        Args:\n            serialized_data: Base64 encoded secure package\n            password: Password for decryption\n            \n        Returns:\n            Deserialized data\n            \n        Raises:\n            ValueError: If data is invalid or verification fails\n        \"\"\"\n        try:\n            # Decode the package\n            package_json = base64.b64decode(serialized_data.encode('ascii'))\n            package = json.loads(package_json.decode('utf-8'))\n            \n            # Extract components\n            salt = base64.b64decode(package['salt'].encode('ascii'))\n            encrypted_data = base64.b64decode(package['data'].encode('ascii'))\n            signature = base64.b64decode(package['signature'].encode('ascii'))\n            hmac_key = base64.b64decode(package['hmac_key'].encode('ascii'))\n            \n            # Verify HMAC integrity\n            expected_signature = hmac.new(hmac_key, encrypted_data, hashlib.sha256).digest()\n            if not hmac.compare_digest(expected_signature, signature):\n                raise ValueError(\"Data integrity verification failed\")\n            \n            # Derive decryption key\n            kdf = PBKDF2HMAC(\n                algorithm=hashes.SHA256(),\n                length=32,\n                salt=salt,\n                iterations=100000,\n            )\n            key = base64.urlsafe_b64encode(kdf.derive(password.encode()))\n            fernet = Fernet(key)\n            \n            # Decrypt data\n            decrypted_data = fernet.decrypt(encrypted_data)\n            payload = json.loads(decrypted_data.decode('utf-8'))\n            \n            # Check TTL if specified\n            if payload.get('ttl'):\n                elapsed = int(time.time()) - payload['timestamp']\n                if elapsed > payload['ttl']:\n                    raise ValueError(\"Data has expired\")\n            \n            return payload['data']\n            \n        except Exception as e:\n            raise ValueError(f\"Deserialization failed: {str(e)}\")\n\n# Usage example with additional security utilities\nclass SecureDataTransfer:\n    @staticmethod\n    def sanitize_data(data: Any) -> Any:\n        \"\"\"Sanitize data before serialization\"\"\"\n        if isinstance(data, dict):\n            # Remove potentially dangerous keys\n            dangerous_keys = ['__class__', '__module__', '__globals__']\n            return {k: SecureDataTransfer.sanitize_data(v) \n                   for k, v in data.items() \n                   if k not in dangerous_keys}\n        elif isinstance(data, list):\n            return [SecureDataTransfer.sanitize_data(item) for item in data]\n        elif isinstance(data, str):\n            # Basic string sanitization\n            return data.replace('\\x00', '').strip()\n        return data\n    \n    @staticmethod\n    def create_secure_token(data: Any, password: str, ttl_minutes: int = 60) -> str:\n        \"\"\"Create a secure token with TTL\"\"\"\n        serializer = SecureSerializer(password)\n        sanitized_data = SecureDataTransfer.sanitize_data(data)\n        return serializer.serialize(sanitized_data, ttl_minutes * 60)\n    \n    @staticmethod\n    def verify_secure_token(token: str, password: str) -> Any:\n        \"\"\"Verify and extract data from secure token\"\"\"\n        serializer = SecureSerializer(password)\n        return serializer.deserialize(token, password)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sensitive data to transfer\n    sensitive_data = {\n        \"user_id\": 12345,\n        \"permissions\": [\"read\", \"write\"],\n        \"session_token\": \"abc123xyz\",\n        \"metadata\": {\"ip\": \"192.168.1.1\", \"timestamp\": \"2024-01-01\"}\n    }\n    \n    password = \"super_secret_password_2024!\"\n    \n    # Create secure token\n    secure_token = SecureDataTransfer.create_secure_token(\n        sensitive_data, \n        password, \n        ttl_minutes=30\n    )\n    \n    print(f\"Secure token: {secure_token[:50]}...\")\n    \n    # Verify and extract data\n    try:\n        extracted_data = SecureDataTransfer.verify_secure_token(secure_token, password)\n        print(f\"Extracted data: {extracted_data}\")\n    except ValueError as e:\n        print(f\"Verification failed: {e}\")\n```\n\nThis implementation provides:\n\n1. **Password-based encryption** using PBKDF2 key derivation\n2. **Data integrity protection** with HMAC signatures\n3. **Time-based expiration** with TTL support\n4. **Input sanitization** to prevent injection attacks\n5. **Secure random salt generation**\n6. **Constant-time comparison** for signature verification\n7. **Comprehensive", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["hmac", "typing", "hashlib", "cryptography", "time", "secrets", "json", "base64"], "package_breakdown": {"install_packages": ["hmac", "typing", "hashlib", "cryptography", "time", "secrets", "json", "base64"], "import_packages": ["hmac", "typing", "hashlib", "cryptography", "time", "secrets", "json", "base64"], "llm_suggested": ["json", "hmac", "hashlib", "secrets", "base64", "cryptography", "typing", "time"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a connection to a D-Bus service using pure Python, sends a method call to the service, and receives the response.", "prompt_index": 163, "generated_code": "```python\nimport socket\nimport struct\nimport os\n\nclass DBusConnection:\n    def __init__(self):\n        self.socket = None\n        self.serial = 1\n        \n    def connect(self):\n        # Connect to session bus\n        session_bus_address = os.environ.get('DBUS_SESSION_BUS_ADDRESS')\n        if not session_bus_address:\n            # Fallback to system bus\n            socket_path = '/var/run/dbus/system_bus_socket'\n        else:\n            # Parse unix socket path from address\n            socket_path = session_bus_address.split('=')[1].split(',')[0]\n        \n        self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        self.socket.connect(socket_path)\n        \n        # Send authentication\n        auth_msg = b'\\0AUTH EXTERNAL ' + str(os.getuid()).encode('ascii').hex().encode('ascii') + b'\\r\\n'\n        self.socket.send(auth_msg)\n        \n        # Read auth response\n        response = self.socket.recv(1024)\n        \n        # Send BEGIN\n        self.socket.send(b'BEGIN\\r\\n')\n        \n    def send_method_call(self, destination, path, interface, method, signature='', *args):\n        # Build D-Bus message\n        message_type = 1  # METHOD_CALL\n        flags = 0\n        protocol_version = 1\n        \n        # Header fields\n        header_fields = []\n        \n        # PATH field (1)\n        path_bytes = path.encode('utf-8') + b'\\0'\n        header_fields.append(struct.pack('<BB', 1, 1))  # field code, signature\n        header_fields.append(struct.pack('<I', len(path_bytes)))\n        header_fields.append(path_bytes)\n        \n        # INTERFACE field (2) \n        if interface:\n            interface_bytes = interface.encode('utf-8') + b'\\0'\n            header_fields.append(struct.pack('<BB', 2, 1))\n            header_fields.append(struct.pack('<I', len(interface_bytes)))\n            header_fields.append(interface_bytes)\n        \n        # MEMBER field (3)\n        member_bytes = method.encode('utf-8') + b'\\0'\n        header_fields.append(struct.pack('<BB', 3, 1))\n        header_fields.append(struct.pack('<I', len(member_bytes)))\n        header_fields.append(member_bytes)\n        \n        # DESTINATION field (6)\n        if destination:\n            dest_bytes = destination.encode('utf-8') + b'\\0'\n            header_fields.append(struct.pack('<BB', 6, 1))\n            header_fields.append(struct.pack('<I', len(dest_bytes)))\n            header_fields.append(dest_bytes)\n        \n        # SIGNATURE field (8)\n        if signature:\n            sig_bytes = signature.encode('utf-8') + b'\\0'\n            header_fields.append(struct.pack('<BB', 8, 1))\n            header_fields.append(struct.pack('<I', len(sig_bytes)))\n            header_fields.append(sig_bytes)\n        \n        # Serialize header fields\n        header_fields_data = b''.join(header_fields)\n        header_fields_length = len(header_fields_data)\n        \n        # Body (simplified - just strings)\n        body = b''\n        if args:\n            for arg in args:\n                if isinstance(arg, str):\n                    arg_bytes = arg.encode('utf-8') + b'\\0'\n                    body += struct.pack('<I', len(arg_bytes) - 1) + arg_bytes\n        \n        body_length = len(body)\n        \n        # Message header\n        header = struct.pack('<BBBBII',\n                           ord('l'),  # little endian\n                           message_type,\n                           flags,\n                           protocol_version,\n                           body_length,\n                           self.serial)\n        \n        # Complete message\n        message = header + struct.pack('<I', header_fields_length) + header_fields_data\n        \n        # Pad to 8-byte boundary\n        padding_needed = (8 - (len(message) % 8)) % 8\n        message += b'\\0' * padding_needed\n        message += body\n        \n        # Send message\n        self.socket.send(message)\n        self.serial += 1\n        \n        # Receive response\n        response_header = self.socket.recv(16)\n        if len(response_header) < 16:\n            return None\n            \n        endian, msg_type, flags, version, body_len, serial = struct.unpack('<BBBBII', response_header)\n        \n        # Read header fields length\n        header_fields_len_data = self.socket.recv(4)\n        header_fields_len = struct.unpack('<I', header_fields_len_data)[0]\n        \n        # Read header fields\n        header_fields = self.socket.recv(header_fields_len)\n        \n        # Read padding\n        total_header_len = 16 + header_fields_len\n        padding = (8 - (total_header_len % 8)) % 8\n        if padding:\n            self.socket.recv(padding)\n        \n        # Read body\n        body = b''\n        if body_len > 0:\n            body = self.socket.recv(body_len)\n        \n        return body\n    \n    def close(self):\n        if self.socket:\n            self.socket.close()\n\n# Usage example\ndbus = DBusConnection()\ndbus.connect()\n\n# Example: Call ListNames method on DBus service\nresponse = dbus.send_method_call(\n    destination='org.freedesktop.DBus',\n    path='/org/freedesktop/DBus', \n    interface='org.freedesktop.DBus',\n    method='ListNames'\n)\n\nprint(f\"Response received: {len(response) if response else 0} bytes\")\n\ndbus.close()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socket", "struct", "os"], "package_breakdown": {"install_packages": ["socket", "struct", "os"], "import_packages": ["socket", "struct", "os"], "llm_suggested": ["socket", "struct", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that queries a specified domain's DNS records and prints out the results, including the IP addresses associated with the domain, using the DNS toolkit package.", "prompt_index": 164, "generated_code": "```python\nimport dns.resolver\nimport dns.rdatatype\n\ndef query_dns_records(domain):\n    \"\"\"Query various DNS records for a domain and print results\"\"\"\n    \n    # Define record types to query\n    record_types = ['A', 'AAAA', 'MX', 'NS', 'TXT', 'CNAME', 'SOA']\n    \n    print(f\"DNS Records for: {domain}\")\n    print(\"=\" * 50)\n    \n    for record_type in record_types:\n        try:\n            # Query the DNS record\n            answers = dns.resolver.resolve(domain, record_type)\n            \n            print(f\"\\n{record_type} Records:\")\n            print(\"-\" * 20)\n            \n            for answer in answers:\n                if record_type == 'A' or record_type == 'AAAA':\n                    print(f\"IP Address: {answer}\")\n                elif record_type == 'MX':\n                    print(f\"Mail Server: {answer.exchange} (Priority: {answer.preference})\")\n                elif record_type == 'NS':\n                    print(f\"Name Server: {answer}\")\n                elif record_type == 'TXT':\n                    print(f\"Text Record: {answer}\")\n                elif record_type == 'CNAME':\n                    print(f\"Canonical Name: {answer}\")\n                elif record_type == 'SOA':\n                    print(f\"Start of Authority: {answer}\")\n                    \n        except dns.resolver.NXDOMAIN:\n            print(f\"\\n{record_type} Records: Domain not found\")\n        except dns.resolver.NoAnswer:\n            print(f\"\\n{record_type} Records: No records found\")\n        except Exception as e:\n            print(f\"\\n{record_type} Records: Error - {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    domain_name = input(\"Enter domain name to query: \")\n    query_dns_records(domain_name)\n```\n\nTo install the required package:\n```bash\npip install dnspython\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dns", "dnspython"], "package_breakdown": {"install_packages": ["dns", "dnspython"], "import_packages": ["dns"], "llm_suggested": ["dnspython"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that downloads a large media file from Google using resumable uploads to ensure uninterrupted downloading in case of network disruptions.", "prompt_index": 165, "generated_code": "```python\nimport requests\nimport os\nimport time\nfrom urllib.parse import urlparse\n\nclass ResumableDownloader:\n    def __init__(self, url, output_path, chunk_size=8192):\n        self.url = url\n        self.output_path = output_path\n        self.chunk_size = chunk_size\n        self.session = requests.Session()\n        \n    def get_file_size(self):\n        \"\"\"Get the total file size from server\"\"\"\n        response = self.session.head(self.url)\n        return int(response.headers.get('content-length', 0))\n    \n    def get_downloaded_size(self):\n        \"\"\"Get the size of already downloaded file\"\"\"\n        if os.path.exists(self.output_path):\n            return os.path.getsize(self.output_path)\n        return 0\n    \n    def download_chunk(self, start_byte, end_byte=None):\n        \"\"\"Download a specific chunk of the file\"\"\"\n        headers = {'Range': f'bytes={start_byte}-'}\n        if end_byte:\n            headers['Range'] = f'bytes={start_byte}-{end_byte}'\n            \n        response = self.session.get(self.url, headers=headers, stream=True)\n        response.raise_for_status()\n        return response\n    \n    def download(self, max_retries=3, retry_delay=5):\n        \"\"\"Download file with resume capability\"\"\"\n        total_size = self.get_file_size()\n        downloaded_size = self.get_downloaded_size()\n        \n        if downloaded_size == total_size:\n            print(f\"File already fully downloaded: {self.output_path}\")\n            return True\n        \n        print(f\"Resuming download from byte {downloaded_size} of {total_size}\")\n        \n        retries = 0\n        while retries < max_retries:\n            try:\n                response = self.download_chunk(downloaded_size)\n                \n                with open(self.output_path, 'ab') as file:\n                    for chunk in response.iter_content(chunk_size=self.chunk_size):\n                        if chunk:\n                            file.write(chunk)\n                            downloaded_size += len(chunk)\n                            \n                            # Progress indicator\n                            progress = (downloaded_size / total_size) * 100\n                            print(f\"\\rProgress: {progress:.1f}% ({downloaded_size}/{total_size} bytes)\", end='')\n                \n                if downloaded_size >= total_size:\n                    print(\"\\nDownload completed successfully!\")\n                    return True\n                    \n            except (requests.exceptions.RequestException, IOError) as e:\n                retries += 1\n                print(f\"\\nError occurred: {e}\")\n                print(f\"Retrying in {retry_delay} seconds... (Attempt {retries}/{max_retries})\")\n                time.sleep(retry_delay)\n                downloaded_size = self.get_downloaded_size()  # Refresh downloaded size\n                \n        print(f\"\\nFailed to download after {max_retries} attempts\")\n        return False\n\n# Example usage for Google Drive files\nclass GoogleDriveDownloader(ResumableDownloader):\n    def __init__(self, file_id, output_path, chunk_size=8192):\n        # Google Drive direct download URL format\n        url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n        super().__init__(url, output_path, chunk_size)\n    \n    def get_direct_download_url(self):\n        \"\"\"Handle Google Drive's virus scan warning for large files\"\"\"\n        response = self.session.get(self.url)\n        \n        # Check if there's a virus scan warning\n        if 'virus scan warning' in response.text.lower():\n            # Extract the actual download URL from the warning page\n            import re\n            match = re.search(r'href=\"(/uc\\?export=download[^\"]+)\"', response.text)\n            if match:\n                self.url = \"https://drive.google.com\" + match.group(1).replace('&amp;', '&')\n        \n        return self.url\n\n# Usage examples:\n\n# For direct URLs\ndef download_from_url(url, output_path):\n    downloader = ResumableDownloader(url, output_path)\n    return downloader.download()\n\n# For Google Drive files\ndef download_from_google_drive(file_id, output_path):\n    downloader = GoogleDriveDownloader(file_id, output_path)\n    downloader.get_direct_download_url()  # Handle potential virus scan warning\n    return downloader.download()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Direct URL download\n    # url = \"https://example.com/large_file.zip\"\n    # download_from_url(url, \"large_file.zip\")\n    \n    # Google Drive download (extract file_id from the sharing URL)\n    # file_id = \"1ABC123def456GHI789jkl\"  # Extract from Google Drive URL\n    # download_from_google_drive(file_id, \"downloaded_file.zip\")\n    \n    pass\n```\n\nThis code provides resumable download functionality with the following features:\n\n- **Resume capability**: Continues downloads from where they left off\n- **Chunk-based downloading**: Downloads in manageable chunks to handle large files\n- **Retry mechanism**: Automatically retries on network failures\n- **Progress tracking**: Shows download progress\n- **Google Drive support**: Handles Google Drive's specific URL format and virus scan warnings\n- **Error handling**: Robust error handling for network disruptions\n\nTo use with a Google Drive file, extract the file ID from the sharing URL (the part after `/d/` and before `/view`) and use the `GoogleDriveDownloader` class.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "time", "urllib.parse", "re", "requests", "os"], "package_breakdown": {"install_packages": ["urllib", "time", "urllib.parse", "re", "requests", "os"], "import_packages": ["urllib", "time", "re", "requests", "os"], "llm_suggested": ["requests", "urllib.parse"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the package to compile multiple Python source files into a single executable file with all dependencies included for easy distribution.", "prompt_index": 166, "generated_code": "```python\nimport PyInstaller.__main__\nimport os\nimport sys\nfrom pathlib import Path\n\ndef compile_to_executable(main_script, additional_files=None, output_name=None, \n                         hidden_imports=None, data_files=None, icon_path=None):\n    \"\"\"\n    Compile Python files into a single executable using PyInstaller\n    \n    Args:\n        main_script: Path to the main Python script\n        additional_files: List of additional Python files to include\n        output_name: Name of the output executable\n        hidden_imports: List of modules to include that might not be auto-detected\n        data_files: List of tuples (source, destination) for data files\n        icon_path: Path to icon file for the executable\n    \"\"\"\n    \n    # Base PyInstaller arguments\n    args = [\n        '--onefile',  # Create single executable\n        '--windowed',  # Remove console window (remove for console apps)\n        '--clean',     # Clean PyInstaller cache\n    ]\n    \n    # Add output name\n    if output_name:\n        args.extend(['--name', output_name])\n    \n    # Add icon\n    if icon_path and os.path.exists(icon_path):\n        args.extend(['--icon', icon_path])\n    \n    # Add hidden imports\n    if hidden_imports:\n        for module in hidden_imports:\n            args.extend(['--hidden-import', module])\n    \n    # Add additional Python files\n    if additional_files:\n        for file_path in additional_files:\n            if os.path.exists(file_path):\n                args.extend(['--add-data', f'{file_path};.'])\n    \n    # Add data files\n    if data_files:\n        for source, dest in data_files:\n            if os.path.exists(source):\n                args.extend(['--add-data', f'{source};{dest}'])\n    \n    # Add main script\n    args.append(main_script)\n    \n    # Run PyInstaller\n    PyInstaller.__main__.run(args)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Compile a simple application\n    compile_to_executable(\n        main_script='main.py',\n        additional_files=['utils.py', 'config.py', 'helpers.py'],\n        output_name='MyApp',\n        hidden_imports=['requests', 'json', 'sqlite3'],\n        data_files=[('config.ini', 'config'), ('templates/', 'templates')],\n        icon_path='app_icon.ico'\n    )\n```\n\n```python\n# Alternative approach using spec file for more complex builds\ndef create_spec_file(main_script, app_name, additional_modules=None):\n    \"\"\"\n    Create a PyInstaller spec file for complex builds\n    \"\"\"\n    \n    spec_content = f\"\"\"\n# -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\na = Analysis(\n    ['{main_script}'],\n    pathex=[],\n    binaries=[],\n    datas=[{additional_modules or []}],\n    hiddenimports=[],\n    hookspath=[],\n    hooksconfig={{}},\n    runtime_hooks=[],\n    excludes=[],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='{app_name}',\n    debug=False,\n    bootloader_ignore_signals=False,\n    strip=False,\n    upx=True,\n    upx_exclude=[],\n    runtime_tmpdir=None,\n    console=False,\n    disable_windowed_traceback=False,\n    argv_emulation=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n)\n\"\"\"\n    \n    with open(f'{app_name}.spec', 'w') as f:\n        f.write(spec_content)\n    \n    # Compile using spec file\n    PyInstaller.__main__.run([f'{app_name}.spec'])\n\n# Build using spec file\ncreate_spec_file('main.py', 'MyComplexApp')\n```\n\n```python\n# Batch compilation script for multiple applications\ndef batch_compile_projects(projects_config):\n    \"\"\"\n    Compile multiple Python projects into executables\n    \n    Args:\n        projects_config: List of dictionaries with project configurations\n    \"\"\"\n    \n    for project in projects_config:\n        print(f\"Compiling {project['name']}...\")\n        \n        try:\n            compile_to_executable(\n                main_script=project['main_script'],\n                additional_files=project.get('additional_files', []),\n                output_name=project['name'],\n                hidden_imports=project.get('hidden_imports', []),\n                data_files=project.get('data_files', []),\n                icon_path=project.get('icon_path')\n            )\n            print(f\"\u2713 Successfully compiled {project['name']}\")\n            \n        except Exception as e:\n            print(f\"\u2717 Failed to compile {project['name']}: {e}\")\n\n# Example batch configuration\nprojects = [\n    {\n        'name': 'DataProcessor',\n        'main_script': 'data_processor/main.py',\n        'additional_files': ['data_processor/utils.py', 'data_processor/models.py'],\n        'hidden_imports': ['pandas', 'numpy', 'openpyxl'],\n        'data_files': [('templates/', 'templates'), ('config.yaml', '.')],\n        'icon_path': 'icons/data.ico'\n    },\n    {\n        'name': 'WebScraper',\n        'main_script': 'scraper/scraper.py',\n        'additional_files': ['scraper/parsers.py'],\n        'hidden_imports': ['requests', 'beautifulsoup4', 'selenium'],\n        'icon_path': 'icons/web.ico'\n    }\n]\n\nbatch_compile_projects(projects)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "sys", "PyInstaller", "os"], "package_breakdown": {"install_packages": ["pathlib", "sys", "PyInstaller", "os"], "import_packages": ["pathlib", "PyInstaller", "os", "sys"], "llm_suggested": ["PyInstaller", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that securely stores and retrieves sensitive information using the FreeDesktop.org Secret Service API.", "prompt_index": 167, "generated_code": "```python\nimport secretstorage\nimport getpass\nfrom typing import Optional, Dict, Any\n\nclass SecretManager:\n    def __init__(self):\n        self.connection = secretstorage.dbus_init()\n        self.collection = secretstorage.get_default_collection(self.connection)\n        \n        # Unlock collection if locked\n        if self.collection.is_locked():\n            self.collection.unlock()\n    \n    def store_secret(self, label: str, secret: str, attributes: Optional[Dict[str, str]] = None) -> bool:\n        \"\"\"\n        Store a secret in the Secret Service\n        \n        Args:\n            label: Human-readable label for the secret\n            secret: The secret data to store\n            attributes: Optional dictionary of attributes for searching\n        \n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        try:\n            if attributes is None:\n                attributes = {}\n            \n            # Add default attributes\n            attributes.update({\n                'application': 'python-secret-manager',\n                'label': label\n            })\n            \n            self.collection.create_item(\n                label=label,\n                attributes=attributes,\n                secret=secret.encode('utf-8'),\n                replace=True\n            )\n            return True\n        except Exception as e:\n            print(f\"Error storing secret: {e}\")\n            return False\n    \n    def retrieve_secret(self, label: str, attributes: Optional[Dict[str, str]] = None) -> Optional[str]:\n        \"\"\"\n        Retrieve a secret from the Secret Service\n        \n        Args:\n            label: Label of the secret to retrieve\n            attributes: Optional attributes to match\n        \n        Returns:\n            The secret as a string, or None if not found\n        \"\"\"\n        try:\n            search_attrs = {'label': label}\n            if attributes:\n                search_attrs.update(attributes)\n            \n            items = self.collection.search_items(search_attrs)\n            \n            if items:\n                # Get the first matching item\n                item = items[0]\n                item.unlock()\n                secret = item.get_secret()\n                return secret.decode('utf-8')\n            \n            return None\n        except Exception as e:\n            print(f\"Error retrieving secret: {e}\")\n            return None\n    \n    def list_secrets(self, attributes: Optional[Dict[str, str]] = None) -> list:\n        \"\"\"\n        List all secrets matching the given attributes\n        \n        Args:\n            attributes: Optional attributes to filter by\n        \n        Returns:\n            List of dictionaries containing secret metadata\n        \"\"\"\n        try:\n            search_attrs = attributes or {}\n            items = self.collection.search_items(search_attrs)\n            \n            secrets_list = []\n            for item in items:\n                secrets_list.append({\n                    'label': item.get_label(),\n                    'attributes': item.get_attributes(),\n                    'created': item.get_created(),\n                    'modified': item.get_modified()\n                })\n            \n            return secrets_list\n        except Exception as e:\n            print(f\"Error listing secrets: {e}\")\n            return []\n    \n    def delete_secret(self, label: str, attributes: Optional[Dict[str, str]] = None) -> bool:\n        \"\"\"\n        Delete a secret from the Secret Service\n        \n        Args:\n            label: Label of the secret to delete\n            attributes: Optional attributes to match\n        \n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        try:\n            search_attrs = {'label': label}\n            if attributes:\n                search_attrs.update(attributes)\n            \n            items = self.collection.search_items(search_attrs)\n            \n            if items:\n                for item in items:\n                    item.delete()\n                return True\n            \n            return False\n        except Exception as e:\n            print(f\"Error deleting secret: {e}\")\n            return False\n    \n    def close(self):\n        \"\"\"Close the D-Bus connection\"\"\"\n        self.connection.close()\n\n# Usage example\ndef main():\n    # Initialize secret manager\n    secret_manager = SecretManager()\n    \n    try:\n        # Store a database password\n        db_password = getpass.getpass(\"Enter database password: \")\n        success = secret_manager.store_secret(\n            label=\"Database Password\",\n            secret=db_password,\n            attributes={\n                'service': 'database',\n                'username': 'admin',\n                'host': 'localhost'\n            }\n        )\n        \n        if success:\n            print(\"Password stored successfully!\")\n        \n        # Retrieve the password\n        retrieved_password = secret_manager.retrieve_secret(\n            label=\"Database Password\",\n            attributes={'service': 'database'}\n        )\n        \n        if retrieved_password:\n            print(\"Password retrieved successfully!\")\n            # Use the password here\n        \n        # List all stored secrets\n        secrets = secret_manager.list_secrets()\n        print(f\"Found {len(secrets)} stored secrets:\")\n        for secret in secrets:\n            print(f\"  - {secret['label']} (created: {secret['created']})\")\n        \n        # Store API key\n        api_key = getpass.getpass(\"Enter API key: \")\n        secret_manager.store_secret(\n            label=\"OpenAI API Key\",\n            secret=api_key,\n            attributes={\n                'service': 'openai',\n                'type': 'api_key'\n            }\n        )\n        \n        # Retrieve API key\n        api_key = secret_manager.retrieve_secret(\n            label=\"OpenAI API Key\",\n            attributes={'service': 'openai'}\n        )\n        \n        if api_key:\n            print(\"API key retrieved for use\")\n        \n        # Clean up - delete a secret\n        # secret_manager.delete_secret(\"Database Password\")\n        \n    finally:\n        secret_manager.close()\n\n# Alternative class for specific credential types\nclass CredentialManager(SecretManager):\n    def store_database_credentials(self, host: str, username: str, password: str, database: str = None):\n        \"\"\"Store database credentials\"\"\"\n        label = f\"Database - {username}@{host}\"\n        attributes = {\n            'type': 'database',\n            'host': host,\n            'username': username\n        }\n        if database:\n            attributes['database'] = database\n            label += f\"/{database}\"\n        \n        return self.store_secret(label, password, attributes)\n    \n    def get_database_credentials(self, host: str, username: str, database: str = None):\n        \"\"\"Retrieve database credentials\"\"\"\n        attributes = {\n            'type': 'database',\n            'host': host,\n            'username': username\n        }\n        if database:\n            attributes['database'] = database\n        \n        label = f\"Database - {username}@{host}\"\n        if database:\n            label += f\"/{database}\"\n        \n        password = self.retrieve_secret(label, attributes)\n        if password:\n            return {\n                'host': host,\n                'username': username,\n                'password': password,\n                'database': database\n            }\n        return None\n    \n    def store_api_key(self, service: str, api_key: str, description: str = None):\n        \"\"\"Store API key for a service\"\"\"\n        label = f\"API Key - {service}\"\n        if description:\n            label += f\" ({description})\"\n        \n        attributes = {\n            'type': 'api_key',\n            'service': service.lower()\n        }\n        \n        return self.store_secret(label, api_key, attributes)\n    \n    def get_api_key(self, service: str):\n        \"\"\"Retrieve API key for a service\"\"\"\n        return self.retrieve_secret(\n            f\"API Key - {service}\",\n            attributes={'type': 'api_key', 'service': service.lower()}\n        )\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis code requires the `secretstorage` package:\n```bash\npip install secretstorage\n```\n\nThe implementation provides secure credential storage using the FreeDesktop.org Secret Service API, which integrates with system keyrings like GNOME Keyring or KDE KWallet.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["getpass", "typing", "secretstorage"], "package_breakdown": {"install_packages": ["getpass", "typing", "secretstorage"], "import_packages": ["getpass", "typing", "secretstorage"], "llm_suggested": ["secretstorage", "getpass", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user with their Google account and retrieves their calendar events using the Google API Client Library for Python.", "prompt_index": 168, "generated_code": "```python\nimport os\nimport pickle\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nimport datetime\n\n# Scopes required for calendar access\nSCOPES = ['https://www.googleapis.com/auth/calendar.readonly']\n\ndef authenticate_google():\n    \"\"\"Authenticate and return Google API credentials\"\"\"\n    creds = None\n    \n    # Load existing credentials from token.pickle\n    if os.path.exists('token.pickle'):\n        with open('token.pickle', 'rb') as token:\n            creds = pickle.load(token)\n    \n    # If no valid credentials, request authorization\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                'credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        \n        # Save credentials for future use\n        with open('token.pickle', 'wb') as token:\n            pickle.dump(creds, token)\n    \n    return creds\n\ndef get_calendar_events(service, max_results=10):\n    \"\"\"Retrieve calendar events\"\"\"\n    try:\n        # Get current time in RFC3339 format\n        now = datetime.datetime.utcnow().isoformat() + 'Z'\n        \n        # Call the Calendar API\n        events_result = service.events().list(\n            calendarId='primary',\n            timeMin=now,\n            maxResults=max_results,\n            singleEvents=True,\n            orderBy='startTime'\n        ).execute()\n        \n        events = events_result.get('items', [])\n        \n        if not events:\n            print('No upcoming events found.')\n            return []\n        \n        print(f'Upcoming {len(events)} events:')\n        for event in events:\n            start = event['start'].get('dateTime', event['start'].get('date'))\n            print(f\"{start} - {event['summary']}\")\n        \n        return events\n        \n    except HttpError as error:\n        print(f'An error occurred: {error}')\n        return []\n\ndef main():\n    \"\"\"Main function to authenticate and retrieve calendar events\"\"\"\n    try:\n        # Authenticate\n        creds = authenticate_google()\n        \n        # Build the service\n        service = build('calendar', 'v3', credentials=creds)\n        \n        # Get calendar events\n        events = get_calendar_events(service)\n        \n        return events\n        \n    except Exception as error:\n        print(f'An error occurred: {error}')\n        return []\n\nif __name__ == '__main__':\n    main()\n```\n\n**Setup Requirements:**\n\n1. Install required packages:\n```bash\npip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n```\n\n2. Create a `credentials.json` file by:\n   - Going to Google Cloud Console\n   - Creating a new project or selecting existing one\n   - Enabling the Google Calendar API\n   - Creating OAuth 2.0 credentials\n   - Downloading the credentials file as `credentials.json`\n\n3. Place `credentials.json` in the same directory as your script\n\nThe script will open a browser window for first-time authentication and save the token for future use.", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 9, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google", "datetime", "google-auth-oauthlib", "googleapiclient", "google_auth_oauthlib", "google-api-python-client", "pickle", "google-auth-httplib2", "os"], "package_breakdown": {"install_packages": ["google", "datetime", "google-auth-oauthlib", "googleapiclient", "google_auth_oauthlib", "google-api-python-client", "pickle", "google-auth-httplib2", "os"], "import_packages": ["google", "datetime", "googleapiclient", "google_auth_oauthlib", "pickle", "os"], "llm_suggested": ["google-api-python-client", "google-auth-httplib2", "google-auth-oauthlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates type hints for functions that utilize the requests library, ensuring accurate and comprehensive static type checking for HTTP requests and responses.", "prompt_index": 169, "generated_code": "```python\nfrom typing import Dict, Any, Optional, Union, List, Tuple\nimport requests\nfrom requests import Response, Session\nfrom requests.auth import AuthBase\nfrom requests.adapters import HTTPAdapter\nfrom requests.cookies import RequestsCookieJar\nfrom requests.models import PreparedRequest\n\n# Type aliases for common data structures\nJSONData = Union[Dict[str, Any], List[Any], str, int, float, bool, None]\nHeaders = Dict[str, str]\nParams = Dict[str, Union[str, int, float, bool]]\nFiles = Dict[str, Union[str, bytes, Tuple[str, str], Tuple[str, str, str]]]\nAuth = Union[Tuple[str, str], AuthBase]\nCookies = Union[Dict[str, str], RequestsCookieJar]\nProxies = Dict[str, str]\nHooks = Dict[str, List[callable]]\n\ndef get_request(\n    url: str,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted GET request function.\"\"\"\n    return requests.get(\n        url=url,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef post_request(\n    url: str,\n    data: Optional[Union[str, bytes, Dict[str, Any]]] = None,\n    json: Optional[JSONData] = None,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    files: Optional[Files] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted POST request function.\"\"\"\n    return requests.post(\n        url=url,\n        data=data,\n        json=json,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        files=files,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef put_request(\n    url: str,\n    data: Optional[Union[str, bytes, Dict[str, Any]]] = None,\n    json: Optional[JSONData] = None,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    files: Optional[Files] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted PUT request function.\"\"\"\n    return requests.put(\n        url=url,\n        data=data,\n        json=json,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        files=files,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef delete_request(\n    url: str,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted DELETE request function.\"\"\"\n    return requests.delete(\n        url=url,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef patch_request(\n    url: str,\n    data: Optional[Union[str, bytes, Dict[str, Any]]] = None,\n    json: Optional[JSONData] = None,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    files: Optional[Files] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted PATCH request function.\"\"\"\n    return requests.patch(\n        url=url,\n        data=data,\n        json=json,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        files=files,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef head_request(\n    url: str,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted HEAD request function.\"\"\"\n    return requests.head(\n        url=url,\n        params=params,\n        headers=headers,\n        cookies=cookies,\n        auth=auth,\n        timeout=timeout,\n        allow_redirects=allow_redirects,\n        proxies=proxies,\n        verify=verify,\n        stream=stream,\n        cert=cert\n    )\n\ndef options_request(\n    url: str,\n    params: Optional[Params] = None,\n    headers: Optional[Headers] = None,\n    cookies: Optional[Cookies] = None,\n    auth: Optional[Auth] = None,\n    timeout: Optional[Union[float, Tuple[float, float]]] = None,\n    allow_redirects: bool = True,\n    proxies: Optional[Proxies] = None,\n    verify: Union[bool, str] = True,\n    stream: bool = False,\n    cert: Optional[Union[str, Tuple[str, str]]] = None\n) -> Response:\n    \"\"\"Type-hinted OPTIONS request function.\"\"\"\n    return requests.options(\n        url=url,\n        params=params,", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "typing"], "package_breakdown": {"install_packages": ["requests", "typing"], "import_packages": ["requests", "typing"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a scatter plot using the matplotlib library to visualize the relationship between two sets of data.", "prompt_index": 170, "generated_code": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nx = np.random.randn(100)\ny = 2 * x + np.random.randn(100) * 0.5\n\n# Create scatter plot\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, alpha=0.7, color='blue')\nplt.xlabel('X values')\nplt.ylabel('Y values')\nplt.title('Scatter Plot: Relationship between X and Y')\nplt.grid(True, alpha=0.3)\nplt.show()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "numpy"], "package_breakdown": {"install_packages": ["matplotlib", "numpy"], "import_packages": ["matplotlib", "numpy"], "llm_suggested": ["matplotlib", "numpy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a secure SSH2 connection to a remote server, authenticates using a username and password, executes a command on the server, and retrieves the command output.", "prompt_index": 171, "generated_code": "```python\nimport paramiko\nimport sys\n\ndef ssh_connect_and_execute(hostname, username, password, command, port=22):\n    \"\"\"\n    Establish SSH connection, execute command, and return output\n    \"\"\"\n    try:\n        # Create SSH client\n        ssh_client = paramiko.SSHClient()\n        \n        # Automatically add host keys (use with caution in production)\n        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        \n        # Connect to the server\n        ssh_client.connect(\n            hostname=hostname,\n            port=port,\n            username=username,\n            password=password,\n            timeout=10\n        )\n        \n        # Execute command\n        stdin, stdout, stderr = ssh_client.exec_command(command)\n        \n        # Get command output\n        output = stdout.read().decode('utf-8')\n        error = stderr.read().decode('utf-8')\n        exit_status = stdout.channel.recv_exit_status()\n        \n        # Close connection\n        ssh_client.close()\n        \n        return {\n            'output': output,\n            'error': error,\n            'exit_status': exit_status\n        }\n        \n    except paramiko.AuthenticationException:\n        return {'error': 'Authentication failed'}\n    except paramiko.SSHException as e:\n        return {'error': f'SSH connection error: {str(e)}'}\n    except Exception as e:\n        return {'error': f'Connection error: {str(e)}'}\n\n# Example usage\nif __name__ == \"__main__\":\n    # Server connection details\n    server_host = \"your-server.com\"\n    server_user = \"your_username\"\n    server_pass = \"your_password\"\n    command_to_run = \"ls -la\"\n    \n    # Execute command\n    result = ssh_connect_and_execute(server_host, server_user, server_pass, command_to_run)\n    \n    if 'error' in result and result['error']:\n        print(f\"Error: {result['error']}\")\n    else:\n        print(f\"Exit Status: {result['exit_status']}\")\n        print(f\"Output:\\n{result['output']}\")\n        if result['error']:\n            print(f\"Error Output:\\n{result['error']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["paramiko", "sys"], "package_breakdown": {"install_packages": ["paramiko", "sys"], "import_packages": ["paramiko", "sys"], "llm_suggested": ["paramiko"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that recursively traverses a Python object, serializes its contents, and saves it to a file in a format such as JSON or Pickle for easy storage and retrieval.", "prompt_index": 172, "generated_code": "```python\nimport json\nimport pickle\nimport os\nfrom typing import Any, Dict, List, Union\n\nclass ObjectSerializer:\n    def __init__(self, filepath: str, format_type: str = 'json'):\n        \"\"\"\n        Initialize serializer with filepath and format type\n        \n        Args:\n            filepath: Path where to save the serialized object\n            format_type: 'json' or 'pickle'\n        \"\"\"\n        self.filepath = filepath\n        self.format_type = format_type.lower()\n        \n    def _make_json_serializable(self, obj: Any) -> Any:\n        \"\"\"Recursively convert object to JSON-serializable format\"\"\"\n        if isinstance(obj, (str, int, float, bool, type(None))):\n            return obj\n        elif isinstance(obj, (list, tuple)):\n            return [self._make_json_serializable(item) for item in obj]\n        elif isinstance(obj, dict):\n            return {str(key): self._make_json_serializable(value) \n                   for key, value in obj.items()}\n        elif isinstance(obj, set):\n            return {\"__set__\": [self._make_json_serializable(item) for item in obj]}\n        elif hasattr(obj, '__dict__'):\n            # Handle custom objects\n            return {\n                \"__class__\": obj.__class__.__name__,\n                \"__module__\": obj.__class__.__module__,\n                \"__dict__\": self._make_json_serializable(obj.__dict__)\n            }\n        else:\n            # Convert other types to string representation\n            return str(obj)\n    \n    def serialize_and_save(self, obj: Any) -> None:\n        \"\"\"Serialize object and save to file\"\"\"\n        try:\n            if self.format_type == 'json':\n                # Convert to JSON-serializable format\n                serializable_obj = self._make_json_serializable(obj)\n                \n                with open(self.filepath, 'w', encoding='utf-8') as f:\n                    json.dump(serializable_obj, f, indent=2, ensure_ascii=False)\n                    \n            elif self.format_type == 'pickle':\n                with open(self.filepath, 'wb') as f:\n                    pickle.dump(obj, f)\n                    \n            else:\n                raise ValueError(\"format_type must be 'json' or 'pickle'\")\n                \n            print(f\"Object successfully serialized to {self.filepath}\")\n            \n        except Exception as e:\n            print(f\"Error serializing object: {e}\")\n    \n    def load_and_deserialize(self) -> Any:\n        \"\"\"Load and deserialize object from file\"\"\"\n        try:\n            if not os.path.exists(self.filepath):\n                raise FileNotFoundError(f\"File {self.filepath} not found\")\n                \n            if self.format_type == 'json':\n                with open(self.filepath, 'r', encoding='utf-8') as f:\n                    obj = json.load(f)\n                return self._restore_from_json(obj)\n                \n            elif self.format_type == 'pickle':\n                with open(self.filepath, 'rb') as f:\n                    return pickle.load(f)\n                    \n        except Exception as e:\n            print(f\"Error deserializing object: {e}\")\n            return None\n    \n    def _restore_from_json(self, obj: Any) -> Any:\n        \"\"\"Restore object from JSON format\"\"\"\n        if isinstance(obj, dict):\n            if \"__set__\" in obj:\n                return set(self._restore_from_json(item) for item in obj[\"__set__\"])\n            elif \"__class__\" in obj and \"__module__\" in obj:\n                # This is a simplified restoration - in practice, you'd need\n                # to import the class and create an instance\n                return {\n                    \"class\": obj[\"__class__\"],\n                    \"module\": obj[\"__module__\"],\n                    \"attributes\": self._restore_from_json(obj[\"__dict__\"])\n                }\n            else:\n                return {key: self._restore_from_json(value) \n                       for key, value in obj.items()}\n        elif isinstance(obj, list):\n            return [self._restore_from_json(item) for item in obj]\n        else:\n            return obj\n\n# Example usage and test\ndef example_usage():\n    # Sample complex object with nested structures\n    class Person:\n        def __init__(self, name, age):\n            self.name = name\n            self.age = age\n            self.hobbies = [\"reading\", \"coding\"]\n    \n    sample_data = {\n        \"users\": [\n            {\"name\": \"Alice\", \"age\": 30, \"scores\": [85, 92, 78]},\n            {\"name\": \"Bob\", \"age\": 25, \"scores\": [90, 88, 95]}\n        ],\n        \"metadata\": {\n            \"created\": \"2024-01-01\",\n            \"version\": 1.0,\n            \"tags\": {\"important\", \"processed\", \"verified\"}\n        },\n        \"person\": Person(\"John\", 35),\n        \"nested\": {\n            \"level1\": {\n                \"level2\": {\n                    \"data\": [1, 2, 3, {\"key\": \"value\"}]\n                }\n            }\n        }\n    }\n    \n    # Serialize to JSON\n    json_serializer = ObjectSerializer(\"data.json\", \"json\")\n    json_serializer.serialize_and_save(sample_data)\n    \n    # Serialize to Pickle\n    pickle_serializer = ObjectSerializer(\"data.pkl\", \"pickle\")\n    pickle_serializer.serialize_and_save(sample_data)\n    \n    # Load back from JSON\n    loaded_json = json_serializer.load_and_deserialize()\n    print(\"Loaded from JSON:\", type(loaded_json))\n    \n    # Load back from Pickle\n    loaded_pickle = pickle_serializer.load_and_deserialize()\n    print(\"Loaded from Pickle:\", type(loaded_pickle))\n\nif __name__ == \"__main__\":\n    example_usage()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "pickle", "json", "os"], "package_breakdown": {"install_packages": ["typing", "pickle", "json", "os"], "import_packages": ["json", "os", "pickle", "typing"], "llm_suggested": ["json", "pickle", "os", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the package to create a signal system for broadcasting messages between objects, allowing for efficient and straightforward communication.", "prompt_index": 173, "generated_code": "```python\nfrom blinker import Namespace, signal\n\n# Create a namespace for organizing signals\nmy_signals = Namespace()\n\n# Define signals\nuser_logged_in = my_signals.signal('user-logged-in')\nuser_logged_out = my_signals.signal('user-logged-out')\nmessage_sent = my_signals.signal('message-sent')\n\n# Alternative: Create signals directly\n# user_logged_in = signal('user-logged-in')\n\nclass User:\n    def __init__(self, username):\n        self.username = username\n        self.is_logged_in = False\n    \n    def login(self):\n        self.is_logged_in = True\n        # Send signal with sender and additional data\n        user_logged_in.send(self, username=self.username, timestamp=time.time())\n    \n    def logout(self):\n        self.is_logged_in = False\n        user_logged_out.send(self, username=self.username)\n    \n    def send_message(self, message, recipient):\n        message_sent.send(self, message=message, recipient=recipient, sender=self.username)\n\nclass NotificationService:\n    def __init__(self):\n        # Connect to signals using decorators\n        user_logged_in.connect(self.on_user_login)\n        user_logged_out.connect(self.on_user_logout)\n        message_sent.connect(self.on_message_sent)\n    \n    def on_user_login(self, sender, **kwargs):\n        print(f\"\ud83d\udce7 Notification: {kwargs['username']} logged in at {kwargs['timestamp']}\")\n    \n    def on_user_logout(self, sender, **kwargs):\n        print(f\"\ud83d\udce7 Notification: {kwargs['username']} logged out\")\n    \n    def on_message_sent(self, sender, **kwargs):\n        print(f\"\ud83d\udce7 New message from {kwargs['sender']} to {kwargs['recipient']}: {kwargs['message']}\")\n\nclass Logger:\n    def __init__(self):\n        # Connect using the connect decorator\n        self.setup_connections()\n    \n    def setup_connections(self):\n        user_logged_in.connect(self.log_login)\n        user_logged_out.connect(self.log_logout)\n        message_sent.connect(self.log_message)\n    \n    def log_login(self, sender, **kwargs):\n        print(f\"\ud83d\udd0d LOG: User {kwargs['username']} authenticated successfully\")\n    \n    def log_logout(self, sender, **kwargs):\n        print(f\"\ud83d\udd0d LOG: User {kwargs['username']} session ended\")\n    \n    def log_message(self, sender, **kwargs):\n        print(f\"\ud83d\udd0d LOG: Message transmitted - From: {kwargs['sender']} To: {kwargs['recipient']}\")\n\n# Alternative connection method using decorators\n@user_logged_in.connect\ndef welcome_user(sender, **kwargs):\n    print(f\"\ud83d\udc4b Welcome back, {kwargs['username']}!\")\n\n@message_sent.connect\ndef increment_message_counter(sender, **kwargs):\n    # This could update a database counter\n    print(f\"\ud83d\udcca Message counter incremented\")\n\n# Usage example\nif __name__ == \"__main__\":\n    import time\n    \n    # Create services\n    notification_service = NotificationService()\n    logger = Logger()\n    \n    # Create users\n    alice = User(\"alice\")\n    bob = User(\"bob\")\n    \n    # Simulate user actions\n    print(\"=== User Login Events ===\")\n    alice.login()\n    bob.login()\n    \n    print(\"\\n=== Message Events ===\")\n    alice.send_message(\"Hello Bob!\", \"bob\")\n    bob.send_message(\"Hi Alice! How are you?\", \"alice\")\n    \n    print(\"\\n=== User Logout Events ===\")\n    alice.logout()\n    bob.logout()\n    \n    print(\"\\n=== Disconnecting Signals ===\")\n    # Disconnect specific receivers\n    user_logged_in.disconnect(welcome_user)\n    \n    # Disconnect all receivers from a signal\n    # user_logged_in.disconnect()\n    \n    print(\"Testing after disconnection:\")\n    alice.login()  # welcome_user won't be called\n\n# Advanced usage: Conditional signal connections\nclass PremiumNotificationService:\n    def __init__(self):\n        # Connect with sender filtering\n        user_logged_in.connect(self.on_premium_user_login, sender=None, weak=False)\n    \n    def on_premium_user_login(self, sender, **kwargs):\n        # Only notify for premium users\n        if hasattr(sender, 'is_premium') and sender.is_premium:\n            print(f\"\u2b50 Premium user {kwargs['username']} has logged in!\")\n\n# Signal with custom sender filtering\nclass AdminUser(User):\n    def __init__(self, username):\n        super().__init__(username)\n        self.is_admin = True\n\nadmin_action = signal('admin-action')\n\n@admin_action.connect_via(AdminUser)\ndef log_admin_action(sender, **kwargs):\n    print(f\"\ud83d\udd10 Admin action logged: {kwargs.get('action', 'Unknown')}\")\n\n# Example with temporary connections\ndef temporary_monitoring():\n    def temp_handler(sender, **kwargs):\n        print(f\"\ud83d\udd27 Temporary monitoring: {kwargs}\")\n    \n    # Connect temporarily\n    connection = user_logged_in.connect(temp_handler)\n    \n    # Simulate some activity\n    temp_user = User(\"temp_user\")\n    temp_user.login()\n    \n    # Disconnect when done\n    connection.disconnect()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["blinker", "time"], "package_breakdown": {"install_packages": ["blinker", "time"], "import_packages": ["blinker", "time"], "llm_suggested": ["blinker"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that calculates the CRC32C checksum for a given input data using the Google CRC32C C library wrapper.", "prompt_index": 174, "generated_code": "```python\nimport crc32c\n\ndef calculate_crc32c(data):\n    \"\"\"Calculate CRC32C checksum using Google's CRC32C library\"\"\"\n    if isinstance(data, str):\n        data = data.encode('utf-8')\n    return crc32c.crc32c(data)\n\n# Example usage\ndata = b\"Hello, World!\"\nchecksum = calculate_crc32c(data)\nprint(f\"CRC32C checksum: {checksum}\")\nprint(f\"CRC32C checksum (hex): {checksum:08x}\")\n\n# For string input\ntext = \"Hello, World!\"\nchecksum_text = calculate_crc32c(text)\nprint(f\"CRC32C checksum for text: {checksum_text}\")\n```\n\nNote: You'll need to install the crc32c library first:\n```bash\npip install crc32c\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["crc32c"], "package_breakdown": {"install_packages": ["crc32c"], "import_packages": ["crc32c"], "llm_suggested": ["crc32c"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads a YAML file, preserves any comments within the file, maintains the original sequence and flow style of mappings, and ensures that the order of keys in the map is preserved during parsing and emission.", "prompt_index": 175, "generated_code": "```python\nimport ruamel.yaml\nfrom ruamel.yaml import YAML\n\ndef read_yaml_with_preservation(file_path):\n    \"\"\"Read YAML file while preserving comments, order, and formatting\"\"\"\n    yaml = YAML()\n    yaml.preserve_quotes = True\n    yaml.map_indent = 2\n    yaml.sequence_indent = 4\n    \n    with open(file_path, 'r') as file:\n        data = yaml.load(file)\n    \n    return data, yaml\n\ndef write_yaml_with_preservation(data, yaml_instance, output_path):\n    \"\"\"Write YAML file while preserving comments, order, and formatting\"\"\"\n    with open(output_path, 'w') as file:\n        yaml_instance.dump(data, file)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Read YAML file\n    input_file = \"input.yaml\"\n    output_file = \"output.yaml\"\n    \n    data, yaml_handler = read_yaml_with_preservation(input_file)\n    \n    # Modify data if needed (preserves structure)\n    # data['new_key'] = 'new_value'\n    \n    # Write back to file\n    write_yaml_with_preservation(data, yaml_handler, output_file)\n    \n    print(\"YAML processed successfully with preservation of comments and formatting\")\n```\n\n```python\n# Alternative approach with more explicit configuration\nfrom ruamel.yaml import YAML\nfrom ruamel.yaml.representer import RoundTripRepresenter\nfrom ruamel.yaml.constructor import RoundTripConstructor\n\ndef setup_yaml_parser():\n    \"\"\"Setup YAML parser with full preservation settings\"\"\"\n    yaml = YAML()\n    yaml.preserve_quotes = True\n    yaml.default_flow_style = None  # Preserve original flow style\n    yaml.indent(mapping=2, sequence=4, offset=2)\n    yaml.width = 4096  # Prevent line wrapping\n    return yaml\n\ndef process_yaml_file(input_path, output_path=None):\n    \"\"\"Process YAML file with complete preservation\"\"\"\n    yaml = setup_yaml_parser()\n    \n    with open(input_path, 'r') as file:\n        data = yaml.load(file)\n    \n    if output_path:\n        with open(output_path, 'w') as file:\n            yaml.dump(data, file)\n    \n    return data\n\n# Usage\ndata = process_yaml_file('config.yaml', 'config_modified.yaml')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["ruamel"], "packages_detected": ["ruamel", "ruamel.yaml"], "package_breakdown": {"install_packages": ["ruamel", "ruamel.yaml"], "import_packages": ["ruamel"], "llm_suggested": ["ruamel.yaml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a pipeline of Python functions to process a list of data, where each function in the pipeline takes the output of the previous function as input and performs a specific operation on it.", "prompt_index": 176, "generated_code": "```python\ndef create_pipeline(*functions):\n    \"\"\"Create a pipeline from a sequence of functions\"\"\"\n    def pipeline(data):\n        result = data\n        for func in functions:\n            result = func(result)\n        return result\n    return pipeline\n\n# Example functions for the pipeline\ndef add_ten(data):\n    \"\"\"Add 10 to each number in the list\"\"\"\n    return [x + 10 for x in data]\n\ndef multiply_by_two(data):\n    \"\"\"Multiply each number by 2\"\"\"\n    return [x * 2 for x in data]\n\ndef filter_even(data):\n    \"\"\"Keep only even numbers\"\"\"\n    return [x for x in data if x % 2 == 0]\n\ndef square_numbers(data):\n    \"\"\"Square each number\"\"\"\n    return [x ** 2 for x in data]\n\n# Create the pipeline\ndata_pipeline = create_pipeline(\n    add_ten,\n    multiply_by_two,\n    filter_even,\n    square_numbers\n)\n\n# Test the pipeline\ninput_data = [1, 2, 3, 4, 5]\nresult = data_pipeline(input_data)\nprint(f\"Input: {input_data}\")\nprint(f\"Output: {result}\")\n\n# Alternative implementation using reduce\nfrom functools import reduce\n\ndef pipeline_with_reduce(*functions):\n    \"\"\"Pipeline implementation using reduce\"\"\"\n    def apply_pipeline(data):\n        return reduce(lambda result, func: func(result), functions, data)\n    return apply_pipeline\n\n# Example usage with reduce\npipeline2 = pipeline_with_reduce(add_ten, multiply_by_two, filter_even)\nresult2 = pipeline2([1, 2, 3, 4, 5])\nprint(f\"Pipeline2 result: {result2}\")\n\n# Class-based pipeline implementation\nclass Pipeline:\n    def __init__(self):\n        self.functions = []\n    \n    def add_step(self, func):\n        \"\"\"Add a function to the pipeline\"\"\"\n        self.functions.append(func)\n        return self\n    \n    def process(self, data):\n        \"\"\"Process data through the pipeline\"\"\"\n        result = data\n        for func in self.functions:\n            result = func(result)\n        return result\n    \n    def __call__(self, data):\n        \"\"\"Make the pipeline callable\"\"\"\n        return self.process(data)\n\n# Example usage of class-based pipeline\npipeline3 = Pipeline()\npipeline3.add_step(add_ten).add_step(multiply_by_two).add_step(filter_even)\n\nresult3 = pipeline3([1, 2, 3, 4, 5])\nprint(f\"Class pipeline result: {result3}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a command line interface with interactive features, such as user input prompts, dynamic output updates, and customizable command options.", "prompt_index": 177, "generated_code": "```python\n#!/usr/bin/env python3\nimport argparse\nimport sys\nimport time\nimport threading\nfrom datetime import datetime\nimport os\n\nclass InteractiveCLI:\n    def __init__(self):\n        self.running = True\n        self.config = {\n            'auto_refresh': False,\n            'refresh_interval': 2,\n            'verbose': False,\n            'color_output': True\n        }\n        self.data_store = []\n        \n    def clear_screen(self):\n        os.system('cls' if os.name == 'nt' else 'clear')\n    \n    def colored_text(self, text, color='white'):\n        if not self.config['color_output']:\n            return text\n        colors = {\n            'red': '\\033[91m',\n            'green': '\\033[92m',\n            'yellow': '\\033[93m',\n            'blue': '\\033[94m',\n            'purple': '\\033[95m',\n            'cyan': '\\033[96m',\n            'white': '\\033[97m',\n            'reset': '\\033[0m'\n        }\n        return f\"{colors.get(color, colors['white'])}{text}{colors['reset']}\"\n    \n    def print_header(self):\n        print(self.colored_text(\"=\" * 60, 'cyan'))\n        print(self.colored_text(\"Interactive Command Line Interface\", 'cyan'))\n        print(self.colored_text(f\"Current Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", 'yellow'))\n        print(self.colored_text(\"=\" * 60, 'cyan'))\n    \n    def show_menu(self):\n        menu_items = [\n            \"1. Add Data Entry\",\n            \"2. View All Data\",\n            \"3. Search Data\",\n            \"4. Delete Entry\",\n            \"5. Configuration Settings\",\n            \"6. Real-time Monitor (Toggle)\",\n            \"7. Export Data\",\n            \"8. Help\",\n            \"9. Exit\"\n        ]\n        \n        print(self.colored_text(\"\\nMain Menu:\", 'green'))\n        for item in menu_items:\n            print(f\"  {item}\")\n        print()\n    \n    def get_user_input(self, prompt, input_type=str, default=None):\n        while True:\n            try:\n                if default:\n                    user_input = input(f\"{prompt} (default: {default}): \").strip()\n                    if not user_input:\n                        return default\n                else:\n                    user_input = input(f\"{prompt}: \").strip()\n                \n                if input_type == int:\n                    return int(user_input)\n                elif input_type == float:\n                    return float(user_input)\n                else:\n                    return user_input\n                    \n            except ValueError:\n                print(self.colored_text(f\"Invalid input. Please enter a {input_type.__name__}.\", 'red'))\n            except KeyboardInterrupt:\n                print(self.colored_text(\"\\nOperation cancelled.\", 'yellow'))\n                return None\n    \n    def add_data_entry(self):\n        print(self.colored_text(\"\\n--- Add New Data Entry ---\", 'green'))\n        \n        name = self.get_user_input(\"Enter name\")\n        if not name:\n            return\n            \n        value = self.get_user_input(\"Enter value (numeric)\", int)\n        if value is None:\n            return\n            \n        category = self.get_user_input(\"Enter category\", default=\"general\")\n        \n        entry = {\n            'id': len(self.data_store) + 1,\n            'name': name,\n            'value': value,\n            'category': category,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        self.data_store.append(entry)\n        print(self.colored_text(f\"\u2713 Entry added successfully! (ID: {entry['id']})\", 'green'))\n    \n    def view_all_data(self):\n        if not self.data_store:\n            print(self.colored_text(\"No data entries found.\", 'yellow'))\n            return\n        \n        print(self.colored_text(f\"\\n--- All Data Entries ({len(self.data_store)} total) ---\", 'green'))\n        print(f\"{'ID':<4} {'Name':<15} {'Value':<10} {'Category':<12} {'Timestamp':<20}\")\n        print(\"-\" * 70)\n        \n        for entry in self.data_store:\n            timestamp = entry['timestamp'][:19].replace('T', ' ')\n            print(f\"{entry['id']:<4} {entry['name']:<15} {entry['value']:<10} \"\n                  f\"{entry['category']:<12} {timestamp:<20}\")\n    \n    def search_data(self):\n        if not self.data_store:\n            print(self.colored_text(\"No data to search.\", 'yellow'))\n            return\n        \n        search_term = self.get_user_input(\"Enter search term (name or category)\")\n        if not search_term:\n            return\n        \n        results = [entry for entry in self.data_store \n                  if search_term.lower() in entry['name'].lower() \n                  or search_term.lower() in entry['category'].lower()]\n        \n        if results:\n            print(self.colored_text(f\"\\n--- Search Results ({len(results)} found) ---\", 'green'))\n            for entry in results:\n                print(f\"ID: {entry['id']}, Name: {entry['name']}, \"\n                      f\"Value: {entry['value']}, Category: {entry['category']}\")\n        else:\n            print(self.colored_text(\"No matching entries found.\", 'yellow'))\n    \n    def delete_entry(self):\n        if not self.data_store:\n            print(self.colored_text(\"No data to delete.\", 'yellow'))\n            return\n        \n        entry_id = self.get_user_input(\"Enter ID of entry to delete\", int)\n        if entry_id is None:\n            return\n        \n        entry_to_delete = next((entry for entry in self.data_store \n                               if entry['id'] == entry_id), None)\n        \n        if entry_to_delete:\n            confirm = self.get_user_input(\n                f\"Delete entry '{entry_to_delete['name']}'? (y/n)\", default=\"n\"\n            )\n            if confirm.lower() == 'y':\n                self.data_store.remove(entry_to_delete)\n                print(self.colored_text(\"\u2713 Entry deleted successfully!\", 'green'))\n            else:\n                print(self.colored_text(\"Deletion cancelled.\", 'yellow'))\n        else:\n            print(self.colored_text(\"Entry not found.\", 'red'))\n    \n    def configuration_menu(self):\n        while True:\n            print(self.colored_text(\"\\n--- Configuration Settings ---\", 'blue'))\n            print(f\"1. Auto-refresh: {self.config['auto_refresh']}\")\n            print(f\"2. Refresh interval: {self.config['refresh_interval']} seconds\")\n            print(f\"3. Verbose mode: {self.config['verbose']}\")\n            print(f\"4. Color output: {self.config['color_output']}\")\n            print(\"5. Reset to defaults\")\n            print(\"6. Back to main menu\")\n            \n            choice = self.get_user_input(\"Select option\", int)\n            if choice == 1:\n                self.config['auto_refresh'] = not self.config['auto_refresh']\n            elif choice == 2:\n                interval = self.get_user_input(\"Enter refresh interval (seconds)\", int)\n                if interval and interval > 0:\n                    self.config['refresh_interval'] = interval\n            elif choice == 3:\n                self.config['verbose'] = not self.config['verbose']\n            elif choice == 4:\n                self.config['color_output'] = not self.config['color_output']\n            elif choice == 5:\n                self.config = {\n                    'auto_refresh': False,\n                    'refresh_interval': 2,\n                    'verbose': False,\n                    'color_output': True\n                }\n                print(self.colored_text(\"", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "argparse", "threading", "time", "os", "sys"], "package_breakdown": {"install_packages": ["datetime", "argparse", "threading", "time", "os", "sys"], "import_packages": ["datetime", "argparse", "sys", "threading", "time", "os"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that decorates a function with a backoff and retry mechanism, allowing the function to automatically retry a specified number of times with increasing delays between attempts in case of failures.", "prompt_index": 179, "generated_code": "```python\nimport time\nimport functools\nfrom typing import Callable, Tuple, Type, Union\n\ndef retry_with_backoff(\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    backoff_factor: float = 2.0,\n    exceptions: Union[Type[Exception], Tuple[Type[Exception], ...]] = Exception\n):\n    \"\"\"\n    Decorator that implements retry logic with exponential backoff.\n    \n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay in seconds\n        backoff_factor: Factor by which delay increases after each retry\n        exceptions: Exception types to catch and retry on\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            delay = initial_delay\n            last_exception = None\n            \n            for attempt in range(max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    if attempt == max_retries:\n                        # Last attempt failed, re-raise the exception\n                        raise e\n                    \n                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    delay *= backoff_factor\n            \n            # This should never be reached, but just in case\n            raise last_exception\n        \n        return wrapper\n    return decorator\n\n# Example usage:\n\n# Basic usage with default parameters\n@retry_with_backoff()\ndef unreliable_function():\n    import random\n    if random.random() < 0.7:  # 70% chance of failure\n        raise ConnectionError(\"Network error\")\n    return \"Success!\"\n\n# Custom configuration\n@retry_with_backoff(\n    max_retries=5,\n    initial_delay=0.5,\n    backoff_factor=1.5,\n    exceptions=(ConnectionError, TimeoutError)\n)\ndef api_call():\n    import random\n    if random.random() < 0.8:\n        raise ConnectionError(\"API unavailable\")\n    return {\"status\": \"ok\", \"data\": \"result\"}\n\n# Usage with specific exception types\n@retry_with_backoff(max_retries=2, exceptions=(ValueError, TypeError))\ndef process_data(data):\n    if not isinstance(data, (list, tuple)):\n        raise TypeError(\"Data must be a list or tuple\")\n    if len(data) == 0:\n        raise ValueError(\"Data cannot be empty\")\n    return sum(data)\n\n# Example with context manager for more complex scenarios\nclass RetryContext:\n    def __init__(self, max_retries=3, initial_delay=1.0, backoff_factor=2.0):\n        self.max_retries = max_retries\n        self.initial_delay = initial_delay\n        self.backoff_factor = backoff_factor\n    \n    def __call__(self, func):\n        return retry_with_backoff(\n            self.max_retries,\n            self.initial_delay,\n            self.backoff_factor\n        )(func)\n\n# Usage of context manager\nretry_config = RetryContext(max_retries=4, initial_delay=0.25, backoff_factor=3.0)\n\n@retry_config\ndef database_operation():\n    import random\n    if random.random() < 0.6:\n        raise ConnectionError(\"Database connection failed\")\n    return \"Data retrieved successfully\"\n\n# Test the decorated functions\nif __name__ == \"__main__\":\n    try:\n        result = unreliable_function()\n        print(f\"Result: {result}\")\n    except Exception as e:\n        print(f\"Final failure: {e}\")\n    \n    try:\n        result = api_call()\n        print(f\"API Result: {result}\")\n    except Exception as e:\n        print(f\"API call failed: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "typing", "time", "random"], "package_breakdown": {"install_packages": ["functools", "typing", "time", "random"], "import_packages": ["functools", "time", "typing", "random"], "llm_suggested": ["time", "functools", "typing", "random"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that automatically instruments a Python application with OpenTelemetry for monitoring and tracing purposes.", "prompt_index": 180, "generated_code": "```python\nimport os\nimport logging\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.auto_instrumentation import sitecustomize\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.django import DjangoInstrumentor\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor\nfrom opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\nfrom opentelemetry.instrumentation.redis import RedisInstrumentor\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\nfrom opentelemetry.sdk.resources import Resource, SERVICE_NAME, SERVICE_VERSION\nfrom opentelemetry.propagate import set_global_textmap\nfrom opentelemetry.propagators.b3 import B3MultiFormat\nfrom opentelemetry.propagators.jaeger import JaegerPropagator\nfrom opentelemetry.propagators.composite import CompositeHTTPPropagator\n\nclass OpenTelemetryInstrumentor:\n    def __init__(self, service_name=\"python-app\", service_version=\"1.0.0\"):\n        self.service_name = service_name\n        self.service_version = service_version\n        self.tracer_provider = None\n        self.tracer = None\n        \n    def setup_tracer_provider(self):\n        \"\"\"Initialize the tracer provider with resource information.\"\"\"\n        resource = Resource.create({\n            SERVICE_NAME: self.service_name,\n            SERVICE_VERSION: self.service_version,\n            \"service.instance.id\": os.getenv(\"HOSTNAME\", \"unknown\"),\n            \"deployment.environment\": os.getenv(\"ENVIRONMENT\", \"development\")\n        })\n        \n        self.tracer_provider = TracerProvider(resource=resource)\n        trace.set_tracer_provider(self.tracer_provider)\n        self.tracer = trace.get_tracer(__name__)\n        \n    def setup_exporters(self):\n        \"\"\"Configure span exporters for different backends.\"\"\"\n        # Console exporter for debugging\n        console_exporter = ConsoleSpanExporter()\n        console_processor = BatchSpanProcessor(console_exporter)\n        self.tracer_provider.add_span_processor(console_processor)\n        \n        # Jaeger exporter\n        jaeger_endpoint = os.getenv(\"JAEGER_ENDPOINT\", \"http://localhost:14268/api/traces\")\n        if jaeger_endpoint:\n            jaeger_exporter = JaegerExporter(\n                agent_host_name=os.getenv(\"JAEGER_AGENT_HOST\", \"localhost\"),\n                agent_port=int(os.getenv(\"JAEGER_AGENT_PORT\", \"6831\")),\n                collector_endpoint=jaeger_endpoint,\n            )\n            jaeger_processor = BatchSpanProcessor(jaeger_exporter)\n            self.tracer_provider.add_span_processor(jaeger_processor)\n        \n        # OTLP exporter (for services like Grafana Tempo, DataDog, etc.)\n        otlp_endpoint = os.getenv(\"OTLP_ENDPOINT\")\n        if otlp_endpoint:\n            otlp_exporter = OTLPSpanExporter(\n                endpoint=otlp_endpoint,\n                headers={\"Authorization\": f\"Bearer {os.getenv('OTLP_TOKEN', '')}\"}\n            )\n            otlp_processor = BatchSpanProcessor(otlp_exporter)\n            self.tracer_provider.add_span_processor(otlp_processor)\n    \n    def setup_propagators(self):\n        \"\"\"Configure trace context propagation.\"\"\"\n        set_global_textmap(\n            CompositeHTTPPropagator([\n                B3MultiFormat(),\n                JaegerPropagator(),\n            ])\n        )\n    \n    def instrument_libraries(self):\n        \"\"\"Automatically instrument common Python libraries.\"\"\"\n        # HTTP requests\n        RequestsInstrumentor().instrument()\n        \n        # Web frameworks\n        try:\n            FlaskInstrumentor().instrument()\n        except Exception:\n            pass\n            \n        try:\n            DjangoInstrumentor().instrument()\n        except Exception:\n            pass\n            \n        try:\n            FastAPIInstrumentor().instrument()\n        except Exception:\n            pass\n        \n        # Database libraries\n        try:\n            Psycopg2Instrumentor().instrument()\n        except Exception:\n            pass\n            \n        try:\n            SQLAlchemyInstrumentor().instrument()\n        except Exception:\n            pass\n        \n        # Cache\n        try:\n            RedisInstrumentor().instrument()\n        except Exception:\n            pass\n    \n    def initialize(self):\n        \"\"\"Initialize complete OpenTelemetry instrumentation.\"\"\"\n        self.setup_tracer_provider()\n        self.setup_exporters()\n        self.setup_propagators()\n        self.instrument_libraries()\n        \n        logging.info(f\"OpenTelemetry instrumentation initialized for service: {self.service_name}\")\n        return self.tracer\n\n# Decorator for manual instrumentation\ndef trace_function(span_name=None):\n    \"\"\"Decorator to trace function calls.\"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            tracer = trace.get_tracer(__name__)\n            name = span_name or f\"{func.__module__}.{func.__name__}\"\n            \n            with tracer.start_as_current_span(name) as span:\n                # Add function metadata\n                span.set_attribute(\"function.name\", func.__name__)\n                span.set_attribute(\"function.module\", func.__module__)\n                \n                try:\n                    result = func(*args, **kwargs)\n                    span.set_attribute(\"function.result\", str(type(result).__name__))\n                    return result\n                except Exception as e:\n                    span.record_exception(e)\n                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n                    raise\n        return wrapper\n    return decorator\n\n# Context manager for manual spans\nclass TracingContext:\n    def __init__(self, span_name, attributes=None):\n        self.span_name = span_name\n        self.attributes = attributes or {}\n        self.tracer = trace.get_tracer(__name__)\n        self.span = None\n    \n    def __enter__(self):\n        self.span = self.tracer.start_span(self.span_name)\n        for key, value in self.attributes.items():\n            self.span.set_attribute(key, value)\n        return self.span\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            self.span.record_exception(exc_val)\n            self.span.set_status(trace.Status(trace.StatusCode.ERROR, str(exc_val)))\n        self.span.end()\n\n# Auto-initialization script\ndef auto_instrument():\n    \"\"\"Automatically instrument the application on import.\"\"\"\n    service_name = os.getenv(\"OTEL_SERVICE_NAME\", \"python-application\")\n    service_version = os.getenv(\"OTEL_SERVICE_VERSION\", \"1.0.0\")\n    \n    instrumentor = OpenTelemetryInstrumentor(service_name, service_version)\n    return instrumentor.initialize()\n\n# Example usage in application code\nif __name__ == \"__main__\":\n    # Initialize instrumentation\n    tracer = auto_instrument()\n    \n    # Example traced function\n    @trace_function(\"example_operation\")\n    def example_function(data):\n        # Simulate some work\n        import time\n        time.sleep(0.1)\n        return f\"Processed {data}\"\n    \n    # Example manual tracing\n    with TracingContext(\"manual_operation\", {\"operation.type\": \"data_processing\"}):\n        result = example_function(\"", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 18, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-sdk", "opentelemetry-instrumentation-psycopg2", "opentelemetry-instrumentation-redis", "time", "opentelemetry-instrumentation", "opentelemetry-instrumentation-requests", "opentelemetry-api", "opentelemetry-instrumentation-django", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-propagator-b3", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-fastapi", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-sqlalchemy", "logging", "os", "opentelemetry-propagator-jaeger"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-sdk", "opentelemetry-instrumentation-psycopg2", "opentelemetry-instrumentation-redis", "time", "opentelemetry-instrumentation", "opentelemetry-instrumentation-requests", "opentelemetry-api", "opentelemetry-instrumentation-django", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-propagator-b3", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-fastapi", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-sqlalchemy", "logging", "os", "opentelemetry-propagator-jaeger"], "import_packages": ["logging", "time", "opentelemetry", "os"], "llm_suggested": ["opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-instrumentation", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-django", "opentelemetry-instrumentation-fastapi", "opentelemetry-instrumentation-psycopg2", "opentelemetry-instrumentation-sqlalchemy", "opentelemetry-instrumentation-redis", "opentelemetry-propagator-b3", "opentelemetry-propagator-jaeger"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that extracts metadata information such as font name, style, and character set from a given font file.", "prompt_index": 181, "generated_code": "```python\nfrom fontTools.ttLib import TTFont\nimport os\n\ndef extract_font_metadata(font_path):\n    \"\"\"\n    Extract metadata information from a font file.\n    \n    Args:\n        font_path (str): Path to the font file\n    \n    Returns:\n        dict: Dictionary containing font metadata\n    \"\"\"\n    try:\n        font = TTFont(font_path)\n        metadata = {}\n        \n        # Get name table\n        name_table = font['name']\n        \n        # Extract font name and style information\n        for record in name_table.names:\n            if record.nameID == 1:  # Font Family name\n                metadata['font_family'] = record.toUnicode()\n            elif record.nameID == 2:  # Font Subfamily name (style)\n                metadata['font_style'] = record.toUnicode()\n            elif record.nameID == 4:  # Full font name\n                metadata['full_name'] = record.toUnicode()\n            elif record.nameID == 6:  # PostScript name\n                metadata['postscript_name'] = record.toUnicode()\n            elif record.nameID == 16:  # Typographic Family name\n                metadata['typographic_family'] = record.toUnicode()\n            elif record.nameID == 17:  # Typographic Subfamily name\n                metadata['typographic_style'] = record.toUnicode()\n        \n        # Get character set information\n        if 'cmap' in font:\n            cmap = font['cmap']\n            unicode_chars = set()\n            \n            for table in cmap.tables:\n                if table.isUnicode():\n                    unicode_chars.update(table.cmap.keys())\n            \n            metadata['character_count'] = len(unicode_chars)\n            metadata['unicode_ranges'] = get_unicode_ranges(unicode_chars)\n        \n        # Get font format and version\n        metadata['font_format'] = 'TrueType' if 'glyf' in font else 'OpenType'\n        \n        if 'head' in font:\n            head_table = font['head']\n            metadata['font_version'] = f\"{head_table.fontRevision:.3f}\"\n            metadata['created'] = head_table.created\n            metadata['modified'] = head_table.modified\n        \n        # Get OS/2 table information\n        if 'OS/2' in font:\n            os2_table = font['OS/2']\n            metadata['weight_class'] = os2_table.usWeightClass\n            metadata['width_class'] = os2_table.usWidthClass\n            \n            # Unicode ranges from OS/2 table\n            metadata['unicode_range_bits'] = {\n                'ulUnicodeRange1': bin(os2_table.ulUnicodeRange1),\n                'ulUnicodeRange2': bin(os2_table.ulUnicodeRange2),\n                'ulUnicodeRange3': bin(os2_table.ulUnicodeRange3),\n                'ulUnicodeRange4': bin(os2_table.ulUnicodeRange4)\n            }\n        \n        # Get glyph count\n        if 'maxp' in font:\n            metadata['glyph_count'] = font['maxp'].numGlyphs\n        \n        font.close()\n        return metadata\n        \n    except Exception as e:\n        return {'error': f\"Failed to extract metadata: {str(e)}\"}\n\ndef get_unicode_ranges(unicode_chars):\n    \"\"\"\n    Identify major Unicode ranges present in the character set.\n    \n    Args:\n        unicode_chars (set): Set of Unicode code points\n    \n    Returns:\n        list: List of Unicode ranges present\n    \"\"\"\n    ranges = []\n    \n    unicode_blocks = {\n        'Basic Latin': (0x0000, 0x007F),\n        'Latin-1 Supplement': (0x0080, 0x00FF),\n        'Latin Extended-A': (0x0100, 0x017F),\n        'Latin Extended-B': (0x0180, 0x024F),\n        'Greek and Coptic': (0x0370, 0x03FF),\n        'Cyrillic': (0x0400, 0x04FF),\n        'Arabic': (0x0600, 0x06FF),\n        'Hebrew': (0x0590, 0x05FF),\n        'CJK Unified Ideographs': (0x4E00, 0x9FFF),\n        'Hiragana': (0x3040, 0x309F),\n        'Katakana': (0x30A0, 0x30FF),\n    }\n    \n    for block_name, (start, end) in unicode_blocks.items():\n        if any(start <= char <= end for char in unicode_chars):\n            ranges.append(block_name)\n    \n    return ranges\n\ndef print_font_metadata(metadata):\n    \"\"\"\n    Pretty print font metadata.\n    \n    Args:\n        metadata (dict): Font metadata dictionary\n    \"\"\"\n    if 'error' in metadata:\n        print(f\"Error: {metadata['error']}\")\n        return\n    \n    print(\"Font Metadata:\")\n    print(\"-\" * 40)\n    \n    for key, value in metadata.items():\n        if key == 'unicode_range_bits':\n            print(f\"{key.replace('_', ' ').title()}:\")\n            for sub_key, sub_value in value.items():\n                print(f\"  {sub_key}: {sub_value}\")\n        elif isinstance(value, list):\n            print(f\"{key.replace('_', ' ').title()}: {', '.join(value)}\")\n        else:\n            print(f\"{key.replace('_', ' ').title()}: {value}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example font file path (replace with actual font file)\n    font_file = \"path/to/your/font.ttf\"  # or .otf\n    \n    if os.path.exists(font_file):\n        metadata = extract_font_metadata(font_file)\n        print_font_metadata(metadata)\n    else:\n        print(\"Font file not found. Please provide a valid font file path.\")\n        \n        # Alternative: Process all fonts in a directory\n        font_directory = \"path/to/fonts/directory\"\n        if os.path.exists(font_directory):\n            print(f\"\\nProcessing fonts in {font_directory}:\")\n            for filename in os.listdir(font_directory):\n                if filename.lower().endswith(('.ttf', '.otf')):\n                    print(f\"\\n{'='*50}\")\n                    print(f\"Font: {filename}\")\n                    print('='*50)\n                    font_path = os.path.join(font_directory, filename)\n                    metadata = extract_font_metadata(font_path)\n                    print_font_metadata(metadata)\n```\n\nThis code requires the `fonttools` library. Install it with:\n```bash\npip install fonttools\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["fonttools", "fontTools", "os"], "package_breakdown": {"install_packages": ["fonttools", "fontTools", "os"], "import_packages": ["fontTools", "os"], "llm_suggested": ["fonttools"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates a user with Google's OAuth 2.0 authentication system and retrieves a user's profile information.", "prompt_index": 182, "generated_code": "```python\nimport requests\nfrom flask import Flask, request, redirect, session, jsonify\nimport secrets\nimport urllib.parse\n\napp = Flask(__name__)\napp.secret_key = secrets.token_hex(16)\n\n# Google OAuth 2.0 configuration\nGOOGLE_CLIENT_ID = 'your_google_client_id'\nGOOGLE_CLIENT_SECRET = 'your_google_client_secret'\nGOOGLE_REDIRECT_URI = 'http://localhost:5000/callback'\nGOOGLE_AUTH_URL = 'https://accounts.google.com/o/oauth2/auth'\nGOOGLE_TOKEN_URL = 'https://oauth2.googleapis.com/token'\nGOOGLE_USERINFO_URL = 'https://www.googleapis.com/oauth2/v2/userinfo'\n\n@app.route('/')\ndef index():\n    return '<a href=\"/login\">Login with Google</a>'\n\n@app.route('/login')\ndef login():\n    # Generate state parameter for security\n    state = secrets.token_urlsafe(32)\n    session['state'] = state\n    \n    # Build authorization URL\n    params = {\n        'client_id': GOOGLE_CLIENT_ID,\n        'redirect_uri': GOOGLE_REDIRECT_URI,\n        'scope': 'openid email profile',\n        'response_type': 'code',\n        'state': state\n    }\n    \n    auth_url = f\"{GOOGLE_AUTH_URL}?{urllib.parse.urlencode(params)}\"\n    return redirect(auth_url)\n\n@app.route('/callback')\ndef callback():\n    # Verify state parameter\n    if request.args.get('state') != session.get('state'):\n        return 'Invalid state parameter', 400\n    \n    # Get authorization code\n    code = request.args.get('code')\n    if not code:\n        return 'Authorization code not found', 400\n    \n    # Exchange code for access token\n    token_data = {\n        'client_id': GOOGLE_CLIENT_ID,\n        'client_secret': GOOGLE_CLIENT_SECRET,\n        'code': code,\n        'grant_type': 'authorization_code',\n        'redirect_uri': GOOGLE_REDIRECT_URI\n    }\n    \n    token_response = requests.post(GOOGLE_TOKEN_URL, data=token_data)\n    token_json = token_response.json()\n    \n    if 'access_token' not in token_json:\n        return 'Failed to obtain access token', 400\n    \n    access_token = token_json['access_token']\n    \n    # Get user profile information\n    headers = {'Authorization': f'Bearer {access_token}'}\n    profile_response = requests.get(GOOGLE_USERINFO_URL, headers=headers)\n    profile_data = profile_response.json()\n    \n    # Store user info in session\n    session['user'] = profile_data\n    \n    return jsonify(profile_data)\n\n@app.route('/profile')\ndef profile():\n    if 'user' not in session:\n        return redirect('/login')\n    \n    return jsonify(session['user'])\n\n@app.route('/logout')\ndef logout():\n    session.pop('user', None)\n    session.pop('state', None)\n    return redirect('/')\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)\n```\n\n```python\n# Alternative implementation using google-auth-oauthlib library\nfrom google_auth_oauthlib.flow import Flow\nfrom google.auth.transport.requests import Request\nimport google.auth.transport.requests\nimport requests\nimport json\n\nclass GoogleOAuthClient:\n    def __init__(self, client_id, client_secret, redirect_uri):\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.redirect_uri = redirect_uri\n        self.flow = None\n        \n    def get_authorization_url(self):\n        \"\"\"Generate authorization URL for Google OAuth\"\"\"\n        self.flow = Flow.from_client_config(\n            {\n                \"web\": {\n                    \"client_id\": self.client_id,\n                    \"client_secret\": self.client_secret,\n                    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n                    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n                    \"redirect_uris\": [self.redirect_uri]\n                }\n            },\n            scopes=['openid', 'email', 'profile']\n        )\n        \n        self.flow.redirect_uri = self.redirect_uri\n        authorization_url, state = self.flow.authorization_url(\n            access_type='offline',\n            include_granted_scopes='true'\n        )\n        \n        return authorization_url, state\n    \n    def exchange_code_for_token(self, authorization_response):\n        \"\"\"Exchange authorization code for access token\"\"\"\n        if not self.flow:\n            raise ValueError(\"Must call get_authorization_url first\")\n            \n        self.flow.fetch_token(authorization_response=authorization_response)\n        return self.flow.credentials\n    \n    def get_user_profile(self, credentials):\n        \"\"\"Get user profile information using access token\"\"\"\n        request_session = requests.Session()\n        request_adapter = google.auth.transport.requests.Request(request_session)\n        \n        # Refresh token if needed\n        credentials.refresh(request_adapter)\n        \n        # Get user info\n        headers = {'Authorization': f'Bearer {credentials.token}'}\n        response = requests.get(\n            'https://www.googleapis.com/oauth2/v2/userinfo',\n            headers=headers\n        )\n        \n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(f\"Failed to get user info: {response.status_code}\")\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize OAuth client\n    oauth_client = GoogleOAuthClient(\n        client_id='your_google_client_id',\n        client_secret='your_google_client_secret',\n        redirect_uri='http://localhost:5000/callback'\n    )\n    \n    # Step 1: Get authorization URL\n    auth_url, state = oauth_client.get_authorization_url()\n    print(f\"Visit this URL to authorize the application: {auth_url}\")\n    \n    # Step 2: After user authorization, get the full callback URL\n    # This would typically come from your web server's callback handler\n    authorization_response = input(\"Enter the full callback URL: \")\n    \n    # Step 3: Exchange code for credentials\n    credentials = oauth_client.exchange_code_for_token(authorization_response)\n    \n    # Step 4: Get user profile\n    profile = oauth_client.get_user_profile(credentials)\n    print(json.dumps(profile, indent=2))\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google", "google-auth-oauthlib", "flask", "google_auth_oauthlib", "urllib", "secrets", "json", "requests"], "package_breakdown": {"install_packages": ["google", "google-auth-oauthlib", "flask", "google_auth_oauthlib", "urllib", "secrets", "json", "requests"], "import_packages": ["google", "flask", "google_auth_oauthlib", "urllib", "secrets", "json", "requests"], "llm_suggested": ["requests", "flask", "google-auth-oauthlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that establishes a WebSocket connection with a server using RFC 6455 & 7692 protocols and sends and receives messages over the connection.", "prompt_index": 183, "generated_code": "```python\nimport websocket\nimport threading\nimport time\nimport json\n\nclass WebSocketClient:\n    def __init__(self, url):\n        self.url = url\n        self.ws = None\n        self.connected = False\n    \n    def on_message(self, ws, message):\n        \"\"\"Handle incoming messages\"\"\"\n        print(f\"Received: {message}\")\n        \n    def on_error(self, ws, error):\n        \"\"\"Handle connection errors\"\"\"\n        print(f\"Error: {error}\")\n        \n    def on_close(self, ws, close_status_code, close_msg):\n        \"\"\"Handle connection close\"\"\"\n        print(\"Connection closed\")\n        self.connected = False\n        \n    def on_open(self, ws):\n        \"\"\"Handle connection open\"\"\"\n        print(\"Connection established\")\n        self.connected = True\n        \n    def connect(self):\n        \"\"\"Establish WebSocket connection with compression support (RFC 7692)\"\"\"\n        # Enable per-message deflate compression (RFC 7692)\n        self.ws = websocket.WebSocketApp(\n            self.url,\n            on_open=self.on_open,\n            on_message=self.on_message,\n            on_error=self.on_error,\n            on_close=self.on_close,\n            # Enable compression extensions\n            header=[\"Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits\"]\n        )\n        \n        # Start connection in separate thread\n        self.ws_thread = threading.Thread(target=self.ws.run_forever)\n        self.ws_thread.daemon = True\n        self.ws_thread.start()\n        \n        # Wait for connection\n        timeout = 10\n        while not self.connected and timeout > 0:\n            time.sleep(0.1)\n            timeout -= 0.1\n            \n        if not self.connected:\n            raise Exception(\"Failed to establish WebSocket connection\")\n    \n    def send_message(self, message):\n        \"\"\"Send message to server\"\"\"\n        if self.connected and self.ws:\n            if isinstance(message, dict):\n                message = json.dumps(message)\n            self.ws.send(message)\n            print(f\"Sent: {message}\")\n        else:\n            print(\"Not connected to server\")\n    \n    def send_binary(self, data):\n        \"\"\"Send binary data to server\"\"\"\n        if self.connected and self.ws:\n            self.ws.send(data, websocket.ABNF.OPCODE_BINARY)\n            print(f\"Sent binary data: {len(data)} bytes\")\n        else:\n            print(\"Not connected to server\")\n    \n    def close(self):\n        \"\"\"Close WebSocket connection\"\"\"\n        if self.ws:\n            self.ws.close()\n\n# Example usage\ndef main():\n    # WebSocket server URL\n    url = \"wss://echo.websocket.org\"  # Test echo server\n    \n    try:\n        # Create WebSocket client\n        client = WebSocketClient(url)\n        \n        # Connect to server\n        print(\"Connecting to WebSocket server...\")\n        client.connect()\n        \n        # Send text messages\n        client.send_message(\"Hello WebSocket!\")\n        client.send_message({\"type\": \"greeting\", \"data\": \"Hello from Python\"})\n        \n        # Send binary data\n        binary_data = b\"Binary message content\"\n        client.send_binary(binary_data)\n        \n        # Keep connection alive for a few seconds\n        time.sleep(5)\n        \n        # Send more messages\n        for i in range(3):\n            client.send_message(f\"Message {i+1}\")\n            time.sleep(1)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        # Close connection\n        client.close()\n        print(\"WebSocket connection closed\")\n\n# Alternative implementation using websockets library (async)\nimport asyncio\nimport websockets\n\nasync def websocket_client_async():\n    \"\"\"Async WebSocket client implementation\"\"\"\n    uri = \"wss://echo.websocket.org\"\n    \n    # Connect with compression support\n    async with websockets.connect(\n        uri, \n        compression=\"deflate\",  # Enable RFC 7692 compression\n        max_size=None,\n        ping_interval=20,\n        ping_timeout=10\n    ) as websocket:\n        print(\"Connected to WebSocket server (async)\")\n        \n        # Send messages\n        await websocket.send(\"Hello async WebSocket!\")\n        response = await websocket.recv()\n        print(f\"Received: {response}\")\n        \n        # Send JSON message\n        json_message = json.dumps({\"type\": \"test\", \"value\": 42})\n        await websocket.send(json_message)\n        response = await websocket.recv()\n        print(f\"Received: {response}\")\n        \n        # Send binary data\n        binary_data = b\"Binary async message\"\n        await websocket.send(binary_data)\n        response = await websocket.recv()\n        print(f\"Received binary: {response}\")\n\n# Run async example\nasync def run_async_example():\n    try:\n        await websocket_client_async()\n    except Exception as e:\n        print(f\"Async error: {e}\")\n\nif __name__ == \"__main__\":\n    print(\"=== Synchronous WebSocket Client ===\")\n    main()\n    \n    print(\"\\n=== Asynchronous WebSocket Client ===\")\n    asyncio.run(run_async_example())\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["threading", "time", "websockets", "websocket", "websocket-client", "json", "asyncio"], "package_breakdown": {"install_packages": ["threading", "time", "websockets", "websocket", "websocket-client", "json", "asyncio"], "import_packages": ["websockets", "asyncio", "threading", "time", "websocket", "json"], "llm_suggested": ["websocket-client", "websockets"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses an XML file using the standard library modules while implementing protection against XML bombs to prevent denial of service attacks.", "prompt_index": 184, "generated_code": "```python\nimport xml.etree.ElementTree as ET\nfrom xml.parsers.expat import ParserCreateNS\nimport xml.parsers.expat as expat\n\nclass SecureXMLParser:\n    def __init__(self, max_entity_expansions=1000, max_entity_depth=20, max_children=10000):\n        self.max_entity_expansions = max_entity_expansions\n        self.max_entity_depth = max_entity_depth\n        self.max_children = max_children\n        self.entity_expansions = 0\n        self.entity_depth = 0\n        self.children_count = 0\n        \n    def parse_file(self, xml_file_path):\n        \"\"\"Parse XML file with protection against XML bombs\"\"\"\n        try:\n            # Create parser with security limits\n            parser = ET.XMLParser()\n            \n            # Disable external entity processing\n            parser.parser.DefaultHandler = self._default_handler\n            parser.parser.ExternalEntityRefHandler = self._external_entity_handler\n            parser.parser.EntityDeclHandler = self._entity_decl_handler\n            parser.parser.StartElementHandler = self._start_element_handler\n            \n            # Parse with size limit\n            with open(xml_file_path, 'rb') as f:\n                content = f.read(10 * 1024 * 1024)  # 10MB limit\n                if len(content) >= 10 * 1024 * 1024:\n                    raise ValueError(\"XML file too large\")\n                \n            tree = ET.parse(xml_file_path, parser)\n            return tree.getroot()\n            \n        except ET.ParseError as e:\n            raise ValueError(f\"XML parsing error: {e}\")\n    \n    def parse_string(self, xml_string):\n        \"\"\"Parse XML string with protection against XML bombs\"\"\"\n        if len(xml_string) > 10 * 1024 * 1024:  # 10MB limit\n            raise ValueError(\"XML string too large\")\n            \n        try:\n            parser = ET.XMLParser()\n            parser.parser.DefaultHandler = self._default_handler\n            parser.parser.ExternalEntityRefHandler = self._external_entity_handler\n            parser.parser.EntityDeclHandler = self._entity_decl_handler\n            parser.parser.StartElementHandler = self._start_element_handler\n            \n            root = ET.fromstring(xml_string, parser)\n            return root\n            \n        except ET.ParseError as e:\n            raise ValueError(f\"XML parsing error: {e}\")\n    \n    def _default_handler(self, data):\n        \"\"\"Handle default parsing events\"\"\"\n        pass\n    \n    def _external_entity_handler(self, context, base, sysId, notationName):\n        \"\"\"Block external entity references\"\"\"\n        raise ValueError(\"External entity references are not allowed\")\n    \n    def _entity_decl_handler(self, entityName, is_parameter_entity, value, base, systemId, publicId, notationName):\n        \"\"\"Monitor entity declarations\"\"\"\n        self.entity_expansions += 1\n        if self.entity_expansions > self.max_entity_expansions:\n            raise ValueError(\"Too many entity expansions detected\")\n        return False\n    \n    def _start_element_handler(self, name, attrs):\n        \"\"\"Monitor element creation\"\"\"\n        self.children_count += 1\n        if self.children_count > self.max_children:\n            raise ValueError(\"Too many child elements detected\")\n\n\n# Alternative implementation using defusedxml-like approach with standard library\nclass DefensiveXMLParser:\n    @staticmethod\n    def safe_parse_file(xml_file_path):\n        \"\"\"Safe XML parsing with built-in protections\"\"\"\n        # Check file size first\n        import os\n        if os.path.getsize(xml_file_path) > 10 * 1024 * 1024:\n            raise ValueError(\"XML file too large\")\n        \n        # Use XMLParser with custom settings\n        parser = ET.XMLParser()\n        \n        # Disable DTD processing and external entities\n        parser.parser.UseForeignDTD(False)\n        parser.parser.SetParamEntityParsing(expat.XML_PARAM_ENTITY_PARSING_NEVER)\n        \n        try:\n            tree = ET.parse(xml_file_path, parser)\n            root = tree.getroot()\n            \n            # Additional validation\n            DefensiveXMLParser._validate_tree(root)\n            return root\n            \n        except ET.ParseError as e:\n            raise ValueError(f\"XML parsing failed: {e}\")\n    \n    @staticmethod\n    def safe_parse_string(xml_string):\n        \"\"\"Safe XML string parsing\"\"\"\n        if len(xml_string) > 10 * 1024 * 1024:\n            raise ValueError(\"XML string too large\")\n        \n        parser = ET.XMLParser()\n        parser.parser.UseForeignDTD(False)\n        parser.parser.SetParamEntityParsing(expat.XML_PARAM_ENTITY_PARSING_NEVER)\n        \n        try:\n            root = ET.fromstring(xml_string, parser)\n            DefensiveXMLParser._validate_tree(root)\n            return root\n            \n        except ET.ParseError as e:\n            raise ValueError(f\"XML parsing failed: {e}\")\n    \n    @staticmethod\n    def _validate_tree(element, max_depth=100, current_depth=0):\n        \"\"\"Validate parsed XML tree for suspicious patterns\"\"\"\n        if current_depth > max_depth:\n            raise ValueError(\"XML tree too deep\")\n        \n        children_count = 0\n        for child in element:\n            children_count += 1\n            if children_count > 10000:\n                raise ValueError(\"Too many child elements\")\n            DefensiveXMLParser._validate_tree(child, max_depth, current_depth + 1)\n\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Using SecureXMLParser\n    secure_parser = SecureXMLParser()\n    \n    # Parse file safely\n    try:\n        root = secure_parser.parse_file(\"example.xml\")\n        print(f\"Root element: {root.tag}\")\n        \n        # Process elements safely\n        for child in root:\n            print(f\"Child: {child.tag} = {child.text}\")\n            \n    except ValueError as e:\n        print(f\"Security violation: {e}\")\n    except FileNotFoundError:\n        print(\"XML file not found\")\n    \n    # Parse string safely\n    xml_data = \"\"\"<?xml version=\"1.0\"?>\n    <root>\n        <item>value1</item>\n        <item>value2</item>\n    </root>\"\"\"\n    \n    try:\n        root = secure_parser.parse_string(xml_data)\n        print(f\"Parsed root: {root.tag}\")\n    except ValueError as e:\n        print(f\"Security violation: {e}\")\n    \n    # Using DefensiveXMLParser\n    try:\n        root = DefensiveXMLParser.safe_parse_string(xml_data)\n        print(f\"Safe parsed root: {root.tag}\")\n    except ValueError as e:\n        print(f\"Security violation: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 2, "hallucinations": ["xml", "xml.etree.ElementTree"], "packages_detected": ["xml", "xml.etree.ElementTree", "xml.parsers.expat", "os"], "package_breakdown": {"install_packages": ["xml", "xml.etree.ElementTree", "xml.parsers.expat", "os"], "import_packages": ["xml", "os"], "llm_suggested": ["xml.etree.ElementTree", "xml.parsers.expat", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves the version of the operating system currently running on the system using the Distro package's OS platform information API.", "prompt_index": 185, "generated_code": "```python\nimport distro\n\n# Get the OS version\nos_version = distro.version()\nprint(f\"OS Version: {os_version}\")\n\n# Get more detailed OS information\nos_name = distro.name()\nos_id = distro.id()\nos_version_full = distro.version(pretty=True)\n\nprint(f\"OS Name: {os_name}\")\nprint(f\"OS ID: {os_id}\")\nprint(f\"OS Version (Full): {os_version_full}\")\n\n# Get complete OS info in one call\nos_info = distro.info()\nprint(f\"Complete OS Info: {os_info}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["distro"], "package_breakdown": {"install_packages": ["distro"], "import_packages": ["distro"], "llm_suggested": ["distro"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the Cassowary constraint solver to dynamically adjust the layout of user interface elements based on specified constraints, such as maintaining a fixed distance between two elements or ensuring certain elements remain aligned.", "prompt_index": 186, "generated_code": "```python\nimport cassowary\nfrom cassowary import Variable, Constraint, STRONG, MEDIUM, WEAK\n\nclass UIElement:\n    def __init__(self, name):\n        self.name = name\n        self.x = Variable(f\"{name}_x\")\n        self.y = Variable(f\"{name}_y\")\n        self.width = Variable(f\"{name}_width\")\n        self.height = Variable(f\"{name}_height\")\n\nclass UILayoutSolver:\n    def __init__(self):\n        self.solver = cassowary.SimplexSolver()\n        self.elements = {}\n        \n    def add_element(self, element):\n        self.elements[element.name] = element\n        \n    def add_constraint(self, constraint):\n        self.solver.add_constraint(constraint)\n        \n    def solve(self):\n        return self.solver.solve()\n        \n    def get_element_bounds(self, element_name):\n        element = self.elements[element_name]\n        return {\n            'x': element.x.value,\n            'y': element.y.value,\n            'width': element.width.value,\n            'height': element.height.value\n        }\n\n# Example usage\ndef create_dynamic_layout():\n    # Create layout solver\n    layout = UILayoutSolver()\n    \n    # Create UI elements\n    button1 = UIElement(\"button1\")\n    button2 = UIElement(\"button2\")\n    label = UIElement(\"label\")\n    container = UIElement(\"container\")\n    \n    # Add elements to solver\n    layout.add_element(button1)\n    layout.add_element(button2)\n    layout.add_element(label)\n    layout.add_element(container)\n    \n    # Container constraints\n    layout.add_constraint(Constraint(container.x == 0, STRONG))\n    layout.add_constraint(Constraint(container.y == 0, STRONG))\n    layout.add_constraint(Constraint(container.width == 400, STRONG))\n    layout.add_constraint(Constraint(container.height == 300, STRONG))\n    \n    # Button dimensions\n    layout.add_constraint(Constraint(button1.width == 100, STRONG))\n    layout.add_constraint(Constraint(button1.height == 30, STRONG))\n    layout.add_constraint(Constraint(button2.width == 100, STRONG))\n    layout.add_constraint(Constraint(button2.height == 30, STRONG))\n    \n    # Label dimensions\n    layout.add_constraint(Constraint(label.width == 200, STRONG))\n    layout.add_constraint(Constraint(label.height == 20, STRONG))\n    \n    # Fixed distance between buttons (20 pixels)\n    layout.add_constraint(Constraint(button2.x - (button1.x + button1.width) == 20, STRONG))\n    \n    # Vertical alignment - buttons on same y level\n    layout.add_constraint(Constraint(button1.y == button2.y, STRONG))\n    \n    # Horizontal alignment - center buttons in container\n    buttons_total_width = button1.width + 20 + button2.width  # including gap\n    layout.add_constraint(Constraint(\n        button1.x == (container.width - buttons_total_width) / 2, MEDIUM\n    ))\n    \n    # Position label above buttons with 30px gap\n    layout.add_constraint(Constraint(label.y + label.height + 30 == button1.y, STRONG))\n    \n    # Center label horizontally\n    layout.add_constraint(Constraint(\n        label.x == (container.width - label.width) / 2, MEDIUM\n    ))\n    \n    # Keep elements within container bounds\n    layout.add_constraint(Constraint(button1.x >= container.x + 10, STRONG))\n    layout.add_constraint(Constraint(button2.x + button2.width <= container.x + container.width - 10, STRONG))\n    layout.add_constraint(Constraint(label.x >= container.x + 10, STRONG))\n    layout.add_constraint(Constraint(label.x + label.width <= container.x + container.width - 10, STRONG))\n    \n    # Vertical positioning within container\n    layout.add_constraint(Constraint(label.y >= container.y + 20, STRONG))\n    layout.add_constraint(Constraint(button1.y + button1.height <= container.y + container.height - 20, STRONG))\n    \n    # Solve the constraints\n    if layout.solve():\n        print(\"Layout solved successfully!\")\n        print(f\"Button1 bounds: {layout.get_element_bounds('button1')}\")\n        print(f\"Button2 bounds: {layout.get_element_bounds('button2')}\")\n        print(f\"Label bounds: {layout.get_element_bounds('label')}\")\n        print(f\"Container bounds: {layout.get_element_bounds('container')}\")\n    else:\n        print(\"Failed to solve layout constraints\")\n    \n    return layout\n\n# Advanced example with dynamic constraint modification\ndef dynamic_layout_example():\n    layout = UILayoutSolver()\n    \n    # Create elements\n    sidebar = UIElement(\"sidebar\")\n    content = UIElement(\"content\")\n    footer = UIElement(\"footer\")\n    window = UIElement(\"window\")\n    \n    layout.add_element(sidebar)\n    layout.add_element(content)\n    layout.add_element(footer)\n    layout.add_element(window)\n    \n    # Window constraints\n    layout.add_constraint(Constraint(window.x == 0, STRONG))\n    layout.add_constraint(Constraint(window.y == 0, STRONG))\n    layout.add_constraint(Constraint(window.width == 800, STRONG))\n    layout.add_constraint(Constraint(window.height == 600, STRONG))\n    \n    # Sidebar constraints\n    layout.add_constraint(Constraint(sidebar.x == window.x, STRONG))\n    layout.add_constraint(Constraint(sidebar.y == window.y, STRONG))\n    layout.add_constraint(Constraint(sidebar.width == 200, STRONG))\n    layout.add_constraint(Constraint(sidebar.height == window.height - 60, STRONG))  # Leave space for footer\n    \n    # Content area constraints\n    layout.add_constraint(Constraint(content.x == sidebar.x + sidebar.width, STRONG))\n    layout.add_constraint(Constraint(content.y == window.y, STRONG))\n    layout.add_constraint(Constraint(content.width == window.width - sidebar.width, STRONG))\n    layout.add_constraint(Constraint(content.height == window.height - 60, STRONG))\n    \n    # Footer constraints\n    layout.add_constraint(Constraint(footer.x == window.x, STRONG))\n    layout.add_constraint(Constraint(footer.y == window.height - 60, STRONG))\n    layout.add_constraint(Constraint(footer.width == window.width, STRONG))\n    layout.add_constraint(Constraint(footer.height == 60, STRONG))\n    \n    # Solve initial layout\n    if layout.solve():\n        print(\"Initial layout:\")\n        print(f\"Sidebar: {layout.get_element_bounds('sidebar')}\")\n        print(f\"Content: {layout.get_element_bounds('content')}\")\n        print(f\"Footer: {layout.get_element_bounds('footer')}\")\n        \n        # Simulate window resize\n        print(\"\\nResizing window to 1000x700...\")\n        \n        # Remove old window size constraints and add new ones\n        layout.solver.remove_constraint(layout.solver.constraints[-4])  # width constraint\n        layout.solver.remove_constraint(layout.solver.constraints[-3])  # height constraint\n        \n        layout.add_constraint(Constraint(window.width == 1000, STRONG))\n        layout.add_constraint(Constraint(window.height == 700, STRONG))\n        \n        # Re-solve with new constraints\n        if layout.solve():\n            print(\"Resized layout:\")\n            print(f\"Sidebar: {layout.get_element_bounds('sidebar')}\")\n            print(f\"Content: {layout.get_element_bounds('content')}\")\n            print(f\"Footer: {layout.get_element_bounds('footer')}\")\n\n# Example with flexible", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cassowary"], "package_breakdown": {"install_packages": ["cassowary"], "import_packages": ["cassowary"], "llm_suggested": ["cassowary"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads a resource file from a specified Python package and outputs its content to the console.", "prompt_index": 187, "generated_code": "```python\nimport pkgutil\n\ndef read_package_resource(package_name, resource_path):\n    \"\"\"Read a resource file from a Python package and output to console\"\"\"\n    try:\n        # Get the resource data as bytes\n        data = pkgutil.get_data(package_name, resource_path)\n        \n        if data is None:\n            print(f\"Resource '{resource_path}' not found in package '{package_name}'\")\n            return\n        \n        # Decode and print the content\n        content = data.decode('utf-8')\n        print(content)\n        \n    except Exception as e:\n        print(f\"Error reading resource: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your package name and resource path\n    package_name = \"your_package\"\n    resource_path = \"data/example.txt\"\n    \n    read_package_resource(package_name, resource_path)\n```\n\nAlternative using `importlib.resources` (Python 3.9+):\n\n```python\nfrom importlib import resources\n\ndef read_package_resource_modern(package_name, resource_name):\n    \"\"\"Read a resource file using importlib.resources (Python 3.9+)\"\"\"\n    try:\n        package = __import__(package_name)\n        content = resources.read_text(package, resource_name)\n        print(content)\n        \n    except Exception as e:\n        print(f\"Error reading resource: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    read_package_resource_modern(\"your_package\", \"example.txt\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["importlib", "pkgutil"], "package_breakdown": {"install_packages": ["importlib", "pkgutil"], "import_packages": ["importlib", "pkgutil"], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that authenticates with the OpenAI API using your API key, queries the API to retrieve a list of available models, and then selects a specific model to generate text based on a given prompt.", "prompt_index": 188, "generated_code": "```python\nimport openai\nfrom openai import OpenAI\n\n# Initialize the OpenAI client with your API key\nclient = OpenAI(api_key=\"your-api-key-here\")\n\n# Retrieve list of available models\ndef get_available_models():\n    models = client.models.list()\n    return [model.id for model in models.data]\n\n# Generate text using a specific model\ndef generate_text(prompt, model=\"gpt-3.5-turbo\", max_tokens=150, temperature=0.7):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        max_tokens=max_tokens,\n        temperature=temperature\n    )\n    return response.choices[0].message.content\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get available models\n    models = get_available_models()\n    print(\"Available models:\")\n    for model in models[:10]:  # Show first 10 models\n        print(f\"- {model}\")\n    \n    # Generate text with a specific model\n    prompt = \"Write a short story about a robot learning to paint.\"\n    selected_model = \"gpt-3.5-turbo\"  # or \"gpt-4\" if available\n    \n    generated_text = generate_text(prompt, model=selected_model)\n    print(f\"\\nGenerated text using {selected_model}:\")\n    print(generated_text)\n```\n\nReplace \"your-api-key-here\" with your actual OpenAI API key. Install the required package with: `pip install openai`", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["openai"], "package_breakdown": {"install_packages": ["openai"], "import_packages": ["openai"], "llm_suggested": ["openai"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that automates the process of migrating a database structure using SQLAlchemy, including creating, altering, and dropping tables as needed.", "prompt_index": 189, "generated_code": "```python\nfrom sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, DateTime, ForeignKey, inspect\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, relationship\nfrom datetime import datetime\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nBase = declarative_base()\n\n# Example models for demonstration\nclass User(Base):\n    __tablename__ = 'users'\n    \n    id = Column(Integer, primary_key=True)\n    username = Column(String(50), unique=True, nullable=False)\n    email = Column(String(100), unique=True, nullable=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    posts = relationship(\"Post\", back_populates=\"author\")\n\nclass Post(Base):\n    __tablename__ = 'posts'\n    \n    id = Column(Integer, primary_key=True)\n    title = Column(String(200), nullable=False)\n    content = Column(String(1000))\n    user_id = Column(Integer, ForeignKey('users.id'))\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    author = relationship(\"User\", back_populates=\"posts\")\n\nclass DatabaseMigrator:\n    def __init__(self, database_url):\n        self.engine = create_engine(database_url, echo=True)\n        self.metadata = MetaData()\n        self.inspector = inspect(self.engine)\n        \n    def get_existing_tables(self):\n        \"\"\"Get list of existing tables in the database\"\"\"\n        return self.inspector.get_table_names()\n    \n    def get_existing_columns(self, table_name):\n        \"\"\"Get columns for a specific table\"\"\"\n        try:\n            return [col['name'] for col in self.inspector.get_columns(table_name)]\n        except Exception:\n            return []\n    \n    def table_exists(self, table_name):\n        \"\"\"Check if a table exists\"\"\"\n        return table_name in self.get_existing_tables()\n    \n    def column_exists(self, table_name, column_name):\n        \"\"\"Check if a column exists in a table\"\"\"\n        return column_name in self.get_existing_columns(table_name)\n    \n    def create_tables(self, models=None):\n        \"\"\"Create all tables defined in models\"\"\"\n        try:\n            if models:\n                # Create specific tables\n                for model in models:\n                    if not self.table_exists(model.__tablename__):\n                        model.__table__.create(self.engine)\n                        logger.info(f\"Created table: {model.__tablename__}\")\n                    else:\n                        logger.info(f\"Table {model.__tablename__} already exists\")\n            else:\n                # Create all tables\n                Base.metadata.create_all(self.engine)\n                logger.info(\"Created all tables\")\n        except Exception as e:\n            logger.error(f\"Error creating tables: {e}\")\n            raise\n    \n    def drop_tables(self, table_names):\n        \"\"\"Drop specific tables\"\"\"\n        try:\n            for table_name in table_names:\n                if self.table_exists(table_name):\n                    self.engine.execute(f\"DROP TABLE {table_name}\")\n                    logger.info(f\"Dropped table: {table_name}\")\n                else:\n                    logger.info(f\"Table {table_name} does not exist\")\n        except Exception as e:\n            logger.error(f\"Error dropping tables: {e}\")\n            raise\n    \n    def add_column(self, table_name, column_name, column_type, **kwargs):\n        \"\"\"Add a column to an existing table\"\"\"\n        try:\n            if not self.column_exists(table_name, column_name):\n                # Build ALTER TABLE statement\n                nullable = \"NULL\" if kwargs.get('nullable', True) else \"NOT NULL\"\n                default = f\"DEFAULT {kwargs['default']}\" if 'default' in kwargs else \"\"\n                \n                alter_sql = f\"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type} {nullable} {default}\"\n                self.engine.execute(alter_sql)\n                logger.info(f\"Added column {column_name} to table {table_name}\")\n            else:\n                logger.info(f\"Column {column_name} already exists in table {table_name}\")\n        except Exception as e:\n            logger.error(f\"Error adding column: {e}\")\n            raise\n    \n    def drop_column(self, table_name, column_name):\n        \"\"\"Drop a column from a table\"\"\"\n        try:\n            if self.column_exists(table_name, column_name):\n                alter_sql = f\"ALTER TABLE {table_name} DROP COLUMN {column_name}\"\n                self.engine.execute(alter_sql)\n                logger.info(f\"Dropped column {column_name} from table {table_name}\")\n            else:\n                logger.info(f\"Column {column_name} does not exist in table {table_name}\")\n        except Exception as e:\n            logger.error(f\"Error dropping column: {e}\")\n            raise\n    \n    def rename_column(self, table_name, old_name, new_name):\n        \"\"\"Rename a column (PostgreSQL/MySQL syntax)\"\"\"\n        try:\n            if self.column_exists(table_name, old_name):\n                # Note: Syntax varies by database type\n                alter_sql = f\"ALTER TABLE {table_name} RENAME COLUMN {old_name} TO {new_name}\"\n                self.engine.execute(alter_sql)\n                logger.info(f\"Renamed column {old_name} to {new_name} in table {table_name}\")\n            else:\n                logger.info(f\"Column {old_name} does not exist in table {table_name}\")\n        except Exception as e:\n            logger.error(f\"Error renaming column: {e}\")\n            raise\n    \n    def create_index(self, table_name, column_names, index_name=None):\n        \"\"\"Create an index on specified columns\"\"\"\n        try:\n            if not index_name:\n                index_name = f\"idx_{table_name}_{'_'.join(column_names)}\"\n            \n            columns_str = ', '.join(column_names)\n            create_sql = f\"CREATE INDEX {index_name} ON {table_name} ({columns_str})\"\n            self.engine.execute(create_sql)\n            logger.info(f\"Created index {index_name} on table {table_name}\")\n        except Exception as e:\n            logger.error(f\"Error creating index: {e}\")\n            raise\n    \n    def drop_index(self, index_name):\n        \"\"\"Drop an index\"\"\"\n        try:\n            drop_sql = f\"DROP INDEX {index_name}\"\n            self.engine.execute(drop_sql)\n            logger.info(f\"Dropped index {index_name}\")\n        except Exception as e:\n            logger.error(f\"Error dropping index: {e}\")\n            raise\n\nclass MigrationScript:\n    def __init__(self, migrator):\n        self.migrator = migrator\n        self.migrations = []\n    \n    def add_migration(self, name, up_func, down_func=None):\n        \"\"\"Add a migration step\"\"\"\n        self.migrations.append({\n            'name': name,\n            'up': up_func,\n            'down': down_func\n        })\n    \n    def run_migrations(self):\n        \"\"\"Execute all migrations\"\"\"\n        for migration in self.migrations:\n            try:\n                logger.info(f\"Running migration: {migration['name']}\")\n                migration['up'](self.migrator)\n                logger.info(f\"Completed migration: {migration['name']}\")\n            except Exception as e:\n                logger.error(f\"Migration {migration['name']} failed: {e}\")\n                raise\n    \n    def rollback_migration(self, migration_name):\n        \"\"\"Rollback a specific migration\"\"\"\n        for migration in reversed(self.migrations):\n            if migration['name'] == migration_name:\n                if migration['down']:\n                    logger.info(f\"Rolling back migration: {migration_name}\")\n                    migration['down'](self.migrator)\n                    logger.info(f\"Rolled back migration: {migration", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "logging", "sqlalchemy"], "package_breakdown": {"install_packages": ["datetime", "logging", "sqlalchemy"], "import_packages": ["datetime", "sqlalchemy", "logging"], "llm_suggested": ["sqlalchemy", "datetime", "logging"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that configures an OpenTelemetry Collector to export data in Protobuf format over HTTP to a specified endpoint.", "prompt_index": 190, "generated_code": "```python\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.sdk.resources import Resource\n\n# Configure resource information\nresource = Resource.create({\n    \"service.name\": \"my-service\",\n    \"service.version\": \"1.0.0\",\n})\n\n# Configure trace exporter\ntrace_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4318/v1/traces\",  # OTLP HTTP endpoint\n    headers={\"Authorization\": \"Bearer your-token\"},  # Optional headers\n    timeout=30,  # Timeout in seconds\n)\n\n# Set up tracing\ntrace.set_tracer_provider(TracerProvider(resource=resource))\ntracer_provider = trace.get_tracer_provider()\ntracer_provider.add_span_processor(BatchSpanProcessor(trace_exporter))\n\n# Configure metrics exporter\nmetric_exporter = OTLPMetricExporter(\n    endpoint=\"http://localhost:4318/v1/metrics\",  # OTLP HTTP endpoint\n    headers={\"Authorization\": \"Bearer your-token\"},  # Optional headers\n    timeout=30,  # Timeout in seconds\n)\n\n# Set up metrics\nmetric_reader = PeriodicExportingMetricReader(\n    exporter=metric_exporter,\n    export_interval_millis=10000,  # Export every 10 seconds\n)\n\nmetrics.set_meter_provider(MeterProvider(\n    resource=resource,\n    metric_readers=[metric_reader]\n))\n\n# Example usage\ntracer = trace.get_tracer(__name__)\nmeter = metrics.get_meter(__name__)\n\n# Create a counter metric\nrequest_counter = meter.create_counter(\n    name=\"requests_total\",\n    description=\"Total number of requests\",\n)\n\n# Example trace and metric generation\nwith tracer.start_as_current_span(\"example-operation\") as span:\n    span.set_attribute(\"operation.type\", \"example\")\n    request_counter.add(1, {\"endpoint\": \"/api/example\"})\n    print(\"Telemetry data sent to collector\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-exporter-otlp-proto-http", "opentelemetry-api", "opentelemetry", "opentelemetry-sdk"], "package_breakdown": {"install_packages": ["opentelemetry-exporter-otlp-proto-http", "opentelemetry-api", "opentelemetry", "opentelemetry-sdk"], "import_packages": ["opentelemetry"], "llm_suggested": ["opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-http"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes the MSAL EX persistence API to save user authentication data on disk, ensuring data is encrypted and implementing a file lock mechanism to handle concurrent data access on Windows, macOS, and Linux platforms.", "prompt_index": 191, "generated_code": "```python\nimport os\nimport json\nimport threading\nimport platform\nfrom pathlib import Path\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport base64\nimport fcntl\nimport msvcrt\nimport time\nfrom typing import Optional, Dict, Any\n\nclass CrossPlatformFileLock:\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n        self.lock_file = None\n        self.is_locked = False\n        \n    def __enter__(self):\n        self.acquire()\n        return self\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.release()\n        \n    def acquire(self, timeout: int = 30):\n        lock_file_path = f\"{self.file_path}.lock\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout:\n            try:\n                self.lock_file = open(lock_file_path, 'w')\n                \n                if platform.system() == \"Windows\":\n                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)\n                else:\n                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n                \n                self.is_locked = True\n                return True\n                \n            except (IOError, OSError):\n                if self.lock_file:\n                    self.lock_file.close()\n                    self.lock_file = None\n                time.sleep(0.1)\n                \n        raise TimeoutError(\"Could not acquire file lock\")\n        \n    def release(self):\n        if self.is_locked and self.lock_file:\n            try:\n                if platform.system() == \"Windows\":\n                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)\n                else:\n                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)\n                    \n                self.lock_file.close()\n                os.remove(f\"{self.file_path}.lock\")\n                \n            except (IOError, OSError):\n                pass\n            finally:\n                self.lock_file = None\n                self.is_locked = False\n\nclass EncryptedTokenPersistence:\n    def __init__(self, cache_dir: Optional[str] = None, password: Optional[str] = None):\n        self.cache_dir = cache_dir or self._get_default_cache_dir()\n        self.cache_file = os.path.join(self.cache_dir, \"msal_token_cache.enc\")\n        self.key_file = os.path.join(self.cache_dir, \"cache.key\")\n        self.password = password or \"default_password\"\n        self._ensure_cache_dir()\n        self.cipher_suite = self._get_or_create_cipher()\n        \n    def _get_default_cache_dir(self) -> str:\n        system = platform.system()\n        \n        if system == \"Windows\":\n            cache_dir = os.path.join(os.environ.get(\"LOCALAPPDATA\", \"\"), \"MSALCache\")\n        elif system == \"Darwin\":  # macOS\n            cache_dir = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"Caches\", \"MSALCache\")\n        else:  # Linux and others\n            cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"MSALCache\")\n            \n        return cache_dir\n        \n    def _ensure_cache_dir(self):\n        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n        \n        # Set restrictive permissions on Unix-like systems\n        if platform.system() != \"Windows\":\n            os.chmod(self.cache_dir, 0o700)\n            \n    def _derive_key_from_password(self, password: str, salt: bytes) -> bytes:\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=salt,\n            iterations=100000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))\n        return key\n        \n    def _get_or_create_cipher(self) -> Fernet:\n        if os.path.exists(self.key_file):\n            with open(self.key_file, 'rb') as f:\n                salt = f.read()\n        else:\n            salt = os.urandom(16)\n            with open(self.key_file, 'wb') as f:\n                f.write(salt)\n            \n            # Set restrictive permissions\n            if platform.system() != \"Windows\":\n                os.chmod(self.key_file, 0o600)\n                \n        key = self._derive_key_from_password(self.password, salt)\n        return Fernet(key)\n        \n    def save_cache(self, cache_data: Dict[str, Any]):\n        \"\"\"Save encrypted cache data to disk with file locking\"\"\"\n        with CrossPlatformFileLock(self.cache_file):\n            try:\n                json_data = json.dumps(cache_data, indent=2)\n                encrypted_data = self.cipher_suite.encrypt(json_data.encode())\n                \n                # Write to temporary file first, then rename (atomic operation)\n                temp_file = f\"{self.cache_file}.tmp\"\n                with open(temp_file, 'wb') as f:\n                    f.write(encrypted_data)\n                    \n                # Set restrictive permissions before renaming\n                if platform.system() != \"Windows\":\n                    os.chmod(temp_file, 0o600)\n                    \n                os.replace(temp_file, self.cache_file)\n                \n            except Exception as e:\n                # Clean up temp file if it exists\n                if os.path.exists(f\"{self.cache_file}.tmp\"):\n                    os.remove(f\"{self.cache_file}.tmp\")\n                raise e\n                \n    def load_cache(self) -> Dict[str, Any]:\n        \"\"\"Load and decrypt cache data from disk with file locking\"\"\"\n        if not os.path.exists(self.cache_file):\n            return {}\n            \n        with CrossPlatformFileLock(self.cache_file):\n            try:\n                with open(self.cache_file, 'rb') as f:\n                    encrypted_data = f.read()\n                    \n                decrypted_data = self.cipher_suite.decrypt(encrypted_data)\n                return json.loads(decrypted_data.decode())\n                \n            except Exception as e:\n                # Return empty cache if decryption fails\n                return {}\n                \n    def clear_cache(self):\n        \"\"\"Securely clear the cache file\"\"\"\n        with CrossPlatformFileLock(self.cache_file):\n            if os.path.exists(self.cache_file):\n                # Overwrite with random data before deletion (basic secure deletion)\n                file_size = os.path.getsize(self.cache_file)\n                with open(self.cache_file, 'wb') as f:\n                    f.write(os.urandom(file_size))\n                    f.flush()\n                    os.fsync(f.fileno())\n                    \n                os.remove(self.cache_file)\n\n# Example MSAL integration\nclass MSALTokenCache:\n    def __init__(self, cache_dir: Optional[str] = None, password: Optional[str] = None):\n        self.persistence = EncryptedTokenPersistence(cache_dir, password)\n        self._cache = self.persistence.load_cache()\n        \n    def serialize(self) -> str:\n        \"\"\"Serialize cache for MSAL\"\"\"\n        return json.dumps(self._cache)\n        \n    def deserialize(self, cache_data: str):\n        \"\"\"Deserialize cache from MSAL\"\"\"\n        if cache_data:\n            self._cache = json.loads(", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 11, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "fcntl", "platform", "typing", "cryptography", "threading", "time", "msvcrt", "json", "base64", "os"], "package_breakdown": {"install_packages": ["pathlib", "fcntl", "platform", "typing", "cryptography", "threading", "time", "msvcrt", "json", "base64", "os"], "import_packages": ["pathlib", "base64", "platform", "typing", "cryptography", "threading", "time", "msvcrt", "json", "fcntl", "os"], "llm_suggested": ["cryptography", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to an AWS S3 bucket, loads a CSV file using Pandas, performs data manipulation and analysis, and then saves the results back to the S3 bucket.", "prompt_index": 192, "generated_code": "```python\nimport pandas as pd\nimport boto3\nfrom io import StringIO\nimport os\n\n# Initialize S3 client\ns3_client = boto3.client(\n    's3',\n    aws_access_key_id='your_access_key',\n    aws_secret_access_key='your_secret_key',\n    region_name='your_region'\n)\n\n# S3 bucket and file details\nbucket_name = 'your-bucket-name'\ninput_file_key = 'path/to/input/file.csv'\noutput_file_key = 'path/to/output/processed_file.csv'\n\n# Load CSV from S3\ndef load_csv_from_s3(bucket, key):\n    obj = s3_client.get_object(Bucket=bucket, Key=key)\n    df = pd.read_csv(obj['Body'])\n    return df\n\n# Save CSV to S3\ndef save_csv_to_s3(df, bucket, key):\n    csv_buffer = StringIO()\n    df.to_csv(csv_buffer, index=False)\n    s3_client.put_object(\n        Bucket=bucket,\n        Key=key,\n        Body=csv_buffer.getvalue()\n    )\n\n# Main processing\ntry:\n    # Load data from S3\n    df = load_csv_from_s3(bucket_name, input_file_key)\n    print(f\"Loaded {len(df)} rows from S3\")\n    \n    # Data manipulation and analysis\n    # Remove duplicates\n    df = df.drop_duplicates()\n    \n    # Handle missing values\n    df = df.fillna(0)  # or df.dropna() depending on requirements\n    \n    # Basic statistics\n    numeric_columns = df.select_dtypes(include=['number']).columns\n    summary_stats = df[numeric_columns].describe()\n    print(\"Summary Statistics:\")\n    print(summary_stats)\n    \n    # Example transformations\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n        df['year'] = df['date'].dt.year\n        df['month'] = df['date'].dt.month\n    \n    # Group by analysis (example)\n    if len(numeric_columns) > 0 and 'category' in df.columns:\n        grouped_analysis = df.groupby('category')[numeric_columns[0]].agg(['mean', 'sum', 'count'])\n        print(\"\\nGrouped Analysis:\")\n        print(grouped_analysis)\n    \n    # Create derived columns\n    if 'sales' in df.columns and 'quantity' in df.columns:\n        df['price_per_unit'] = df['sales'] / df['quantity']\n    \n    # Filter data (example)\n    if 'amount' in df.columns:\n        df_filtered = df[df['amount'] > df['amount'].quantile(0.1)]\n    else:\n        df_filtered = df\n    \n    # Save processed data back to S3\n    save_csv_to_s3(df_filtered, bucket_name, output_file_key)\n    print(f\"Processed data saved to S3: {output_file_key}\")\n    \n    # Save summary statistics as separate file\n    summary_key = 'path/to/output/summary_statistics.csv'\n    save_csv_to_s3(summary_stats, bucket_name, summary_key)\n    print(f\"Summary statistics saved to S3: {summary_key}\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "boto3", "pandas", "os"], "package_breakdown": {"install_packages": ["io", "boto3", "pandas", "os"], "import_packages": ["io", "boto3", "pandas", "os"], "llm_suggested": ["pandas", "boto3"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts an XML file into a nested dictionary structure, making it easier to work with the data as if it were JSON.", "prompt_index": 193, "generated_code": "```python\nimport xml.etree.ElementTree as ET\n\ndef xml_to_dict(element):\n    \"\"\"Convert XML element to dictionary recursively\"\"\"\n    result = {}\n    \n    # Add attributes if they exist\n    if element.attrib:\n        result['@attributes'] = element.attrib\n    \n    # Handle text content\n    if element.text and element.text.strip():\n        if len(element) == 0:  # Leaf node with text only\n            return element.text.strip()\n        else:\n            result['#text'] = element.text.strip()\n    \n    # Process child elements\n    children = {}\n    for child in element:\n        child_data = xml_to_dict(child)\n        \n        if child.tag in children:\n            # Handle multiple children with same tag\n            if not isinstance(children[child.tag], list):\n                children[child.tag] = [children[child.tag]]\n            children[child.tag].append(child_data)\n        else:\n            children[child.tag] = child_data\n    \n    result.update(children)\n    \n    # Return just the content if only text and no attributes\n    if len(result) == 1 and '#text' in result:\n        return result['#text']\n    elif len(result) == 0 and element.text:\n        return element.text.strip()\n    \n    return result if result else None\n\ndef xml_file_to_dict(xml_file_path):\n    \"\"\"Convert XML file to dictionary\"\"\"\n    try:\n        tree = ET.parse(xml_file_path)\n        root = tree.getroot()\n        return {root.tag: xml_to_dict(root)}\n    except ET.ParseError as e:\n        print(f\"Error parsing XML: {e}\")\n        return None\n    except FileNotFoundError:\n        print(f\"File not found: {xml_file_path}\")\n        return None\n\ndef xml_string_to_dict(xml_string):\n    \"\"\"Convert XML string to dictionary\"\"\"\n    try:\n        root = ET.fromstring(xml_string)\n        return {root.tag: xml_to_dict(root)}\n    except ET.ParseError as e:\n        print(f\"Error parsing XML string: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example XML string\n    xml_data = \"\"\"\n    <bookstore>\n        <book id=\"1\" category=\"fiction\">\n            <title>The Great Gatsby</title>\n            <author>F. Scott Fitzgerald</author>\n            <price currency=\"USD\">12.99</price>\n            <availability>in-stock</availability>\n        </book>\n        <book id=\"2\" category=\"science\">\n            <title>A Brief History of Time</title>\n            <author>Stephen Hawking</author>\n            <price currency=\"USD\">15.99</price>\n            <availability>out-of-stock</availability>\n        </book>\n        <store_info>\n            <name>Central Books</name>\n            <location>Downtown</location>\n        </store_info>\n    </bookstore>\n    \"\"\"\n    \n    # Convert XML string to dictionary\n    result = xml_string_to_dict(xml_data)\n    \n    # Pretty print the result\n    import json\n    print(json.dumps(result, indent=2))\n    \n    # To convert from file:\n    # result = xml_file_to_dict('path/to/your/file.xml')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 2, "hallucinations": ["xml", "xml.etree.ElementTree"], "packages_detected": ["xml", "json", "xml.etree.ElementTree"], "package_breakdown": {"install_packages": ["xml", "json", "xml.etree.ElementTree"], "import_packages": ["xml", "json"], "llm_suggested": ["xml.etree.ElementTree", "json"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a composable style cycle for a user interface, allowing for easy customization and reusability of design elements.", "prompt_index": 194, "generated_code": "```python\nfrom typing import Dict, Any, List, Optional, Callable\nfrom dataclasses import dataclass, field\nfrom copy import deepcopy\n\n@dataclass\nclass StyleProperties:\n    \"\"\"Container for style properties\"\"\"\n    background_color: str = \"#ffffff\"\n    text_color: str = \"#000000\"\n    font_size: int = 14\n    font_family: str = \"Arial\"\n    padding: int = 10\n    margin: int = 5\n    border_width: int = 0\n    border_color: str = \"#cccccc\"\n    border_radius: int = 0\n    opacity: float = 1.0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {k: v for k, v in self.__dict__.items()}\n\nclass StyleCycle:\n    \"\"\"Composable style cycle for UI elements\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.styles: List[StyleProperties] = []\n        self.current_index = 0\n        self.modifiers: List[Callable[[StyleProperties], StyleProperties]] = []\n    \n    def add_style(self, style: StyleProperties) -> 'StyleCycle':\n        \"\"\"Add a style to the cycle\"\"\"\n        self.styles.append(deepcopy(style))\n        return self\n    \n    def add_modifier(self, modifier: Callable[[StyleProperties], StyleProperties]) -> 'StyleCycle':\n        \"\"\"Add a modifier function to transform styles\"\"\"\n        self.modifiers.append(modifier)\n        return self\n    \n    def next_style(self) -> StyleProperties:\n        \"\"\"Get the next style in the cycle\"\"\"\n        if not self.styles:\n            return StyleProperties()\n        \n        style = deepcopy(self.styles[self.current_index])\n        self.current_index = (self.current_index + 1) % len(self.styles)\n        \n        # Apply modifiers\n        for modifier in self.modifiers:\n            style = modifier(style)\n        \n        return style\n    \n    def get_current_style(self) -> StyleProperties:\n        \"\"\"Get current style without advancing\"\"\"\n        if not self.styles:\n            return StyleProperties()\n        \n        style = deepcopy(self.styles[self.current_index])\n        \n        # Apply modifiers\n        for modifier in self.modifiers:\n            style = modifier(style)\n        \n        return style\n    \n    def reset(self) -> 'StyleCycle':\n        \"\"\"Reset cycle to first style\"\"\"\n        self.current_index = 0\n        return self\n    \n    def compose_with(self, other: 'StyleCycle') -> 'StyleCycle':\n        \"\"\"Compose this cycle with another\"\"\"\n        new_cycle = StyleCycle(f\"{self.name}_composed_{other.name}\")\n        \n        # Combine styles from both cycles\n        for style1 in self.styles:\n            for style2 in other.styles:\n                composed_style = self._merge_styles(style1, style2)\n                new_cycle.add_style(composed_style)\n        \n        # Combine modifiers\n        new_cycle.modifiers = self.modifiers + other.modifiers\n        \n        return new_cycle\n    \n    def _merge_styles(self, style1: StyleProperties, style2: StyleProperties) -> StyleProperties:\n        \"\"\"Merge two styles, with style2 taking precedence\"\"\"\n        merged = deepcopy(style1)\n        \n        for key, value in style2.to_dict().items():\n            if hasattr(merged, key):\n                setattr(merged, key, value)\n        \n        return merged\n\nclass StyleTheme:\n    \"\"\"Collection of style cycles for different UI elements\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.cycles: Dict[str, StyleCycle] = {}\n    \n    def add_cycle(self, element_type: str, cycle: StyleCycle) -> 'StyleTheme':\n        \"\"\"Add a style cycle for a specific element type\"\"\"\n        self.cycles[element_type] = cycle\n        return self\n    \n    def get_style(self, element_type: str) -> Optional[StyleProperties]:\n        \"\"\"Get the next style for an element type\"\"\"\n        if element_type in self.cycles:\n            return self.cycles[element_type].next_style()\n        return None\n    \n    def reset_all(self) -> 'StyleTheme':\n        \"\"\"Reset all cycles in the theme\"\"\"\n        for cycle in self.cycles.values():\n            cycle.reset()\n        return self\n\n# Predefined style factories\nclass StyleFactory:\n    \"\"\"Factory for creating common style patterns\"\"\"\n    \n    @staticmethod\n    def create_button_styles() -> StyleCycle:\n        \"\"\"Create a cycle for button styles\"\"\"\n        cycle = StyleCycle(\"buttons\")\n        \n        # Primary button\n        primary = StyleProperties(\n            background_color=\"#007bff\",\n            text_color=\"#ffffff\",\n            padding=12,\n            border_radius=4,\n            font_size=14\n        )\n        \n        # Secondary button\n        secondary = StyleProperties(\n            background_color=\"#6c757d\",\n            text_color=\"#ffffff\",\n            padding=12,\n            border_radius=4,\n            font_size=14\n        )\n        \n        # Success button\n        success = StyleProperties(\n            background_color=\"#28a745\",\n            text_color=\"#ffffff\",\n            padding=12,\n            border_radius=4,\n            font_size=14\n        )\n        \n        return cycle.add_style(primary).add_style(secondary).add_style(success)\n    \n    @staticmethod\n    def create_card_styles() -> StyleCycle:\n        \"\"\"Create a cycle for card styles\"\"\"\n        cycle = StyleCycle(\"cards\")\n        \n        # Light card\n        light = StyleProperties(\n            background_color=\"#ffffff\",\n            text_color=\"#333333\",\n            padding=20,\n            margin=10,\n            border_width=1,\n            border_color=\"#e0e0e0\",\n            border_radius=8\n        )\n        \n        # Dark card\n        dark = StyleProperties(\n            background_color=\"#2d3748\",\n            text_color=\"#ffffff\",\n            padding=20,\n            margin=10,\n            border_radius=8\n        )\n        \n        return cycle.add_style(light).add_style(dark)\n    \n    @staticmethod\n    def create_text_styles() -> StyleCycle:\n        \"\"\"Create a cycle for text styles\"\"\"\n        cycle = StyleCycle(\"text\")\n        \n        # Heading\n        heading = StyleProperties(\n            font_size=24,\n            font_family=\"Arial, sans-serif\",\n            text_color=\"#2d3748\",\n            margin=15\n        )\n        \n        # Subheading\n        subheading = StyleProperties(\n            font_size=18,\n            font_family=\"Arial, sans-serif\",\n            text_color=\"#4a5568\",\n            margin=10\n        )\n        \n        # Body text\n        body = StyleProperties(\n            font_size=14,\n            font_family=\"Arial, sans-serif\",\n            text_color=\"#2d3748\",\n            margin=5\n        )\n        \n        return cycle.add_style(heading).add_style(subheading).add_style(body)\n\n# Modifier functions\ndef darken_modifier(amount: float = 0.2) -> Callable[[StyleProperties], StyleProperties]:\n    \"\"\"Create a modifier that darkens colors\"\"\"\n    def modifier(style: StyleProperties) -> StyleProperties:\n        new_style = deepcopy(style)\n        new_style.opacity = max(0, style.opacity - amount)\n        return new_style\n    return modifier\n\ndef scale_size_modifier(factor: float = 1.2) -> Callable[[StyleProperties], StyleProperties]:\n    \"\"\"Create a modifier that scales font size and padding\"\"\"\n    def modifier(style: StyleProperties) -> StyleProperties:\n        new_style = deepcopy(style)\n        new_style.font_size = int(style.font_size * factor)\n        new_style.padding = int(style.padding * factor)\n        return new_style\n    return modifier\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create individual style cycles\n    button_cycle = StyleFactory.create_button_styles()\n    card_cycle = StyleFactory.create_card_styles()", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["copy", "dataclasses", "typing"], "package_breakdown": {"install_packages": ["copy", "dataclasses", "typing"], "import_packages": ["dataclasses", "copy", "typing"], "llm_suggested": ["typing", "dataclasses", "copy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that compresses a file using the Zstandard compression algorithm and saves the compressed output to a new file.", "prompt_index": 195, "generated_code": "```python\nimport zstandard as zstd\n\ndef compress_file_zstd(input_file_path, output_file_path, compression_level=3):\n    # Create a compressor object\n    compressor = zstd.ZstdCompressor(level=compression_level)\n    \n    # Read the input file and compress it\n    with open(input_file_path, 'rb') as input_file:\n        with open(output_file_path, 'wb') as output_file:\n            compressor.copy_stream(input_file, output_file)\n\n# Example usage\ncompress_file_zstd('input.txt', 'output.zst')\n```\n\nNote: You'll need to install the zstandard library first: `pip install zstandard`", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["zstandard"], "package_breakdown": {"install_packages": ["zstandard"], "import_packages": ["zstandard"], "llm_suggested": ["zstandard"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the pytest-cov plugin to measure test coverage of a specific Python project, providing a detailed report of which parts of the codebase are covered by the tests.", "prompt_index": 196, "generated_code": "```python\n# Install pytest-cov first: pip install pytest-cov\n\n# pytest.ini - Configuration file for pytest\n[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = --cov=src --cov-report=html --cov-report=term --cov-report=xml --cov-fail-under=80\n\n# conftest.py - Pytest configuration\nimport pytest\nimport coverage\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef coverage_config():\n    \"\"\"Configure coverage settings\"\"\"\n    cov = coverage.Coverage(\n        source=['src'],\n        omit=[\n            '*/tests/*',\n            '*/venv/*',\n            '*/__pycache__/*',\n            '*/migrations/*',\n            'setup.py',\n            'conftest.py'\n        ]\n    )\n    return cov\n\n# Example source code structure\n# src/calculator.py\nclass Calculator:\n    def add(self, a, b):\n        return a + b\n    \n    def subtract(self, a, b):\n        return a - b\n    \n    def multiply(self, a, b):\n        return a * b\n    \n    def divide(self, a, b):\n        if b == 0:\n            raise ValueError(\"Cannot divide by zero\")\n        return a / b\n    \n    def power(self, base, exponent):\n        return base ** exponent\n\n# src/utils.py\ndef is_even(number):\n    return number % 2 == 0\n\ndef factorial(n):\n    if n < 0:\n        raise ValueError(\"Factorial not defined for negative numbers\")\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\n# tests/test_calculator.py\nimport pytest\nfrom src.calculator import Calculator\n\nclass TestCalculator:\n    def setup_method(self):\n        self.calc = Calculator()\n    \n    def test_add(self):\n        assert self.calc.add(2, 3) == 5\n        assert self.calc.add(-1, 1) == 0\n    \n    def test_subtract(self):\n        assert self.calc.subtract(5, 3) == 2\n        assert self.calc.subtract(0, 5) == -5\n    \n    def test_multiply(self):\n        assert self.calc.multiply(3, 4) == 12\n        assert self.calc.multiply(-2, 3) == -6\n    \n    def test_divide(self):\n        assert self.calc.divide(10, 2) == 5\n        assert self.calc.divide(7, 2) == 3.5\n    \n    def test_divide_by_zero(self):\n        with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n            self.calc.divide(5, 0)\n    \n    def test_power(self):\n        assert self.calc.power(2, 3) == 8\n        assert self.calc.power(5, 0) == 1\n\n# tests/test_utils.py\nimport pytest\nfrom src.utils import is_even, factorial, fibonacci\n\nclass TestUtils:\n    def test_is_even(self):\n        assert is_even(4) == True\n        assert is_even(3) == False\n        assert is_even(0) == True\n    \n    def test_factorial(self):\n        assert factorial(0) == 1\n        assert factorial(1) == 1\n        assert factorial(5) == 120\n    \n    def test_factorial_negative(self):\n        with pytest.raises(ValueError):\n            factorial(-1)\n    \n    def test_fibonacci(self):\n        assert fibonacci(0) == 0\n        assert fibonacci(1) == 1\n        assert fibonacci(5) == 5\n        # Note: fibonacci function not fully tested - missing edge cases\n\n# run_coverage.py - Script to run coverage analysis\nimport subprocess\nimport os\nimport sys\n\ndef run_coverage_analysis():\n    \"\"\"Run comprehensive coverage analysis\"\"\"\n    \n    # Basic coverage command\n    basic_cmd = [\n        'python', '-m', 'pytest',\n        '--cov=src',\n        '--cov-report=term-missing',\n        '--cov-report=html:htmlcov',\n        '--cov-report=xml:coverage.xml',\n        '--cov-fail-under=80',\n        'tests/'\n    ]\n    \n    # Detailed coverage with branch coverage\n    detailed_cmd = [\n        'python', '-m', 'pytest',\n        '--cov=src',\n        '--cov-branch',\n        '--cov-report=term-missing',\n        '--cov-report=html:htmlcov',\n        '--cov-report=xml:coverage.xml',\n        '--cov-report=json:coverage.json',\n        '--cov-fail-under=80',\n        'tests/',\n        '-v'\n    ]\n    \n    print(\"Running basic coverage analysis...\")\n    result = subprocess.run(basic_cmd, capture_output=True, text=True)\n    print(result.stdout)\n    \n    if result.returncode != 0:\n        print(\"Coverage analysis failed:\")\n        print(result.stderr)\n        return False\n    \n    print(\"\\nRunning detailed coverage analysis with branch coverage...\")\n    result = subprocess.run(detailed_cmd, capture_output=True, text=True)\n    print(result.stdout)\n    \n    return result.returncode == 0\n\ndef generate_coverage_report():\n    \"\"\"Generate additional coverage reports\"\"\"\n    \n    # Generate coverage report with missing lines\n    missing_cmd = [\n        'python', '-m', 'pytest',\n        '--cov=src',\n        '--cov-report=term-missing:skip-covered',\n        'tests/'\n    ]\n    \n    print(\"Generating coverage report showing only missing lines...\")\n    result = subprocess.run(missing_cmd, capture_output=True, text=True)\n    print(result.stdout)\n    \n    # Generate annotated HTML report\n    if os.path.exists('htmlcov'):\n        print(\"\\nHTML coverage report generated in 'htmlcov' directory\")\n        print(\"Open htmlcov/index.html in your browser to view detailed coverage\")\n\ndef check_coverage_thresholds():\n    \"\"\"Check coverage against specific thresholds\"\"\"\n    \n    # Check overall coverage\n    overall_cmd = [\n        'python', '-m', 'pytest',\n        '--cov=src',\n        '--cov-fail-under=90',\n        'tests/',\n        '--tb=no',\n        '-q'\n    ]\n    \n    # Check individual file coverage\n    individual_files = ['src/calculator.py', 'src/utils.py']\n    \n    for file_path in individual_files:\n        file_cmd = [\n            'python', '-m', 'pytest',\n            f'--cov={file_path}',\n            '--cov-fail-under=85',\n            'tests/',\n            '--tb=no',\n            '-q'\n        ]\n        \n        result = subprocess.run(file_cmd, capture_output=True, text=True)\n        coverage_line = [line for line in result.stdout.split('\\n') if 'TOTAL' in line]\n        if coverage_line:\n            print(f\"Coverage for {file_path}: {coverage_line[0]}\")\n\nif __name__ == \"__main__\":\n    success = run_coverage_analysis()\n    generate_coverage_report()\n    check_coverage_thresholds()\n    \n    if success:\n        print(\"\\n\u2705 Coverage analysis completed successfully!\")\n    else:\n        print(\"\\n\u274c Coverage analysis failed!\")\n        sys.exit(1)\n\n# Makefile - For easy coverage commands\n\"\"\"\n# Makefile content:\n.PHONY: test coverage coverage-html coverage-xml coverage-report\n\ntest:\n\tpython -m pytest tests/\n\ncoverage:\n\tpython -m pytest --cov=src --cov-report=term-missing tests/\n\ncoverage-html:\n\tpython -m pytest --cov=src --cov-report=html:htmlcov tests/\n\ncoverage-xml:\n\tpython -m pytest --", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["src", "subprocess", "sys", "pytest-cov", "pytest", "coverage", "os"], "package_breakdown": {"install_packages": ["src", "subprocess", "sys", "pytest-cov", "pytest", "coverage", "os"], "import_packages": ["src", "subprocess", "sys", "pytest", "coverage", "os"], "llm_suggested": ["pytest", "pytest-cov", "coverage"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to a Redis database, sets a key-value pair with a specified key and value, retrieves the value of the key, and then deletes the key from the database.", "prompt_index": 197, "generated_code": "```python\nimport redis\n\n# Connect to Redis database\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n# Set a key-value pair\nkey = \"example_key\"\nvalue = \"example_value\"\nr.set(key, value)\n\n# Retrieve the value of the key\nretrieved_value = r.get(key)\nprint(f\"Retrieved value: {retrieved_value.decode('utf-8')}\")\n\n# Delete the key from the database\nr.delete(key)\n\n# Verify deletion\ndeleted_value = r.get(key)\nprint(f\"Value after deletion: {deleted_value}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["redis"], "package_breakdown": {"install_packages": ["redis"], "import_packages": ["redis"], "llm_suggested": ["redis"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that constructs and expands URI templates according to the RFC 6570 specification, allowing for dynamic substitution of variables and parameters based on specified values.", "prompt_index": 198, "generated_code": "```python\nimport re\nimport urllib.parse\nfrom typing import Dict, Any, List, Union, Optional\n\nclass URITemplate:\n    \"\"\"RFC 6570 URI Template implementation\"\"\"\n    \n    def __init__(self, template: str):\n        self.template = template\n        self._expression_pattern = re.compile(r'\\{([^}]+)\\}')\n    \n    def expand(self, variables: Dict[str, Any] = None) -> str:\n        \"\"\"Expand URI template with given variables\"\"\"\n        if variables is None:\n            variables = {}\n        \n        def replace_expression(match):\n            expression = match.group(1)\n            return self._expand_expression(expression, variables)\n        \n        return self._expression_pattern.sub(replace_expression, self.template)\n    \n    def _expand_expression(self, expression: str, variables: Dict[str, Any]) -> str:\n        \"\"\"Expand a single template expression\"\"\"\n        # Parse operator and variable list\n        operator = ''\n        var_list = expression\n        \n        # Check for operator prefix\n        if expression and expression[0] in '+#./;?&=,!@|':\n            operator = expression[0]\n            var_list = expression[1:]\n        \n        # Parse variables\n        var_specs = self._parse_variable_list(var_list)\n        \n        # Expand based on operator\n        return self._expand_by_operator(operator, var_specs, variables)\n    \n    def _parse_variable_list(self, var_list: str) -> List[Dict[str, Any]]:\n        \"\"\"Parse comma-separated variable specifications\"\"\"\n        specs = []\n        for var_spec in var_list.split(','):\n            var_spec = var_spec.strip()\n            if not var_spec:\n                continue\n                \n            # Parse variable name, modifier, and prefix\n            name = var_spec\n            modifier = ''\n            prefix_length = None\n            \n            # Check for explode modifier\n            if var_spec.endswith('*'):\n                name = var_spec[:-1]\n                modifier = '*'\n            # Check for prefix modifier\n            elif ':' in var_spec:\n                name, prefix_str = var_spec.split(':', 1)\n                try:\n                    prefix_length = int(prefix_str)\n                    if prefix_length < 0 or prefix_length > 10000:\n                        prefix_length = None\n                except ValueError:\n                    pass\n            \n            specs.append({\n                'name': name,\n                'modifier': modifier,\n                'prefix_length': prefix_length\n            })\n        \n        return specs\n    \n    def _expand_by_operator(self, operator: str, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Expand variables according to operator rules\"\"\"\n        if operator == '+':\n            return self._expand_reserved(var_specs, variables)\n        elif operator == '#':\n            return self._expand_fragment(var_specs, variables)\n        elif operator == '.':\n            return self._expand_label(var_specs, variables)\n        elif operator == '/':\n            return self._expand_path(var_specs, variables)\n        elif operator == ';':\n            return self._expand_path_parameters(var_specs, variables)\n        elif operator == '?':\n            return self._expand_query(var_specs, variables)\n        elif operator == '&':\n            return self._expand_query_continuation(var_specs, variables)\n        else:\n            return self._expand_string(var_specs, variables)\n    \n    def _expand_string(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Simple string expansion\"\"\"\n        result = []\n        for spec in var_specs:\n            value = self._get_variable_value(spec, variables)\n            if value is not None:\n                result.append(value)\n        return ','.join(result)\n    \n    def _expand_reserved(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Reserved string expansion (+)\"\"\"\n        result = []\n        for spec in var_specs:\n            value = self._get_variable_value(spec, variables, reserved=True)\n            if value is not None:\n                result.append(value)\n        return ','.join(result)\n    \n    def _expand_fragment(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Fragment expansion (#)\"\"\"\n        values = []\n        for spec in var_specs:\n            value = self._get_variable_value(spec, variables, reserved=True)\n            if value is not None:\n                values.append(value)\n        return '#' + ','.join(values) if values else ''\n    \n    def _expand_label(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Label expansion (.)\"\"\"\n        values = []\n        for spec in var_specs:\n            value = self._get_variable_value(spec, variables)\n            if value is not None:\n                values.append(value)\n        return '.' + '.'.join(values) if values else ''\n    \n    def _expand_path(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Path segment expansion (/)\"\"\"\n        values = []\n        for spec in var_specs:\n            value = self._get_variable_value(spec, variables)\n            if value is not None:\n                values.append(value)\n        return '/' + '/'.join(values) if values else ''\n    \n    def _expand_path_parameters(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Path-style parameter expansion (;)\"\"\"\n        result = []\n        for spec in var_specs:\n            name = spec['name']\n            if name in variables:\n                value = variables[name]\n                if value is None or (isinstance(value, str) and not value):\n                    continue\n                \n                if spec['modifier'] == '*' and isinstance(value, dict):\n                    # Explode associative array\n                    pairs = []\n                    for k, v in value.items():\n                        pairs.append(f\"{self._encode_value(k)}={self._encode_value(v)}\")\n                    result.append(';' + ';'.join(pairs))\n                elif spec['modifier'] == '*' and isinstance(value, list):\n                    # Explode list\n                    encoded_values = [self._encode_value(v) for v in value]\n                    result.append(';' + f';{name}='.join([''] + encoded_values))\n                else:\n                    encoded_value = self._get_variable_value(spec, variables)\n                    if encoded_value:\n                        result.append(f';{name}={encoded_value}')\n        \n        return ''.join(result)\n    \n    def _expand_query(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Query expansion (?)\"\"\"\n        pairs = self._build_query_pairs(var_specs, variables)\n        return '?' + '&'.join(pairs) if pairs else ''\n    \n    def _expand_query_continuation(self, var_specs: List[Dict], variables: Dict[str, Any]) -> str:\n        \"\"\"Query continuation expansion (&)\"\"\"\n        pairs = self._build_query_pairs(var_specs, variables)\n        return '&' + '&'.join(pairs) if pairs else ''\n    \n    def _build_query_pairs(self, var_specs: List[Dict], variables: Dict[str, Any]) -> List[str]:\n        \"\"\"Build query parameter pairs\"\"\"\n        pairs = []\n        for spec in var_specs:\n            name = spec['name']\n            if name in variables:\n                value = variables[name]\n                if value is None:\n                    continue\n                \n                if spec['modifier'] == '*' and isinstance(value, dict):\n                    # Explode associative array\n                    for k, v in value.items():\n                        pairs.append(f\"{self._encode_value(k)}={self._encode_value(v)}\")\n                elif spec['modifier'] == '*' and isinstance(value, list):\n                    # Explode list\n                    for v in value:\n                        pairs.append(f\"{name}={self._encode_value(v)}\")", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "re", "typing"], "package_breakdown": {"install_packages": ["urllib", "re", "typing"], "import_packages": ["urllib", "re", "typing"], "llm_suggested": ["urllib", "typing", "re"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that loads data from a JSON-like format file and dumps it into typed data structures using the package's functionalities.", "prompt_index": 199, "generated_code": "```python\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport json\nfrom dacite import from_dict\n\n# Define typed data structures\n@dataclass\nclass Address:\n    street: str\n    city: str\n    zip_code: str\n    country: str\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    email: str\n    address: Address\n    phone: Optional[str] = None\n\n@dataclass\nclass Company:\n    name: str\n    employees: List[Person]\n    founded_year: int\n\n# Load JSON data\ndef load_and_convert_data(json_file_path: str) -> Company:\n    with open(json_file_path, 'r') as file:\n        json_data = json.load(file)\n    \n    # Convert JSON to typed data structures\n    company = from_dict(data_class=Company, data=json_data)\n    return company\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample JSON file content (save as 'company_data.json')\n    sample_data = {\n        \"name\": \"Tech Corp\",\n        \"founded_year\": 2010,\n        \"employees\": [\n            {\n                \"name\": \"John Doe\",\n                \"age\": 30,\n                \"email\": \"john@techcorp.com\",\n                \"phone\": \"123-456-7890\",\n                \"address\": {\n                    \"street\": \"123 Main St\",\n                    \"city\": \"San Francisco\",\n                    \"zip_code\": \"94105\",\n                    \"country\": \"USA\"\n                }\n            },\n            {\n                \"name\": \"Jane Smith\",\n                \"age\": 28,\n                \"email\": \"jane@techcorp.com\",\n                \"address\": {\n                    \"street\": \"456 Oak Ave\",\n                    \"city\": \"New York\",\n                    \"zip_code\": \"10001\",\n                    \"country\": \"USA\"\n                }\n            }\n        ]\n    }\n    \n    # Save sample data to file\n    with open('company_data.json', 'w') as f:\n        json.dump(sample_data, f, indent=2)\n    \n    # Load and convert data\n    company = load_and_convert_data('company_data.json')\n    \n    # Access typed data\n    print(f\"Company: {company.name}\")\n    print(f\"Founded: {company.founded_year}\")\n    print(f\"Employees: {len(company.employees)}\")\n    \n    for employee in company.employees:\n        print(f\"- {employee.name} ({employee.age}) - {employee.email}\")\n        print(f\"  Address: {employee.address.street}, {employee.address.city}\")\n```\n\nAlternative using Pydantic:\n\n```python\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport json\n\n# Define typed data structures with Pydantic\nclass Address(BaseModel):\n    street: str\n    city: str\n    zip_code: str\n    country: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n    address: Address\n    phone: Optional[str] = None\n\nclass Company(BaseModel):\n    name: str\n    employees: List[Person]\n    founded_year: int\n\n# Load and convert JSON data\ndef load_and_convert_pydantic(json_file_path: str) -> Company:\n    with open(json_file_path, 'r') as file:\n        json_data = json.load(file)\n    \n    # Pydantic automatically validates and converts\n    company = Company(**json_data)\n    return company\n\n# Usage\ncompany = load_and_convert_pydantic('company_data.json')\nprint(company.model_dump_json(indent=2))\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dataclasses", "json", "dacite", "pydantic", "typing"], "package_breakdown": {"install_packages": ["dataclasses", "json", "dacite", "pydantic", "typing"], "import_packages": ["dacite", "typing", "dataclasses", "json", "pydantic"], "llm_suggested": ["dacite", "pydantic"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sets up a web application to collect and propagate distributed traces using OpenTelemetry for monitoring and observability purposes.", "prompt_index": 200, "generated_code": "```python\nfrom flask import Flask, request, jsonify\nimport requests\nimport time\nimport os\nfrom opentelemetry import trace, baggage\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.propagate import inject, extract\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\n\n# Configure OpenTelemetry\ndef configure_tracer():\n    # Set up resource with service name\n    resource = Resource(attributes={\n        SERVICE_NAME: \"web-app-service\"\n    })\n    \n    # Configure tracer provider\n    trace.set_tracer_provider(TracerProvider(resource=resource))\n    tracer = trace.get_tracer(__name__)\n    \n    # Configure Jaeger exporter\n    jaeger_exporter = JaegerExporter(\n        agent_host_name=os.getenv(\"JAEGER_HOST\", \"localhost\"),\n        agent_port=int(os.getenv(\"JAEGER_PORT\", \"6831\")),\n    )\n    \n    # Add span processor\n    span_processor = BatchSpanProcessor(jaeger_exporter)\n    trace.get_tracer_provider().add_span_processor(span_processor)\n    \n    return tracer\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Configure OpenTelemetry\ntracer = configure_tracer()\n\n# Instrument Flask and Requests\nFlaskInstrumentor().instrument_app(app)\nRequestsInstrumentor().instrument()\n\n@app.route('/api/users/<user_id>')\ndef get_user(user_id):\n    \"\"\"Get user information with distributed tracing\"\"\"\n    \n    # Create a custom span\n    with tracer.start_as_current_span(\"get_user_operation\") as span:\n        # Add attributes to span\n        span.set_attribute(\"user.id\", user_id)\n        span.set_attribute(\"operation.type\", \"user_lookup\")\n        \n        try:\n            # Simulate database call\n            user_data = fetch_user_from_db(user_id)\n            \n            # Make downstream service call\n            profile_data = fetch_user_profile(user_id)\n            \n            # Combine data\n            result = {\n                \"user\": user_data,\n                \"profile\": profile_data,\n                \"timestamp\": time.time()\n            }\n            \n            span.set_attribute(\"response.status\", \"success\")\n            span.set_attribute(\"response.size\", len(str(result)))\n            \n            return jsonify(result)\n            \n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return jsonify({\"error\": str(e)}), 500\n\ndef fetch_user_from_db(user_id):\n    \"\"\"Simulate database operation with tracing\"\"\"\n    \n    with tracer.start_as_current_span(\"database_query\") as span:\n        span.set_attribute(\"db.operation\", \"SELECT\")\n        span.set_attribute(\"db.table\", \"users\")\n        span.set_attribute(\"db.user_id\", user_id)\n        \n        # Simulate database delay\n        time.sleep(0.1)\n        \n        # Mock user data\n        user_data = {\n            \"id\": user_id,\n            \"name\": f\"User {user_id}\",\n            \"email\": f\"user{user_id}@example.com\"\n        }\n        \n        span.set_attribute(\"db.rows_affected\", 1)\n        return user_data\n\ndef fetch_user_profile(user_id):\n    \"\"\"Make downstream service call with trace propagation\"\"\"\n    \n    with tracer.start_as_current_span(\"downstream_service_call\") as span:\n        span.set_attribute(\"service.name\", \"profile-service\")\n        span.set_attribute(\"user.id\", user_id)\n        \n        # Prepare headers for trace propagation\n        headers = {}\n        inject(headers)\n        \n        try:\n            # Mock downstream service URL\n            url = f\"http://profile-service:8080/profiles/{user_id}\"\n            \n            # In real scenario, this would be an actual HTTP call\n            # response = requests.get(url, headers=headers, timeout=5)\n            \n            # Mock response for demo\n            time.sleep(0.05)  # Simulate network delay\n            profile_data = {\n                \"user_id\": user_id,\n                \"bio\": f\"Bio for user {user_id}\",\n                \"preferences\": {\"theme\": \"dark\", \"notifications\": True}\n            }\n            \n            span.set_attribute(\"http.status_code\", 200)\n            span.set_attribute(\"http.url\", url)\n            \n            return profile_data\n            \n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            raise\n\n@app.route('/api/health')\ndef health_check():\n    \"\"\"Health check endpoint\"\"\"\n    \n    with tracer.start_as_current_span(\"health_check\") as span:\n        span.set_attribute(\"endpoint\", \"/api/health\")\n        \n        health_status = {\n            \"status\": \"healthy\",\n            \"timestamp\": time.time(),\n            \"service\": \"web-app-service\"\n        }\n        \n        return jsonify(health_status)\n\n@app.route('/api/process-order', methods=['POST'])\ndef process_order():\n    \"\"\"Process order with baggage propagation\"\"\"\n    \n    # Extract baggage from incoming request\n    ctx = extract(request.headers)\n    \n    with tracer.start_as_current_span(\"process_order\", context=ctx) as span:\n        order_data = request.get_json()\n        \n        # Set baggage for downstream services\n        ctx = baggage.set_baggage(\"customer.tier\", \"premium\", ctx)\n        ctx = baggage.set_baggage(\"order.priority\", \"high\", ctx)\n        \n        span.set_attribute(\"order.id\", order_data.get(\"order_id\"))\n        span.set_attribute(\"order.amount\", order_data.get(\"amount\", 0))\n        \n        # Process order steps\n        validate_order(order_data, ctx)\n        reserve_inventory(order_data, ctx)\n        process_payment(order_data, ctx)\n        \n        result = {\n            \"order_id\": order_data.get(\"order_id\"),\n            \"status\": \"processed\",\n            \"timestamp\": time.time()\n        }\n        \n        return jsonify(result)\n\ndef validate_order(order_data, ctx=None):\n    \"\"\"Validate order with context propagation\"\"\"\n    \n    with tracer.start_as_current_span(\"validate_order\", context=ctx) as span:\n        # Access baggage\n        customer_tier = baggage.get_baggage(\"customer.tier\", ctx)\n        span.set_attribute(\"customer.tier\", customer_tier or \"standard\")\n        \n        # Simulate validation\n        time.sleep(0.02)\n        span.set_attribute(\"validation.result\", \"passed\")\n\ndef reserve_inventory(order_data, ctx=None):\n    \"\"\"Reserve inventory with context propagation\"\"\"\n    \n    with tracer.start_as_current_span(\"reserve_inventory\", context=ctx) as span:\n        order_priority = baggage.get_baggage(\"order.priority\", ctx)\n        span.set_attribute(\"order.priority\", order_priority or \"normal\")\n        \n        # Simulate inventory reservation\n        time.sleep(0.03)\n        span.set_attribute(\"inventory.reserved\", True)\n\ndef process_payment(order_data, ctx=None):\n    \"\"\"Process payment with context propagation\"\"\"\n    \n    with tracer.start_as_current_span(\"process_payment\", context=ctx) as span:\n        span.set_attribute(\"payment.amount\", order_data.get(\"amount\", 0))\n        span.set_attribute(\"payment.method\", order_data.get(\"payment_method\", \"card\"))\n        \n        # Simulate payment processing\n        time.sleep(0.05", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 10, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "time", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests", "os"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-sdk", "flask", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "time", "opentelemetry-api", "opentelemetry-instrumentation-requests", "requests", "os"], "import_packages": ["opentelemetry", "flask", "time", "requests", "os"], "llm_suggested": ["flask", "requests", "opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-jaeger-thrift", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to Google BigQuery API, retrieves data from a specific dataset and table, and performs a simple query to aggregate and summarize the data before storing the result in a new table.", "prompt_index": 201, "generated_code": "```python\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nimport pandas as pd\n\n# Initialize BigQuery client\n# Option 1: Using service account key file\n# credentials = service_account.Credentials.from_service_account_file('path/to/service-account-key.json')\n# client = bigquery.Client(credentials=credentials, project='your-project-id')\n\n# Option 2: Using default credentials (if running on GCP or with gcloud auth)\nclient = bigquery.Client(project='your-project-id')\n\n# Define source and destination details\nsource_dataset = 'your_source_dataset'\nsource_table = 'your_source_table'\ndestination_dataset = 'your_destination_dataset'\ndestination_table = 'aggregated_results'\n\n# SQL query to aggregate and summarize data\nquery = f\"\"\"\nSELECT \n    category,\n    COUNT(*) as total_records,\n    AVG(amount) as avg_amount,\n    SUM(amount) as total_amount,\n    MIN(date) as earliest_date,\n    MAX(date) as latest_date\nFROM `{client.project}.{source_dataset}.{source_table}`\nWHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\nGROUP BY category\nORDER BY total_amount DESC\n\"\"\"\n\n# Execute query and get results\nprint(\"Executing query...\")\nquery_job = client.query(query)\nresults = query_job.result()\n\n# Convert to DataFrame for easier manipulation\ndf = results.to_dataframe()\nprint(f\"Query returned {len(df)} rows\")\nprint(df.head())\n\n# Define destination table reference\ndestination_table_ref = client.dataset(destination_dataset).table(destination_table)\n\n# Configure job settings\njob_config = bigquery.LoadJobConfig(\n    write_disposition=\"WRITE_TRUNCATE\",  # Overwrite table if exists\n    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n)\n\n# Load DataFrame to BigQuery table\nprint(f\"Loading results to {destination_dataset}.{destination_table}...\")\njob = client.load_table_from_dataframe(df, destination_table_ref, job_config=job_config)\njob.result()  # Wait for job to complete\n\nprint(f\"Successfully loaded {job.output_rows} rows to {destination_table}\")\n\n# Alternative: Direct query-to-table approach (more efficient for large datasets)\ndef create_table_from_query():\n    destination_table_id = f\"{client.project}.{destination_dataset}.{destination_table}_direct\"\n    \n    job_config = bigquery.QueryJobConfig(\n        destination=destination_table_id,\n        write_disposition=\"WRITE_TRUNCATE\"\n    )\n    \n    query_job = client.query(query, job_config=job_config)\n    query_job.result()  # Wait for job to complete\n    \n    print(f\"Created table {destination_table_id} with {query_job.num_dml_affected_rows} rows\")\n\n# Uncomment to use direct query-to-table approach\n# create_table_from_query()\n\n# Verify the new table\nverify_query = f\"\"\"\nSELECT COUNT(*) as row_count \nFROM `{client.project}.{destination_dataset}.{destination_table}`\n\"\"\"\nverify_result = client.query(verify_query).result()\nfor row in verify_result:\n    print(f\"New table contains {row.row_count} rows\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-bigquery", "google-auth", "pandas"], "package_breakdown": {"install_packages": ["google", "google-cloud-bigquery", "google-auth", "pandas"], "import_packages": ["google", "pandas"], "llm_suggested": ["google-cloud-bigquery", "google-auth", "pandas"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that parses a given SQL query without performing validation checks.", "prompt_index": 202, "generated_code": "```python\nimport re\nfrom typing import Dict, List, Optional, Union\n\nclass SQLParser:\n    def __init__(self):\n        self.keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'HAVING', 'ORDER BY', 'LIMIT', 'OFFSET']\n    \n    def parse(self, query: str) -> Dict[str, Union[str, List[str]]]:\n        \"\"\"Parse SQL query and extract components\"\"\"\n        query = query.strip().rstrip(';')\n        result = {}\n        \n        # Extract SELECT clause\n        select_match = re.search(r'SELECT\\s+(.*?)\\s+FROM', query, re.IGNORECASE | re.DOTALL)\n        if select_match:\n            result['SELECT'] = [col.strip() for col in select_match.group(1).split(',')]\n        \n        # Extract FROM clause\n        from_match = re.search(r'FROM\\s+([^\\s]+)', query, re.IGNORECASE)\n        if from_match:\n            result['FROM'] = from_match.group(1)\n        \n        # Extract WHERE clause\n        where_match = re.search(r'WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)', query, re.IGNORECASE | re.DOTALL)\n        if where_match:\n            result['WHERE'] = where_match.group(1).strip()\n        \n        # Extract GROUP BY clause\n        group_by_match = re.search(r'GROUP\\s+BY\\s+(.*?)(?:\\s+HAVING|\\s+ORDER\\s+BY|\\s+LIMIT|$)', query, re.IGNORECASE | re.DOTALL)\n        if group_by_match:\n            result['GROUP BY'] = [col.strip() for col in group_by_match.group(1).split(',')]\n        \n        # Extract HAVING clause\n        having_match = re.search(r'HAVING\\s+(.*?)(?:\\s+ORDER\\s+BY|\\s+LIMIT|$)', query, re.IGNORECASE | re.DOTALL)\n        if having_match:\n            result['HAVING'] = having_match.group(1).strip()\n        \n        # Extract ORDER BY clause\n        order_by_match = re.search(r'ORDER\\s+BY\\s+(.*?)(?:\\s+LIMIT|$)', query, re.IGNORECASE | re.DOTALL)\n        if order_by_match:\n            result['ORDER BY'] = [col.strip() for col in order_by_match.group(1).split(',')]\n        \n        # Extract LIMIT clause\n        limit_match = re.search(r'LIMIT\\s+(\\d+)', query, re.IGNORECASE)\n        if limit_match:\n            result['LIMIT'] = int(limit_match.group(1))\n        \n        # Extract OFFSET clause\n        offset_match = re.search(r'OFFSET\\s+(\\d+)', query, re.IGNORECASE)\n        if offset_match:\n            result['OFFSET'] = int(offset_match.group(1))\n        \n        return result\n\n    def extract_tables(self, query: str) -> List[str]:\n        \"\"\"Extract table names from query\"\"\"\n        tables = []\n        \n        # FROM clause tables\n        from_match = re.search(r'FROM\\s+(.*?)(?:\\s+WHERE|\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)', query, re.IGNORECASE | re.DOTALL)\n        if from_match:\n            from_clause = from_match.group(1)\n            # Handle JOINs\n            join_pattern = r'(?:LEFT|RIGHT|INNER|OUTER)?\\s*JOIN\\s+([^\\s]+)'\n            joins = re.findall(join_pattern, from_clause, re.IGNORECASE)\n            tables.extend(joins)\n            \n            # Main table (remove JOIN parts)\n            main_table = re.sub(r'\\s+(?:LEFT|RIGHT|INNER|OUTER)?\\s*JOIN.*', '', from_clause, flags=re.IGNORECASE)\n            tables.insert(0, main_table.strip())\n        \n        return [table.strip() for table in tables if table.strip()]\n\n    def extract_columns(self, query: str) -> List[str]:\n        \"\"\"Extract column names from SELECT clause\"\"\"\n        select_match = re.search(r'SELECT\\s+(.*?)\\s+FROM', query, re.IGNORECASE | re.DOTALL)\n        if select_match:\n            columns_str = select_match.group(1)\n            columns = [col.strip() for col in columns_str.split(',')]\n            return columns\n        return []\n\n# Usage example\nparser = SQLParser()\n\n# Example queries\nqueries = [\n    \"SELECT id, name, email FROM users WHERE age > 18 ORDER BY name\",\n    \"SELECT COUNT(*) FROM orders WHERE status = 'completed' GROUP BY customer_id\",\n    \"SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id WHERE o.date > '2023-01-01' LIMIT 10\"\n]\n\nfor query in queries:\n    parsed = parser.parse(query)\n    print(f\"Query: {query}\")\n    print(f\"Parsed: {parsed}\")\n    print(f\"Tables: {parser.extract_tables(query)}\")\n    print(f\"Columns: {parser.extract_columns(query)}\")\n    print(\"-\" * 50)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "typing"], "package_breakdown": {"install_packages": ["re", "typing"], "import_packages": ["re", "typing"], "llm_suggested": ["re", "typing"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that retrieves metadata information (such as name, version, dependencies) from a specific package installed on the system.", "prompt_index": 203, "generated_code": "```python\nimport pkg_resources\nimport importlib.metadata\n\ndef get_package_metadata(package_name):\n    \"\"\"Get metadata for an installed package using multiple methods\"\"\"\n    \n    print(f\"=== Metadata for package: {package_name} ===\\n\")\n    \n    # Method 1: Using importlib.metadata (Python 3.8+)\n    try:\n        print(\"--- Using importlib.metadata ---\")\n        metadata = importlib.metadata.metadata(package_name)\n        \n        print(f\"Name: {metadata['Name']}\")\n        print(f\"Version: {metadata['Version']}\")\n        print(f\"Summary: {metadata.get('Summary', 'N/A')}\")\n        print(f\"Author: {metadata.get('Author', 'N/A')}\")\n        print(f\"License: {metadata.get('License', 'N/A')}\")\n        print(f\"Home Page: {metadata.get('Home-page', 'N/A')}\")\n        \n        # Get dependencies\n        requires = importlib.metadata.requires(package_name)\n        if requires:\n            print(f\"Dependencies: {', '.join(requires)}\")\n        else:\n            print(\"Dependencies: None\")\n            \n        # Get files\n        try:\n            files = importlib.metadata.files(package_name)\n            if files:\n                print(f\"Number of files: {len(files)}\")\n        except:\n            print(\"Files: Unable to retrieve\")\n            \n    except importlib.metadata.PackageNotFoundError:\n        print(f\"Package '{package_name}' not found using importlib.metadata\")\n    except Exception as e:\n        print(f\"Error with importlib.metadata: {e}\")\n    \n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    # Method 2: Using pkg_resources\n    try:\n        print(\"--- Using pkg_resources ---\")\n        dist = pkg_resources.get_distribution(package_name)\n        \n        print(f\"Name: {dist.project_name}\")\n        print(f\"Version: {dist.version}\")\n        print(f\"Location: {dist.location}\")\n        print(f\"Platform: {dist.platform}\")\n        \n        # Get requirements\n        if dist.requires():\n            print(f\"Requirements: {[str(req) for req in dist.requires()]}\")\n        else:\n            print(\"Requirements: None\")\n            \n        # Get metadata from PKG-INFO\n        if hasattr(dist, 'get_metadata'):\n            try:\n                pkg_info = dist.get_metadata('PKG-INFO')\n                print(f\"PKG-INFO available: Yes\")\n            except:\n                print(f\"PKG-INFO available: No\")\n                \n    except pkg_resources.DistributionNotFound:\n        print(f\"Package '{package_name}' not found using pkg_resources\")\n    except Exception as e:\n        print(f\"Error with pkg_resources: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example with a common package\n    get_package_metadata(\"requests\")\n    \n    # To check a specific package, replace \"requests\" with the package name\n    # get_package_metadata(\"numpy\")\n    # get_package_metadata(\"pandas\")\n```\n\n```python\n# Alternative simpler version for quick metadata lookup\ndef quick_package_info(package_name):\n    \"\"\"Quick package information lookup\"\"\"\n    try:\n        import importlib.metadata as metadata\n        \n        info = metadata.metadata(package_name)\n        requires = metadata.requires(package_name) or []\n        \n        return {\n            'name': info['Name'],\n            'version': info['Version'],\n            'summary': info.get('Summary', ''),\n            'author': info.get('Author', ''),\n            'dependencies': requires,\n            'home_page': info.get('Home-page', '')\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\n# Usage example\npackage_info = quick_package_info(\"requests\")\nprint(package_info)\n```\n\n```python\n# List all installed packages with basic info\ndef list_all_packages():\n    \"\"\"List all installed packages with basic metadata\"\"\"\n    import importlib.metadata\n    \n    packages = []\n    for dist in importlib.metadata.distributions():\n        packages.append({\n            'name': dist.metadata['Name'],\n            'version': dist.metadata['Version'],\n            'summary': dist.metadata.get('Summary', '')[:100] + '...' if dist.metadata.get('Summary', '') else ''\n        })\n    \n    return sorted(packages, key=lambda x: x['name'].lower())\n\n# Usage\nall_packages = list_all_packages()\nfor pkg in all_packages[:10]:  # Show first 10 packages\n    print(f\"{pkg['name']} ({pkg['version']}): {pkg['summary']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 2, "hallucinations": ["importlib.metadata", "pkg_resources"], "packages_detected": ["importlib.metadata", "importlib", "pkg_resources"], "package_breakdown": {"install_packages": ["importlib.metadata", "importlib", "pkg_resources"], "import_packages": ["importlib", "pkg_resources"], "llm_suggested": ["pkg_resources", "importlib.metadata"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that utilizes IPython to create an interactive computing environment for data analysis and visualization, including running code snippets, displaying results, and facilitating data exploration.", "prompt_index": 204, "generated_code": "```python\n# Install required packages (run in terminal or notebook cell)\n# !pip install ipython jupyter matplotlib pandas numpy seaborn plotly\n\nimport IPython\nfrom IPython.display import display, HTML, Markdown, Image\nfrom IPython.core.magic import register_line_magic, register_cell_magic\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import iplot, init_notebook_mode\n\n# Initialize IPython environment\ndef setup_interactive_environment():\n    \"\"\"Set up IPython for interactive data analysis\"\"\"\n    # Enable inline plotting\n    get_ipython().run_line_magic('matplotlib', 'inline')\n    \n    # Set up plotly for notebooks\n    init_notebook_mode(connected=True)\n    \n    # Configure pandas display options\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', 100)\n    pd.set_option('display.width', None)\n    pd.set_option('display.max_colwidth', 100)\n    \n    # Set matplotlib style\n    plt.style.use('seaborn-v0_8')\n    sns.set_palette(\"husl\")\n\n# Custom magic commands for data analysis\n@register_line_magic\ndef quick_stats(line):\n    \"\"\"Magic command to quickly display dataset statistics\"\"\"\n    var_name = line.strip()\n    if var_name in globals():\n        df = globals()[var_name]\n        if isinstance(df, pd.DataFrame):\n            display(HTML(f\"<h3>Quick Stats for {var_name}</h3>\"))\n            display(df.describe())\n            display(HTML(f\"<p><strong>Shape:</strong> {df.shape}</p>\"))\n            display(HTML(f\"<p><strong>Missing Values:</strong></p>\"))\n            display(df.isnull().sum())\n        else:\n            print(f\"{var_name} is not a DataFrame\")\n    else:\n        print(f\"Variable {var_name} not found\")\n\n@register_cell_magic\ndef analyze_column(line, cell):\n    \"\"\"Magic command to analyze a specific column\"\"\"\n    parts = line.strip().split()\n    df_name, col_name = parts[0], parts[1]\n    \n    if df_name in globals():\n        df = globals()[df_name]\n        if col_name in df.columns:\n            display(HTML(f\"<h3>Analysis for {df_name}['{col_name}']</h3>\"))\n            \n            # Basic stats\n            display(HTML(\"<h4>Basic Statistics:</h4>\"))\n            if df[col_name].dtype in ['int64', 'float64']:\n                display(df[col_name].describe())\n            else:\n                display(df[col_name].value_counts().head(10))\n            \n            # Execute additional analysis code from cell\n            exec(cell)\n\n# Interactive data exploration class\nclass DataExplorer:\n    def __init__(self, dataframe, name=\"dataset\"):\n        self.df = dataframe\n        self.name = name\n        \n    def overview(self):\n        \"\"\"Display comprehensive dataset overview\"\"\"\n        display(HTML(f\"<h2>Dataset Overview: {self.name}</h2>\"))\n        \n        # Basic info\n        display(HTML(\"<h3>Basic Information</h3>\"))\n        display(HTML(f\"\"\"\n        <ul>\n            <li><strong>Shape:</strong> {self.df.shape[0]} rows \u00d7 {self.df.shape[1]} columns</li>\n            <li><strong>Memory Usage:</strong> {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB</li>\n            <li><strong>Duplicate Rows:</strong> {self.df.duplicated().sum()}</li>\n        </ul>\n        \"\"\"))\n        \n        # Data types\n        display(HTML(\"<h3>Data Types</h3>\"))\n        dtype_counts = self.df.dtypes.value_counts()\n        display(dtype_counts)\n        \n        # Missing values\n        display(HTML(\"<h3>Missing Values</h3>\"))\n        missing = self.df.isnull().sum()\n        missing_pct = (missing / len(self.df)) * 100\n        missing_df = pd.DataFrame({\n            'Missing Count': missing,\n            'Percentage': missing_pct\n        }).sort_values('Missing Count', ascending=False)\n        display(missing_df[missing_df['Missing Count'] > 0])\n        \n        # Sample data\n        display(HTML(\"<h3>Sample Data</h3>\"))\n        display(self.df.head())\n        \n    def plot_missing_values(self):\n        \"\"\"Visualize missing values pattern\"\"\"\n        if self.df.isnull().sum().sum() > 0:\n            plt.figure(figsize=(12, 6))\n            sns.heatmap(self.df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n            plt.title(f'Missing Values Pattern - {self.name}')\n            plt.tight_layout()\n            plt.show()\n        else:\n            display(HTML(\"<p>No missing values found in the dataset.</p>\"))\n    \n    def correlation_matrix(self, method='pearson'):\n        \"\"\"Display interactive correlation matrix\"\"\"\n        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n        if len(numeric_cols) > 1:\n            corr = self.df[numeric_cols].corr(method=method)\n            \n            # Plotly heatmap\n            fig = go.Figure(data=go.Heatmap(\n                z=corr.values,\n                x=corr.columns,\n                y=corr.columns,\n                colorscale='RdBu',\n                zmid=0\n            ))\n            fig.update_layout(\n                title=f'Correlation Matrix - {self.name}',\n                width=800,\n                height=600\n            )\n            fig.show()\n        else:\n            display(HTML(\"<p>Not enough numeric columns for correlation analysis.</p>\"))\n    \n    def distribution_plots(self, columns=None, max_cols=4):\n        \"\"\"Create distribution plots for numeric columns\"\"\"\n        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n        if columns:\n            numeric_cols = [col for col in columns if col in numeric_cols]\n        \n        if len(numeric_cols) == 0:\n            display(HTML(\"<p>No numeric columns found.</p>\"))\n            return\n        \n        n_cols = min(max_cols, len(numeric_cols))\n        n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n        \n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n        if n_rows == 1 and n_cols == 1:\n            axes = [axes]\n        elif n_rows == 1:\n            axes = axes\n        else:\n            axes = axes.flatten()\n        \n        for i, col in enumerate(numeric_cols):\n            if i < len(axes):\n                self.df[col].hist(bins=30, ax=axes[i], alpha=0.7)\n                axes[i].set_title(f'Distribution of {col}')\n                axes[i].set_xlabel(col)\n                axes[i].set_ylabel('Frequency')\n        \n        # Hide empty subplots\n        for i in range(len(numeric_cols), len(axes)):\n            axes[i].set_visible(False)\n        \n        plt.tight_layout()\n        plt.show()\n\n# Interactive visualization functions\ndef interactive_scatter(df, x, y, color=None, size=None, title=None):\n    \"\"\"Create interactive scatter plot\"\"\"\n    fig = px.scatter(\n        df, x=x, y=y, color=color, size=size,\n        title=title or f'{y} vs {x}',\n        hover_data=df.columns\n    )\n    fig.update_layout(height=500)\n    fig.show()\n\ndef interactive_histogram(df, column, bins=30, color=None, title=", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["IPython", "plotly", "pandas", "numpy", "seaborn", "matplotlib"], "package_breakdown": {"install_packages": ["IPython", "plotly", "pandas", "numpy", "seaborn", "matplotlib"], "import_packages": ["IPython", "plotly", "pandas", "numpy", "seaborn", "matplotlib"], "llm_suggested": ["IPython", "pandas", "numpy", "matplotlib", "seaborn", "plotly"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that converts a nested dictionary containing various datatypes (such as lists, tuples, dictionaries, and sets) into native Python datatypes, and then converts it back to the original complex datatype structure.", "prompt_index": 205, "generated_code": "```python\nimport json\nfrom collections import defaultdict\n\ndef convert_to_native(obj):\n    \"\"\"Convert nested dictionary with complex datatypes to native Python types\"\"\"\n    if isinstance(obj, dict):\n        result = {}\n        for key, value in obj.items():\n            result[key] = convert_to_native(value)\n        return result\n    elif isinstance(obj, list):\n        return [convert_to_native(item) for item in obj]\n    elif isinstance(obj, tuple):\n        return {\"__type__\": \"tuple\", \"__data__\": [convert_to_native(item) for item in obj]}\n    elif isinstance(obj, set):\n        return {\"__type__\": \"set\", \"__data__\": [convert_to_native(item) for item in obj]}\n    elif isinstance(obj, frozenset):\n        return {\"__type__\": \"frozenset\", \"__data__\": [convert_to_native(item) for item in obj]}\n    else:\n        return obj\n\ndef convert_from_native(obj):\n    \"\"\"Convert native Python types back to original complex datatypes\"\"\"\n    if isinstance(obj, dict):\n        if \"__type__\" in obj and \"__data__\" in obj:\n            data = [convert_from_native(item) for item in obj[\"__data__\"]]\n            if obj[\"__type__\"] == \"tuple\":\n                return tuple(data)\n            elif obj[\"__type__\"] == \"set\":\n                return set(data)\n            elif obj[\"__type__\"] == \"frozenset\":\n                return frozenset(data)\n        else:\n            result = {}\n            for key, value in obj.items():\n                result[key] = convert_from_native(value)\n            return result\n    elif isinstance(obj, list):\n        return [convert_from_native(item) for item in obj]\n    else:\n        return obj\n\n# Example usage\noriginal_data = {\n    'name': 'example',\n    'numbers': [1, 2, 3],\n    'coordinates': (10, 20, 30),\n    'unique_items': {1, 2, 3, 4},\n    'frozen_data': frozenset(['a', 'b', 'c']),\n    'nested': {\n        'inner_list': [4, 5, 6],\n        'inner_tuple': (7, 8, 9),\n        'inner_set': {'x', 'y', 'z'}\n    }\n}\n\n# Convert to native types\nnative_data = convert_to_native(original_data)\nprint(\"Native data:\")\nprint(json.dumps(native_data, indent=2))\n\n# Convert back to original structure\nrestored_data = convert_from_native(native_data)\nprint(\"\\nRestored data:\")\nprint(restored_data)\n\n# Verify restoration\nprint(\"\\nVerification:\")\nprint(f\"Original tuple type: {type(original_data['coordinates'])}\")\nprint(f\"Restored tuple type: {type(restored_data['coordinates'])}\")\nprint(f\"Original set type: {type(original_data['unique_items'])}\")\nprint(f\"Restored set type: {type(restored_data['unique_items'])}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections", "json"], "package_breakdown": {"install_packages": ["collections", "json"], "import_packages": ["collections", "json"], "llm_suggested": ["json", "collections"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that reads a YAML file, parses its content using the C version of the ruamel.yaml library derived from libyaml, and then emits the parsed data back into a new YAML file.", "prompt_index": 206, "generated_code": "```python\nfrom ruamel.yaml import YAML\n\n# Initialize YAML parser with C loader/dumper\nyaml = YAML()\nyaml.preserve_quotes = True\nyaml.width = 4096\n\n# Read and parse YAML file\nwith open('input.yaml', 'r') as file:\n    data = yaml.load(file)\n\n# Write parsed data to new YAML file\nwith open('output.yaml', 'w') as file:\n    yaml.dump(data, file)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["ruamel"], "packages_detected": ["ruamel", "ruamel.yaml"], "package_breakdown": {"install_packages": ["ruamel", "ruamel.yaml"], "import_packages": ["ruamel"], "llm_suggested": ["ruamel.yaml"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses functools to create a memoized function that caches the results of a slow computation, improving performance by storing previously computed values.", "prompt_index": 207, "generated_code": "```python\nimport functools\nimport time\n\n# Example 1: Basic memoization with @lru_cache\n@functools.lru_cache(maxsize=128)\ndef fibonacci(n):\n    \"\"\"Compute fibonacci number with memoization\"\"\"\n    if n < 2:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\n# Example 2: Memoizing a slow computation function\n@functools.lru_cache(maxsize=None)  # Unlimited cache size\ndef expensive_computation(x, y):\n    \"\"\"Simulate expensive computation with sleep\"\"\"\n    time.sleep(1)  # Simulate slow operation\n    return x ** 2 + y ** 2 + x * y\n\n# Example 3: Custom memoization decorator using functools.wraps\ndef memoize(func):\n    cache = {}\n    \n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Create a key from arguments\n        key = str(args) + str(sorted(kwargs.items()))\n        \n        if key not in cache:\n            cache[key] = func(*args, **kwargs)\n        return cache[key]\n    \n    # Add cache inspection methods\n    wrapper.cache = cache\n    wrapper.cache_clear = lambda: cache.clear()\n    wrapper.cache_info = lambda: f\"Cache size: {len(cache)}\"\n    \n    return wrapper\n\n@memoize\ndef complex_calculation(a, b, operation='multiply'):\n    \"\"\"Another slow computation example\"\"\"\n    time.sleep(0.5)\n    if operation == 'multiply':\n        return a * b\n    elif operation == 'power':\n        return a ** b\n    else:\n        return a + b\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Test fibonacci\n    print(\"Fibonacci results:\")\n    start = time.time()\n    result1 = fibonacci(35)\n    print(f\"First call: {result1} (took {time.time() - start:.2f}s)\")\n    \n    start = time.time()\n    result2 = fibonacci(35)\n    print(f\"Second call: {result2} (took {time.time() - start:.2f}s)\")\n    \n    # Check cache info\n    print(f\"Fibonacci cache info: {fibonacci.cache_info()}\")\n    \n    # Test expensive computation\n    print(\"\\nExpensive computation:\")\n    start = time.time()\n    result = expensive_computation(5, 3)\n    print(f\"First call result: {result} (took {time.time() - start:.2f}s)\")\n    \n    start = time.time()\n    result = expensive_computation(5, 3)\n    print(f\"Second call result: {result} (took {time.time() - start:.2f}s)\")\n    \n    # Test custom memoization\n    print(\"\\nCustom memoized function:\")\n    start = time.time()\n    result = complex_calculation(4, 3, 'power')\n    print(f\"First call: {result} (took {time.time() - start:.2f}s)\")\n    \n    start = time.time()\n    result = complex_calculation(4, 3, 'power')\n    print(f\"Second call: {result} (took {time.time() - start:.2f}s)\")\n    \n    print(f\"Custom cache info: {complex_calculation.cache_info()}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "time"], "package_breakdown": {"install_packages": ["functools", "time"], "import_packages": ["functools", "time"], "llm_suggested": ["functools", "time"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that encodes data using the OpenTelemetry Protobuf format for sending telemetry data to a remote server.", "prompt_index": 208, "generated_code": "```python\nimport time\nfrom opentelemetry.proto.trace.v1 import trace_pb2\nfrom opentelemetry.proto.resource.v1 import resource_pb2\nfrom opentelemetry.proto.common.v1 import common_pb2\nfrom opentelemetry.proto.collector.trace.v1 import trace_service_pb2\nimport requests\n\ndef create_telemetry_data():\n    # Create resource attributes\n    resource_attributes = [\n        common_pb2.KeyValue(\n            key=\"service.name\",\n            value=common_pb2.AnyValue(string_value=\"my-python-service\")\n        ),\n        common_pb2.KeyValue(\n            key=\"service.version\",\n            value=common_pb2.AnyValue(string_value=\"1.0.0\")\n        )\n    ]\n    \n    # Create resource\n    resource = resource_pb2.Resource(attributes=resource_attributes)\n    \n    # Create span attributes\n    span_attributes = [\n        common_pb2.KeyValue(\n            key=\"http.method\",\n            value=common_pb2.AnyValue(string_value=\"GET\")\n        ),\n        common_pb2.KeyValue(\n            key=\"http.url\",\n            value=common_pb2.AnyValue(string_value=\"https://api.example.com/users\")\n        ),\n        common_pb2.KeyValue(\n            key=\"http.status_code\",\n            value=common_pb2.AnyValue(int_value=200)\n        )\n    ]\n    \n    # Create span\n    current_time_ns = int(time.time() * 1_000_000_000)\n    span = trace_pb2.Span(\n        trace_id=b'\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10',\n        span_id=b'\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08',\n        name=\"HTTP GET /users\",\n        kind=trace_pb2.Span.SPAN_KIND_CLIENT,\n        start_time_unix_nano=current_time_ns - 100_000_000,  # 100ms ago\n        end_time_unix_nano=current_time_ns,\n        attributes=span_attributes,\n        status=trace_pb2.Status(code=trace_pb2.Status.STATUS_CODE_OK)\n    )\n    \n    # Create instrumentation scope\n    scope = common_pb2.InstrumentationScope(\n        name=\"my-tracer\",\n        version=\"1.0.0\"\n    )\n    \n    # Create scope spans\n    scope_spans = trace_pb2.ScopeSpans(\n        scope=scope,\n        spans=[span]\n    )\n    \n    # Create resource spans\n    resource_spans = trace_pb2.ResourceSpans(\n        resource=resource,\n        scope_spans=[scope_spans]\n    )\n    \n    # Create export request\n    export_request = trace_service_pb2.ExportTraceServiceRequest(\n        resource_spans=[resource_spans]\n    )\n    \n    return export_request\n\ndef send_telemetry_data(endpoint_url, data):\n    \"\"\"Send protobuf-encoded telemetry data to remote server\"\"\"\n    headers = {\n        'Content-Type': 'application/x-protobuf',\n        'Content-Encoding': 'gzip'\n    }\n    \n    # Serialize to protobuf bytes\n    serialized_data = data.SerializeToString()\n    \n    # Optional: compress the data\n    import gzip\n    compressed_data = gzip.compress(serialized_data)\n    \n    try:\n        response = requests.post(\n            endpoint_url,\n            data=compressed_data,\n            headers=headers,\n            timeout=30\n        )\n        response.raise_for_status()\n        print(f\"Telemetry data sent successfully. Status: {response.status_code}\")\n        return True\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to send telemetry data: {e}\")\n        return False\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create telemetry data\n    telemetry_data = create_telemetry_data()\n    \n    # Send to OTLP endpoint (adjust URL as needed)\n    otlp_endpoint = \"http://localhost:4318/v1/traces\"\n    send_telemetry_data(otlp_endpoint, telemetry_data)\n    \n    # Print serialized data size for debugging\n    serialized = telemetry_data.SerializeToString()\n    print(f\"Serialized data size: {len(serialized)} bytes\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-proto", "time", "gzip", "opentelemetry", "requests"], "package_breakdown": {"install_packages": ["opentelemetry-proto", "time", "gzip", "opentelemetry", "requests"], "import_packages": ["opentelemetry", "gzip", "time", "requests"], "llm_suggested": ["opentelemetry-proto", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a tzinfo object for the local timezone and returns the current date and time in that timezone.", "prompt_index": 209, "generated_code": "```python\nfrom datetime import datetime, timezone\nimport time\n\n# Create tzinfo object for local timezone\nlocal_tz = timezone(timezone.utc.utcoffset(datetime.now()) or timezone.utc.utcoffset(datetime.utcnow()))\n\n# Alternative method using time module for more accurate local timezone\nlocal_offset = time.timezone if time.daylight == 0 else time.altzone\nlocal_tz = timezone(timezone.timedelta(seconds=-local_offset))\n\n# Get current date and time in local timezone\ncurrent_datetime = datetime.now(local_tz)\n\nprint(current_datetime)\n```\n\nOr using the more straightforward approach:\n\n```python\nfrom datetime import datetime\n\n# Get current date and time in local timezone\ncurrent_datetime = datetime.now().astimezone()\n\nprint(current_datetime)\nprint(f\"Timezone: {current_datetime.tzinfo}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "time"], "package_breakdown": {"install_packages": ["datetime", "time"], "import_packages": ["datetime", "time"], "llm_suggested": ["datetime", "time"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that creates a composite application using Language Model Modules (LLMs) to perform sentiment analysis on a given text input and generate a summary based on the analysis results.", "prompt_index": 210, "generated_code": "```python\nimport openai\nfrom transformers import pipeline\nimport json\nfrom typing import Dict, List, Tuple\n\nclass SentimentAnalysisLLM:\n    \"\"\"Language Model Module for sentiment analysis\"\"\"\n    \n    def __init__(self, model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n        self.sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name)\n    \n    def analyze_sentiment(self, text: str) -> Dict:\n        \"\"\"Analyze sentiment of input text\"\"\"\n        result = self.sentiment_analyzer(text)[0]\n        return {\n            'label': result['label'],\n            'confidence': result['score'],\n            'text': text\n        }\n\nclass TextSummarizerLLM:\n    \"\"\"Language Model Module for text summarization\"\"\"\n    \n    def __init__(self, api_key: str = None):\n        if api_key:\n            openai.api_key = api_key\n        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n    \n    def generate_summary(self, text: str, max_length: int = 130) -> str:\n        \"\"\"Generate summary of input text\"\"\"\n        if len(text.split()) < 50:\n            return text  # Return original if too short\n        \n        summary = self.summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n        return summary[0]['summary_text']\n\nclass CompositeAnalysisApp:\n    \"\"\"Composite application combining sentiment analysis and summarization\"\"\"\n    \n    def __init__(self, openai_api_key: str = None):\n        self.sentiment_module = SentimentAnalysisLLM()\n        self.summary_module = TextSummarizerLLM(openai_api_key)\n    \n    def process_text(self, text: str) -> Dict:\n        \"\"\"Process text through sentiment analysis and generate summary\"\"\"\n        # Perform sentiment analysis\n        sentiment_result = self.sentiment_module.analyze_sentiment(text)\n        \n        # Generate summary\n        summary = self.summary_module.generate_summary(text)\n        \n        # Analyze sentiment of summary as well\n        summary_sentiment = self.sentiment_module.analyze_sentiment(summary)\n        \n        return {\n            'original_text': text,\n            'original_sentiment': {\n                'label': sentiment_result['label'],\n                'confidence': round(sentiment_result['confidence'], 3)\n            },\n            'summary': summary,\n            'summary_sentiment': {\n                'label': summary_sentiment['label'],\n                'confidence': round(summary_sentiment['confidence'], 3)\n            },\n            'analysis_report': self._generate_analysis_report(sentiment_result, summary_sentiment)\n        }\n    \n    def _generate_analysis_report(self, original_sentiment: Dict, summary_sentiment: Dict) -> str:\n        \"\"\"Generate a comprehensive analysis report\"\"\"\n        report = f\"Analysis Report:\\n\"\n        report += f\"- Original text sentiment: {original_sentiment['label']} (confidence: {original_sentiment['confidence']:.3f})\\n\"\n        report += f\"- Summary sentiment: {summary_sentiment['label']} (confidence: {summary_sentiment['confidence']:.3f})\\n\"\n        \n        if original_sentiment['label'] == summary_sentiment['label']:\n            report += \"- Sentiment consistency: HIGH (original and summary have same sentiment)\\n\"\n        else:\n            report += \"- Sentiment consistency: LOW (sentiment changed during summarization)\\n\"\n        \n        return report\n    \n    def batch_process(self, texts: List[str]) -> List[Dict]:\n        \"\"\"Process multiple texts\"\"\"\n        return [self.process_text(text) for text in texts]\n\n# Example usage and testing\ndef main():\n    # Initialize the composite application\n    app = CompositeAnalysisApp()\n    \n    # Sample texts for testing\n    sample_texts = [\n        \"\"\"\n        I absolutely love this new smartphone! The camera quality is outstanding, \n        the battery life exceeds my expectations, and the user interface is incredibly \n        intuitive. After using it for a month, I can confidently say this is the best \n        phone I've ever owned. The customer service was also exceptional when I had \n        a minor question about the settings. I would definitely recommend this to \n        anyone looking for a premium mobile experience.\n        \"\"\",\n        \"\"\"\n        This restaurant was a complete disaster. The food arrived cold, the service \n        was incredibly slow, and the staff seemed completely disinterested in helping \n        customers. We waited over an hour for our meal, and when it finally arrived, \n        it was poorly prepared and tasted terrible. The prices were way too high for \n        the quality we received. I will never return to this establishment and would \n        strongly advise others to avoid it as well.\n        \"\"\",\n        \"\"\"\n        The weather today is quite pleasant with partly cloudy skies and a gentle breeze. \n        Temperature is around 72 degrees Fahrenheit, which makes it perfect for outdoor \n        activities. Many people are taking advantage of the nice conditions to go for \n        walks in the park or have picnics with their families.\n        \"\"\"\n    ]\n    \n    # Process each text\n    for i, text in enumerate(sample_texts, 1):\n        print(f\"\\n{'='*50}\")\n        print(f\"PROCESSING TEXT {i}\")\n        print(f\"{'='*50}\")\n        \n        result = app.process_text(text.strip())\n        \n        print(f\"\\nOriginal Text: {result['original_text'][:100]}...\")\n        print(f\"\\nSummary: {result['summary']}\")\n        print(f\"\\nOriginal Sentiment: {result['original_sentiment']}\")\n        print(f\"Summary Sentiment: {result['summary_sentiment']}\")\n        print(f\"\\n{result['analysis_report']}\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["transformers", "openai", "json", "typing", "torch"], "package_breakdown": {"install_packages": ["transformers", "openai", "json", "typing", "torch"], "import_packages": ["transformers", "json", "openai", "typing"], "llm_suggested": ["openai", "transformers", "torch"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sends an HTTP GET request to a specified URL and retrieves the response data using the comprehensive HTTP client library.", "prompt_index": 211, "generated_code": "```python\nimport requests\n\ndef send_get_request(url, headers=None, params=None, timeout=30):\n    \"\"\"\n    Send HTTP GET request and retrieve response data\n    \n    Args:\n        url (str): The URL to send the request to\n        headers (dict): Optional headers to include\n        params (dict): Optional query parameters\n        timeout (int): Request timeout in seconds\n    \n    Returns:\n        dict: Response data including status, headers, and content\n    \"\"\"\n    try:\n        response = requests.get(\n            url=url,\n            headers=headers,\n            params=params,\n            timeout=timeout\n        )\n        \n        # Raise an exception for bad status codes\n        response.raise_for_status()\n        \n        return {\n            'status_code': response.status_code,\n            'headers': dict(response.headers),\n            'content': response.text,\n            'json': response.json() if response.headers.get('content-type', '').startswith('application/json') else None,\n            'url': response.url\n        }\n        \n    except requests.exceptions.RequestException as e:\n        return {\n            'error': str(e),\n            'status_code': None\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Basic GET request\n    url = \"https://jsonplaceholder.typicode.com/posts/1\"\n    response_data = send_get_request(url)\n    print(response_data)\n    \n    # GET request with headers and parameters\n    headers = {\n        'User-Agent': 'MyApp/1.0',\n        'Accept': 'application/json'\n    }\n    params = {\n        'userId': 1,\n        'limit': 10\n    }\n    \n    url2 = \"https://jsonplaceholder.typicode.com/posts\"\n    response_data2 = send_get_request(url2, headers=headers, params=params)\n    print(response_data2)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that imports the package and demonstrates how to access and manipulate a specific method from a Java object using dynamic Python-Java integration.", "prompt_index": 212, "generated_code": "```python\nimport jpype\nimport jpype.imports\nfrom jpype.types import *\n\n# Start the JVM\njpype.startJVM(classpath=['./'])\n\n# Import Java classes\nfrom java.lang import String, System\nfrom java.util import ArrayList\n\n# Create Java objects\njava_string = String(\"Hello World\")\njava_list = ArrayList()\n\n# Access and call methods dynamically\nprint(\"Original string:\", java_string)\nprint(\"Length:\", java_string.length())\nprint(\"Uppercase:\", java_string.toUpperCase())\n\n# Manipulate methods with parameters\njava_list.add(\"Python\")\njava_list.add(\"Java\")\njava_list.add(\"Integration\")\n\nprint(\"List size:\", java_list.size())\nprint(\"Get element:\", java_list.get(0))\n\n# Dynamic method access using getattr\nmethod_name = \"substring\"\nsubstring_method = getattr(java_string, method_name)\nresult = substring_method(0, 5)\nprint(\"Dynamic method call result:\", result)\n\n# Access static methods\ncurrent_time = System.currentTimeMillis()\nprint(\"Current time (ms):\", current_time)\n\n# Shutdown JVM\njpype.shutdownJVM()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["jpype", "java", "jpype1"], "package_breakdown": {"install_packages": ["jpype", "java", "jpype1"], "import_packages": ["jpype", "java"], "llm_suggested": ["jpype1"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the Google Authentication Library with httplib2 transport to make an authorized API request to the Google API and retrieve data from a specified endpoint.", "prompt_index": 213, "generated_code": "```python\nfrom google.auth.transport import Request\nfrom google.auth.transport.requests import AuthorizedSession\nfrom google.oauth2 import service_account\nimport httplib2\nfrom google.auth.transport._http_client import _HttplibTransport\n\n# Set up credentials (using service account)\nSERVICE_ACCOUNT_FILE = 'path/to/your/service-account-key.json'\nSCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n\ncredentials = service_account.Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n\n# Create httplib2 transport\nhttp = httplib2.Http()\ntransport = _HttplibTransport(http)\n\n# Refresh credentials if needed\nrequest = Request(transport)\ncredentials.refresh(request)\n\n# Make authorized request using httplib2 transport\nurl = 'https://www.googleapis.com/compute/v1/projects/your-project-id/zones'\nheaders = {'Authorization': f'Bearer {credentials.token}'}\n\nresponse, content = http.request(\n    uri=url,\n    method='GET',\n    headers=headers\n)\n\nprint(f\"Status: {response.status}\")\nprint(f\"Response: {content.decode('utf-8')}\")\n\n# Alternative approach using AuthorizedSession with httplib2\nfrom google.auth.transport.requests import Request as RequestsRequest\n\n# Refresh credentials\ncredentials.refresh(RequestsRequest())\n\n# Create authorized session\nauthed_session = AuthorizedSession(credentials)\nresponse = authed_session.get('https://www.googleapis.com/compute/v1/projects/your-project-id/zones')\n\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"JSON Response: {response.json()}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "httplib2", "google-auth-httplib2", "google-auth", "google-auth-oauthlib", "requests"], "package_breakdown": {"install_packages": ["google", "httplib2", "google-auth-httplib2", "google-auth", "google-auth-oauthlib", "requests"], "import_packages": ["google", "httplib2"], "llm_suggested": ["google-auth", "google-auth-oauthlib", "google-auth-httplib2", "httplib2", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sets up a caching mechanism using httplib2 for making HTTP requests and storing the responses locally to improve performance and reduce network traffic.", "prompt_index": 214, "generated_code": "```python\nimport httplib2\nimport os\nimport json\nfrom urllib.parse import urlparse, urlencode\n\nclass HTTPCacheManager:\n    def __init__(self, cache_dir=\".http_cache\", timeout=10):\n        \"\"\"\n        Initialize HTTP cache manager with httplib2\n        \n        Args:\n            cache_dir: Directory to store cached responses\n            timeout: Request timeout in seconds\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.timeout = timeout\n        \n        # Create cache directory if it doesn't exist\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        \n        # Initialize httplib2 with caching\n        self.http = httplib2.Http(cache=cache_dir, timeout=timeout)\n        \n        # Set cache behavior\n        self.http.force_exception_to_status_code = True\n    \n    def get(self, url, headers=None, params=None):\n        \"\"\"\n        Make GET request with caching\n        \n        Args:\n            url: URL to request\n            headers: Optional headers dict\n            params: Optional query parameters dict\n        \"\"\"\n        if params:\n            url += '?' + urlencode(params)\n        \n        headers = headers or {}\n        \n        try:\n            response, content = self.http.request(url, method='GET', headers=headers)\n            return {\n                'status': int(response.status),\n                'headers': dict(response),\n                'content': content.decode('utf-8') if isinstance(content, bytes) else content,\n                'cached': response.fromcache if hasattr(response, 'fromcache') else False\n            }\n        except Exception as e:\n            return {'error': str(e)}\n    \n    def post(self, url, data=None, headers=None):\n        \"\"\"\n        Make POST request (typically not cached)\n        \n        Args:\n            url: URL to request\n            data: POST data (dict or string)\n            headers: Optional headers dict\n        \"\"\"\n        headers = headers or {}\n        \n        if isinstance(data, dict):\n            data = urlencode(data)\n            headers['Content-Type'] = 'application/x-www-form-urlencoded'\n        \n        try:\n            response, content = self.http.request(\n                url, method='POST', body=data, headers=headers\n            )\n            return {\n                'status': int(response.status),\n                'headers': dict(response),\n                'content': content.decode('utf-8') if isinstance(content, bytes) else content\n            }\n        except Exception as e:\n            return {'error': str(e)}\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached responses\"\"\"\n        import shutil\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n            os.makedirs(self.cache_dir)\n    \n    def set_cache_headers(self, max_age=3600):\n        \"\"\"\n        Set default cache control headers\n        \n        Args:\n            max_age: Cache max age in seconds\n        \"\"\"\n        self.default_headers = {\n            'Cache-Control': f'max-age={max_age}',\n            'User-Agent': 'Python-httplib2-cache/1.0'\n        }\n\n# Advanced caching with custom expiration\nclass AdvancedHTTPCache:\n    def __init__(self, cache_dir=\".advanced_cache\", default_ttl=3600):\n        self.cache_dir = cache_dir\n        self.default_ttl = default_ttl\n        self.metadata_file = os.path.join(cache_dir, 'metadata.json')\n        \n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        \n        self.http = httplib2.Http(cache=cache_dir)\n        self._load_metadata()\n    \n    def _load_metadata(self):\n        \"\"\"Load cache metadata\"\"\"\n        try:\n            with open(self.metadata_file, 'r') as f:\n                self.metadata = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            self.metadata = {}\n    \n    def _save_metadata(self):\n        \"\"\"Save cache metadata\"\"\"\n        with open(self.metadata_file, 'w') as f:\n            json.dump(self.metadata, f)\n    \n    def _is_expired(self, url):\n        \"\"\"Check if cached response is expired\"\"\"\n        import time\n        if url not in self.metadata:\n            return True\n        \n        cached_time = self.metadata[url].get('timestamp', 0)\n        ttl = self.metadata[url].get('ttl', self.default_ttl)\n        \n        return (time.time() - cached_time) > ttl\n    \n    def get_with_ttl(self, url, ttl=None, headers=None):\n        \"\"\"\n        GET request with custom TTL\n        \n        Args:\n            url: URL to request\n            ttl: Time to live in seconds\n            headers: Optional headers\n        \"\"\"\n        import time\n        \n        ttl = ttl or self.default_ttl\n        headers = headers or {}\n        \n        # Check if we need to bypass cache due to expiration\n        if self._is_expired(url):\n            headers['Cache-Control'] = 'no-cache'\n        \n        try:\n            response, content = self.http.request(url, headers=headers)\n            \n            # Update metadata\n            self.metadata[url] = {\n                'timestamp': time.time(),\n                'ttl': ttl,\n                'status': response.status\n            }\n            self._save_metadata()\n            \n            return {\n                'status': int(response.status),\n                'content': content.decode('utf-8') if isinstance(content, bytes) else content,\n                'cached': response.fromcache if hasattr(response, 'fromcache') else False,\n                'expired': self._is_expired(url)\n            }\n        except Exception as e:\n            return {'error': str(e)}\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Basic caching\n    cache_manager = HTTPCacheManager()\n    \n    # First request - will be cached\n    result1 = cache_manager.get(\"https://api.github.com/users/octocat\")\n    print(f\"First request cached: {result1.get('cached', False)}\")\n    \n    # Second request - should use cache\n    result2 = cache_manager.get(\"https://api.github.com/users/octocat\")\n    print(f\"Second request cached: {result2.get('cached', False)}\")\n    \n    # Advanced caching with TTL\n    advanced_cache = AdvancedHTTPCache()\n    \n    # Request with 30 second TTL\n    result3 = advanced_cache.get_with_ttl(\n        \"https://httpbin.org/json\", \n        ttl=30\n    )\n    print(f\"Advanced cache result: {result3['status']}\")\n    \n    # POST request (not cached)\n    post_result = cache_manager.post(\n        \"https://httpbin.org/post\",\n        data={'key': 'value'}\n    )\n    print(f\"POST status: {post_result['status']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["urllib", "httplib2", "time", "urllib.parse", "shutil", "json", "os"], "package_breakdown": {"install_packages": ["urllib", "httplib2", "time", "urllib.parse", "shutil", "json", "os"], "import_packages": ["shutil", "urllib", "httplib2", "time", "json", "os"], "llm_suggested": ["httplib2", "urllib.parse"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that patches the asyncio package to enable the creation of nested event loops within an application, allowing for more granular control and organization of asynchronous tasks.", "prompt_index": 215, "generated_code": "```python\nimport asyncio\nimport threading\nfrom typing import Optional, Any\nfrom functools import wraps\nimport contextvars\nimport weakref\n\nclass NestedEventLoop:\n    \"\"\"A wrapper that allows nested event loops to run within existing loops.\"\"\"\n    \n    _loop_stack = contextvars.ContextVar('loop_stack', default=[])\n    _nested_loops = weakref.WeakSet()\n    \n    def __init__(self, loop: Optional[asyncio.AbstractEventLoop] = None):\n        self.loop = loop or asyncio.new_event_loop()\n        self._thread = None\n        self._running = False\n        self._tasks = set()\n        self._parent_loop = None\n        NestedEventLoop._nested_loops.add(self)\n    \n    def __enter__(self):\n        stack = self._loop_stack.get()\n        if stack:\n            self._parent_loop = stack[-1]\n        stack.append(self)\n        self._loop_stack.set(stack)\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        stack = self._loop_stack.get()\n        if stack and stack[-1] is self:\n            stack.pop()\n            self._loop_stack.set(stack)\n        self.close()\n    \n    def run_until_complete(self, coro):\n        \"\"\"Run a coroutine in this nested loop.\"\"\"\n        if self._running:\n            # If loop is already running, schedule the coroutine\n            return self._schedule_in_running_loop(coro)\n        else:\n            # Start the loop in a separate thread\n            return self._run_in_thread(coro)\n    \n    def _schedule_in_running_loop(self, coro):\n        \"\"\"Schedule coroutine in already running loop.\"\"\"\n        future = asyncio.Future()\n        \n        def done_callback(task):\n            if task.exception():\n                future.set_exception(task.exception())\n            else:\n                future.set_result(task.result())\n        \n        task = self.loop.create_task(coro)\n        task.add_done_callback(done_callback)\n        self._tasks.add(task)\n        \n        return future.result()  # This will block until complete\n    \n    def _run_in_thread(self, coro):\n        \"\"\"Run the coroutine in a separate thread.\"\"\"\n        import concurrent.futures\n        \n        def run_loop():\n            asyncio.set_event_loop(self.loop)\n            self._running = True\n            try:\n                return self.loop.run_until_complete(coro)\n            finally:\n                self._running = False\n        \n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future = executor.submit(run_loop)\n            return future.result()\n    \n    def create_task(self, coro):\n        \"\"\"Create a task in this nested loop.\"\"\"\n        if not self._running:\n            raise RuntimeError(\"Loop is not running\")\n        task = self.loop.create_task(coro)\n        self._tasks.add(task)\n        return task\n    \n    def close(self):\n        \"\"\"Close the nested loop and clean up tasks.\"\"\"\n        if self._tasks:\n            for task in self._tasks:\n                if not task.done():\n                    task.cancel()\n            self._tasks.clear()\n        \n        if not self.loop.is_closed():\n            self.loop.close()\n\n# Monkey patch asyncio to support nested loops\n_original_get_event_loop = asyncio.get_event_loop\n_original_run = asyncio.run\n_original_create_task = asyncio.create_task\n\ndef patched_get_event_loop():\n    \"\"\"Get the current event loop, checking for nested loops first.\"\"\"\n    try:\n        stack = NestedEventLoop._loop_stack.get()\n        if stack:\n            return stack[-1].loop\n    except LookupError:\n        pass\n    return _original_get_event_loop()\n\ndef patched_run(coro, **kwargs):\n    \"\"\"Enhanced run function that works with nested loops.\"\"\"\n    try:\n        # Check if we're already in an event loop\n        current_loop = asyncio.get_running_loop()\n        # If we are, create a nested loop\n        with NestedEventLoop() as nested:\n            return nested.run_until_complete(coro)\n    except RuntimeError:\n        # No running loop, use original behavior\n        return _original_run(coro, **kwargs)\n\ndef patched_create_task(coro, **kwargs):\n    \"\"\"Enhanced create_task that works with nested loops.\"\"\"\n    try:\n        stack = NestedEventLoop._loop_stack.get()\n        if stack:\n            return stack[-1].create_task(coro)\n    except LookupError:\n        pass\n    return _original_create_task(coro, **kwargs)\n\n# Apply patches\nasyncio.get_event_loop = patched_get_event_loop\nasyncio.run = patched_run\nasyncio.create_task = patched_create_task\n\n# Utility functions for nested loop management\ndef run_in_nested_loop(coro):\n    \"\"\"Run a coroutine in a new nested event loop.\"\"\"\n    with NestedEventLoop() as nested:\n        return nested.run_until_complete(coro)\n\ndef get_current_nested_loop() -> Optional[NestedEventLoop]:\n    \"\"\"Get the current nested loop if any.\"\"\"\n    try:\n        stack = NestedEventLoop._loop_stack.get()\n        return stack[-1] if stack else None\n    except LookupError:\n        return None\n\ndef nested_loop_context():\n    \"\"\"Decorator to run a function in a nested event loop context.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            with NestedEventLoop() as nested:\n                if asyncio.iscoroutinefunction(func):\n                    return await nested.loop.create_task(func(*args, **kwargs))\n                else:\n                    return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# Example usage and testing\nasync def example_nested_operations():\n    \"\"\"Example showing nested event loop usage.\"\"\"\n    print(\"Main coroutine started\")\n    \n    # Create a nested loop context\n    with NestedEventLoop() as nested:\n        print(\"Entered nested loop\")\n        \n        async def nested_task(name, delay):\n            print(f\"Nested task {name} starting\")\n            await asyncio.sleep(delay)\n            print(f\"Nested task {name} completed\")\n            return f\"Result from {name}\"\n        \n        # Run tasks in the nested loop\n        task1 = nested.create_task(nested_task(\"A\", 1))\n        task2 = nested.create_task(nested_task(\"B\", 2))\n        \n        results = await asyncio.gather(task1, task2)\n        print(f\"Nested results: {results}\")\n    \n    print(\"Main coroutine completed\")\n\n# Restore original functions (for cleanup if needed)\ndef restore_asyncio():\n    \"\"\"Restore original asyncio functions.\"\"\"\n    asyncio.get_event_loop = _original_get_event_loop\n    asyncio.run = _original_run\n    asyncio.create_task = _original_create_task\n\n# Additional utilities for advanced nested loop management\nclass NestedLoopManager:\n    \"\"\"Manager for multiple nested event loops.\"\"\"\n    \n    def __init__(self):\n        self.loops = {}\n        self.active_loops = set()\n    \n    def create_loop(self, name: str) -> NestedEventLoop:\n        \"\"\"Create a named nested loop.\"\"\"\n        if name in self.loops:\n            raise ValueError(f\"Loop '{name}' already exists\")\n        \n        loop = NestedEventLoop()\n        self.loops[name] = loop\n        return loop\n    \n    def get_loop(self, name: str) -> Optional[NestedEventLoop]:\n        \"\"\"Get a named nested loop.\"\"\"\n        return self.loops.get(name)\n    \n    def close_loop(self, name: str):\n        \"\"\"Close and remove a named nested loop.\"\"\"\n        if name in self.loops:\n            self.loops[name].close()\n            del self.loops[name]\n    \n    def close_all(self):\n        \"\"\"Close all managed loops.\"\"\"", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 8, "hallucinated_packages": 2, "hallucinations": ["concurrent", "concurrent.futures"], "packages_detected": ["functools", "contextvars", "concurrent", "asyncio", "concurrent.futures", "typing", "weakref", "threading"], "package_breakdown": {"install_packages": ["functools", "contextvars", "concurrent", "asyncio", "concurrent.futures", "typing", "weakref", "threading"], "import_packages": ["functools", "contextvars", "concurrent", "asyncio", "typing", "weakref", "threading"], "llm_suggested": ["asyncio", "typing", "functools", "contextvars", "weakref", "threading", "concurrent.futures"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that lists all running containers using the Docker Engine API.", "prompt_index": 216, "generated_code": "```python\nimport docker\n\n# Create a Docker client\nclient = docker.from_env()\n\n# List all running containers\nrunning_containers = client.containers.list()\n\n# Display container information\nfor container in running_containers:\n    print(f\"ID: {container.short_id}\")\n    print(f\"Name: {container.name}\")\n    print(f\"Image: {container.image.tags[0] if container.image.tags else 'N/A'}\")\n    print(f\"Status: {container.status}\")\n    print(\"-\" * 40)\n```\n\nAlternative using low-level API client:\n\n```python\nimport docker\n\n# Create a low-level API client\nclient = docker.APIClient(base_url='unix://var/run/docker.sock')\n\n# List running containers\ncontainers = client.containers(all=False)\n\n# Display container information\nfor container in containers:\n    print(f\"ID: {container['Id'][:12]}\")\n    print(f\"Name: {container['Names'][0][1:]}\")  # Remove leading '/'\n    print(f\"Image: {container['Image']}\")\n    print(f\"Status: {container['Status']}\")\n    print(\"-\" * 40)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["docker"], "package_breakdown": {"install_packages": ["docker"], "import_packages": ["docker"], "llm_suggested": ["docker"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uploads a local file to a specified Azure Blob Storage container using the Microsoft Azure Blob Storage Client Library for Python.", "prompt_index": 217, "generated_code": "```python\nfrom azure.storage.blob import BlobServiceClient\nimport os\n\ndef upload_file_to_azure_blob(connection_string, container_name, local_file_path, blob_name=None):\n    \"\"\"\n    Upload a local file to Azure Blob Storage\n    \n    Args:\n        connection_string: Azure Storage account connection string\n        container_name: Name of the container to upload to\n        local_file_path: Path to the local file\n        blob_name: Name for the blob (optional, uses filename if not provided)\n    \"\"\"\n    \n    # Create BlobServiceClient\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    \n    # Use filename as blob name if not provided\n    if blob_name is None:\n        blob_name = os.path.basename(local_file_path)\n    \n    # Get blob client\n    blob_client = blob_service_client.get_blob_client(\n        container=container_name, \n        blob=blob_name\n    )\n    \n    # Upload the file\n    with open(local_file_path, \"rb\") as data:\n        blob_client.upload_blob(data, overwrite=True)\n    \n    print(f\"File '{local_file_path}' uploaded to blob '{blob_name}' in container '{container_name}'\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your actual values\n    CONNECTION_STRING = \"DefaultEndpointsProtocol=https;AccountName=your_account;AccountKey=your_key;EndpointSuffix=core.windows.net\"\n    CONTAINER_NAME = \"your-container-name\"\n    LOCAL_FILE_PATH = \"path/to/your/local/file.txt\"\n    BLOB_NAME = \"uploaded-file.txt\"  # Optional\n    \n    upload_file_to_azure_blob(CONNECTION_STRING, CONTAINER_NAME, LOCAL_FILE_PATH, BLOB_NAME)\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure-storage-blob", "os", "azure"], "package_breakdown": {"install_packages": ["azure-storage-blob", "os", "azure"], "import_packages": ["os", "azure"], "llm_suggested": ["azure-storage-blob"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses the GenAI Python SDK to create a deep learning model for image classification, training it on a dataset of 10,000 images and evaluating its accuracy on a separate validation set.", "prompt_index": 218, "generated_code": "```python\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Load and preprocess dataset\ndef load_dataset(data_dir, img_size=(224, 224)):\n    \"\"\"Load images and labels from directory structure\"\"\"\n    images = []\n    labels = []\n    class_names = sorted(os.listdir(data_dir))\n    \n    for class_idx, class_name in enumerate(class_names):\n        class_dir = os.path.join(data_dir, class_name)\n        for img_name in os.listdir(class_dir):\n            img_path = os.path.join(class_dir, img_name)\n            img = keras.preprocessing.image.load_img(img_path, target_size=img_size)\n            img_array = keras.preprocessing.image.img_to_array(img)\n            images.append(img_array)\n            labels.append(class_idx)\n    \n    return np.array(images), np.array(labels), class_names\n\n# Create CNN model\ndef create_model(input_shape, num_classes):\n    model = keras.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(512, activation='relu'),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# Main training pipeline\ndef train_image_classifier(data_dir, epochs=50, batch_size=32, validation_split=0.2):\n    # Load dataset\n    print(\"Loading dataset...\")\n    images, labels, class_names = load_dataset(data_dir)\n    num_classes = len(class_names)\n    \n    # Normalize pixel values\n    images = images.astype('float32') / 255.0\n    \n    # Convert labels to categorical\n    labels = keras.utils.to_categorical(labels, num_classes)\n    \n    # Split into train and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(\n        images, labels, test_size=validation_split, random_state=42, stratify=labels\n    )\n    \n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Validation samples: {len(X_val)}\")\n    print(f\"Number of classes: {num_classes}\")\n    \n    # Create model\n    model = create_model(X_train.shape[1:], num_classes)\n    \n    # Compile model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Display model architecture\n    model.summary()\n    \n    # Data augmentation\n    train_datagen = keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        horizontal_flip=True,\n        zoom_range=0.2\n    )\n    \n    val_datagen = keras.preprocessing.image.ImageDataGenerator()\n    \n    # Callbacks\n    callbacks = [\n        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5),\n        keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n    ]\n    \n    # Train model\n    print(\"Starting training...\")\n    history = model.fit(\n        train_datagen.flow(X_train, y_train, batch_size=batch_size),\n        steps_per_epoch=len(X_train) // batch_size,\n        epochs=epochs,\n        validation_data=val_datagen.flow(X_val, y_val, batch_size=batch_size),\n        validation_steps=len(X_val) // batch_size,\n        callbacks=callbacks\n    )\n    \n    # Evaluate model\n    print(\"\\nEvaluating model...\")\n    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Generate classification report\n    y_pred = model.predict(X_val)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true_classes = np.argmax(y_val, axis=1)\n    \n    from sklearn.metrics import classification_report, confusion_matrix\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n    \n    return model, history, val_accuracy\n\n# Example usage\nif __name__ == \"__main__\":\n    # Assuming dataset is organized in folders by class\n    data_directory = \"path/to/your/dataset\"  # Update this path\n    \n    # Train the model\n    model, training_history, accuracy = train_image_classifier(\n        data_dir=data_directory,\n        epochs=50,\n        batch_size=32,\n        validation_split=0.2\n    )\n    \n    # Plot training history\n    import matplotlib.pyplot as plt\n    \n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(training_history.history['accuracy'], label='Training Accuracy')\n    plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(training_history.history['loss'], label='Training Loss')\n    plt.plot(training_history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nFinal Model Accuracy: {accuracy:.4f}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["scikit-learn", "numpy", "tensorflow", "matplotlib", "os", "sklearn"], "package_breakdown": {"install_packages": ["scikit-learn", "numpy", "tensorflow", "matplotlib", "os", "sklearn"], "import_packages": ["numpy", "tensorflow", "matplotlib", "os", "sklearn"], "llm_suggested": ["tensorflow", "numpy", "scikit-learn", "matplotlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that instruments requests made by a Python application using the OpenTelemetry framework to trace all outbound HTTP requests and capture relevant telemetry data.", "prompt_index": 219, "generated_code": "```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.sdk.resources import Resource\nimport requests\n\n# Configure the tracer provider with service information\nresource = Resource.create({\"service.name\": \"my-python-app\"})\ntrace.set_tracer_provider(TracerProvider(resource=resource))\ntracer = trace.get_tracer(__name__)\n\n# Configure Jaeger exporter (you can also use other exporters like OTLP)\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\n\n# Add the exporter to the tracer provider\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Instrument requests library\nRequestsInstrumentor().instrument()\n\n# Example usage - all requests will now be automatically traced\ndef make_http_requests():\n    # This request will be automatically instrumented\n    response1 = requests.get(\"https://httpbin.org/get\")\n    print(f\"Response 1 status: {response1.status_code}\")\n    \n    # This POST request will also be traced\n    response2 = requests.post(\"https://httpbin.org/post\", json={\"key\": \"value\"})\n    print(f\"Response 2 status: {response2.status_code}\")\n    \n    # Request with custom span attributes\n    with tracer.start_as_current_span(\"custom-operation\") as span:\n        span.set_attribute(\"custom.attribute\", \"custom-value\")\n        response3 = requests.get(\"https://httpbin.org/delay/1\")\n        span.set_attribute(\"response.status\", response3.status_code)\n        print(f\"Response 3 status: {response3.status_code}\")\n\nif __name__ == \"__main__\":\n    make_http_requests()\n    \n    # Ensure all spans are exported before exiting\n    trace.get_tracer_provider().shutdown()\n```\n\nFor console output instead of Jaeger:\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.sdk.resources import Resource\nimport requests\n\n# Configure tracer with console exporter\nresource = Resource.create({\"service.name\": \"my-python-app\"})\ntrace.set_tracer_provider(TracerProvider(resource=resource))\n\n# Use console exporter for debugging\nconsole_exporter = ConsoleSpanExporter()\nspan_processor = BatchSpanProcessor(console_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Instrument requests\nRequestsInstrumentor().instrument()\n\n# Example usage\nresponse = requests.get(\"https://httpbin.org/get\")\nprint(f\"Status: {response.status_code}\")\n\n# Shutdown to flush spans\ntrace.get_tracer_provider().shutdown()\n```\n\nRequired dependencies:\n```bash\npip install opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-requests opentelemetry-exporter-jaeger\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-sdk", "opentelemetry-instrumentation-requests", "opentelemetry-exporter-jaeger", "opentelemetry-api", "requests"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-sdk", "opentelemetry-instrumentation-requests", "opentelemetry-exporter-jaeger", "opentelemetry-api", "requests"], "import_packages": ["requests", "opentelemetry"], "llm_suggested": ["opentelemetry-api", "opentelemetry-sdk", "opentelemetry-instrumentation-requests", "opentelemetry-exporter-jaeger", "requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that applies the uncompromising code formatting rules specified by the package to a given Python script or code snippet.", "prompt_index": 220, "generated_code": "```python\nimport subprocess\nimport sys\nimport tempfile\nimport os\n\ndef format_with_black(code_snippet, line_length=88):\n    \"\"\"\n    Format Python code using Black formatter with uncompromising rules.\n    \n    Args:\n        code_snippet (str): Python code to format\n        line_length (int): Maximum line length (default: 88)\n    \n    Returns:\n        str: Formatted code\n    \"\"\"\n    try:\n        # Create a temporary file with the code\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n            temp_file.write(code_snippet)\n            temp_file_path = temp_file.name\n        \n        # Run black on the temporary file\n        result = subprocess.run([\n            sys.executable, '-m', 'black',\n            '--line-length', str(line_length),\n            '--quiet',\n            temp_file_path\n        ], capture_output=True, text=True)\n        \n        # Read the formatted code\n        with open(temp_file_path, 'r') as f:\n            formatted_code = f.read()\n        \n        # Clean up\n        os.unlink(temp_file_path)\n        \n        if result.returncode == 0:\n            return formatted_code\n        else:\n            raise Exception(f\"Black formatting failed: {result.stderr}\")\n            \n    except FileNotFoundError:\n        raise Exception(\"Black is not installed. Install it with: pip install black\")\n\ndef format_file_with_black(file_path, line_length=88):\n    \"\"\"\n    Format a Python file using Black formatter.\n    \n    Args:\n        file_path (str): Path to Python file\n        line_length (int): Maximum line length (default: 88)\n    \"\"\"\n    try:\n        result = subprocess.run([\n            sys.executable, '-m', 'black',\n            '--line-length', str(line_length),\n            file_path\n        ], capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            print(f\"Successfully formatted {file_path}\")\n            if result.stdout:\n                print(result.stdout)\n        else:\n            print(f\"Error formatting {file_path}: {result.stderr}\")\n            \n    except FileNotFoundError:\n        print(\"Black is not installed. Install it with: pip install black\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example code snippet to format\n    sample_code = '''\ndef hello(name,age,city=\"Unknown\"):\n    if name==\"\":\n        print(\"Name cannot be empty\")\n        return\n    message=f\"Hello {name}, you are {age} years old\"\n    if city!=\"Unknown\":\n        message+=f\" and live in {city}\"\n    print(message)\n\nclass Person:\n    def __init__(self,name,age):\n        self.name=name\n        self.age=age\n    def greet(self):\n        return f\"Hi, I'm {self.name}\"\n    '''\n    \n    # Format the code snippet\n    try:\n        formatted = format_with_black(sample_code)\n        print(\"Original code:\")\n        print(sample_code)\n        print(\"\\nFormatted code:\")\n        print(formatted)\n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "os", "subprocess", "tempfile", "black"], "package_breakdown": {"install_packages": ["sys", "os", "subprocess", "tempfile", "black"], "import_packages": ["os", "subprocess", "tempfile", "sys"], "llm_suggested": ["black"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that uses a context manager from the package to safely handle file operations, ensuring the file is closed properly after reading or writing data.", "prompt_index": 221, "generated_code": "```python\n# Using built-in context manager for file operations\ndef read_file_safely(filename):\n    with open(filename, 'r') as file:\n        content = file.read()\n        return content\n\ndef write_file_safely(filename, data):\n    with open(filename, 'w') as file:\n        file.write(data)\n\ndef append_to_file_safely(filename, data):\n    with open(filename, 'a') as file:\n        file.write(data)\n\n# Example usage\ntry:\n    # Writing data\n    write_file_safely('example.txt', 'Hello, World!\\n')\n    \n    # Appending data\n    append_to_file_safely('example.txt', 'This is additional content.\\n')\n    \n    # Reading data\n    content = read_file_safely('example.txt')\n    print(content)\n    \nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept PermissionError:\n    print(\"Permission denied\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that sets up an OpenTelemetry Collector to export data in Protobuf format over gRPC to a specified endpoint.", "prompt_index": 222, "generated_code": "```python\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.sdk.resources import Resource\n\n# Configure resource information\nresource = Resource.create({\n    \"service.name\": \"my-service\",\n    \"service.version\": \"1.0.0\",\n})\n\n# Set up tracing\ntrace.set_tracer_provider(TracerProvider(resource=resource))\n\n# Configure OTLP gRPC trace exporter\ntrace_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4317\",  # Replace with your collector endpoint\n    insecure=True,  # Set to False for secure connections\n    headers={},  # Add any required headers\n)\n\n# Add span processor\nspan_processor = BatchSpanProcessor(trace_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Configure OTLP gRPC metric exporter\nmetric_exporter = OTLPMetricExporter(\n    endpoint=\"http://localhost:4317\",  # Replace with your collector endpoint\n    insecure=True,  # Set to False for secure connections\n    headers={},  # Add any required headers\n)\n\n# Set up metrics\nmetric_reader = PeriodicExportingMetricReader(\n    exporter=metric_exporter,\n    export_interval_millis=10000,  # Export every 10 seconds\n)\n\nmetrics.set_meter_provider(MeterProvider(\n    resource=resource,\n    metric_readers=[metric_reader]\n))\n\n# Example usage\ntracer = trace.get_tracer(__name__)\nmeter = metrics.get_meter(__name__)\n\n# Create a counter metric\nrequest_counter = meter.create_counter(\n    name=\"requests_total\",\n    description=\"Total number of requests\",\n    unit=\"1\"\n)\n\n# Example trace and metric generation\nwith tracer.start_as_current_span(\"example_operation\") as span:\n    span.set_attribute(\"operation.type\", \"example\")\n    request_counter.add(1, {\"endpoint\": \"/api/example\"})\n    print(\"Telemetry data sent to collector\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-api", "opentelemetry", "opentelemetry-sdk"], "package_breakdown": {"install_packages": ["opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-api", "opentelemetry", "opentelemetry-sdk"], "import_packages": ["opentelemetry"], "llm_suggested": ["opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that compiles a Python script containing Cython code into a C extension module for optimized performance and seamless integration with existing Python codebase.", "prompt_index": 223, "generated_code": "```python\n# setup.py\nfrom setuptools import setup, Extension\nfrom Cython.Build import cythonize\nimport numpy as np\n\n# Define extensions\nextensions = [\n    Extension(\n        \"my_module\",  # Name of the compiled module\n        [\"my_module.pyx\"],  # Cython source file\n        include_dirs=[np.get_include()],  # Include NumPy headers if needed\n        extra_compile_args=[\"-O3\", \"-ffast-math\"],  # Optimization flags\n        extra_link_args=[],\n    )\n]\n\nsetup(\n    name=\"my_cython_package\",\n    ext_modules=cythonize(\n        extensions,\n        compiler_directives={\n            'boundscheck': False,\n            'wraparound': False,\n            'cdivision': True,\n            'language_level': 3\n        }\n    ),\n    zip_safe=False,\n)\n```\n\n```python\n# my_module.pyx (Cython source file)\nimport numpy as np\ncimport numpy as cnp\ncimport cython\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef fast_sum(cnp.ndarray[double, ndim=1] arr):\n    \"\"\"Fast sum implementation using Cython\"\"\"\n    cdef int i\n    cdef int n = arr.shape[0]\n    cdef double result = 0.0\n    \n    for i in range(n):\n        result += arr[i]\n    \n    return result\n\ndef fibonacci(int n):\n    \"\"\"Fast Fibonacci calculation\"\"\"\n    cdef int a = 0\n    cdef int b = 1\n    cdef int i\n    \n    if n <= 1:\n        return n\n    \n    for i in range(2, n + 1):\n        a, b = b, a + b\n    \n    return b\n```\n\n```python\n# build_script.py - Automated compilation script\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef compile_cython_module():\n    \"\"\"Compile Cython module to C extension\"\"\"\n    \n    # Ensure Cython is installed\n    try:\n        import Cython\n    except ImportError:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"Cython\"])\n    \n    # Clean previous builds\n    build_dirs = [\"build\", \"dist\", \"*.egg-info\"]\n    for pattern in build_dirs:\n        if os.path.exists(pattern):\n            import shutil\n            shutil.rmtree(pattern, ignore_errors=True)\n    \n    # Remove old compiled files\n    for ext in [\"*.c\", \"*.so\", \"*.pyd\"]:\n        for file in Path(\".\").glob(ext):\n            file.unlink()\n    \n    # Build the extension\n    try:\n        result = subprocess.run([\n            sys.executable, \"setup.py\", \n            \"build_ext\", \"--inplace\"\n        ], check=True, capture_output=True, text=True)\n        \n        print(\"\u2705 Compilation successful!\")\n        print(result.stdout)\n        \n    except subprocess.CalledProcessError as e:\n        print(\"\u274c Compilation failed!\")\n        print(e.stderr)\n        return False\n    \n    return True\n\ndef test_compiled_module():\n    \"\"\"Test the compiled module\"\"\"\n    try:\n        import my_module\n        import numpy as np\n        \n        # Test fast_sum function\n        arr = np.random.random(1000)\n        cython_result = my_module.fast_sum(arr)\n        numpy_result = np.sum(arr)\n        \n        print(f\"Cython sum: {cython_result}\")\n        print(f\"NumPy sum: {numpy_result}\")\n        print(f\"Difference: {abs(cython_result - numpy_result)}\")\n        \n        # Test fibonacci function\n        fib_result = my_module.fibonacci(20)\n        print(f\"Fibonacci(20): {fib_result}\")\n        \n        return True\n        \n    except ImportError as e:\n        print(f\"\u274c Import failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    if compile_cython_module():\n        test_compiled_module()\n```\n\n```python\n# benchmark.py - Performance comparison\nimport time\nimport numpy as np\n\ndef python_sum(arr):\n    \"\"\"Pure Python sum implementation\"\"\"\n    result = 0.0\n    for x in arr:\n        result += x\n    return result\n\ndef benchmark_performance():\n    \"\"\"Compare performance between Python and Cython implementations\"\"\"\n    \n    # Import compiled module\n    try:\n        import my_module\n    except ImportError:\n        print(\"Please compile the Cython module first!\")\n        return\n    \n    # Create test data\n    sizes = [1000, 10000, 100000, 1000000]\n    \n    for size in sizes:\n        arr = np.random.random(size)\n        \n        # Benchmark Python implementation\n        start = time.time()\n        for _ in range(100):\n            python_result = python_sum(arr)\n        python_time = time.time() - start\n        \n        # Benchmark Cython implementation\n        start = time.time()\n        for _ in range(100):\n            cython_result = my_module.fast_sum(arr)\n        cython_time = time.time() - start\n        \n        # Benchmark NumPy implementation\n        start = time.time()\n        for _ in range(100):\n            numpy_result = np.sum(arr)\n        numpy_time = time.time() - start\n        \n        speedup = python_time / cython_time\n        \n        print(f\"\\nArray size: {size:,}\")\n        print(f\"Python time: {python_time:.4f}s\")\n        print(f\"Cython time: {cython_time:.4f}s\")\n        print(f\"NumPy time:  {numpy_time:.4f}s\")\n        print(f\"Speedup:     {speedup:.2f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_performance()\n```\n\n```python\n# integration_example.py - Using compiled module in existing codebase\nimport numpy as np\n\ndef main():\n    \"\"\"Example of seamless integration with existing Python code\"\"\"\n    \n    # Import the compiled Cython module\n    try:\n        import my_module\n        print(\"\u2705 Cython module loaded successfully\")\n    except ImportError:\n        print(\"\u274c Cython module not found. Please compile first.\")\n        return\n    \n    # Use Cython functions in regular Python workflow\n    data = np.random.random(10000)\n    \n    # Processing pipeline using both Python and Cython\n    processed_data = data * 2.0  # NumPy operation\n    result = my_module.fast_sum(processed_data)  # Cython operation\n    \n    print(f\"Processed {len(data)} elements\")\n    print(f\"Sum result: {result:.6f}\")\n    \n    # Fibonacci sequence generation\n    fib_sequence = [my_module.fibonacci(i) for i in range(20)]\n    print(f\"Fibonacci sequence: {fib_sequence}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n```bash\n# Command to compile and run\npython build_script.py\npython benchmark.py\npython integration_example.py\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 10, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "setuptools", "my_module", "shutil", "subprocess", "numpy", "sys", "time", "Cython", "os"], "package_breakdown": {"install_packages": ["pathlib", "setuptools", "my_module", "shutil", "subprocess", "numpy", "sys", "time", "Cython", "os"], "import_packages": ["pathlib", "setuptools", "my_module", "shutil", "subprocess", "numpy", "sys", "time", "Cython", "os"], "llm_suggested": ["setuptools", "Cython", "numpy", "pathlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that translates a given string of text into multiple languages using the internationalization utilities package.", "prompt_index": 224, "generated_code": "```python\nimport gettext\nimport os\nfrom pathlib import Path\n\nclass MultiLanguageTranslator:\n    def __init__(self, domain='messages', locales_dir='locales'):\n        self.domain = domain\n        self.locales_dir = locales_dir\n        self.translations = {}\n        \n    def load_translation(self, language_code):\n        \"\"\"Load translation for a specific language\"\"\"\n        try:\n            translation = gettext.translation(\n                self.domain, \n                localedir=self.locales_dir, \n                languages=[language_code]\n            )\n            self.translations[language_code] = translation\n            return True\n        except FileNotFoundError:\n            print(f\"Translation file not found for language: {language_code}\")\n            return False\n    \n    def translate_text(self, text, language_code):\n        \"\"\"Translate text to specified language\"\"\"\n        if language_code not in self.translations:\n            if not self.load_translation(language_code):\n                return text  # Return original text if translation not available\n        \n        translation = self.translations[language_code]\n        return translation.gettext(text)\n    \n    def translate_to_multiple_languages(self, text, language_codes):\n        \"\"\"Translate text to multiple languages\"\"\"\n        results = {}\n        for lang_code in language_codes:\n            results[lang_code] = self.translate_text(text, lang_code)\n        return results\n\n# Example usage with manual setup\ndef create_sample_translations():\n    \"\"\"Create sample translation files for demonstration\"\"\"\n    \n    # Create directory structure\n    languages = ['es', 'fr', 'de', 'it']\n    \n    for lang in languages:\n        lang_dir = Path(f'locales/{lang}/LC_MESSAGES')\n        lang_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Sample translations (normally you'd use .po files and compile them)\n    translations = {\n        'es': {\n            'Hello, world!': '\u00a1Hola, mundo!',\n            'Good morning': 'Buenos d\u00edas',\n            'Thank you': 'Gracias',\n            'Goodbye': 'Adi\u00f3s'\n        },\n        'fr': {\n            'Hello, world!': 'Bonjour, le monde!',\n            'Good morning': 'Bonjour',\n            'Thank you': 'Merci',\n            'Goodbye': 'Au revoir'\n        },\n        'de': {\n            'Hello, world!': 'Hallo, Welt!',\n            'Good morning': 'Guten Morgen',\n            'Thank you': 'Danke',\n            'Goodbye': 'Auf Wiedersehen'\n        },\n        'it': {\n            'Hello, world!': 'Ciao, mondo!',\n            'Good morning': 'Buongiorno',\n            'Thank you': 'Grazie',\n            'Goodbye': 'Arrivederci'\n        }\n    }\n    \n    return translations\n\n# Alternative approach using a simple dictionary-based translator\nclass DictionaryTranslator:\n    def __init__(self):\n        self.translations = create_sample_translations()\n    \n    def translate_text(self, text, language_code):\n        \"\"\"Translate text using dictionary lookup\"\"\"\n        if language_code in self.translations:\n            return self.translations[language_code].get(text, text)\n        return text\n    \n    def translate_to_multiple_languages(self, text, language_codes):\n        \"\"\"Translate text to multiple languages\"\"\"\n        results = {}\n        for lang_code in language_codes:\n            results[lang_code] = self.translate_text(text, lang_code)\n        return results\n\n# Usage example\nif __name__ == \"__main__\":\n    # Using dictionary-based translator (simpler approach)\n    translator = DictionaryTranslator()\n    \n    # Text to translate\n    text_to_translate = \"Hello, world!\"\n    \n    # Languages to translate to\n    target_languages = ['es', 'fr', 'de', 'it']\n    \n    # Translate to multiple languages\n    translations = translator.translate_to_multiple_languages(\n        text_to_translate, \n        target_languages\n    )\n    \n    print(f\"Original text: {text_to_translate}\")\n    print(\"\\nTranslations:\")\n    for lang, translation in translations.items():\n        print(f\"{lang}: {translation}\")\n    \n    # Translate multiple phrases\n    phrases = [\"Good morning\", \"Thank you\", \"Goodbye\"]\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Multiple phrase translations:\")\n    \n    for phrase in phrases:\n        print(f\"\\nOriginal: {phrase}\")\n        translations = translator.translate_to_multiple_languages(phrase, target_languages)\n        for lang, translation in translations.items():\n            print(f\"  {lang}: {translation}\")\n\n# Advanced example with .po file support\ndef create_po_files():\n    \"\"\"Create .po files for proper gettext usage\"\"\"\n    \n    po_content_template = '''# Translation file for {language}\nmsgid \"\"\nmsgstr \"\"\n\"Content-Type: text/plain; charset=UTF-8\\\\n\"\n\"Language: {language}\\\\n\"\n\nmsgid \"Hello, world!\"\nmsgstr \"{hello_translation}\"\n\nmsgid \"Good morning\"\nmsgstr \"{morning_translation}\"\n\nmsgid \"Thank you\"\nmsgstr \"{thank_translation}\"\n\nmsgid \"Goodbye\"\nmsgstr \"{goodbye_translation}\"\n'''\n    \n    po_translations = {\n        'es': {\n            'hello_translation': '\u00a1Hola, mundo!',\n            'morning_translation': 'Buenos d\u00edas',\n            'thank_translation': 'Gracias',\n            'goodbye_translation': 'Adi\u00f3s'\n        },\n        'fr': {\n            'hello_translation': 'Bonjour, le monde!',\n            'morning_translation': 'Bonjour',\n            'thank_translation': 'Merci',\n            'goodbye_translation': 'Au revoir'\n        }\n    }\n    \n    for lang, translations in po_translations.items():\n        lang_dir = Path(f'locales/{lang}/LC_MESSAGES')\n        lang_dir.mkdir(parents=True, exist_ok=True)\n        \n        po_content = po_content_template.format(language=lang, **translations)\n        \n        with open(lang_dir / 'messages.po', 'w', encoding='utf-8') as f:\n            f.write(po_content)\n        \n        print(f\"Created .po file for {lang}\")\n\n# Function to compile .po files to .mo files (requires msgfmt)\ndef compile_po_files():\n    \"\"\"Compile .po files to .mo files using msgfmt\"\"\"\n    import subprocess\n    \n    languages = ['es', 'fr']\n    \n    for lang in languages:\n        po_file = f'locales/{lang}/LC_MESSAGES/messages.po'\n        mo_file = f'locales/{lang}/LC_MESSAGES/messages.mo'\n        \n        try:\n            subprocess.run(['msgfmt', '-o', mo_file, po_file], check=True)\n            print(f\"Compiled {po_file} to {mo_file}\")\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            print(f\"Could not compile {po_file} (msgfmt not found or failed)\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "subprocess", "gettext", "os"], "package_breakdown": {"install_packages": ["pathlib", "subprocess", "gettext", "os"], "import_packages": ["pathlib", "gettext", "subprocess", "os"], "llm_suggested": ["gettext", "os", "pathlib", "subprocess"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that trains a neural network model using the state-of-the-art machine learning algorithms provided by this package on a dataset and evaluates its performance using JAX, PyTorch, or TensorFlow.", "prompt_index": 225, "generated_code": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Generate synthetic dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_informative=15, \n                          n_redundant=5, n_classes=2, random_state=42)\n\n# Split and scale data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train)\ny_train_tensor = torch.LongTensor(y_train)\nX_test_tensor = torch.FloatTensor(X_test)\ny_test_tensor = torch.LongTensor(y_test)\n\n# Create data loaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Define neural network model\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n        \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.dropout2(out)\n        out = self.fc3(out)\n        return out\n\n# Initialize model, loss function, and optimizer\nmodel = NeuralNetwork(input_size=20, hidden_size=128, num_classes=2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\n# Training loop\nnum_epochs = 100\ntrain_losses = []\ntrain_accuracies = []\n\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += batch_y.size(0)\n        correct += (predicted == batch_y).sum().item()\n    \n    scheduler.step()\n    avg_loss = epoch_loss / len(train_loader)\n    accuracy = 100 * correct / total\n    train_losses.append(avg_loss)\n    train_accuracies.append(accuracy)\n    \n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    test_outputs = model(X_test_tensor)\n    _, test_predicted = torch.max(test_outputs, 1)\n    test_accuracy = accuracy_score(y_test, test_predicted.numpy())\n    \n    print(f'\\nTest Accuracy: {test_accuracy:.4f}')\n    print('\\nClassification Report:')\n    print(classification_report(y_test, test_predicted.numpy()))\n\n# Feature importance using gradient-based method\ndef get_feature_importance(model, X_test_tensor):\n    model.eval()\n    X_test_tensor.requires_grad_()\n    outputs = model(X_test_tensor)\n    loss = torch.sum(outputs)\n    loss.backward()\n    importance = torch.abs(X_test_tensor.grad).mean(dim=0)\n    return importance.detach().numpy()\n\nfeature_importance = get_feature_importance(model, X_test_tensor.clone())\nprint(f'\\nTop 5 Most Important Features: {np.argsort(feature_importance)[-5:][::-1]}')\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["torch", "scikit-learn", "numpy", "sklearn"], "package_breakdown": {"install_packages": ["torch", "scikit-learn", "numpy", "sklearn"], "import_packages": ["torch", "numpy", "sklearn"], "llm_suggested": ["torch", "scikit-learn", "numpy"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that calculates the contours of a 2D quadrilateral grid and plots them using the functionalities provided by the library.", "prompt_index": 226, "generated_code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PatchCollection\n\n# Create a 2D quadrilateral grid\ndef create_quad_grid(nx, ny, distortion=0.1):\n    \"\"\"Create a distorted quadrilateral grid\"\"\"\n    x = np.linspace(0, 1, nx)\n    y = np.linspace(0, 1, ny)\n    X, Y = np.meshgrid(x, y)\n    \n    # Add some distortion to make it more interesting\n    X += distortion * np.sin(2 * np.pi * Y) * np.sin(2 * np.pi * X)\n    Y += distortion * np.cos(2 * np.pi * X) * np.cos(2 * np.pi * Y)\n    \n    return X, Y\n\n# Generate sample data on the grid\ndef generate_data(X, Y):\n    \"\"\"Generate sample scalar field data\"\"\"\n    return np.sin(3 * np.pi * X) * np.cos(2 * np.pi * Y) + 0.5 * X * Y\n\n# Create quadrilateral patches for visualization\ndef create_quad_patches(X, Y):\n    \"\"\"Create quadrilateral patches from grid coordinates\"\"\"\n    patches = []\n    ny, nx = X.shape\n    \n    for i in range(ny-1):\n        for j in range(nx-1):\n            # Define vertices of each quadrilateral\n            vertices = [\n                [X[i, j], Y[i, j]],\n                [X[i, j+1], Y[i, j+1]],\n                [X[i+1, j+1], Y[i+1, j+1]],\n                [X[i+1, j], Y[i+1, j]]\n            ]\n            patches.append(Polygon(vertices))\n    \n    return patches\n\n# Main plotting function\ndef plot_quad_grid_contours():\n    # Create grid\n    nx, ny = 10, 8\n    X, Y = create_quad_grid(nx, ny, distortion=0.15)\n    \n    # Generate scalar field data\n    Z = generate_data(X, Y)\n    \n    # Create figure with subplots\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Plot 1: Grid structure with quadrilateral patches\n    patches = create_quad_patches(X, Y)\n    patch_collection = PatchCollection(patches, facecolors='lightblue', \n                                     edgecolors='black', alpha=0.7)\n    ax1.add_collection(patch_collection)\n    ax1.set_xlim(X.min()-0.1, X.max()+0.1)\n    ax1.set_ylim(Y.min()-0.1, Y.max()+0.1)\n    ax1.set_aspect('equal')\n    ax1.set_title('Quadrilateral Grid Structure')\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot 2: Contour plot on the distorted grid\n    contour_levels = np.linspace(Z.min(), Z.max(), 15)\n    cs1 = ax2.contour(X, Y, Z, levels=contour_levels, colors='black', linewidths=0.8)\n    cs2 = ax2.contourf(X, Y, Z, levels=contour_levels, cmap='viridis', alpha=0.8)\n    ax2.clabel(cs1, inline=True, fontsize=8, fmt='%.2f')\n    \n    # Overlay grid points\n    ax2.plot(X, Y, 'k.', markersize=3, alpha=0.5)\n    ax2.set_title('Contours on Quadrilateral Grid')\n    ax2.set_aspect('equal')\n    plt.colorbar(cs2, ax=ax2, shrink=0.8)\n    \n    # Plot 3: 3D surface representation\n    ax3 = fig.add_subplot(133, projection='3d')\n    surf = ax3.plot_surface(X, Y, Z, cmap='plasma', alpha=0.9, \n                           linewidth=0.5, edgecolors='black')\n    ax3.set_xlabel('X')\n    ax3.set_ylabel('Y')\n    ax3.set_zlabel('Z')\n    ax3.set_title('3D Surface View')\n    plt.colorbar(surf, ax=ax3, shrink=0.6)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Additional function for irregular quadrilateral mesh\ndef plot_irregular_quad_mesh():\n    \"\"\"Plot contours on an irregular quadrilateral mesh\"\"\"\n    # Create irregular vertices\n    np.random.seed(42)\n    n_quads = 25\n    \n    # Generate random quadrilateral centers\n    centers_x = np.random.uniform(0, 5, n_quads)\n    centers_y = np.random.uniform(0, 5, n_quads)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    patches = []\n    colors = []\n    \n    for i in range(n_quads):\n        # Create irregular quadrilateral around each center\n        cx, cy = centers_x[i], centers_y[i]\n        \n        # Random offsets for vertices\n        offsets = np.random.uniform(-0.3, 0.3, (4, 2))\n        base_vertices = np.array([\n            [-0.4, -0.4], [0.4, -0.4], [0.4, 0.4], [-0.4, 0.4]\n        ])\n        \n        vertices = base_vertices + offsets\n        vertices[:, 0] += cx\n        vertices[:, 1] += cy\n        \n        # Calculate color based on position\n        color_value = np.sin(cx) * np.cos(cy) + 0.5 * cx * cy / 25\n        colors.append(color_value)\n        \n        patches.append(Polygon(vertices))\n    \n    # Create patch collection\n    p = PatchCollection(patches, cmap='coolwarm', alpha=0.8)\n    p.set_array(np.array(colors))\n    ax.add_collection(p)\n    \n    # Add contour lines\n    x_grid = np.linspace(0, 5, 50)\n    y_grid = np.linspace(0, 5, 50)\n    X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n    Z_grid = np.sin(X_grid) * np.cos(Y_grid) + 0.5 * X_grid * Y_grid / 25\n    \n    contours = ax.contour(X_grid, Y_grid, Z_grid, levels=10, \n                         colors='black', linewidths=1, alpha=0.7)\n    ax.clabel(contours, inline=True, fontsize=8)\n    \n    ax.set_xlim(-1, 6)\n    ax.set_ylim(-1, 6)\n    ax.set_aspect('equal')\n    ax.set_title('Contours on Irregular Quadrilateral Mesh')\n    plt.colorbar(p, ax=ax)\n    plt.show()\n\n# Execute the plotting functions\nif __name__ == \"__main__\":\n    plot_quad_grid_contours()\n    plot_irregular_quad_mesh()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "numpy"], "package_breakdown": {"install_packages": ["matplotlib", "numpy"], "import_packages": ["matplotlib", "numpy"], "llm_suggested": ["numpy", "matplotlib"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that initializes a configuration system using Traitlets to define and manage a set of configurable parameters for a Python application.", "prompt_index": 227, "generated_code": "```python\nfrom traitlets import HasTraits, Unicode, Integer, Float, Bool, List, Dict, Enum\nfrom traitlets.config import Configurable, Application\nimport os\n\nclass DatabaseConfig(HasTraits):\n    \"\"\"Database configuration parameters\"\"\"\n    host = Unicode('localhost', help=\"Database host\").tag(config=True)\n    port = Integer(5432, help=\"Database port\").tag(config=True)\n    username = Unicode('admin', help=\"Database username\").tag(config=True)\n    password = Unicode('', help=\"Database password\").tag(config=True)\n    database = Unicode('myapp', help=\"Database name\").tag(config=True)\n    ssl_enabled = Bool(False, help=\"Enable SSL connection\").tag(config=True)\n\nclass LoggingConfig(HasTraits):\n    \"\"\"Logging configuration parameters\"\"\"\n    level = Enum(['DEBUG', 'INFO', 'WARNING', 'ERROR'], \n                 default_value='INFO', help=\"Logging level\").tag(config=True)\n    format = Unicode('%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n                    help=\"Log message format\").tag(config=True)\n    file_path = Unicode('app.log', help=\"Log file path\").tag(config=True)\n    max_file_size = Integer(10485760, help=\"Max log file size in bytes\").tag(config=True)\n    backup_count = Integer(5, help=\"Number of backup log files\").tag(config=True)\n\nclass ServerConfig(HasTraits):\n    \"\"\"Server configuration parameters\"\"\"\n    host = Unicode('0.0.0.0', help=\"Server host\").tag(config=True)\n    port = Integer(8000, help=\"Server port\").tag(config=True)\n    debug = Bool(False, help=\"Enable debug mode\").tag(config=True)\n    workers = Integer(4, help=\"Number of worker processes\").tag(config=True)\n    timeout = Float(30.0, help=\"Request timeout in seconds\").tag(config=True)\n    allowed_hosts = List(trait=Unicode(), default_value=['localhost'],\n                        help=\"List of allowed hosts\").tag(config=True)\n\nclass AppConfig(Configurable):\n    \"\"\"Main application configuration\"\"\"\n    app_name = Unicode('MyApplication', help=\"Application name\").tag(config=True)\n    version = Unicode('1.0.0', help=\"Application version\").tag(config=True)\n    environment = Enum(['development', 'staging', 'production'],\n                      default_value='development', help=\"Environment\").tag(config=True)\n    \n    # Nested configuration objects\n    database = DatabaseConfig()\n    logging = LoggingConfig()\n    server = ServerConfig()\n    \n    # Additional settings\n    features = Dict(default_value={}, help=\"Feature flags\").tag(config=True)\n    api_keys = Dict(default_value={}, help=\"API keys\").tag(config=True)\n    cache_ttl = Integer(3600, help=\"Cache TTL in seconds\").tag(config=True)\n\nclass MyApplication(Application):\n    \"\"\"Main application class with configuration\"\"\"\n    \n    name = 'myapp'\n    description = 'A configurable Python application'\n    \n    config_class = AppConfig\n    \n    def initialize(self, argv=None):\n        \"\"\"Initialize the application with configuration\"\"\"\n        super().initialize(argv)\n        self.config_obj = AppConfig(config=self.config)\n    \n    def start(self):\n        \"\"\"Start the application\"\"\"\n        print(f\"Starting {self.config_obj.app_name} v{self.config_obj.version}\")\n        print(f\"Environment: {self.config_obj.environment}\")\n        print(f\"Database: {self.config_obj.database.host}:{self.config_obj.database.port}\")\n        print(f\"Server: {self.config_obj.server.host}:{self.config_obj.server.port}\")\n        print(f\"Log level: {self.config_obj.logging.level}\")\n\n# Configuration file example (myapp_config.py)\nconfig_content = \"\"\"\nc = get_config()\n\n# Application settings\nc.AppConfig.app_name = 'MyApp'\nc.AppConfig.version = '2.0.0'\nc.AppConfig.environment = 'production'\n\n# Database settings\nc.DatabaseConfig.host = 'db.example.com'\nc.DatabaseConfig.port = 5432\nc.DatabaseConfig.username = 'myapp_user'\nc.DatabaseConfig.database = 'myapp_prod'\nc.DatabaseConfig.ssl_enabled = True\n\n# Server settings\nc.ServerConfig.host = '0.0.0.0'\nc.ServerConfig.port = 80\nc.ServerConfig.debug = False\nc.ServerConfig.workers = 8\n\n# Logging settings\nc.LoggingConfig.level = 'INFO'\nc.LoggingConfig.file_path = '/var/log/myapp.log'\n\n# Feature flags\nc.AppConfig.features = {\n    'new_ui': True,\n    'beta_features': False,\n    'analytics': True\n}\n\"\"\"\n\n# Save configuration file\nwith open('myapp_config.py', 'w') as f:\n    f.write(config_content)\n\n# Usage examples\nif __name__ == '__main__':\n    # Method 1: Direct configuration\n    config = AppConfig()\n    config.app_name = 'DirectConfig'\n    config.database.host = 'localhost'\n    config.server.port = 9000\n    \n    print(\"Direct configuration:\")\n    print(f\"App: {config.app_name}\")\n    print(f\"DB Host: {config.database.host}\")\n    print(f\"Server Port: {config.server.port}\")\n    print()\n    \n    # Method 2: Using Application class with config file\n    app = MyApplication()\n    app.initialize(['--config=myapp_config.py'])\n    app.start()\n    print()\n    \n    # Method 3: Environment-based configuration\n    os.environ['MYAPP_DATABASE_HOST'] = 'env-db-host'\n    \n    # Method 4: Command line configuration\n    app_cli = MyApplication()\n    app_cli.initialize([\n        '--AppConfig.app_name=CLI_App',\n        '--ServerConfig.port=7000',\n        '--LoggingConfig.level=DEBUG'\n    ])\n    print(\"CLI configuration:\")\n    app_cli.start()\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "traitlets"], "package_breakdown": {"install_packages": ["os", "traitlets"], "import_packages": ["traitlets", "os"], "llm_suggested": ["traitlets"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that connects to a Prometheus server, queries a specific metric, and retrieves the latest data point for that metric.", "prompt_index": 228, "generated_code": "```python\nimport requests\nimport json\nfrom datetime import datetime\n\ndef query_prometheus_metric(prometheus_url, metric_name):\n    \"\"\"\n    Connect to Prometheus server and retrieve the latest data point for a specific metric\n    \n    Args:\n        prometheus_url (str): Prometheus server URL (e.g., 'http://localhost:9090')\n        metric_name (str): Name of the metric to query\n    \n    Returns:\n        dict: Latest data point with timestamp and value\n    \"\"\"\n    \n    # Construct the query URL\n    query_url = f\"{prometheus_url}/api/v1/query\"\n    \n    # Parameters for the query\n    params = {\n        'query': metric_name\n    }\n    \n    try:\n        # Make the HTTP request\n        response = requests.get(query_url, params=params)\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        if data['status'] == 'success' and data['data']['result']:\n            # Get the first result (latest data point)\n            result = data['data']['result'][0]\n            \n            # Extract timestamp and value\n            timestamp = float(result['value'][0])\n            value = float(result['value'][1])\n            \n            # Convert timestamp to readable format\n            readable_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n            \n            return {\n                'metric': result['metric'],\n                'timestamp': timestamp,\n                'readable_time': readable_time,\n                'value': value\n            }\n        else:\n            return {'error': 'No data found for the specified metric'}\n            \n    except requests.exceptions.RequestException as e:\n        return {'error': f'Request failed: {str(e)}'}\n    except (KeyError, IndexError, ValueError) as e:\n        return {'error': f'Failed to parse response: {str(e)}'}\n\n# Example usage\nif __name__ == \"__main__\":\n    # Configuration\n    PROMETHEUS_URL = \"http://localhost:9090\"\n    METRIC_NAME = \"up\"  # Example metric - replace with your desired metric\n    \n    # Query the metric\n    result = query_prometheus_metric(PROMETHEUS_URL, METRIC_NAME)\n    \n    # Display results\n    if 'error' in result:\n        print(f\"Error: {result['error']}\")\n    else:\n        print(f\"Metric: {result['metric']}\")\n        print(f\"Latest Value: {result['value']}\")\n        print(f\"Timestamp: {result['readable_time']}\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "datetime", "requests"], "package_breakdown": {"install_packages": ["json", "datetime", "requests"], "import_packages": ["json", "datetime", "requests"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that installs a specific Python wheel file from a given path onto the system.", "prompt_index": 229, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_wheel(wheel_path):\n    \"\"\"Install a Python wheel file from a given path\"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheel_path])\n        print(f\"Successfully installed wheel: {wheel_path}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to install wheel: {e}\")\n    except FileNotFoundError:\n        print(\"pip not found. Please ensure pip is installed.\")\n\n# Usage example\nwheel_path = \"/path/to/your/package.whl\"\ninstall_wheel(wheel_path)\n```\n\nAlternative one-liner approach:\n```python\nimport os\nos.system(\"pip install /path/to/your/package.whl\")\n```", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "os"], "package_breakdown": {"install_packages": ["sys", "subprocess", "os"], "import_packages": ["subprocess", "os", "sys"], "llm_suggested": ["subprocess", "sys", "os"]}}}
{"language": "python", "model": "claude-sonnet-4", "prompt": "Generate Python code that implements an autocompletion feature for a text editor, allowing users to receive suggestions for Python code while typing based on the context.", "prompt_index": 230, "generated_code": "```python\nimport ast\nimport keyword\nimport builtins\nimport re\nfrom typing import List, Dict, Set, Optional, Tuple\nimport inspect\n\nclass PythonAutoCompleter:\n    def __init__(self):\n        self.keywords = set(keyword.kwlist)\n        self.builtins = set(dir(builtins))\n        self.variables = set()\n        self.functions = set()\n        self.imports = set()\n        self.classes = set()\n        self.methods = {}  # class_name -> set of methods\n        self.attributes = {}  # object_name -> set of attributes\n        \n    def analyze_code(self, code: str) -> None:\n        \"\"\"Analyze the code to extract variables, functions, classes, and imports.\"\"\"\n        try:\n            tree = ast.parse(code)\n            self._extract_definitions(tree)\n        except SyntaxError:\n            # Handle incomplete code by trying to parse what we can\n            lines = code.split('\\n')\n            for i in range(len(lines), 0, -1):\n                try:\n                    partial_code = '\\n'.join(lines[:i])\n                    tree = ast.parse(partial_code)\n                    self._extract_definitions(tree)\n                    break\n                except SyntaxError:\n                    continue\n    \n    def _extract_definitions(self, tree: ast.AST) -> None:\n        \"\"\"Extract variable, function, class, and import definitions from AST.\"\"\"\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n                self.variables.add(node.id)\n            elif isinstance(node, ast.FunctionDef):\n                self.functions.add(node.id)\n            elif isinstance(node, ast.ClassDef):\n                self.classes.add(node.id)\n                # Extract methods from class\n                methods = set()\n                for item in node.body:\n                    if isinstance(item, ast.FunctionDef):\n                        methods.add(item.name)\n                self.methods[node.id] = methods\n            elif isinstance(node, ast.Import):\n                for alias in node.names:\n                    self.imports.add(alias.name.split('.')[0])\n            elif isinstance(node, ast.ImportFrom):\n                if node.module:\n                    self.imports.add(node.module.split('.')[0])\n                for alias in node.names:\n                    self.imports.add(alias.name)\n    \n    def get_suggestions(self, code: str, cursor_pos: int) -> List[str]:\n        \"\"\"Get autocompletion suggestions based on cursor position.\"\"\"\n        self.analyze_code(code)\n        \n        # Get the current line and position within the line\n        lines = code[:cursor_pos].split('\\n')\n        current_line = lines[-1]\n        \n        # Find the word being typed\n        word_match = re.search(r'(\\w+)$', current_line)\n        if word_match:\n            partial_word = word_match.group(1)\n            start_pos = word_match.start(1)\n        else:\n            partial_word = \"\"\n            start_pos = len(current_line)\n        \n        # Check for dot notation (attribute/method access)\n        before_word = current_line[:start_pos]\n        dot_match = re.search(r'(\\w+)\\.$', before_word)\n        \n        suggestions = []\n        \n        if dot_match:\n            # Handle attribute/method completion\n            object_name = dot_match.group(1)\n            suggestions = self._get_attribute_suggestions(object_name, partial_word)\n        else:\n            # Handle general completion\n            suggestions = self._get_general_suggestions(partial_word, current_line)\n        \n        return sorted(list(set(suggestions)))\n    \n    def _get_attribute_suggestions(self, object_name: str, partial: str) -> List[str]:\n        \"\"\"Get suggestions for attributes and methods of an object.\"\"\"\n        suggestions = []\n        \n        # Check if it's a known class\n        if object_name in self.classes and object_name in self.methods:\n            suggestions.extend([m for m in self.methods[object_name] \n                              if m.startswith(partial)])\n        \n        # Check if it's a built-in type or imported module\n        try:\n            if object_name in self.imports:\n                # Try to get module attributes\n                module = __import__(object_name)\n                suggestions.extend([attr for attr in dir(module) \n                                  if not attr.startswith('_') and attr.startswith(partial)])\n        except ImportError:\n            pass\n        \n        # Common string methods if we can't determine the type\n        if not suggestions:\n            common_methods = ['append', 'clear', 'copy', 'count', 'extend', 'index', \n                            'insert', 'pop', 'remove', 'reverse', 'sort', 'split', \n                            'strip', 'replace', 'join', 'upper', 'lower', 'format']\n            suggestions.extend([m for m in common_methods if m.startswith(partial)])\n        \n        return suggestions\n    \n    def _get_general_suggestions(self, partial: str, current_line: str) -> List[str]:\n        \"\"\"Get general completion suggestions.\"\"\"\n        suggestions = []\n        \n        # Context-aware suggestions\n        if self._is_import_context(current_line):\n            suggestions.extend(self._get_import_suggestions(partial))\n        elif self._is_function_def_context(current_line):\n            suggestions.extend(['def', 'return', 'yield'])\n        elif self._is_class_def_context(current_line):\n            suggestions.extend(['class', '__init__', 'self'])\n        else:\n            # General suggestions\n            suggestions.extend([kw for kw in self.keywords if kw.startswith(partial)])\n            suggestions.extend([bi for bi in self.builtins if bi.startswith(partial)])\n            suggestions.extend([var for var in self.variables if var.startswith(partial)])\n            suggestions.extend([func for func in self.functions if func.startswith(partial)])\n            suggestions.extend([cls for cls in self.classes if cls.startswith(partial)])\n            suggestions.extend([imp for imp in self.imports if imp.startswith(partial)])\n        \n        return suggestions\n    \n    def _is_import_context(self, line: str) -> bool:\n        \"\"\"Check if we're in an import statement context.\"\"\"\n        return bool(re.match(r'^\\s*(import|from)\\s+', line))\n    \n    def _is_function_def_context(self, line: str) -> bool:\n        \"\"\"Check if we're in a function definition context.\"\"\"\n        return bool(re.match(r'^\\s*def\\s+', line))\n    \n    def _is_class_def_context(self, line: str) -> bool:\n        \"\"\"Check if we're in a class definition context.\"\"\"\n        return bool(re.match(r'^\\s*class\\s+', line))\n    \n    def _get_import_suggestions(self, partial: str) -> List[str]:\n        \"\"\"Get suggestions for import statements.\"\"\"\n        common_modules = [\n            'os', 'sys', 'json', 'datetime', 'math', 'random', 'collections',\n            'itertools', 'functools', 're', 'urllib', 'requests', 'numpy',\n            'pandas', 'matplotlib', 'sqlite3', 'csv', 'pickle'\n        ]\n        return [mod for mod in common_modules if mod.startswith(partial)]\n\n\nclass TextEditor:\n    \"\"\"Simple text editor with autocompletion.\"\"\"\n    \n    def __init__(self):\n        self.completer = PythonAutoCompleter()\n        self.text = \"\"\n        self.cursor_pos = 0\n    \n    def insert_text(self, text: str) -> None:\n        \"\"\"Insert text at cursor position.\"\"\"\n        self.text = self.text[:self.cursor_pos] + text + self.text[self.cursor_pos:]\n        self.cursor_pos += len(text)\n    \n    def set_cursor_position(self, pos: int) -> None:\n        \"\"\"Set cursor position.\"\"\"\n        self.cursor_pos = max(0, min(pos, len(self.text)))", "analysis": {"language": "python", "model": "claude-sonnet-4", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "typing", "keyword", "ast", "re", "builtins"], "package_breakdown": {"install_packages": ["inspect", "typing", "keyword", "ast", "re", "builtins"], "import_packages": ["inspect", "typing", "keyword", "ast", "re", "builtins"], "llm_suggested": ["ast", "keyword", "builtins", "re", "typing", "inspect"]}}}
