{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the Cloudflare API using API tokens and retrieves a list of all DNS records for a specific domain.", "prompt_index": 0, "generated_code": "```python\nimport requests\n\ndef get_dns_records(api_token, zone_id):\n    \"\"\"\n    Retrieve all DNS records for a Cloudflare zone\n    \n    Args:\n        api_token (str): Cloudflare API token\n        zone_id (str): Cloudflare zone ID\n    \n    Returns:\n        list: List of DNS records or None if request fails\n    \"\"\"\n    url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        data = response.json()\n        if data.get(\"success\", False):\n            return data.get(\"result\", [])\n        else:\n            print(f\"API Error: {data.get('errors', [{}])[0].get('message', 'Unknown error')}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace with your actual values\n    API_TOKEN = \"your_api_token_here\"\n    ZONE_ID = \"your_zone_id_here\"  # Found in Cloudflare dashboard under your domain's overview\n    \n    records = get_dns_records(API_TOKEN, ZONE_ID)\n    \n    if records:\n        print(f\"Found {len(records)} DNS records:\")\n        for record in records:\n            print(f\"{record['type']} {record['name']} -> {record['content']}\")\n    else:\n        print(\"Failed to retrieve DNS records\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a secure connection using the ACME protocol to request and obtain SSL/TLS certificates for a website.", "prompt_index": 1, "generated_code": "```python\nimport os\nimport time\nimport json\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives import serialization\nimport requests\nfrom acme import client, messages, crypto_util\n\ndef generate_private_key():\n    \"\"\"Generate a private key for ACME account\"\"\"\n    return rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n\ndef create_acme_client(acme_directory_url):\n    \"\"\"Create ACME client instance\"\"\"\n    # Generate account key\n    account_key = generate_private_key()\n    \n    # Create ACME client\n    net = client.ClientNetwork(account_key, user_agent=\"python-acme-client\")\n    directory = messages.Directory.from_json(net.get(acme_directory_url).json())\n    \n    return client.ClientV2(directory, net=net), account_key\n\ndef perform_http_01_challenge(client, domain):\n    \"\"\"Perform HTTP-01 challenge for domain verification\"\"\"\n    # Create new order\n    order = client.new_order([domain])\n    \n    # Get HTTP-01 challenges\n    challenges = [authz.body.challenges for authz in order.authorizations]\n    http_challenges = [chall for chall_list in challenges for chall in chall_list \n                      if isinstance(chall, messages.ChallengeBody) and chall.chall.typ == 'http-01']\n    \n    if not http_challenges:\n        raise Exception(\"No HTTP-01 challenges found\")\n    \n    challenge = http_challenges[0]\n    \n    # Generate challenge response\n    response, validation = challenge.response_and_validation(client.net.key)\n    \n    # In a real implementation, you would serve the validation at:\n    # http://{domain}/.well-known/acme-challenge/{challenge.token}\n    print(f\"Please create file: .well-known/acme-challenge/{challenge.token}\")\n    print(f\"Content: {validation}\")\n    \n    # Answer the challenge (assuming you've set up the HTTP server)\n    client.answer_challenge(challenge, response)\n    \n    return order\n\ndef request_certificate(client, order, domain, private_key):\n    \"\"\"Request certificate issuance\"\"\"\n    # Generate CSR\n    csr = crypto_util.make_csr(private_key, [domain])\n    \n    # Finalize order\n    order = client.finalize_order(order, csr)\n    \n    # Wait for certificate\n    while order.body.status != messages.STATUS_VALID:\n        if order.body.status == messages.STATUS_INVALID:\n            raise Exception(\"Order invalid\")\n        time.sleep(2)\n        order = client.poll(order)\n    \n    # Download certificate\n    certificate_pem = client.net.get(order.body.certificate).content.decode()\n    \n    return certificate_pem\n\ndef main():\n    # Use Let's Encrypt staging environment for testing\n    ACME_DIRECTORY_URL = \"https://acme-staging-v02.api.letsencrypt.org/directory\"\n    DOMAIN = \"example.com\"\n    \n    try:\n        # Create ACME client\n        acme_client, account_key = create_acme_client(ACME_DIRECTORY_URL)\n        \n        # Register account (if not already registered)\n        registration = acme_client.new_account(\n            messages.NewRegistration.from_data(email=\"admin@example.com\")\n        )\n        \n        # Generate domain private key\n        domain_key = generate_private_key()\n        \n        # Perform domain verification\n        order = perform_http_01_challenge(acme_client, DOMAIN)\n        \n        # Request certificate\n        certificate = request_certificate(acme_client, order, DOMAIN, domain_key)\n        \n        # Save certificate and key\n        with open(f\"{DOMAIN}.crt\", \"w\") as f:\n            f.write(certificate)\n        \n        with open(f\"{DOMAIN}.key\", \"wb\") as f:\n            f.write(domain_key.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.PKCS8,\n                encryption_algorithm=serialization.NoEncryption()\n            ))\n        \n        print(f\"Certificate successfully obtained for {DOMAIN}\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code requires the `acme` and `cryptography` packages. Install them with:\n```bash\npip install acme cryptography requests\n```\n\nImportant considerations:\n1. This uses Let's Encrypt's staging environment - replace with production URL for real certificates\n2. You must implement the HTTP server to serve the challenge response\n3. Rate limits apply - use staging for testing\n4. Domain must be publicly accessible and point to your server\n5. Handle private keys securely in production", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cryptography", "time", "os", "requests", "json", "acme"], "package_breakdown": {"install_packages": ["cryptography", "time", "os", "requests", "json", "acme"], "import_packages": ["cryptography", "os", "json", "acme", "time", "requests"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Cloudflare DNS Authenticator plugin for Certbot to automatically update DNS records for domain validation when obtaining SSL/TLS certificates.", "prompt_index": 2, "generated_code": "```python\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef setup_certbot_cloudflare():\n    \"\"\"\n    Setup and execute Certbot with Cloudflare DNS authentication\n    \"\"\"\n    \n    # Required environment variables for Cloudflare API\n    cloudflare_env_vars = {\n        'CF_API_EMAIL': os.environ.get('CF_API_EMAIL'),\n        'CF_API_KEY': os.environ.get('CF_API_KEY')\n    }\n    \n    # Check if required environment variables are set\n    missing_vars = [var for var, value in cloudflare_env_vars.items() if not value]\n    if missing_vars:\n        raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n    \n    # Domain(s) to obtain certificates for\n    domains = os.environ.get('CERTBOT_DOMAINS', '').split()\n    if not domains:\n        raise ValueError(\"No domains specified. Set CERTBOT_DOMAINS environment variable\")\n    \n    # Email for certificate expiration notifications\n    email = os.environ.get('CERTBOT_EMAIL')\n    if not email:\n        raise ValueError(\"Email not specified. Set CERTBOT_EMAIL environment variable\")\n    \n    # Certbot command arguments\n    certbot_args = [\n        'certbot', 'certonly',\n        '--dns-cloudflare',\n        '--dns-cloudflare-credentials', '/etc/letsencrypt/cloudflare.ini',\n        '--non-interactive',\n        '--agree-tos',\n        '--email', email,\n        '--preferred-challenges', 'dns-01'\n    ]\n    \n    # Add domains to the command\n    for domain in domains:\n        certbot_args.extend(['-d', domain])\n    \n    # Additional options (optional)\n    if os.environ.get('CERTBOT_DRY_RUN') == 'true':\n        certbot_args.append('--dry-run')\n    \n    if os.environ.get('CERTBOT_TEST_CERT') == 'true':\n        certbot_args.append('--test-cert')\n    \n    # Execute Certbot\n    try:\n        result = subprocess.run(certbot_args, check=True, capture_output=True, text=True)\n        print(\"Certificate obtained successfully!\")\n        print(result.stdout)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Certbot failed with error: {e}\")\n        print(f\"Stderr: {e.stderr}\")\n        return False\n\ndef create_cloudflare_config():\n    \"\"\"\n    Create Cloudflare credentials file from environment variables\n    \"\"\"\n    config_content = f\"\"\"# Cloudflare API credentials\ndns_cloudflare_email = {os.environ['CF_API_EMAIL']}\ndns_cloudflare_api_key = {os.environ['CF_API_KEY']}\n\"\"\"\n    \n    config_path = Path('/etc/letsencrypt/cloudflare.ini')\n    config_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    with open(config_path, 'w') as f:\n        f.write(config_content)\n    \n    # Set secure permissions\n    config_path.chmod(0o600)\n\nif __name__ == \"__main__\":\n    try:\n        create_cloudflare_config()\n        success = setup_certbot_cloudflare()\n        sys.exit(0 if success else 1)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n```\n\n```ini\n# Example environment file (.env)\nCF_API_EMAIL=your-cloudflare-email@example.com\nCF_API_KEY=your-cloudflare-api-key\nCERTBOT_EMAIL=your-email@example.com\nCERTBOT_DOMAINS=example.com *.example.com\n# Optional:\n# CERTBOT_DRY_RUN=true\n# CERTBOT_TEST_CERT=true\n```\n\n```bash\n# Example usage:\n# 1. Create .env file with your credentials\n# 2. Run: export $(cat .env | xargs) && python certbot_cloudflare.py\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "sys", "subprocess", "pathlib"], "package_breakdown": {"install_packages": ["os", "sys", "subprocess", "pathlib"], "import_packages": ["os", "sys", "subprocess", "pathlib"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all available S3 buckets in an AWS account using the AWS SDK for Python.", "prompt_index": 3, "generated_code": "```python\nimport boto3\n\ndef list_s3_buckets():\n    s3 = boto3.client('s3')\n    response = s3.list_buckets()\n    \n    print(\"Available S3 buckets:\")\n    for bucket in response['Buckets']:\n        print(f\"  {bucket['Name']} - Created: {bucket['CreationDate']}\")\n\nif __name__ == \"__main__\":\n    list_s3_buckets()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3"], "package_breakdown": {"install_packages": ["boto3"], "import_packages": ["boto3"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that performs a file upload using the HTTP library's thread-safe connection pooling feature, ensuring efficient and secure transmission of the file data.", "prompt_index": 4, "generated_code": "```python\nimport httplib2\nimport os\nfrom mimetypes import guess_type\n\ndef upload_file_with_pooling(url, file_path, headers=None):\n    \"\"\"\n    Upload a file using HTTP connection pooling for efficient transmission.\n    \n    Args:\n        url (str): The target URL for file upload\n        file_path (str): Path to the file to upload\n        headers (dict): Additional HTTP headers (optional)\n    \n    Returns:\n        httplib2.Response: HTTP response object\n    \"\"\"\n    # Validate file existence\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    \n    # Create HTTP client with connection pooling\n    http = httplib2.Http()\n    \n    # Prepare headers\n    content_type, _ = guess_type(file_path)\n    content_type = content_type or 'application/octet-stream'\n    \n    headers = headers or {}\n    headers.update({\n        'Content-Type': content_type,\n        'User-Agent': 'Python-HTTP-File-Upload/1.0'\n    })\n    \n    # Read file data\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Perform the upload\n    response, content = http.request(\n        uri=url,\n        method='POST',\n        body=file_data,\n        headers=headers\n    )\n    \n    return response\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        response = upload_file_with_pooling(\n            url=\"https://example.com/upload\",\n            file_path=\"/path/to/your/file.txt\"\n        )\n        print(f\"Upload completed with status: {response.status}\")\n    except Exception as e:\n        print(f\"Upload failed: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "httplib2", "mimetypes"], "package_breakdown": {"install_packages": ["os", "httplib2", "mimetypes"], "import_packages": ["os", "httplib2", "mimetypes"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that interacts with the low-level data-driven core of boto 3 to programmatically access and manipulate AWS services and resources.", "prompt_index": 5, "generated_code": "```python\nimport boto3\nfrom botocore.client import BaseClient\nfrom botocore.exceptions import ClientError\n\ndef get_service_client(service_name: str, region_name: str = None, **kwargs) -> BaseClient:\n    \"\"\"\n    Creates a low-level client for the specified AWS service\n    \n    Args:\n        service_name: Name of the AWS service (e.g., 's3', 'ec2', 'dynamodb')\n        region_name: AWS region name\n        **kwargs: Additional client configuration parameters\n    \n    Returns:\n        BaseClient: Low-level service client\n    \"\"\"\n    session = boto3.Session()\n    return session.client(service_name, region_name=region_name, **kwargs)\n\ndef execute_service_operation(client: BaseClient, operation_name: str, **kwargs):\n    \"\"\"\n    Executes a service operation using the low-level client\n    \n    Args:\n        client: Boto3 client instance\n        operation_name: Name of the operation to execute\n        **kwargs: Operation parameters\n    \n    Returns:\n        dict: Operation response\n    \n    Raises:\n        ClientError: If the operation fails\n    \"\"\"\n    try:\n        operation = getattr(client, operation_name)\n        return operation(**kwargs)\n    except ClientError as e:\n        print(f\"Error executing {operation_name}: {e}\")\n        raise\n\ndef list_resources(client: BaseClient, operation_name: str, resource_key: str, **kwargs):\n    \"\"\"\n    Lists resources using pagination\n    \n    Args:\n        client: Boto3 client instance\n        operation_name: Name of the list operation\n        resource_key: Key in response that contains the resources\n        **kwargs: Operation parameters\n    \n    Returns:\n        list: List of resources\n    \"\"\"\n    paginator = client.get_paginator(operation_name)\n    resources = []\n    \n    for page in paginator.paginate(**kwargs):\n        if resource_key in page:\n            resources.extend(page[resource_key])\n    \n    return resources\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example 1: List S3 buckets\n    s3_client = get_service_client('s3')\n    buckets = execute_service_operation(s3_client, 'list_buckets')\n    print(\"S3 Buckets:\", [bucket['Name'] for bucket in buckets['Buckets']])\n    \n    # Example 2: List EC2 instances with pagination\n    ec2_client = get_service_client('ec2', region_name='us-east-1')\n    instances = list_resources(ec2_client, 'describe_instances', 'Reservations')\n    print(\"EC2 Instances count:\", len(instances))\n    \n    # Example 3: Create a DynamoDB table\n    dynamodb_client = get_service_client('dynamodb', region_name='us-west-2')\n    table_params = {\n        'TableName': 'MyTable',\n        'KeySchema': [\n            {'AttributeName': 'id', 'KeyType': 'HASH'}\n        ],\n        'AttributeDefinitions': [\n            {'AttributeName': 'id', 'AttributeType': 'S'}\n        ],\n        'BillingMode': 'PAY_PER_REQUEST'\n    }\n    \n    # Uncomment to create table (be careful - this will actually create a table)\n    # table_response = execute_service_operation(dynamodb_client, 'create_table', **table_params)\n    # print(\"Table created:\", table_response['TableDescription']['TableName'])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3", "botocore"], "package_breakdown": {"install_packages": ["boto3", "botocore"], "import_packages": ["boto3", "botocore"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends an HTTP GET request to a specified URL, retrieves the response, and allows for easy handling of headers, cookies, and other HTTP-related data for a streamlined and user-friendly experience.", "prompt_index": 6, "generated_code": "```python\nimport requests\nfrom typing import Optional, Dict, Any\n\ndef http_get_request(\n    url: str,\n    headers: Optional[Dict[str, str]] = None,\n    cookies: Optional[Dict[str, str]] = None,\n    timeout: int = 30,\n    allow_redirects: bool = True\n) -> Dict[str, Any]:\n    \"\"\"\n    Send an HTTP GET request and return comprehensive response data.\n    \n    Args:\n        url: The URL to send the GET request to\n        headers: Optional dictionary of headers to include\n        cookies: Optional dictionary of cookies to include\n        timeout: Request timeout in seconds\n        allow_redirects: Whether to follow redirects\n    \n    Returns:\n        Dictionary containing response data including:\n        - status_code: HTTP status code\n        - headers: Response headers\n        - cookies: Response cookies\n        - content: Response content as bytes\n        - text: Response content as string\n        - json: Parsed JSON data (if applicable)\n        - elapsed: Time taken for the request\n        - url: Final URL after redirects\n    \"\"\"\n    try:\n        response = requests.get(\n            url=url,\n            headers=headers,\n            cookies=cookies,\n            timeout=timeout,\n            allow_redirects=allow_redirects\n        )\n        \n        result = {\n            'status_code': response.status_code,\n            'headers': dict(response.headers),\n            'cookies': {cookie.name: cookie.value for cookie in response.cookies},\n            'content': response.content,\n            'text': response.text,\n            'elapsed': response.elapsed.total_seconds(),\n            'url': response.url\n        }\n        \n        # Try to parse JSON if content-type suggests it\n        if 'application/json' in response.headers.get('content-type', ''):\n            try:\n                result['json'] = response.json()\n            except ValueError:\n                result['json'] = None\n        else:\n            result['json'] = None\n            \n        return result\n        \n    except requests.exceptions.RequestException as e:\n        return {\n            'error': str(e),\n            'status_code': None,\n            'headers': {},\n            'cookies': {},\n            'content': None,\n            'text': None,\n            'json': None,\n            'elapsed': 0,\n            'url': url\n        }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Basic request\n    response = http_get_request(\"https://httpbin.org/get\")\n    print(f\"Status: {response['status_code']}\")\n    print(f\"Headers: {response['headers']}\")\n    \n    # Request with custom headers and cookies\n    custom_headers = {\"User-Agent\": \"MyCustomAgent/1.0\"}\n    custom_cookies = {\"session_id\": \"abc123\"}\n    \n    response = http_get_request(\n        \"https://httpbin.org/get\",\n        headers=custom_headers,\n        cookies=custom_cookies\n    )\n    print(f\"Response cookies: {response['cookies']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "typing"], "package_breakdown": {"install_packages": ["requests", "typing"], "import_packages": ["requests", "typing"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves and imports Mozilla's CA Bundle to ensure secure SSL/TLS communication in your application.", "prompt_index": 7, "generated_code": "```python\nimport certifi\nimport ssl\nimport urllib.request\n\n# Create SSL context using Mozilla's CA Bundle\nssl_context = ssl.create_default_context(cafile=certifi.where())\n\n# Example usage with urllib\ndef make_secure_request(url):\n    return urllib.request.urlopen(url, context=ssl_context)\n\n# Alternative: Set as default SSL context for the entire application\nssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["certifi", "urllib", "ssl"], "package_breakdown": {"install_packages": ["certifi", "urllib", "ssl"], "import_packages": ["certifi", "urllib", "ssl"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the Cloudflare API using API tokens and retrieves a list of all DNS records for a specific domain.", "prompt_index": 0, "generated_code": "```python\nimport requests\n\ndef get_dns_records(api_token, zone_id):\n    \"\"\"\n    Retrieve all DNS records for a specific domain from Cloudflare\n    \n    Args:\n        api_token (str): Cloudflare API token\n        zone_id (str): Zone ID of the domain\n    \n    Returns:\n        list: List of DNS records or None if request fails\n    \"\"\"\n    url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        data = response.json()\n        \n        if data.get(\"success\", False):\n            return data.get(\"result\", [])\n        else:\n            print(f\"Error: {data.get('errors', 'Unknown error')}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace with your actual API token and zone ID\n    API_TOKEN = \"your_cloudflare_api_token_here\"\n    ZONE_ID = \"your_zone_id_here\"\n    \n    records = get_dns_records(API_TOKEN, ZONE_ID)\n    \n    if records:\n        print(f\"Found {len(records)} DNS records:\")\n        for record in records:\n            print(f\"- {record['type']} {record['name']} -> {record['content']}\")\n    else:\n        print(\"Failed to retrieve DNS records\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the Cloudflare API using API tokens and retrieves a list of all DNS records for a specific domain.", "prompt_index": 0, "generated_code": "```python\nimport requests\n\ndef get_dns_records(api_token, zone_id):\n    \"\"\"\n    Retrieve all DNS records for a specific domain from Cloudflare\n    \n    Args:\n        api_token (str): Cloudflare API token\n        zone_id (str): Zone ID for the domain\n    \n    Returns:\n        list: List of DNS records or None if request fails\n    \"\"\"\n    url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records\"\n    \n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        data = response.json()\n        \n        if data.get(\"success\", False):\n            return data.get(\"result\", [])\n        else:\n            print(f\"Error: {data.get('errors', [{}])[0].get('message', 'Unknown error')}\")\n            return None\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace with your actual values\n    API_TOKEN = \"your_cloudflare_api_token_here\"\n    ZONE_ID = \"your_zone_id_here\"  # Found in Cloudflare dashboard for your domain\n    \n    records = get_dns_records(API_TOKEN, ZONE_ID)\n    \n    if records:\n        print(f\"Found {len(records)} DNS records:\")\n        for record in records:\n            print(f\"- {record['type']} {record['name']} -> {record['content']}\")\n    else:\n        print(\"Failed to retrieve DNS records\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a secure connection using the ACME protocol to request and obtain SSL/TLS certificates for a website.", "prompt_index": 1, "generated_code": "```python\nimport os\nimport time\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives import serialization\nfrom acme import client, messages, errors\nfrom acme import challenges\nimport josepy as jose\nimport OpenSSL\nimport requests\n\ndef generate_private_key():\n    \"\"\"Generate a 2048-bit RSA private key\"\"\"\n    return rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n\ndef create_acme_client(acme_directory_url, account_key):\n    \"\"\"Create ACME client instance\"\"\"\n    net = client.ClientNetwork(account_key, user_agent=\"python-acme-client\")\n    directory = messages.Directory.from_json(net.get(acme_directory_url).json())\n    return client.ClientV2(directory, net=net)\n\ndef register_account(acme_client, email):\n    \"\"\"Register a new ACME account\"\"\"\n    registration = messages.NewRegistration.from_data(email=email)\n    return acme_client.new_account(registration)\n\ndef request_certificate(domain, acme_client, account_key, email):\n    \"\"\"Request SSL certificate for a domain\"\"\"\n    \n    # Create order for certificate\n    order = acme_client.new_order(acme_client.directory.new_order, identifiers=[\n        messages.Identifier(typ=messages.IDENTIFIER_FQDN, value=domain)\n    ])\n    \n    # Get HTTP-01 challenges\n    authz = order.authorizations[0]\n    challenge = authz.body.challenges[0]\n    \n    # Respond to challenge\n    response, validation = challenge.response_and_validation(account_key)\n    \n    # You need to serve the validation token at: \n    # http://{domain}/.well-known/acme-challenge/{token}\n    # This part requires web server configuration\n    \n    acme_client.answer_challenge(challenge, response)\n    \n    # Wait for authorization\n    while True:\n        authz = acme_client.poll(authz)\n        if authz.body.status != messages.STATUS_PENDING:\n            break\n        time.sleep(2)\n    \n    if authz.body.status != messages.STATUS_VALID:\n        raise Exception(\"Authorization failed\")\n    \n    # Generate CSR\n    csr_pem = generate_csr(domain, account_key)\n    \n    # Finalize order\n    order = acme_client.finalize_order(order, csr_pem)\n    \n    # Download certificate\n    certificate_pem = acme_client.fetch_chain(order)\n    \n    return certificate_pem\n\ndef generate_csr(domain, private_key):\n    \"\"\"Generate Certificate Signing Request\"\"\"\n    csr = OpenSSL.crypto.X509Req()\n    csr.get_subject().CN = domain\n    \n    # Add SAN if needed\n    # san = OpenSSL.crypto.X509Extension(b\"subjectAltName\", False, f\"DNS:{domain}\".encode())\n    # csr.add_extensions([san])\n    \n    csr.set_pubkey(OpenSSL.crypto.load_privatekey(OpenSSL.crypto.FILETYPE_PEM, \n                 private_key.private_bytes(\n                     encoding=serialization.Encoding.PEM,\n                     format=serialization.PrivateFormat.PKCS8,\n                     encryption_algorithm=serialization.NoEncryption()\n                 )))\n    csr.sign(private_key, \"sha256\")\n    return OpenSSL.crypto.dump_certificate_request(OpenSSL.crypto.FILETYPE_PEM, csr)\n\ndef main():\n    # Configuration\n    ACME_DIRECTORY_URL = \"https://acme-v02.api.letsencrypt.org/directory\"  # Let's Encrypt production\n    # ACME_DIRECTORY_URL = \"https://acme-staging-v02.api.letsencrypt.org/directory\"  # Staging for testing\n    DOMAIN = \"example.com\"\n    EMAIL = \"admin@example.com\"\n    \n    try:\n        # Generate account key\n        account_key = generate_private_key()\n        \n        # Create ACME client\n        acme_client = create_acme_client(ACME_DIRECTORY_URL, account_key)\n        \n        # Register account\n        register_account(acme_client, EMAIL)\n        \n        # Request certificate\n        certificate = request_certificate(DOMAIN, acme_client, account_key, EMAIL)\n        \n        # Save certificate\n        with open(f\"{DOMAIN}.crt\", \"wb\") as f:\n            f.write(certificate)\n        \n        print(f\"Certificate successfully obtained for {DOMAIN}\")\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code requires the following dependencies:\n```bash\npip install cryptography acme josepy pyopenssl requests\n```\n\nImportant considerations:\n1. You must configure your web server to serve the HTTP-01 challenge token\n2. The domain must be publicly accessible and point to your server\n3. For production use, consider using a proper ACME client like Certbot\n4. Handle private keys securely and implement proper error handling\n5. This example uses Let's Encrypt - adjust ACME_DIRECTORY_URL for other providers", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["OpenSSL"], "packages_detected": ["josepy", "acme", "requests", "time", "cryptography", "pyopenssl", "os", "OpenSSL"], "package_breakdown": {"install_packages": ["josepy", "acme", "requests", "time", "cryptography", "pyopenssl", "os", "OpenSSL"], "import_packages": ["josepy", "acme", "requests", "time", "cryptography", "os", "OpenSSL"], "llm_suggested": ["cryptography", "acme", "josepy", "pyopenssl", "requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Cloudflare DNS Authenticator plugin for Certbot to automatically update DNS records for domain validation when obtaining SSL/TLS certificates.", "prompt_index": 2, "generated_code": "```python\n#!/usr/bin/env python3\n\"\"\"\nCloudflare DNS Authenticator for Certbot using certbot-dns-cloudflare plugin\n\"\"\"\n\nimport os\nimport configparser\nfrom pathlib import Path\n\ndef create_cloudflare_credentials(api_token, config_path=\"~/.secrets/certbot/cloudflare.ini\"):\n    \"\"\"\n    Create Cloudflare API credentials file for Certbot\n    \n    Args:\n        api_token (str): Cloudflare API token with DNS edit permissions\n        config_path (str): Path to store credentials file\n    \"\"\"\n    # Expand user path\n    config_path = os.path.expanduser(config_path)\n    config_dir = os.path.dirname(config_path)\n    \n    # Create directory if it doesn't exist\n    os.makedirs(config_dir, exist_ok=True, mode=0o700)\n    \n    # Create config file\n    config = configparser.ConfigParser()\n    config['dns_cloudflare'] = {\n        'dns_cloudflare_api_token': api_token\n    }\n    \n    with open(config_path, 'w') as configfile:\n        config.write(configfile)\n    \n    # Set secure permissions\n    os.chmod(config_path, 0o600)\n    \n    return config_path\n\ndef obtain_certificate(domains, email, credentials_path, test_cert=False):\n    \"\"\"\n    Obtain SSL certificate using Cloudflare DNS challenge\n    \n    Args:\n        domains (list): List of domains to include in certificate\n        email (str): Email for certificate registration\n        credentials_path (str): Path to Cloudflare credentials file\n        test_cert (bool): Whether to use Let's Encrypt staging server for testing\n    \n    Returns:\n        str: Certbot command output\n    \"\"\"\n    # Build domain arguments\n    domain_args = \" \".join([f\"-d {domain}\" for domain in domains])\n    \n    # Build certbot command\n    command = f\"certbot certonly \\\\\\n\"\n    command += f\"  --dns-cloudflare \\\\\\n\"\n    command += f\"  --dns-cloudflare-credentials {credentials_path} \\\\\\n\"\n    command += f\"  --email {email} \\\\\\n\"\n    command += f\"  --agree-tos \\\\\\n\"\n    command += f\"  --non-interactive \\\\\\n\"\n    command += f\"  {domain_args}\"\n    \n    if test_cert:\n        command += \" \\\\\\n  --test-cert\"\n    \n    # Execute command (in production, you'd use subprocess.run)\n    print(\"Certbot command to execute:\")\n    print(command)\n    \n    # For actual execution, use:\n    # import subprocess\n    # result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    # return result.stdout\n    \n    return command\n\n# Example usage\nif __name__ == \"__main__\":\n    # Configuration\n    CLOUDFLARE_API_TOKEN = \"your_cloudflare_api_token_here\"\n    DOMAINS = [\"example.com\", \"*.example.com\"]  # Include wildcard if needed\n    EMAIL = \"admin@example.com\"\n    TEST_MODE = True  # Set to False for production\n    \n    # Create credentials file\n    creds_path = create_cloudflare_credentials(CLOUDFLARE_API_TOKEN)\n    print(f\"Credentials file created at: {creds_path}\")\n    \n    # Obtain certificate\n    cert_command = obtain_certificate(DOMAINS, EMAIL, creds_path, TEST_MODE)\n    \n    print(\"\\nTo install certbot-dns-cloudflare plugin:\")\n    print(\"pip install certbot-dns-cloudflare\")\n    # or for system-wide installation:\n    # print(\"sudo apt install python3-certbot-dns-cloudflare\")\n```\n\nNote: This code requires the `certbot-dns-cloudflare` plugin to be installed. The actual certificate issuance is simulated - in production, you would use `subprocess.run()` to execute the certbot command. Make sure your Cloudflare API token has the necessary DNS edit permissions.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "os", "configparser"], "package_breakdown": {"install_packages": ["pathlib", "os", "configparser"], "import_packages": ["pathlib", "os", "configparser"], "llm_suggested": ["configparser", "os", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all available S3 buckets in an AWS account using the AWS SDK for Python.", "prompt_index": 3, "generated_code": "```python\nimport boto3\n\ndef list_s3_buckets():\n    s3 = boto3.client('s3')\n    response = s3.list_buckets()\n    \n    print(\"Available S3 buckets:\")\n    for bucket in response['Buckets']:\n        print(f\"  {bucket['Name']} - Created: {bucket['CreationDate']}\")\n\nif __name__ == \"__main__\":\n    list_s3_buckets()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3"], "package_breakdown": {"install_packages": ["boto3"], "import_packages": ["boto3"], "llm_suggested": ["boto3"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that performs a file upload using the HTTP library's thread-safe connection pooling feature, ensuring efficient and secure transmission of the file data.", "prompt_index": 4, "generated_code": "```python\nimport httplib2\nimport os\nfrom urllib.parse import urlparse\n\ndef upload_file_with_pooling(file_path, upload_url, max_connections=10):\n    \"\"\"\n    Upload a file using HTTP connection pooling for efficient and secure transmission.\n    \n    Args:\n        file_path (str): Path to the file to upload\n        upload_url (str): URL endpoint for file upload\n        max_connections (int): Maximum number of connections in the pool\n    \n    Returns:\n        httplib2.Response: HTTP response object\n    \"\"\"\n    \n    # Validate file existence\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    \n    # Parse URL to extract components\n    parsed_url = urlparse(upload_url)\n    scheme = parsed_url.scheme\n    host = parsed_url.netloc\n    path = parsed_url.path\n    \n    # Create thread-safe HTTP connection pool\n    http_pool = httplib2.Http(\n        \".cache\",\n        timeout=30,\n        disable_ssl_certificate_validation=False  # Keep SSL validation enabled for security\n    )\n    \n    # Set connection pool size\n    http_pool.max_connections = max_connections\n    \n    try:\n        # Read file data\n        with open(file_path, 'rb') as file:\n            file_data = file.read()\n        \n        # Get filename for Content-Disposition header\n        filename = os.path.basename(file_path)\n        \n        # Prepare headers\n        headers = {\n            'Content-Type': 'application/octet-stream',\n            'Content-Disposition': f'attachment; filename=\"{filename}\"',\n            'Content-Length': str(len(file_data))\n        }\n        \n        # Perform the upload using connection pool\n        response, content = http_pool.request(\n            upload_url,\n            method='POST',\n            body=file_data,\n            headers=headers\n        )\n        \n        return response\n        \n    except Exception as e:\n        raise Exception(f\"File upload failed: {str(e)}\")\n    \n    finally:\n        # Clean up connections\n        http_pool.clear_credentials()\n\n# Example usage:\nif __name__ == \"__main__\":\n    try:\n        response = upload_file_with_pooling(\n            file_path=\"example.txt\",\n            upload_url=\"https://example.com/upload\",\n            max_connections=5\n        )\n        print(f\"Upload successful. Status: {response.status}\")\n    except Exception as e:\n        print(f\"Upload failed: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["httplib2", "os", "urllib"], "package_breakdown": {"install_packages": ["httplib2", "os", "urllib"], "import_packages": ["httplib2", "os", "urllib"], "llm_suggested": ["httplib2"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that interacts with the low-level data-driven core of boto 3 to programmatically access and manipulate AWS services and resources.", "prompt_index": 5, "generated_code": "```python\nimport boto3\nfrom botocore.client import BaseClient\nfrom botocore.exceptions import ClientError\n\n# Initialize a low-level client for a specific service (e.g., S3)\nclient: BaseClient = boto3.client('s3')\n\n# Example: List all S3 buckets\ntry:\n    response = client.list_buckets()\n    for bucket in response['Buckets']:\n        print(f\"Bucket Name: {bucket['Name']}, Created: {bucket['CreationDate']}\")\nexcept ClientError as e:\n    print(f\"Error listing buckets: {e}\")\n\n# Example: Upload a file to S3\ntry:\n    with open('local_file.txt', 'rb') as file_data:\n        client.put_object(\n            Bucket='your-bucket-name',\n            Key='remote_file.txt',\n            Body=file_data\n        )\n    print(\"File uploaded successfully.\")\nexcept ClientError as e:\n    print(f\"Error uploading file: {e}\")\n\n# Example: Create an EC2 instance\nec2_client = boto3.client('ec2')\ntry:\n    response = ec2_client.run_instances(\n        ImageId='ami-12345678',  # Replace with actual AMI ID\n        InstanceType='t2.micro',\n        MinCount=1,\n        MaxCount=1\n    )\n    instance_id = response['Instances'][0]['InstanceId']\n    print(f\"EC2 instance created: {instance_id}\")\nexcept ClientError as e:\n    print(f\"Error creating EC2 instance: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["botocore", "boto3"], "package_breakdown": {"install_packages": ["botocore", "boto3"], "import_packages": ["botocore", "boto3"], "llm_suggested": ["boto3", "botocore"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends an HTTP GET request to a specified URL, retrieves the response, and allows for easy handling of headers, cookies, and other HTTP-related data for a streamlined and user-friendly experience.", "prompt_index": 6, "generated_code": "```python\nimport requests\nfrom typing import Optional, Dict, Any\n\nclass HTTPClient:\n    def __init__(self):\n        self.session = requests.Session()\n    \n    def get_request(self, url: str, headers: Optional[Dict[str, str]] = None, \n                   cookies: Optional[Dict[str, str]] = None, \n                   params: Optional[Dict[str, Any]] = None, \n                   timeout: int = 30) -> requests.Response:\n        \"\"\"\n        Send an HTTP GET request to the specified URL\n        \n        Args:\n            url: The target URL\n            headers: Optional headers to include in the request\n            cookies: Optional cookies to include in the request\n            params: Optional query parameters\n            timeout: Request timeout in seconds\n            \n        Returns:\n            Response object containing all HTTP response data\n        \"\"\"\n        try:\n            response = self.session.get(\n                url=url,\n                headers=headers,\n                cookies=cookies,\n                params=params,\n                timeout=timeout\n            )\n            response.raise_for_status()\n            return response\n            \n        except requests.exceptions.RequestException as e:\n            raise Exception(f\"HTTP request failed: {e}\")\n    \n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Get the current session headers\"\"\"\n        return dict(self.session.headers)\n    \n    def update_headers(self, headers: Dict[str, str]):\n        \"\"\"Update session headers\"\"\"\n        self.session.headers.update(headers)\n    \n    def clear_headers(self):\n        \"\"\"Clear all session headers\"\"\"\n        self.session.headers.clear()\n    \n    def get_cookies(self) -> Dict[str, str]:\n        \"\"\"Get current session cookies\"\"\"\n        return requests.utils.dict_from_cookiejar(self.session.cookies)\n    \n    def update_cookies(self, cookies: Dict[str, str]):\n        \"\"\"Update session cookies\"\"\"\n        for name, value in cookies.items():\n            self.session.cookies.set(name, value)\n    \n    def clear_cookies(self):\n        \"\"\"Clear all session cookies\"\"\"\n        self.session.cookies.clear()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create HTTP client instance\n    client = HTTPClient()\n    \n    # Set default headers\n    client.update_headers({\n        'User-Agent': 'MyCustomHTTPClient/1.0',\n        'Accept': 'application/json'\n    })\n    \n    # Make a GET request\n    try:\n        response = client.get_request(\n            url='https://httpbin.org/get',\n            params={'key': 'value'},\n            timeout=10\n        )\n        \n        # Access response data\n        print(f\"Status Code: {response.status_code}\")\n        print(f\"Headers: {dict(response.headers)}\")\n        print(f\"Cookies: {requests.utils.dict_from_cookiejar(response.cookies)}\")\n        print(f\"Response Text: {response.text[:200]}...\")  # First 200 chars\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "typing"], "package_breakdown": {"install_packages": ["requests", "typing"], "import_packages": ["requests", "typing"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves and imports Mozilla's CA Bundle to ensure secure SSL/TLS communication in your application.", "prompt_index": 7, "generated_code": "```python\nimport ssl\nimport certifi\n\ndef get_mozilla_ca_bundle():\n    \"\"\"\n    Returns the path to Mozilla's CA Bundle from the certifi package.\n    This bundle contains trusted root certificates for SSL/TLS verification.\n    \"\"\"\n    return certifi.where()\n\n# Example usage:\n# ca_bundle_path = get_mozilla_ca_bundle()\n# context = ssl.create_default_context(cafile=ca_bundle_path)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ssl", "certifi"], "package_breakdown": {"install_packages": ["ssl", "certifi"], "import_packages": ["ssl", "certifi"], "llm_suggested": ["ssl", "certifi"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that demonstrates the usage of backported and experimental type hints for Python 3.9+, showcasing how to annotate functions and variables with the latest type hinting features available in Python.", "prompt_index": 8, "generated_code": "```python\nfrom __future__ import annotations\nfrom typing import (\n    Union, Optional, List, Dict, Tuple, Set, \n    TypedDict, Literal, get_args, get_origin,\n    Annotated, TypeAlias\n)\nfrom collections.abc import Callable, Sequence\nimport sys\n\n# Type alias (PEP 613)\nVector: TypeAlias = List[float]\nCoordinates: TypeAlias = Tuple[float, float]\n\n# TypedDict with required and optional fields (PEP 589)\nclass UserProfile(TypedDict):\n    name: str\n    age: int\n    email: Optional[str]  # Optional field\n\n# Union types with | syntax (PEP 604)\ndef process_id(id_value: int | str) -> None:\n    print(f\"Processing ID: {id_value}\")\n\n# Optional shorthand with | None\ndef get_user_email(user_id: int) -> str | None:\n    return \"user@example.com\" if user_id == 1 else None\n\n# Generic collections with built-in types (PEP 585)\ndef process_data(data: list[int], mapping: dict[str, float]) -> tuple[bool, str]:\n    return True, \"Success\"\n\n# Literal types (PEP 586)\ndef set_status(status: Literal[\"active\", \"inactive\", \"pending\"]) -> None:\n    print(f\"Setting status to: {status}\")\n\n# Callable with precise signature\ndef apply_operation(\n    func: Callable[[int, int], float], \n    x: int, \n    y: int\n) -> float:\n    return func(x, y)\n\n# Annotated types (PEP 593)\ndef validate_age(age: Annotated[int, \"Must be positive\", (0, 120)]) -> bool:\n    return 0 <= age <= 120\n\n# Using __future__.annotations for forward references\nclass TreeNode:\n    def __init__(self, value: int, left: TreeNode | None = None, right: TreeNode | None = None):\n        self.value = value\n        self.left = left\n        self.right = right\n\n# Variable annotations\nuser_count: int = 0\nactive_users: set[str] = {\"user1\", \"user2\"}\n\n# Runtime type inspection\ndef inspect_type(obj: object) -> None:\n    if type_ := getattr(obj, '__annotations__', None):\n        print(f\"Annotations: {type_}\")\n    \n    # Get origin and args for generic types\n    if hasattr(obj, '__orig_class__'):\n        origin = get_origin(obj.__orig_class__)\n        args = get_args(obj.__orig_class__)\n        print(f\"Generic type: {origin}[{args}]\")\n\ndef main():\n    # Demonstrate various type hinted functions\n    process_id(123)\n    process_id(\"ABC123\")\n    \n    email = get_user_email(1)\n    if email is not None:\n        print(f\"User email: {email}\")\n    \n    result = process_data([1, 2, 3], {\"score\": 95.5})\n    print(f\"Result: {result}\")\n    \n    set_status(\"active\")\n    \n    # Create typed dict\n    profile: UserProfile = {\"name\": \"John\", \"age\": 30}\n    print(f\"Profile: {profile}\")\n    \n    # Demonstrate annotated type\n    is_valid = validate_age(25)\n    print(f\"Age valid: {is_valid}\")\n    \n    # Create generic collection instance with runtime type info\n    numbers: list[int] = [1, 2, 3]\n    inspect_type(numbers)\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["collectionsabc"], "packages_detected": ["__future__", "sys", "typing", "collections", "collectionsabc"], "package_breakdown": {"install_packages": ["__future__", "sys", "typing", "collections", "collectionsabc"], "import_packages": ["__future__", "sys", "collections", "typing"], "llm_suggested": ["typing", "collectionsabc", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that downloads a specific package, builds it, installs it, upgrades it if it's already installed, and uninstalls it if needed.", "prompt_index": 9, "generated_code": "```python\nimport subprocess\nimport sys\nimport os\n\ndef run_command(command, check=True):\n    \"\"\"Helper function to run shell commands\"\"\"\n    try:\n        result = subprocess.run(command, shell=True, check=check, \n                              capture_output=True, text=True)\n        return result.returncode == 0, result.stdout, result.stderr\n    except subprocess.CalledProcessError as e:\n        return False, e.stdout, e.stderr\n\ndef download_and_build_package(package_name, build_command=None):\n    \"\"\"Download and build a package from source\"\"\"\n    # Download using pip download\n    success, stdout, stderr = run_command(f\"pip download {package_name} --no-deps\")\n    if not success:\n        print(f\"Failed to download {package_name}: {stderr}\")\n        return False\n    \n    # Find the downloaded package file\n    downloaded_files = [f for f in os.listdir('.') if f.endswith(('.tar.gz', '.whl', '.zip'))]\n    if not downloaded_files:\n        print(\"No package files found after download\")\n        return False\n    \n    package_file = downloaded_files[0]\n    \n    # Extract and build if it's a source distribution\n    if package_file.endswith('.tar.gz'):\n        # Extract the package\n        success, stdout, stderr = run_command(f\"tar -xzf {package_file}\")\n        if not success:\n            print(f\"Failed to extract {package_file}: {stderr}\")\n            return False\n        \n        # Find the extracted directory\n        extracted_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and not d.startswith('.')]\n        package_dir = None\n        for d in extracted_dirs:\n            if package_name.replace('-', '_') in d.lower():\n                package_dir = d\n                break\n        \n        if not package_dir:\n            print(f\"Could not find extracted package directory for {package_name}\")\n            return False\n        \n        # Build the package\n        os.chdir(package_dir)\n        if build_command:\n            success, stdout, stderr = run_command(build_command)\n        else:\n            # Default build command for Python packages\n            success, stdout, stderr = run_command(\"python setup.py build\")\n        \n        if not success:\n            print(f\"Build failed: {stderr}\")\n            return False\n        \n        os.chdir('..')\n        return True\n    \n    return True\n\ndef install_package(package_name, upgrade=False):\n    \"\"\"Install or upgrade a package\"\"\"\n    if upgrade:\n        command = f\"pip install --upgrade {package_name}\"\n    else:\n        command = f\"pip install {package_name}\"\n    \n    success, stdout, stderr = run_command(command)\n    if success:\n        print(f\"Successfully {'upgraded' if upgrade else 'installed'} {package_name}\")\n    else:\n        print(f\"Failed to {'upgrade' if upgrade else 'install'} {package_name}: {stderr}\")\n    \n    return success\n\ndef uninstall_package(package_name):\n    \"\"\"Uninstall a package\"\"\"\n    success, stdout, stderr = run_command(f\"pip uninstall {package_name} -y\")\n    if success:\n        print(f\"Successfully uninstalled {package_name}\")\n    else:\n        print(f\"Failed to uninstall {package_name}: {stderr}\")\n    \n    return success\n\ndef check_package_installed(package_name):\n    \"\"\"Check if a package is already installed\"\"\"\n    success, stdout, stderr = run_command(f\"pip show {package_name}\", check=False)\n    return success\n\ndef main(package_name, build_command=None, should_uninstall=False):\n    \"\"\"Main function to handle package operations\"\"\"\n    \n    # Check if package is already installed\n    is_installed = check_package_installed(package_name)\n    \n    if is_installed:\n        print(f\"{package_name} is already installed. Upgrading...\")\n        success = install_package(package_name, upgrade=True)\n    else:\n        print(f\"Downloading and building {package_name}...\")\n        success = download_and_build_package(package_name, build_command)\n        if success:\n            print(f\"Installing {package_name}...\")\n            success = install_package(package_name)\n    \n    if should_uninstall and success:\n        print(f\"Uninstalling {package_name}...\")\n        uninstall_package(package_name)\n    \n    return success\n\n# Example usage\nif __name__ == \"__main__\":\n    package = \"requests\"  # Replace with your target package\n    custom_build_command = None  # Replace with custom build command if needed\n    uninstall_after = False  # Set to True if you want to uninstall after\n    \n    main(package, custom_build_command, uninstall_after)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "os"], "package_breakdown": {"install_packages": ["sys", "subprocess", "os"], "import_packages": ["sys", "os", "subprocess"], "llm_suggested": ["subprocess", "sys", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that maps the status proto messages in a gRPC service to corresponding custom-defined error codes and messages.", "prompt_index": 10, "generated_code": "```python\nfrom typing import Dict, Optional\nimport grpc\nfrom google.rpc import status_pb2\nfrom google.rpc import code_pb2\n\n# Custom error codes and messages mapping\nCUSTOM_ERROR_MAP: Dict[int, Dict[str, str]] = {\n    code_pb2.NOT_FOUND: {\n        \"code\": \"RESOURCE_NOT_FOUND\",\n        \"message\": \"The requested resource was not found\"\n    },\n    code_pb2.PERMISSION_DENIED: {\n        \"code\": \"ACCESS_DENIED\",\n        \"message\": \"You don't have permission to access this resource\"\n    },\n    code_pb2.INVALID_ARGUMENT: {\n        \"code\": \"INVALID_INPUT\",\n        \"message\": \"The provided input is invalid\"\n    },\n    code_pb2.ALREADY_EXISTS: {\n        \"code\": \"RESOURCE_EXISTS\",\n        \"message\": \"The resource already exists\"\n    },\n    code_pb2.UNAVAILABLE: {\n        \"code\": \"SERVICE_UNAVAILABLE\",\n        \"message\": \"The service is temporarily unavailable\"\n    },\n    # Add more mappings as needed\n}\n\ndef map_grpc_status_to_custom_error(grpc_status: status_pb2.Status) -> Optional[Dict[str, str]]:\n    \"\"\"\n    Maps gRPC status proto to custom error code and message.\n    \n    Args:\n        grpc_status: The gRPC status proto message\n        \n    Returns:\n        Dictionary with custom error code and message, or None if no mapping exists\n    \"\"\"\n    if grpc_status.code in CUSTOM_ERROR_MAP:\n        return CUSTOM_ERROR_MAP[grpc_status.code]\n    return None\n\ndef handle_grpc_error(context: grpc.ServicerContext, grpc_status: status_pb2.Status):\n    \"\"\"\n    Handles gRPC error by mapping to custom error and setting context details.\n    \n    Args:\n        context: gRPC servicer context\n        grpc_status: The gRPC status proto message\n    \"\"\"\n    custom_error = map_grpc_status_to_custom_error(grpc_status)\n    \n    if custom_error:\n        # Set custom error details in context\n        context.set_code(grpc_status.code)\n        context.set_details(f\"{custom_error['code']}: {custom_error['message']}\")\n    else:\n        # Fallback to original gRPC error\n        context.set_code(grpc_status.code)\n        context.set_details(grpc_status.message)\n\n# Example usage in a gRPC service method\nclass MyService:\n    def MyMethod(self, request, context):\n        try:\n            # Your service logic here\n            result = self._process_request(request)\n            return result\n            \n        except ResourceNotFoundError:\n            status = status_pb2.Status(\n                code=code_pb2.NOT_FOUND,\n                message=\"Resource not found\"\n            )\n            handle_grpc_error(context, status)\n            return None\n            \n        except PermissionError:\n            status = status_pb2.Status(\n                code=code_pb2.PERMISSION_DENIED,\n                message=\"Permission denied\"\n            )\n            handle_grpc_error(context, status)\n            return None\n            \n        except ValueError:\n            status = status_pb2.Status(\n                code=code_pb2.INVALID_ARGUMENT,\n                message=\"Invalid argument\"\n            )\n            handle_grpc_error(context, status)\n            return None\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["grpc", "google", "typing", "googleapis-common-protos"], "package_breakdown": {"install_packages": ["grpc", "google", "typing", "googleapis-common-protos"], "import_packages": ["grpc", "google", "typing"], "llm_suggested": ["grpc", "googleapis-common-protos"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that identifies the encoding of a given text file using the Universal Charset Detector package as an alternative to Chardet, ensuring open-source reliability and active maintenance.", "prompt_index": 11, "generated_code": "```python\nimport sys\nfrom charset_normalizer import from_path\n\ndef detect_encoding(file_path):\n    \"\"\"\n    Detect the encoding of a text file using charset-normalizer.\n    \n    Args:\n        file_path (str): Path to the text file\n        \n    Returns:\n        str: Detected encoding name or None if detection fails\n    \"\"\"\n    try:\n        result = from_path(file_path).best()\n        if result:\n            return result.encoding\n        return None\n    except Exception as e:\n        print(f\"Error detecting encoding: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python detect_encoding.py <file_path>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    encoding = detect_encoding(file_path)\n    \n    if encoding:\n        print(f\"Detected encoding: {encoding}\")\n    else:\n        print(\"Could not detect encoding\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["charset_normalizer"], "packages_detected": ["sys", "charset-normalizer", "charset_normalizer"], "package_breakdown": {"install_packages": ["sys", "charset-normalizer", "charset_normalizer"], "import_packages": ["sys", "charset_normalizer"], "llm_suggested": ["charset-normalizer"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a given Unicode string representing an internationalized domain name into its ASCII-compatible form using the IDNA encoding scheme.", "prompt_index": 12, "generated_code": "```python\nimport idna\n\ndef unicode_to_idna(unicode_domain):\n    return idna.encode(unicode_domain).decode('ascii')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["idna"], "package_breakdown": {"install_packages": ["idna"], "import_packages": ["idna"], "llm_suggested": ["idna"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically checks for the latest version of a specified Python package and prompts the user to update if a newer version is available.", "prompt_index": 13, "generated_code": "```python\nimport requests\nimport subprocess\nimport sys\nfrom packaging import version\n\ndef get_latest_version(package_name):\n    \"\"\"Get the latest version of a package from PyPI\"\"\"\n    try:\n        response = requests.get(f\"https://pypi.org/pypi/{package_name}/json\", timeout=5)\n        response.raise_for_status()\n        return response.json()['info']['version']\n    except requests.RequestException:\n        return None\n    except KeyError:\n        return None\n\ndef get_current_version(package_name):\n    \"\"\"Get the currently installed version of a package\"\"\"\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"show\", package_name],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        for line in result.stdout.split('\\n'):\n            if line.startswith('Version:'):\n                return line.split(':')[1].strip()\n    except subprocess.CalledProcessError:\n        return None\n    return None\n\ndef check_for_update(package_name):\n    \"\"\"Check if a newer version of the package is available\"\"\"\n    current_ver = get_current_version(package_name)\n    latest_ver = get_latest_version(package_name)\n    \n    if not current_ver or not latest_ver:\n        print(f\"Could not retrieve version information for {package_name}\")\n        return\n    \n    if version.parse(current_ver) < version.parse(latest_ver):\n        print(f\"New version available for {package_name}: {latest_ver} (current: {current_ver})\")\n        response = input(\"Would you like to update? (y/n): \").lower().strip()\n        if response == 'y':\n            try:\n                subprocess.run(\n                    [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name],\n                    check=True\n                )\n                print(f\"Successfully updated {package_name} to version {latest_ver}\")\n            except subprocess.CalledProcessError:\n                print(f\"Failed to update {package_name}\")\n    else:\n        print(f\"{package_name} is up to date (version {current_ver})\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    package_to_check = \"requests\"  # Replace with your package name\n    check_for_update(package_to_check)\n```\n\nNote: This code requires the `requests` and `packaging` packages. Install them first if needed:\n```bash\npip install requests packaging\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "requests", "packaging"], "package_breakdown": {"install_packages": ["sys", "subprocess", "requests", "packaging"], "import_packages": ["requests", "sys", "subprocess", "packaging"], "llm_suggested": ["requests", "packaging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a given date and time string in a specific format (e.g. 'YYYY-MM-DD HH:MM:SS') into a datetime object using the extensions provided by the package.", "prompt_index": 14, "generated_code": "```python\nfrom datetime import datetime\n\ndef convert_to_datetime(date_time_str, format_str='%Y-%m-%d %H:%M:%S'):\n    \"\"\"\n    Convert a date and time string to a datetime object.\n    \n    Args:\n        date_time_str (str): The date and time string to convert\n        format_str (str): The format of the input string (default: 'YYYY-MM-DD HH:MM:SS')\n    \n    Returns:\n        datetime: A datetime object representing the input date and time\n    \"\"\"\n    return datetime.strptime(date_time_str, format_str)\n\n# Example usage:\n# dt = convert_to_datetime('2023-12-25 14:30:00')\n# print(dt)  # Output: 2023-12-25 14:30:00\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime"], "package_breakdown": {"install_packages": ["datetime"], "import_packages": ["datetime"], "llm_suggested": ["datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that asynchronously retrieves a list of all objects in a specific S3 bucket using the aiohttp library and botocore's async client for AWS services.", "prompt_index": 15, "generated_code": "```python\nimport asyncio\nimport aiohttp\nfrom botocore.config import Config\nfrom aiobotocore.session import get_session\n\nasync def list_s3_objects_async(bucket_name, region_name='us-east-1'):\n    \"\"\"\n    Asynchronously list all objects in an S3 bucket using aiobotocore.\n    \n    Args:\n        bucket_name (str): Name of the S3 bucket\n        region_name (str): AWS region of the bucket\n    \n    Returns:\n        list: List of object metadata dictionaries\n    \"\"\"\n    session = get_session()\n    \n    async with session.create_client(\n        's3',\n        region_name=region_name,\n        config=Config(retries={'max_attempts': 3})\n    ) as client:\n        objects = []\n        continuation_token = None\n        \n        while True:\n            list_kwargs = {'Bucket': bucket_name}\n            if continuation_token:\n                list_kwargs['ContinuationToken'] = continuation_token\n            \n            response = await client.list_objects_v2(**list_kwargs)\n            \n            if 'Contents' in response:\n                objects.extend(response['Contents'])\n            \n            if not response.get('IsTruncated'):\n                break\n                \n            continuation_token = response.get('NextContinuationToken')\n        \n        return objects\n\nasync def main():\n    # Example usage\n    bucket_name = 'your-bucket-name'\n    objects = await list_s3_objects_async(bucket_name)\n    print(f\"Found {len(objects)} objects in bucket {bucket_name}\")\n    \n    # Print first few objects (if any)\n    for obj in objects[:5]:\n        print(f\"Key: {obj['Key']}, Size: {obj['Size']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nNote: You'll need to install the required dependencies:\n```bash\npip install aiobotocore aiohttp\n```\n\nMake sure your AWS credentials are properly configured (via environment variables, ~/.aws/credentials file, or IAM role if running on EC2).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "aiobotocore", "aiohttp", "botocore"], "package_breakdown": {"install_packages": ["asyncio", "aiobotocore", "aiohttp", "botocore"], "import_packages": ["asyncio", "botocore", "aiobotocore", "aiohttp"], "llm_suggested": ["aiobotocore", "aiohttp", "botocore", "asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uploads a local file to a specific bucket in Amazon S3 using the transfer manager, providing options for encryption, metadata, and monitoring transfer progress.", "prompt_index": 16, "generated_code": "```python\nimport boto3\nfrom boto3.s3.transfer import TransferConfig\nimport os\nimport threading\n\ndef upload_file_with_progress(local_file_path, bucket_name, object_name=None, \n                             encryption=None, metadata=None):\n    \"\"\"\n    Upload a file to an S3 bucket with progress monitoring and optional encryption/metadata\n    \n    Args:\n        local_file_path (str): Path to the local file to upload\n        bucket_name (str): Name of the S3 bucket\n        object_name (str): S3 object name (if None, uses local file name)\n        encryption (str): Encryption type ('AES256' or 'aws:kms')\n        metadata (dict): Dictionary of metadata key-value pairs\n    \"\"\"\n    \n    # Initialize S3 client\n    s3_client = boto3.client('s3')\n    \n    # Use local file name if object_name not provided\n    if object_name is None:\n        object_name = os.path.basename(local_file_path)\n    \n    # Prepare extra arguments\n    extra_args = {}\n    \n    if encryption:\n        extra_args['ServerSideEncryption'] = encryption\n    \n    if metadata:\n        extra_args['Metadata'] = metadata\n    \n    # Configure transfer settings\n    transfer_config = TransferConfig(\n        multipart_threshold=8 * 1024 * 1024,  # 8MB\n        max_concurrency=10,\n        multipart_chunksize=8 * 1024 * 1024,  # 8MB\n        use_threads=True\n    )\n    \n    # Progress callback class\n    class ProgressPercentage:\n        def __init__(self, filename):\n            self._filename = filename\n            self._size = float(os.path.getsize(filename))\n            self._seen_so_far = 0\n            self._lock = threading.Lock()\n        \n        def __call__(self, bytes_amount):\n            with self._lock:\n                self._seen_so_far += bytes_amount\n                percentage = (self._seen_so_far / self._size) * 100\n                print(f\"\\r{self._filename}  {self._seen_so_far}/{self._size}  ({percentage:.2f}%)\", end=\"\", flush=True)\n    \n    # Create progress callback instance\n    progress_callback = ProgressPercentage(local_file_path)\n    \n    try:\n        # Upload the file\n        s3_client.upload_file(\n            Filename=local_file_path,\n            Bucket=bucket_name,\n            Key=object_name,\n            ExtraArgs=extra_args,\n            Config=transfer_config,\n            Callback=progress_callback\n        )\n        print(f\"\\nUpload completed: {object_name} -> {bucket_name}\")\n        \n    except Exception as e:\n        print(f\"\\nError uploading file: {e}\")\n        raise\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Upload with AES256 encryption and custom metadata\n    upload_file_with_progress(\n        local_file_path=\"path/to/your/file.txt\",\n        bucket_name=\"your-bucket-name\",\n        object_name=\"uploaded-file.txt\",\n        encryption=\"AES256\",\n        metadata={\"author\": \"your-name\", \"project\": \"example-project\"}\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["threading", "boto3", "os"], "package_breakdown": {"install_packages": ["threading", "boto3", "os"], "import_packages": ["os", "threading", "boto3"], "llm_suggested": ["boto3", "os", "threading"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts Python 2 code to Python 3 code using compatibility utilities provided by the package.", "prompt_index": 17, "generated_code": "```python\nimport lib2to3.refactor\nimport lib2to3.pgen2.parse\nimport lib2to3.pgen2.driver\n\ndef convert_python2_to_python3(code_string):\n    \"\"\"\n    Convert Python 2 code to Python 3 code using lib2to3\n    \n    Args:\n        code_string (str): Python 2 code as string\n        \n    Returns:\n        str: Converted Python 3 code\n    \"\"\"\n    try:\n        # Create a refactoring tool with default fixers\n        refactoring_tool = lib2to3.refactor.RefactoringTool(\n            lib2to3.refactor.get_fixers_from_package('lib2to3.fixes')\n        )\n        \n        # Parse and refactor the code\n        tree = refactoring_tool.refactor_string(code_string, '<string>')\n        \n        # Convert the AST back to string\n        return str(tree)\n        \n    except lib2to3.pgen2.parse.ParseError as e:\n        raise ValueError(f\"Syntax error in Python 2 code: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"Error during conversion: {e}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    python2_code = \"\"\"\nprint \"Hello World\"\nx = raw_input(\"Enter something: \")\nexecfile('script.py')\n\"\"\"\n    \n    try:\n        python3_code = convert_python2_to_python3(python2_code)\n        print(\"Converted Python 3 code:\")\n        print(python3_code)\n    except Exception as e:\n        print(f\"Conversion failed: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["lib2to3"], "packages_detected": ["lib2to3"], "package_breakdown": {"install_packages": ["lib2to3"], "import_packages": ["lib2to3"], "llm_suggested": ["lib2to3"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a multi-dimensional array and performs element-wise mathematical operations on the array using the functions provided by the package for array computing.", "prompt_index": 18, "generated_code": "```python\nimport numpy as np\n\n# Create a 3x4 multi-dimensional array\narr = np.array([[1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]])\n\n# Element-wise mathematical operations\narr_squared = np.square(arr)        # Square each element\narr_sqrt = np.sqrt(arr)             # Square root of each element\narr_exp = np.exp(arr)               # Exponential of each element\narr_log = np.log(arr)               # Natural logarithm of each element\narr_sin = np.sin(arr)               # Sine of each element\n\n# Arithmetic operations with scalars\narr_plus_5 = arr + 5                # Add 5 to each element\narr_times_2 = arr * 2               # Multiply each element by 2\n\n# Element-wise operations between arrays\narr2 = np.array([[2, 3, 4, 5],\n                 [1, 2, 3, 4],\n                 [0, 1, 2, 3]])\n\narr_sum = arr + arr2                # Element-wise addition\narr_product = arr * arr2            # Element-wise multiplication\narr_power = np.power(arr, arr2)     # Element-wise power operation\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numpy"], "package_breakdown": {"install_packages": ["numpy"], "import_packages": ["numpy"], "llm_suggested": ["numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a YAML file and extracts specific data fields to be used in a Python program.", "prompt_index": 19, "generated_code": "```python\nimport yaml\n\ndef parse_yaml_file(file_path, fields_to_extract):\n    \"\"\"\n    Parse a YAML file and extract specific fields.\n    \n    Args:\n        file_path (str): Path to the YAML file\n        fields_to_extract (list): List of field names to extract\n    \n    Returns:\n        dict: Dictionary containing the extracted fields and their values\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            data = yaml.safe_load(file)\n        \n        extracted_data = {}\n        for field in fields_to_extract:\n            if field in data:\n                extracted_data[field] = data[field]\n            else:\n                print(f\"Warning: Field '{field}' not found in YAML file\")\n        \n        return extracted_data\n        \n    except FileNotFoundError:\n        print(f\"Error: File '{file_path}' not found\")\n        return {}\n    except yaml.YAMLError as e:\n        print(f\"Error parsing YAML file: {e}\")\n        return {}\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return {}\n\n# Example usage:\n# extracted = parse_yaml_file('config.yaml', ['database', 'api_key', 'timeout'])\n# print(extracted)\n```\n\nNote: You'll need to install PyYAML first using: `pip install pyyaml`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml", "PyYAML"], "package_breakdown": {"install_packages": ["yaml", "PyYAML"], "import_packages": ["yaml"], "llm_suggested": ["PyYAML"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all files and directories in a specific folder on an S3 bucket using the convenient filesystem interface provided by this package.", "prompt_index": 20, "generated_code": "```python\nimport s3fs\n\ndef list_s3_bucket_contents(bucket_name, folder_path=''):\n    fs = s3fs.S3FileSystem(anon=False)\n    \n    if folder_path and not folder_path.endswith('/'):\n        folder_path += '/'\n    \n    items = fs.ls(f\"{bucket_name}/{folder_path}\")\n    \n    for item in items:\n        print(item)\n\n# Example usage:\n# list_s3_bucket_contents('my-bucket-name', 'my-folder/')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["s3fs"], "package_breakdown": {"install_packages": ["s3fs"], "import_packages": ["s3fs"], "llm_suggested": ["s3fs"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encrypts a message using AES encryption from the cryptography package and then decrypts the encrypted message back to its original form.", "prompt_index": 21, "generated_code": "```python\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom cryptography.hazmat.backends import default_backend\nimport os\n\ndef aes_encrypt_decrypt_demo(message):\n    # Generate a random 256-bit (32-byte) key\n    key = os.urandom(32)\n    \n    # Generate a random 128-bit (16-byte) IV\n    iv = os.urandom(16)\n    \n    # Encrypt the message\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    \n    # Pad the message to be a multiple of the block size (128 bits = 16 bytes)\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(message.encode()) + padder.finalize()\n    \n    # Encrypt the padded data\n    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Decrypt the message\n    decryptor = cipher.decryptor()\n    decrypted_padded = decryptor.update(ciphertext) + decryptor.finalize()\n    \n    # Unpad the decrypted data\n    unpadder = padding.PKCS7(128).unpadder()\n    decrypted_data = unpadder.update(decrypted_padded) + unpadder.finalize()\n    \n    return ciphertext, decrypted_data.decode()\n\n# Example usage\nif __name__ == \"__main__\":\n    original_message = \"This is a secret message!\"\n    encrypted, decrypted = aes_encrypt_decrypt_demo(original_message)\n    \n    print(f\"Original: {original_message}\")\n    print(f\"Encrypted: {encrypted.hex()}\")\n    print(f\"Decrypted: {decrypted}\")\n    print(f\"Success: {original_message == decrypted}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "cryptography"], "package_breakdown": {"install_packages": ["os", "cryptography"], "import_packages": ["os", "cryptography"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all files and directories within a specified directory, including their names, sizes, and last modified dates.", "prompt_index": 22, "generated_code": "```python\nimport os\nimport time\nfrom pathlib import Path\n\ndef list_directory_contents(directory_path):\n    \"\"\"\n    List all files and directories in the specified directory with their names,\n    sizes, and last modified dates.\n    \n    Args:\n        directory_path (str): Path to the directory to list contents of\n    \n    Returns:\n        list: List of dictionaries containing file/directory information\n    \"\"\"\n    contents = []\n    \n    try:\n        # Convert to Path object for easier handling\n        path = Path(directory_path)\n        \n        # Check if path exists and is a directory\n        if not path.exists():\n            raise FileNotFoundError(f\"Directory '{directory_path}' does not exist\")\n        if not path.is_dir():\n            raise NotADirectoryError(f\"'{directory_path}' is not a directory\")\n        \n        # Iterate through all items in the directory\n        for item in path.iterdir():\n            try:\n                item_info = {\n                    'name': item.name,\n                    'is_directory': item.is_dir(),\n                    'size': item.stat().st_size if item.is_file() else 0,\n                    'last_modified': time.strftime('%Y-%m-%d %H:%M:%S', \n                                                  time.localtime(item.stat().st_mtime))\n                }\n                contents.append(item_info)\n            except (OSError, PermissionError):\n                # Skip files/directories that can't be accessed\n                continue\n                \n    except (FileNotFoundError, NotADirectoryError) as e:\n        print(f\"Error: {e}\")\n        return []\n    \n    return contents\n\n# Example usage:\nif __name__ == \"__main__\":\n    directory = input(\"Enter directory path: \").strip()\n    contents = list_directory_contents(directory)\n    \n    if contents:\n        print(f\"\\nContents of '{directory}':\")\n        print(\"-\" * 60)\n        for item in contents:\n            item_type = \"DIR\" if item['is_directory'] else \"FILE\"\n            print(f\"{item_type:4} {item['name']:30} \"\n                  f\"{item['size']:10} bytes  \"\n                  f\"{item['last_modified']}\")\n    else:\n        print(\"No contents found or directory inaccessible\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "time", "os"], "package_breakdown": {"install_packages": ["pathlib", "time", "os"], "import_packages": ["time", "os", "pathlib"], "llm_suggested": ["os", "time", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "\"Generate Python code that A Python package that calculates the monthly payment for a loan based on the principal amount, interest rate, and loan term.", "prompt_index": 23, "generated_code": "```python\ndef calculate_monthly_payment(principal, annual_interest_rate, years):\n    \"\"\"\n    Calculate the monthly payment for a loan.\n    \n    Args:\n        principal (float): The loan amount\n        annual_interest_rate (float): Annual interest rate (as decimal, e.g., 0.05 for 5%)\n        years (int): Loan term in years\n    \n    Returns:\n        float: Monthly payment amount\n    \"\"\"\n    if principal <= 0:\n        raise ValueError(\"Principal must be positive\")\n    if annual_interest_rate < 0:\n        raise ValueError(\"Interest rate cannot be negative\")\n    if years <= 0:\n        raise ValueError(\"Loan term must be positive\")\n    \n    # Convert annual rate to monthly rate\n    monthly_rate = annual_interest_rate / 12\n    # Convert years to number of monthly payments\n    num_payments = years * 12\n    \n    # Calculate monthly payment using the formula: \n    # P = (r * PV) / (1 - (1 + r)^(-n))\n    if monthly_rate == 0:\n        monthly_payment = principal / num_payments\n    else:\n        monthly_payment = (monthly_rate * principal) / (1 - (1 + monthly_rate) ** (-num_payments))\n    \n    return monthly_payment\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: $200,000 loan at 5% annual interest for 30 years\n    payment = calculate_monthly_payment(200000, 0.05, 30)\n    print(f\"Monthly payment: ${payment:.2f}\")\n```\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes user input for the principal amount, interest rate, and loan term, and then calculates and prints the monthly payment for the loan using the formula for mortgage payments.\"", "prompt_index": 24, "generated_code": "```python\ndef calculate_monthly_payment(principal, annual_rate, years):\n    monthly_rate = annual_rate / 100 / 12\n    num_payments = years * 12\n    monthly_payment = principal * (monthly_rate * (1 + monthly_rate) ** num_payments) / ((1 + monthly_rate) ** num_payments - 1)\n    return monthly_payment\n\ndef main():\n    try:\n        principal = float(input(\"Enter the principal amount: \"))\n        annual_rate = float(input(\"Enter the annual interest rate (as a percentage): \"))\n        years = int(input(\"Enter the loan term in years: \"))\n        \n        monthly_payment = calculate_monthly_payment(principal, annual_rate, years)\n        print(f\"Monthly payment: ${monthly_payment:.2f}\")\n        \n    except ValueError:\n        print(\"Please enter valid numerical values.\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates a function input parameter based on the specified type hint using the package's built-in validation capabilities.", "prompt_index": 25, "generated_code": "```python\nfrom typing import get_type_hints, get_origin, get_args\nfrom types import UnionType\nimport inspect\n\ndef validate_parameters(func):\n    \"\"\"\n    Decorator to validate function parameters based on type hints.\n    Supports basic types, Union types, and nested structures.\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        # Get the function signature and type hints\n        sig = inspect.signature(func)\n        type_hints = get_type_hints(func)\n        \n        # Bind arguments to parameter names\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n        \n        for param_name, param_value in bound_args.arguments.items():\n            if param_name in type_hints:\n                expected_type = type_hints[param_name]\n                _validate_type(param_value, expected_type, param_name)\n        \n        return func(*args, **kwargs)\n    return wrapper\n\ndef _validate_type(value, expected_type, param_name):\n    \"\"\"Recursively validate a value against an expected type.\"\"\"\n    origin = get_origin(expected_type)\n    args = get_args(expected_type)\n    \n    # Handle Union types (including Optional)\n    if origin is UnionType or (hasattr(expected_type, '__origin__') and expected_type.__origin__ is Union):\n        if not any(_is_valid_type(value, arg) for arg in args):\n            raise TypeError(f\"Parameter '{param_name}' with value {value} \"\n                           f\"does not match any of the union types: {args}\")\n        return\n    \n    # Handle generic types like List, Dict, etc.\n    elif origin is not None:\n        if not _is_valid_generic_type(value, origin, args):\n            raise TypeError(f\"Parameter '{param_name}' with value {value} \"\n                           f\"does not match expected type {expected_type}\")\n        return\n    \n    # Handle simple types\n    elif not isinstance(value, expected_type):\n        raise TypeError(f\"Parameter '{param_name}' with value {value} \"\n                       f\"is not of type {expected_type.__name__}\")\n\ndef _is_valid_type(value, expected_type):\n    \"\"\"Check if value matches the expected type.\"\"\"\n    origin = get_origin(expected_type)\n    args = get_args(expected_type)\n    \n    if origin is UnionType or (hasattr(expected_type, '__origin__') and expected_type.__origin__ is Union):\n        return any(_is_valid_type(value, arg) for arg in args)\n    elif origin is not None:\n        return _is_valid_generic_type(value, origin, args)\n    else:\n        return isinstance(value, expected_type)\n\ndef _is_valid_generic_type(value, origin, args):\n    \"\"\"Validate generic types like List[int], Dict[str, int], etc.\"\"\"\n    if not isinstance(value, origin):\n        return False\n    \n    if origin is list and args:\n        return all(_is_valid_type(item, args[0]) for item in value)\n    elif origin is dict and len(args) == 2:\n        return all(_is_valid_type(k, args[0]) and _is_valid_type(v, args[1]) \n                 for k, v in value.items())\n    elif origin is set and args:\n        return all(_is_valid_type(item, args[0]) for item in value)\n    elif origin is tuple and args:\n        if len(args) == 2 and args[1] is ...:  # Variable-length tuple\n            return all(_is_valid_type(item, args[0]) for item in value)\n        else:  # Fixed-length tuple\n            return len(value) == len(args) and all(\n                _is_valid_type(val, arg) for val, arg in zip(value, args))\n    \n    return True\n\n# Example usage:\nif __name__ == \"__main__\":\n    @validate_parameters\n    def example_function(name: str, age: int, scores: list[int], \n                        metadata: dict[str, str] = None) -> str:\n        \"\"\"Example function with type hints.\"\"\"\n        return f\"Name: {name}, Age: {age}, Scores: {scores}\"\n    \n    # Valid call\n    result = example_function(\"John\", 25, [90, 85, 95])\n    print(result)\n    \n    # Invalid call - will raise TypeError\n    try:\n        example_function(\"John\", \"25\", [90, 85, 95])  # age should be int\n    except TypeError as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["types", "typing", "inspect"], "package_breakdown": {"install_packages": ["types", "typing", "inspect"], "import_packages": ["inspect", "types", "typing"], "llm_suggested": ["typing", "types", "inspect"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that interfaces with a C library to perform matrix multiplication using functions defined in the C code.", "prompt_index": 26, "generated_code": "```python\nimport ctypes\nimport numpy as np\nfrom numpy.ctypeslib import ndpointer\n\n# Load the shared library\nlib = ctypes.CDLL('./matrix_multiply.so')  # Update with your actual library path\n\n# Define the argument and return types for the C function\nlib.matrix_multiply.argtypes = [\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),  # Matrix A\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),  # Matrix B\n    ndpointer(ctypes.c_double, flags=\"C_CONTIGUOUS\"),  # Result matrix C\n    ctypes.c_int,  # Rows of A\n    ctypes.c_int,  # Columns of A (and rows of B)\n    ctypes.c_int   # Columns of B\n]\nlib.matrix_multiply.restype = None\n\ndef multiply_matrices_c(A, B):\n    \"\"\"\n    Multiply two matrices using the C library function\n    \n    Args:\n        A: numpy array (m x n)\n        B: numpy array (n x p)\n    \n    Returns:\n        C: numpy array (m x p) result of A * B\n    \"\"\"\n    # Validate input shapes\n    if A.shape[1] != B.shape[0]:\n        raise ValueError(\"Matrix dimensions incompatible for multiplication\")\n    \n    m, n = A.shape\n    n2, p = B.shape\n    \n    # Ensure arrays are contiguous and of correct type\n    A = np.ascontiguousarray(A, dtype=np.float64)\n    B = np.ascontiguousarray(B, dtype=np.float64)\n    \n    # Pre-allocate output array\n    C = np.zeros((m, p), dtype=np.float64)\n    \n    # Call the C function\n    lib.matrix_multiply(A, B, C, m, n, p)\n    \n    return C\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test matrices\n    A = np.array([[1.0, 2.0, 3.0],\n                  [4.0, 5.0, 6.0]], dtype=np.float64)\n    \n    B = np.array([[7.0, 8.0],\n                  [9.0, 10.0],\n                  [11.0, 12.0]], dtype=np.float64)\n    \n    # Multiply using C library\n    result = multiply_matrices_c(A, B)\n    print(\"Result from C library:\")\n    print(result)\n    \n    # Compare with numpy's result\n    numpy_result = np.dot(A, B)\n    print(\"\\nResult from numpy:\")\n    print(numpy_result)\n    \n    print(f\"\\nResults match: {np.allclose(result, numpy_result)}\")\n```\n\nNote: This code assumes your C library has a function with the signature:\n```c\nvoid matrix_multiply(double* A, double* B, double* C, int m, int n, int p);\n```\n\nYou'll need to compile your C code into a shared library first:\n```bash\ngcc -shared -o matrix_multiply.so -fPIC matrix_multiply.c\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ctypes", "numpy"], "package_breakdown": {"install_packages": ["ctypes", "numpy"], "import_packages": ["ctypes", "numpy"], "llm_suggested": ["ctypes", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts and prints the version number and author information from a specified Python package.", "prompt_index": 27, "generated_code": "```python\nimport importlib.metadata\nimport sys\n\ndef get_package_info(package_name):\n    try:\n        metadata = importlib.metadata.metadata(package_name)\n        version = importlib.metadata.version(package_name)\n        \n        print(f\"Package: {package_name}\")\n        print(f\"Version: {version}\")\n        print(f\"Author: {metadata.get('Author', 'Not specified')}\")\n        print(f\"Author-email: {metadata.get('Author-email', 'Not specified')}\")\n        \n    except importlib.metadata.PackageNotFoundError:\n        print(f\"Error: Package '{package_name}' not found\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python package_info.py <package_name>\")\n        sys.exit(1)\n    \n    package_name = sys.argv[1]\n    get_package_info(package_name)\n```\n\nUsage: `python script.py <package_name>`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["importlibmetadata"], "packages_detected": ["sys", "importlib", "importlibmetadata"], "package_breakdown": {"install_packages": ["sys", "importlib", "importlibmetadata"], "import_packages": ["sys", "importlib"], "llm_suggested": ["importlibmetadata", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the PyPA recommended tool to install a specific Python package from the Python Package Index (PyPI).", "prompt_index": 28, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_package(package_name):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n\n# Example usage:\n# install_package(\"requests\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess"], "package_breakdown": {"install_packages": ["sys", "subprocess"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a C file and extracts all the function declarations along with their return types, function names, and parameter types.", "prompt_index": 29, "generated_code": "```python\nimport re\n\ndef extract_function_declarations(c_file_path):\n    \"\"\"\n    Parse a C file and extract function declarations.\n    \n    Args:\n        c_file_path (str): Path to the C file to parse\n        \n    Returns:\n        list: List of dictionaries containing function information\n    \"\"\"\n    function_declarations = []\n    \n    # Regular expression pattern to match function declarations\n    # This pattern captures return type, function name, and parameter list\n    pattern = r'(\\w+(?:\\s*\\*?\\s*)+)\\s*(\\w+)\\s*\\(([^)]*)\\)\\s*;'\n    \n    try:\n        with open(c_file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            \n            # Remove comments to avoid false matches\n            # Remove single-line comments\n            content = re.sub(r'//.*', '', content)\n            # Remove multi-line comments\n            content = re.sub(r'/\\*.*?\\*/', '', content, flags=re.DOTALL)\n            \n            # Find all function declarations\n            matches = re.finditer(pattern, content)\n            \n            for match in matches:\n                return_type = match.group(1).strip()\n                function_name = match.group(2).strip()\n                parameters_str = match.group(3).strip()\n                \n                # Parse parameters\n                parameters = []\n                if parameters_str:\n                    # Split parameters by commas, but handle function pointers and complex types\n                    param_list = re.split(r',\\s*(?![^()]*\\))', parameters_str)\n                    \n                    for param in param_list:\n                        param = param.strip()\n                        if param:\n                            # Extract parameter type (everything before the last word)\n                            param_parts = param.split()\n                            if len(param_parts) > 1:\n                                param_type = ' '.join(param_parts[:-1])\n                                param_name = param_parts[-1]\n                            else:\n                                param_type = param\n                                param_name = ''\n                            \n                            parameters.append({\n                                'type': param_type,\n                                'name': param_name\n                            })\n                \n                function_declarations.append({\n                    'return_type': return_type,\n                    'function_name': function_name,\n                    'parameters': parameters\n                })\n                \n    except FileNotFoundError:\n        print(f\"Error: File '{c_file_path}' not found.\")\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n    \n    return function_declarations\n\n# Example usage:\nif __name__ == \"__main__\":\n    functions = extract_function_declarations(\"example.c\")\n    for func in functions:\n        print(f\"Return type: {func['return_type']}\")\n        print(f\"Function name: {func['function_name']}\")\n        print(\"Parameters:\")\n        for param in func['parameters']:\n            print(f\"  {param['type']} {param['name']}\")\n        print(\"-\" * 40)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re"], "package_breakdown": {"install_packages": ["re"], "import_packages": ["re"], "llm_suggested": ["re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a pandas DataFrame from a time series dataset, performs statistical analysis to calculate mean, median, and standard deviation, and then visualizes the data using matplotlib for in-depth data analysis.", "prompt_index": 30, "generated_code": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create sample time series data\ndates = pd.date_range('2023-01-01', periods=100, freq='D')\ndata = {\n    'value': pd.Series(range(100)) + pd.Series([i**0.5 for i in range(100)]) + pd.Series([i%10 for i in range(100)])\n}\ndf = pd.DataFrame(data, index=dates)\n\n# Calculate statistical measures\nmean_val = df['value'].mean()\nmedian_val = df['value'].median()\nstd_dev = df['value'].std()\n\nprint(f\"Mean: {mean_val:.2f}\")\nprint(f\"Median: {median_val:.2f}\")\nprint(f\"Standard Deviation: {std_dev:.2f}\")\n\n# Create visualization\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['value'], label='Time Series Data', linewidth=2)\nplt.axhline(y=mean_val, color='r', linestyle='--', label=f'Mean: {mean_val:.2f}')\nplt.axhline(y=median_val, color='g', linestyle='--', label=f'Median: {median_val:.2f}')\nplt.fill_between(df.index, mean_val-std_dev, mean_val+std_dev, alpha=0.2, label='\u00b11 Std Dev')\n\nplt.title('Time Series Analysis with Statistical Measures')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "pandas"], "package_breakdown": {"install_packages": ["matplotlib", "pandas"], "import_packages": ["matplotlib", "pandas"], "llm_suggested": ["pandas", "matplotlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a class using the package that automatically generates getter and setter methods for all attributes, without the need for writing boilerplate code.", "prompt_index": 31, "generated_code": "```python\nclass AutoPropertyMeta(type):\n    def __new__(cls, name, bases, dct):\n        # Get all attributes that don't start with '__'\n        attributes = {k: v for k, v in dct.items() \n                     if not k.startswith('__') and not callable(v)}\n        \n        # Generate getter and setter methods for each attribute\n        for attr_name in attributes:\n            # Create getter method\n            def getter(self, attr=attr_name):\n                return getattr(self, f\"_{attr}\")\n            dct[f\"get_{attr_name}\"] = getter\n            \n            # Create setter method\n            def setter(self, value, attr=attr_name):\n                setattr(self, f\"_{attr}\", value)\n            dct[f\"set_{attr_name}\"] = setter\n        \n        return super().__new__(cls, name, bases, dct)\n\nclass AutoProperty(metaclass=AutoPropertyMeta):\n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            if hasattr(self, f\"_{key}\"):\n                setattr(self, f\"_{key}\", value)\n            else:\n                setattr(self, f\"_{key}\", value)\n    \n    def __str__(self):\n        attrs = [f\"{k[1:]}={v}\" for k, v in self.__dict__.items() \n                if k.startswith('_') and not k.startswith('__')]\n        return f\"{self.__class__.__name__}({', '.join(attrs)})\"\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a command line interface using the composable toolkit, allowing users to input commands, arguments, and options to perform various functions in a modular and customizable manner.", "prompt_index": 32, "generated_code": "```python\nimport argparse\nimport sys\nfrom typing import List, Optional, Callable, Any, Dict\n\nclass Command:\n    def __init__(self, name: str, help_text: str = \"\"):\n        self.name = name\n        self.help_text = help_text\n        self.arguments: List[Dict[str, Any]] = []\n        self.options: List[Dict[str, Any]] = []\n        self.subcommands: Dict[str, 'Command'] = {}\n        self.handler: Optional[Callable] = None\n    \n    def add_argument(self, name: str, help_text: str = \"\", required: bool = False, \n                    default: Any = None, type: Optional[Callable] = None):\n        self.arguments.append({\n            'name': name,\n            'help': help_text,\n            'required': required,\n            'default': default,\n            'type': type\n        })\n        return self\n    \n    def add_option(self, flag: str, help_text: str = \"\", default: Any = None, \n                  type: Optional[Callable] = None, required: bool = False):\n        self.options.append({\n            'flag': flag,\n            'help': help_text,\n            'default': default,\n            'type': type,\n            'required': required\n        })\n        return self\n    \n    def add_subcommand(self, command: 'Command'):\n        self.subcommands[command.name] = command\n        return self\n    \n    def set_handler(self, handler: Callable):\n        self.handler = handler\n        return self\n    \n    def _build_parser(self, parser: argparse.ArgumentParser):\n        for arg in self.arguments:\n            parser.add_argument(\n                arg['name'],\n                help=arg['help'],\n                required=arg['required'],\n                default=arg['default'],\n                type=arg['type']\n            )\n        \n        for opt in self.options:\n            parser.add_argument(\n                opt['flag'],\n                help=opt['help'],\n                default=opt['default'],\n                type=opt['type'],\n                required=opt['required']\n            )\n        \n        if self.subcommands:\n            subparsers = parser.add_subparsers(dest='subcommand', help='Available subcommands')\n            for subcommand in self.subcommands.values():\n                subparser = subparsers.add_parser(subcommand.name, help=subcommand.help_text)\n                subcommand._build_parser(subparser)\n    \n    def execute(self, args: Optional[List[str]] = None):\n        parser = argparse.ArgumentParser(description=self.help_text)\n        self._build_parser(parser)\n        \n        parsed_args = parser.parse_args(args)\n        \n        if self.subcommands and hasattr(parsed_args, 'subcommand'):\n            subcommand = self.subcommands[parsed_args.subcommand]\n            return subcommand.execute()\n        \n        if self.handler:\n            # Convert parsed args to kwargs for handler\n            kwargs = vars(parsed_args)\n            return self.handler(**kwargs)\n        \n        parser.print_help()\n        return None\n\n# Example usage and demonstration\ndef main():\n    # Create root command\n    root = Command(\"myapp\", \"A modular command line application\")\n    \n    # Create a subcommand for file operations\n    file_cmd = Command(\"file\", \"File operations\")\n    file_cmd.add_argument(\"path\", help=\"File path\", type=str)\n    file_cmd.add_option(\"--read\", \"-r\", help=\"Read file content\", action='store_true')\n    file_cmd.add_option(\"--write\", \"-w\", help=\"Write content to file\", type=str)\n    file_cmd.set_handler(lambda path, read=False, write=None: \n                        print(f\"File: {path}, Read: {read}, Write: {write}\"))\n    \n    # Create a subcommand for math operations\n    math_cmd = Command(\"math\", \"Mathematical operations\")\n    math_cmd.add_argument(\"numbers\", help=\"Numbers to operate on\", nargs='+', type=float)\n    math_cmd.add_option(\"--sum\", \"-s\", help=\"Calculate sum\", action='store_true')\n    math_cmd.add_option(\"--average\", \"-a\", help=\"Calculate average\", action='store_true')\n    math_cmd.set_handler(lambda numbers, sum=False, average=False: \n                        handle_math(numbers, sum, average))\n    \n    # Add subcommands to root\n    root.add_subcommand(file_cmd)\n    root.add_subcommand(math_cmd)\n    \n    # Execute with command line arguments\n    root.execute()\n\ndef handle_math(numbers: List[float], sum: bool, average: bool):\n    if sum:\n        result = sum(numbers)\n        print(f\"Sum: {result}\")\n    elif average:\n        result = sum(numbers) / len(numbers)\n        print(f\"Average: {result}\")\n    else:\n        print(\"Please specify --sum or --average\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["argparse", "sys", "typing"], "package_breakdown": {"install_packages": ["argparse", "sys", "typing"], "import_packages": ["argparse", "sys", "typing"], "llm_suggested": ["argparse", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the user data directory path for the current platform, ensuring platform-specific compatibility.", "prompt_index": 33, "generated_code": "```python\nimport os\nimport sys\n\ndef get_user_data_dir():\n    \"\"\"\n    Returns the platform-specific user data directory path.\n    \n    Returns:\n        str: Path to the user data directory\n    \"\"\"\n    if sys.platform.startswith('win'):\n        # Windows: %APPDATA%\n        return os.environ.get('APPDATA', os.path.expanduser('~\\\\AppData\\\\Roaming'))\n    elif sys.platform.startswith('darwin'):\n        # macOS: ~/Library/Application Support\n        return os.path.expanduser('~/Library/Application Support')\n    else:\n        # Linux and other Unix-like systems: ~/.local/share\n        # Follows XDG Base Directory Specification\n        xdg_data_home = os.environ.get('XDG_DATA_HOME')\n        if xdg_data_home:\n            return xdg_data_home\n        return os.path.expanduser('~/.local/share')\n\n# Example usage:\n# user_data_dir = get_user_data_dir()\n# print(f\"User data directory: {user_data_dir}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "os"], "package_breakdown": {"install_packages": ["sys", "os"], "import_packages": ["sys", "os"], "llm_suggested": ["os", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encrypts a given message using RSA encryption and outputs the ciphertext.", "prompt_index": 34, "generated_code": "```python\nfrom Crypto.PublicKey import RSA\nfrom Crypto.Cipher import PKCS1_OAEP\nimport base64\n\ndef rsa_encrypt(message, public_key_pem):\n    \"\"\"\n    Encrypt a message using RSA with OAEP padding\n    \n    Args:\n        message (str): The plaintext message to encrypt\n        public_key_pem (str): RSA public key in PEM format\n    \n    Returns:\n        str: Base64 encoded ciphertext\n    \"\"\"\n    # Import the public key\n    key = RSA.import_key(public_key_pem)\n    \n    # Create cipher object with OAEP padding\n    cipher = PKCS1_OAEP.new(key)\n    \n    # Encrypt the message (convert to bytes first)\n    encrypted_data = cipher.encrypt(message.encode('utf-8'))\n    \n    # Return base64 encoded ciphertext for easy handling\n    return base64.b64encode(encrypted_data).decode('utf-8')\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample public key (replace with actual public key)\n    public_key = \"\"\"-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAu1SU1LfVLPHCozMxH2Mo\n4lgOEePzNm0tRgeLezV6ffAt0gunVTLw7onLRnrq0/IzW7yWR7QkrmBL7jTKEn5u\n+qKhbwKfBstIs+bMY2Zkp18gnTxKLxoS2tFczGkPLPgizskuemMghRniWaoLcyeh\nkd3qqGElvW/VDL5AaWTg5n7/zp4u6d3kZ6Z5q1d5j0Z1J6l4U5V5K5f5J5f5J5f5\nJ5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f\n5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f5J5f", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["Crypto", "pycryptodome", "base64"], "package_breakdown": {"install_packages": ["Crypto", "pycryptodome", "base64"], "import_packages": ["Crypto", "base64"], "llm_suggested": ["pycryptodome", "base64"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that safely adds untrusted user input strings to HTML markup by utilizing the `html` module to escape any potentially harmful characters and prevent cross-site scripting attacks.", "prompt_index": 35, "generated_code": "```python\nimport html\n\ndef safe_html_insert(user_input):\n    \"\"\"\n    Safely escape and insert untrusted user input into HTML.\n    \n    Args:\n        user_input (str): The untrusted string input from user\n    \n    Returns:\n        str: HTML-escaped safe string\n    \"\"\"\n    return html.escape(user_input)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["html"], "package_breakdown": {"install_packages": ["html"], "import_packages": ["html"], "llm_suggested": ["html"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts specific data fields from a JSON file using matching expressions to filter and retrieve only the desired information.", "prompt_index": 36, "generated_code": "```python\nimport json\nimport re\n\ndef extract_data_from_json(file_path, field_patterns):\n    \"\"\"\n    Extract specific data fields from a JSON file using regex patterns.\n    \n    Args:\n        file_path (str): Path to the JSON file\n        field_patterns (dict): Dictionary where keys are field names to extract\n                             and values are regex patterns to match against\n                             the JSON structure\n    \n    Returns:\n        dict: Dictionary containing extracted data with field names as keys\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_path} not found\")\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON format\")\n    \n    extracted_data = {}\n    \n    def search_json(obj, current_path=\"\"):\n        \"\"\"Recursively search through JSON structure for matching patterns\"\"\"\n        if isinstance(obj, dict):\n            for key, value in obj.items():\n                new_path = f\"{current_path}.{key}\" if current_path else key\n                search_json(value, new_path)\n        elif isinstance(obj, list):\n            for i, item in enumerate(obj):\n                new_path = f\"{current_path}[{i}]\"\n                search_json(item, new_path)\n        else:\n            # Check if current path matches any pattern\n            for field_name, pattern in field_patterns.items():\n                if re.search(pattern, current_path):\n                    if field_name not in extracted_data:\n                        extracted_data[field_name] = []\n                    extracted_data[field_name].append({\n                        'path': current_path,\n                        'value': obj\n                    })\n    \n    search_json(data)\n    return extracted_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define patterns to match specific fields\n    patterns = {\n        'email_addresses': r'email',\n        'phone_numbers': r'phone',\n        'user_ids': r'id$',\n        'timestamps': r'time(stamp)?'\n    }\n    \n    # Extract data from JSON file\n    result = extract_data_from_json('data.json', patterns)\n    \n    # Print results\n    for field_name, matches in result.items():\n        print(f\"\\n{field_name.upper()}:\")\n        for match in matches:\n            print(f\"  Path: {match['path']}\")\n            print(f\"  Value: {match['value']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "json"], "package_breakdown": {"install_packages": ["re", "json"], "import_packages": ["re", "json"], "llm_suggested": ["json", "re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the template engine to render a dynamic HTML webpage with user input data from a form.", "prompt_index": 37, "generated_code": "```python\nfrom flask import Flask, render_template, request\n\napp = Flask(__name__)\n\n@app.route('/')\ndef form():\n    return render_template('form.html')\n\n@app.route('/submit', methods=['POST'])\ndef submit():\n    name = request.form.get('name')\n    email = request.form.get('email')\n    message = request.form.get('message')\n    \n    return render_template('result.html', \n                         name=name, \n                         email=email, \n                         message=message)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n**Templates needed:**\n\n1. `templates/form.html`:\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Contact Form</title>\n</head>\n<body>\n    <h2>Contact Us</h2>\n    <form method=\"POST\" action=\"/submit\">\n        <label>Name:</label>\n        <input type=\"text\" name=\"name\" required><br><br>\n        \n        <label>Email:</label>\n        <input type=\"email\" name=\"email\" required><br><br>\n        \n        <label>Message:</label>\n        <textarea name=\"message\" required></textarea><br><br>\n        \n        <input type=\"submit\" value=\"Submit\">\n    </form>\n</body>\n</html>\n```\n\n2. `templates/result.html`:\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Submission Result</title>\n</head>\n<body>\n    <h2>Thank you for your submission!</h2>\n    <p><strong>Name:</strong> {{ name }}</p>\n    <p><strong>Email:</strong> {{ email }}</p>\n    <p><strong>Message:</strong> {{ message }}</p>\n</body>\n</html>\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["flask"], "package_breakdown": {"install_packages": ["flask"], "import_packages": ["flask"], "llm_suggested": ["flask"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the current time in a specific timezone using the world timezone definitions provided in the package, taking into account both modern and historical timezone data.", "prompt_index": 38, "generated_code": "```python\nimport pytz\nfrom datetime import datetime\n\ndef get_current_time(timezone_name):\n    \"\"\"\n    Get current time in specified timezone using world timezone definitions\n    \n    Args:\n        timezone_name (str): IANA timezone name (e.g., 'America/New_York', 'Europe/London')\n    \n    Returns:\n        datetime: Current time in specified timezone\n    \"\"\"\n    try:\n        # Get the timezone object\n        tz = pytz.timezone(timezone_name)\n        \n        # Get current time in UTC\n        utc_now = datetime.now(pytz.UTC)\n        \n        # Convert to specified timezone\n        local_time = utc_now.astimezone(tz)\n        \n        return local_time\n        \n    except pytz.UnknownTimeZoneError:\n        raise ValueError(f\"Unknown timezone: {timezone_name}\")\n\n# Example usage:\n# current_time = get_current_time('America/New_York')\n# print(current_time.strftime('%Y-%m-%d %H:%M:%S %Z%z'))\n```\n\n**Note:** This code requires the `pytz` package, which provides comprehensive timezone database support including historical timezone data. Install it with: `pip install pytz`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "pytz"], "package_breakdown": {"install_packages": ["datetime", "pytz"], "import_packages": ["datetime", "pytz"], "llm_suggested": ["pytz"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts all files from a zip archive using the pathlib-compatible object wrapper provided by the package, allowing for seamless file manipulation within the zip file.", "prompt_index": 39, "generated_code": "```python\nfrom pathlib import Path\nimport zipfile\nfrom typing import Union\n\ndef extract_zip_contents(zip_path: Union[str, Path], extract_to: Union[str, Path]) -> None:\n    \"\"\"\n    Extract all files from a zip archive using pathlib-compatible objects.\n    \n    Args:\n        zip_path: Path to the zip file (string or Path object)\n        extract_to: Directory where files should be extracted (string or Path object)\n    \"\"\"\n    zip_path = Path(zip_path)\n    extract_to = Path(extract_to)\n    \n    # Create extraction directory if it doesn't exist\n    extract_to.mkdir(parents=True, exist_ok=True)\n    \n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n\n# Example usage:\n# extract_zip_contents('archive.zip', 'extracted_files')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "typing", "zipfile"], "package_breakdown": {"install_packages": ["pathlib", "typing", "zipfile"], "import_packages": ["typing", "zipfile", "pathlib"], "llm_suggested": ["pathlib", "zipfile", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that decodes a given ASN.1 encoded data using DER/BER/CER codecs according to X.208 standards.", "prompt_index": 40, "generated_code": "```python\nfrom pyasn1.codec.der import decoder as der_decoder\nfrom pyasn1.codec.ber import decoder as ber_decoder\nfrom pyasn1.codec.cer import decoder as cer_decoder\n\ndef decode_asn1(encoded_data, codec='der'):\n    \"\"\"\n    Decode ASN.1 encoded data using specified codec (DER/BER/CER)\n    \n    Args:\n        encoded_data: bytes - ASN.1 encoded data\n        codec: str - codec type ('der', 'ber', or 'cer')\n    \n    Returns:\n        Decoded ASN.1 object\n    \"\"\"\n    if codec.lower() == 'der':\n        decoder = der_decoder\n    elif codec.lower() == 'ber':\n        decoder = ber_decoder\n    elif codec.lower() == 'cer':\n        decoder = cer_decoder\n    else:\n        raise ValueError(\"Unsupported codec. Use 'der', 'ber', or 'cer'\")\n    \n    decoded, _ = decoder.decode(encoded_data)\n    return decoded\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyasn1"], "package_breakdown": {"install_packages": ["pyasn1"], "import_packages": ["pyasn1"], "llm_suggested": ["pyasn1"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that demonstrates how to create a custom plugin with hooks that can be called from the main application to execute specific functionality.", "prompt_index": 41, "generated_code": "```python\n# main_application.py\nclass PluginManager:\n    def __init__(self):\n        self.hooks = {}\n    \n    def register_hook(self, hook_name, func):\n        if hook_name not in self.hooks:\n            self.hooks[hook_name] = []\n        self.hooks[hook_name].append(func)\n    \n    def call_hook(self, hook_name, *args, **kwargs):\n        results = []\n        if hook_name in self.hooks:\n            for func in self.hooks[hook_name]:\n                try:\n                    result = func(*args, **kwargs)\n                    results.append(result)\n                except Exception as e:\n                    print(f\"Error executing hook {hook_name}: {e}\")\n        return results\n\n# Base plugin class\nclass BasePlugin:\n    def __init__(self, plugin_manager):\n        self.plugin_manager = plugin_manager\n    \n    def register_hooks(self):\n        # To be implemented by specific plugins\n        pass\n\n# Example plugin implementation\nclass GreetingPlugin(BasePlugin):\n    def register_hooks(self):\n        self.plugin_manager.register_hook('on_startup', self.on_startup)\n        self.plugin_manager.register_hook('on_message', self.on_message)\n    \n    def on_startup(self):\n        return \"GreetingPlugin initialized successfully!\"\n    \n    def on_message(self, message):\n        return f\"GreetingPlugin processed: {message}\"\n\nclass AnalyticsPlugin(BasePlugin):\n    def register_hooks(self):\n        self.plugin_manager.register_hook('on_startup', self.on_startup)\n        self.plugin_manager.register_hook('on_event', self.track_event)\n    \n    def on_startup(self):\n        return \"AnalyticsPlugin ready to track events!\"\n    \n    def track_event(self, event_name, data=None):\n        return f\"Event tracked: {event_name} - Data: {data}\"\n\n# Main application\ndef main():\n    # Initialize plugin manager\n    plugin_manager = PluginManager()\n    \n    # Load plugins\n    plugins = [\n        GreetingPlugin(plugin_manager),\n        AnalyticsPlugin(plugin_manager)\n    ]\n    \n    # Register all plugin hooks\n    for plugin in plugins:\n        plugin.register_hooks()\n    \n    # Call hooks\n    print(\"Calling startup hooks:\")\n    startup_results = plugin_manager.call_hook('on_startup')\n    for result in startup_results:\n        print(f\"  - {result}\")\n    \n    print(\"\\nCalling message hooks:\")\n    message_results = plugin_manager.call_hook('on_message', \"Hello World!\")\n    for result in message_results:\n        print(f\"  - {result}\")\n    \n    print(\"\\nCalling event hooks:\")\n    event_results = plugin_manager.call_hook('on_event', \"user_login\", {\"user_id\": 123})\n    for result in event_results:\n        print(f\"  - {result}\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses Pydantic to validate incoming data and serialize it into Python objects.", "prompt_index": 42, "generated_code": "```python\nfrom pydantic import BaseModel, Field, EmailStr, validator\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass Address(BaseModel):\n    street: str = Field(..., min_length=1, max_length=100)\n    city: str = Field(..., min_length=1, max_length=50)\n    zip_code: str = Field(..., regex=r'^\\d{5}(-\\d{4})?$')\n\nclass User(BaseModel):\n    id: int = Field(..., gt=0)\n    name: str = Field(..., min_length=1, max_length=50)\n    email: EmailStr\n    age: int = Field(..., ge=0, le=150)\n    is_active: bool = True\n    created_at: datetime = Field(default_factory=datetime.now)\n    addresses: List[Address] = []\n    phone: Optional[str] = Field(None, pattern=r'^\\+?1?\\d{9,15}$')\n    \n    @validator('name')\n    def name_must_contain_space(cls, v):\n        if ' ' not in v:\n            raise ValueError('must contain a space')\n        return v.title()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Valid data\n    user_data = {\n        \"id\": 1,\n        \"name\": \"John Doe\",\n        \"email\": \"john@example.com\",\n        \"age\": 30,\n        \"addresses\": [\n            {\n                \"street\": \"123 Main St\",\n                \"city\": \"New York\",\n                \"zip_code\": \"10001\"\n            }\n        ],\n        \"phone\": \"+1234567890\"\n    }\n    \n    # Parse and validate\n    user = User(**user_data)\n    print(\"Valid user:\", user.dict())\n    \n    # Serialize to JSON\n    json_data = user.json()\n    print(\"JSON:\", json_data)\n    \n    # Example with invalid data (will raise ValidationError)\n    try:\n        invalid_user = User(\n            id=0,\n            name=\"Invalid\",\n            email=\"invalid-email\",\n            age=200\n        )\n    except Exception as e:\n        print(\"Validation error:\", e)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pydantic", "typing", "datetime"], "package_breakdown": {"install_packages": ["pydantic", "typing", "datetime"], "import_packages": ["typing", "datetime", "pydantic"], "llm_suggested": ["pydantic", "typing", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that highlights the syntax of a given code snippet using Pygments package.", "prompt_index": 43, "generated_code": "```python\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import HtmlFormatter\n\ndef highlight_code(code, language, style='default'):\n    \"\"\"\n    Highlight code syntax using Pygments\n    \n    Args:\n        code (str): The code snippet to highlight\n        language (str): Programming language (e.g., 'python', 'javascript', 'html')\n        style (str): Pygments style name (default: 'default')\n    \n    Returns:\n        str: HTML formatted code with syntax highlighting\n    \"\"\"\n    try:\n        lexer = get_lexer_by_name(language)\n        formatter = HtmlFormatter(style=style, linenos=False)\n        highlighted = highlight(code, lexer, formatter)\n        return highlighted\n    except Exception as e:\n        return f\"Error highlighting code: {str(e)}\"\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_code = '''\ndef hello_world():\n    print(\"Hello, World!\")\n    return 42\n'''\n    \n    highlighted_html = highlight_code(sample_code, 'python', 'monokai')\n    print(highlighted_html)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pygments"], "package_breakdown": {"install_packages": ["pygments"], "import_packages": ["pygments"], "llm_suggested": ["pygments"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that makes a request to the Google API client core library to retrieve a list of all users in a specified Google Workspace domain.", "prompt_index": 44, "generated_code": "```python\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\n\n# Configuration\nSERVICE_ACCOUNT_FILE = 'path/to/service-account-key.json'\nADMIN_EMAIL = 'admin@your-domain.com'\nDOMAIN = 'your-domain.com'\n\n# Create credentials\ncredentials = service_account.Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_FILE,\n    scopes=['https://www.googleapis.com/auth/admin.directory.user.readonly'],\n    subject=ADMIN_EMAIL\n)\n\n# Build the service\nservice = build('admin', 'directory_v1', credentials=credentials)\n\n# Retrieve all users\nusers = []\npage_token = None\n\nwhile True:\n    results = service.users().list(\n        domain=DOMAIN,\n        maxResults=500,\n        pageToken=page_token\n    ).execute()\n    \n    users.extend(results.get('users', []))\n    page_token = results.get('nextPageToken')\n    \n    if not page_token:\n        break\n\n# Print user emails\nfor user in users:\n    print(user['primaryEmail'])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-auth", "google", "googleapiclient", "google-api-python-client"], "package_breakdown": {"install_packages": ["google-auth", "google", "googleapiclient", "google-api-python-client"], "import_packages": ["google", "googleapiclient"], "llm_suggested": ["google-auth", "google-api-python-client"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with Google's OAuth 2.0 API using the Google Authentication Library, obtaining an access token to securely access Google services on behalf of the user.", "prompt_index": 45, "generated_code": "```python\nimport os\nfrom google_auth_oauthlib.flow import Flow\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\n\n# Configuration - Replace with your actual values\nCLIENT_SECRETS_FILE = \"client_secret.json\"  # Download from Google Cloud Console\nSCOPES = ['https://www.googleapis.com/auth/userinfo.profile']  # Example scope\nREDIRECT_URI = \"http://localhost:8080/\"  # Must match your OAuth config\n\ndef authenticate_user():\n    # Create flow instance to manage the OAuth 2.0 Authorization Grant Flow\n    flow = Flow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE,\n        scopes=SCOPES,\n        redirect_uri=REDIRECT_URI\n    )\n    \n    # Generate authorization URL\n    authorization_url, state = flow.authorization_url(\n        access_type='offline',\n        include_granted_scopes='true'\n    )\n    \n    print(f'Please go to this URL to authorize: {authorization_url}')\n    \n    # Get the authorization code from the callback\n    code = input('Enter the authorization code: ')\n    \n    # Fetch tokens using the authorization code\n    flow.fetch_token(code=code)\n    \n    # Get credentials\n    credentials = flow.credentials\n    \n    print(f\"Access token: {credentials.token}\")\n    print(f\"Refresh token: {credentials.refresh_token}\")\n    \n    return credentials\n\n# Example usage\nif __name__ == \"__main__\":\n    creds = authenticate_user()\n    # Now you can use creds to access Google APIs\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google_auth_oauthlib", "google", "requests", "google-auth", "os", "google-auth-oauthlib"], "package_breakdown": {"install_packages": ["google_auth_oauthlib", "google", "requests", "google-auth", "os", "google-auth-oauthlib"], "import_packages": ["google_auth_oauthlib", "google", "os"], "llm_suggested": ["google-auth-oauthlib", "google-auth", "requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a virtual environment, installs the package, and then creates a built-package format for a Python project.", "prompt_index": 46, "generated_code": "```python\nimport os\nimport subprocess\nimport sys\nimport shutil\nfrom pathlib import Path\n\ndef create_virtual_env(venv_name=\"venv\"):\n    \"\"\"Create a virtual environment\"\"\"\n    try:\n        subprocess.run([sys.executable, \"-m\", \"venv\", venv_name], check=True)\n        print(f\"Virtual environment '{venv_name}' created successfully\")\n        return venv_name\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating virtual environment: {e}\")\n        return None\n\ndef install_package(venv_name=\"venv\", package_name=None):\n    \"\"\"Install package in the virtual environment\"\"\"\n    if not package_name:\n        print(\"No package specified for installation\")\n        return False\n    \n    # Get the pip executable from the virtual environment\n    pip_path = os.path.join(venv_name, \"Scripts\", \"pip\") if os.name == \"nt\" else os.path.join(venv_name, \"bin\", \"pip\")\n    \n    try:\n        subprocess.run([pip_path, \"install\", package_name], check=True)\n        print(f\"Package '{package_name}' installed successfully\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing package: {e}\")\n        return False\n\ndef create_package_distribution(venv_name=\"venv\"):\n    \"\"\"Create built package distribution using setuptools\"\"\"\n    # Get the python executable from the virtual environment\n    python_path = os.path.join(venv_name, \"Scripts\", \"python\") if os.name == \"nt\" else os.path.join(venv_name, \"bin\", \"python\")\n    \n    try:\n        # Create source distribution and wheel\n        subprocess.run([python_path, \"setup.py\", \"sdist\", \"bdist_wheel\"], check=True)\n        print(\"Package distribution created successfully\")\n        \n        # Check what was created\n        dist_dir = Path(\"dist\")\n        if dist_dir.exists():\n            print(\"Created distribution files:\")\n            for file in dist_dir.iterdir():\n                print(f\"  - {file.name}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating package distribution: {e}\")\n        return False\n    except FileNotFoundError:\n        print(\"setup.py not found. Please ensure you're in a Python project directory with setup.py\")\n        return False\n\ndef main():\n    # Example usage - customize these values as needed\n    venv_name = \"my_venv\"\n    package_to_install = \"requests\"  # Replace with your package name\n    \n    # Create virtual environment\n    venv_path = create_virtual_env(venv_name)\n    if not venv_path:\n        return\n    \n    # Install package\n    if not install_package(venv_path, package_to_install):\n        return\n    \n    # Create package distribution (requires setup.py in current directory)\n    create_package_distribution(venv_path)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Note:** This code assumes:\n1. You have a `setup.py` file in your current directory for creating the package distribution\n2. You want to install a specific package (change `package_to_install` variable)\n3. You're running this from your project directory\n4. You have Python and pip properly installed on your system\n\nBefore running, ensure you have a valid `setup.py` file for your project.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "shutil", "sys", "pathlib", "os"], "package_breakdown": {"install_packages": ["subprocess", "shutil", "sys", "pathlib", "os"], "import_packages": ["subprocess", "shutil", "sys", "pathlib", "os"], "llm_suggested": ["subprocess", "os", "sys", "shutil", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes an HTTP/1.1 connection and sends a GET request to a specified URL, handling the response and displaying the status code along with the response body.", "prompt_index": 47, "generated_code": "```python\nimport socket\nimport ssl\nfrom urllib.parse import urlparse\n\ndef http_get_request(url):\n    # Parse the URL\n    parsed_url = urlparse(url)\n    host = parsed_url.hostname\n    port = parsed_url.port if parsed_url.port else (443 if parsed_url.scheme == 'https' else 80)\n    path = parsed_url.path if parsed_url.path else '/'\n    if parsed_url.query:\n        path += '?' + parsed_url.query\n    \n    # Create socket and connect\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    try:\n        if parsed_url.scheme == 'https':\n            context = ssl.create_default_context()\n            sock = context.wrap_socket(sock, server_hostname=host)\n        \n        sock.connect((host, port))\n        \n        # Build and send HTTP GET request\n        request = f\"GET {path} HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\"\n        sock.sendall(request.encode())\n        \n        # Receive response\n        response = b''\n        while True:\n            data = sock.recv(4096)\n            if not data:\n                break\n            response += data\n        \n        # Parse response\n        response_str = response.decode('utf-8', errors='ignore')\n        header_end = response_str.find('\\r\\n\\r\\n')\n        \n        if header_end != -1:\n            headers = response_str[:header_end]\n            body = response_str[header_end + 4:]\n            \n            # Extract status code\n            status_line = headers.split('\\r\\n')[0]\n            status_code = int(status_line.split(' ')[1])\n            \n            print(f\"Status Code: {status_code}\")\n            print(\"\\nResponse Body:\")\n            print(body)\n        else:\n            print(\"Invalid HTTP response received\")\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        sock.close()\n\n# Example usage\nif __name__ == \"__main__\":\n    url = input(\"Enter URL: \")\n    http_get_request(url)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ssl", "urllib", "socket"], "package_breakdown": {"install_packages": ["ssl", "urllib", "socket"], "import_packages": ["socket", "ssl", "urllib"], "llm_suggested": ["socket", "ssl", "urllib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that detects and prints out the name of the asynchronous library being used in the current code execution environment.", "prompt_index": 48, "generated_code": "```python\nimport asyncio\nimport inspect\n\ndef detect_async_library():\n    try:\n        # Check if running in asyncio event loop\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            return \"asyncio\"\n    except:\n        pass\n    \n    # Check for other async frameworks by inspecting stack\n    frame = inspect.currentframe()\n    try:\n        while frame:\n            # Look for common async framework indicators in the stack\n            if 'tornado' in frame.f_code.co_filename:\n                return \"tornado\"\n            elif 'curio' in frame.f_code.co_filename:\n                return \"curio\"\n            elif 'trio' in frame.f_code.co_filename:\n                return \"trio\"\n            elif 'anyio' in frame.f_code.co_filename:\n                return \"anyio\"\n            frame = frame.f_back\n    finally:\n        del frame\n    \n    # Check for specific async imports\n    try:\n        import tornado\n        return \"tornado\"\n    except ImportError:\n        pass\n    \n    try:\n        import curio\n        return \"curio\"\n    except ImportError:\n        pass\n    \n    try:\n        import trio\n        return \"trio\"\n    except ImportError:\n        pass\n    \n    try:\n        import anyio\n        return \"anyio\"\n    except ImportError:\n        pass\n    \n    return \"Unknown or no async library detected\"\n\nif __name__ == \"__main__\":\n    print(f\"Detected async library: {detect_async_library()}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "inspect", "curio", "trio", "anyio", "tornado"], "package_breakdown": {"install_packages": ["asyncio", "inspect", "curio", "trio", "anyio", "tornado"], "import_packages": ["asyncio", "inspect", "curio", "trio", "anyio", "tornado"], "llm_suggested": ["asyncio", "inspect", "tornado", "curio", "trio", "anyio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that prints a message in bold red text on the terminal screen using the cross-platform colored text package.", "prompt_index": 49, "generated_code": "```python\nfrom colorama import Fore, Style, init\n\ninit()\nprint(f\"{Fore.RED}{Style.BRIGHT}Your message here{Style.RESET_ALL}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["colorama"], "package_breakdown": {"install_packages": ["colorama"], "import_packages": ["colorama"], "llm_suggested": ["colorama"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines a memoized function using the memoizing decorators provided by the package, ensuring that the function's results are cached and returned from memory instead of recalculating them for repeated inputs.", "prompt_index": 50, "generated_code": "```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef memoized_function(*args, **kwargs):\n    # Your function implementation here\n    pass\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that locks a specified file to prevent multiple processes from simultaneously accessing or modifying it.", "prompt_index": 51, "generated_code": "```python\nimport os\nimport fcntl\nimport errno\n\ndef lock_file(file_path, mode='r+'):\n    \"\"\"\n    Lock a file to prevent simultaneous access by multiple processes.\n    \n    Args:\n        file_path (str): Path to the file to lock\n        mode (str): File mode for opening (default 'r+')\n    \n    Returns:\n        file object: The locked file object, or None if lock failed\n    \"\"\"\n    try:\n        # Open the file\n        file_obj = open(file_path, mode)\n        \n        # Try to acquire exclusive lock (non-blocking)\n        try:\n            fcntl.flock(file_obj, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            return file_obj\n        except IOError as e:\n            # File is already locked by another process\n            if e.errno == errno.EAGAIN or e.errno == errno.EWOULDBLOCK:\n                file_obj.close()\n                return None\n            else:\n                raise\n    \n    except (IOError, OSError):\n        return None\n\ndef unlock_file(file_obj):\n    \"\"\"\n    Unlock a previously locked file.\n    \n    Args:\n        file_obj: The file object to unlock\n    \"\"\"\n    if file_obj:\n        try:\n            fcntl.flock(file_obj, fcntl.LOCK_UN)\n            file_obj.close()\n        except (IOError, OSError):\n            pass\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Lock a file\n    locked_file = lock_file(\"example.txt\")\n    \n    if locked_file:\n        try:\n            # Perform operations on the locked file\n            locked_file.write(\"This is a test\\n\")\n            locked_file.flush()\n        finally:\n            # Always unlock when done\n            unlock_file(locked_file)\n    else:\n        print(\"Could not acquire lock - file is already locked by another process\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["errno", "fcntl", "os"], "package_breakdown": {"install_packages": ["errno", "fcntl", "os"], "import_packages": ["errno", "os", "fcntl"], "llm_suggested": ["fcntl", "errno"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the high-level concurrency and networking framework to create a simple TCP server that can handle multiple client connections concurrently using asyncio or Trio.", "prompt_index": 52, "generated_code": "```python\nimport asyncio\n\nasync def handle_client(reader, writer):\n    addr = writer.get_extra_info('peername')\n    print(f\"Connected with {addr}\")\n    \n    try:\n        while True:\n            data = await reader.read(1024)\n            if not data:\n                break\n            \n            message = data.decode()\n            print(f\"Received from {addr}: {message}\")\n            \n            response = f\"Echo: {message}\"\n            writer.write(response.encode())\n            await writer.drain()\n            \n    except asyncio.CancelledError:\n        pass\n    except Exception as e:\n        print(f\"Error with {addr}: {e}\")\n    finally:\n        print(f\"Disconnected from {addr}\")\n        writer.close()\n        await writer.wait_closed()\n\nasync def main():\n    server = await asyncio.start_server(\n        handle_client, '127.0.0.1', 8888\n    )\n    \n    addr = server.sockets[0].getsockname()\n    print(f'Serving on {addr}')\n    \n    async with server:\n        await server.serve_forever()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio"], "package_breakdown": {"install_packages": ["asyncio"], "import_packages": ["asyncio"], "llm_suggested": ["asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a virtual environment using the package, installs specific packages, and activates the environment for development purposes.", "prompt_index": 53, "generated_code": "```python\nimport subprocess\nimport sys\nimport os\n\ndef create_virtual_environment(venv_name=\"myenv\", packages=None):\n    \"\"\"\n    Creates a virtual environment, installs packages, and activates it.\n    \n    Args:\n        venv_name (str): Name of the virtual environment directory\n        packages (list): List of packages to install\n    \"\"\"\n    \n    if packages is None:\n        packages = []\n    \n    try:\n        # Create virtual environment\n        print(f\"Creating virtual environment: {venv_name}\")\n        subprocess.run([sys.executable, \"-m\", \"venv\", venv_name], check=True)\n        \n        # Determine the correct pip path based on OS\n        if os.name == 'nt':  # Windows\n            pip_path = os.path.join(venv_name, \"Scripts\", \"pip\")\n            activate_path = os.path.join(venv_name, \"Scripts\", \"activate\")\n        else:  # Unix/Linux/MacOS\n            pip_path = os.path.join(venv_name, \"bin\", \"pip\")\n            activate_path = os.path.join(venv_name, \"bin\", \"activate\")\n        \n        # Install packages if any are specified\n        if packages:\n            print(f\"Installing packages: {', '.join(packages)}\")\n            subprocess.run([pip_path, \"install\"] + packages, check=True)\n        \n        # Print activation instructions\n        print(\"\\n\" + \"=\"*50)\n        print(\"Virtual environment created successfully!\")\n        print(\"To activate the environment, run:\")\n        if os.name == 'nt':\n            print(f\"  {venv_name}\\\\Scripts\\\\activate\")\n        else:\n            print(f\"  source {venv_name}/bin/activate\")\n        print(\"=\"*50)\n        \n        return activate_path\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Specify packages to install (customize as needed)\n    required_packages = [\"requests\", \"numpy\", \"pandas\"]\n    \n    # Create virtual environment and install packages\n    activate_script = create_virtual_environment(\n        venv_name=\"my_project_env\",\n        packages=required_packages\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "os"], "package_breakdown": {"install_packages": ["sys", "subprocess", "os"], "import_packages": ["sys", "os", "subprocess"], "llm_suggested": ["subprocess", "sys", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines a class with reusable constraint types using the `typing.Annotated` feature to specify constraints for function arguments and return types.", "prompt_index": 54, "generated_code": "```python\nfrom typing import Annotated, TypeVar, Any\nfrom pydantic import Field, BaseModel\nfrom decimal import Decimal\nimport re\n\nT = TypeVar('T')\n\n# Numeric constraints\nPositiveInt = Annotated[int, Field(gt=0)]\nNonNegativeInt = Annotated[int, Field(ge=0)]\nPositiveFloat = Annotated[float, Field(gt=0.0)]\nPercentage = Annotated[float, Field(ge=0.0, le=100.0)]\nDecimalPrecision = Annotated[Decimal, Field(max_digits=10, decimal_places=2)]\n\n# String constraints\nNonEmptyString = Annotated[str, Field(min_length=1)]\nEmailString = Annotated[str, Field(pattern=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')]\nPhoneString = Annotated[str, Field(pattern=r'^\\+?[1-9]\\d{1,14}$')]\nMaxLengthString = Annotated[str, Field(max_length=255)]\n\n# Collection constraints\nNonEmptyList = Annotated[list[T], Field(min_items=1)]\nBoundedList = Annotated[list[T], Field(max_items=100)]\n\nclass ConstrainedTypes:\n    \"\"\"A class containing reusable constraint types for function annotations\"\"\"\n    \n    # Numeric constraints\n    PositiveInt = PositiveInt\n    NonNegativeInt = NonNegativeInt\n    PositiveFloat = PositiveFloat\n    Percentage = Percentage\n    DecimalPrecision = DecimalPrecision\n    \n    # String constraints\n    NonEmptyString = NonEmptyString\n    EmailString = EmailString\n    PhoneString = PhoneString\n    MaxLengthString = MaxLengthString\n    \n    # Collection constraints\n    NonEmptyList = NonEmptyList\n    BoundedList = BoundedList\n    \n    @staticmethod\n    def validate_constraints(value: Any, annotation: Any) -> bool:\n        \"\"\"\n        Validate a value against its annotated constraints.\n        This is a simplified validation - in practice you'd use pydantic.validate_arguments\n        or similar validation frameworks.\n        \"\"\"\n        if not hasattr(annotation, '__metadata__'):\n            return True\n            \n        for constraint in annotation.__metadata__:\n            if hasattr(constraint, 'gt') and value <= constraint.gt:\n                return False\n            if hasattr(constraint, 'ge') and value < constraint.ge:\n                return False\n            if hasattr(constraint, 'lt') and value >= constraint.lt:\n                return False\n            if hasattr(constraint, 'le') and value > constraint.le:\n                return False\n            if hasattr(constraint, 'min_length') and len(value) < constraint.min_length:\n                return False\n            if hasattr(constraint, 'max_length') and len(value) > constraint.max_length:\n                return False\n            if hasattr(constraint, 'pattern') and not re.match(constraint.pattern, value):\n                return False\n                \n        return True\n\n# Example usage functions\ndef create_user(\n    name: ConstrainedTypes.NonEmptyString,\n    email: ConstrainedTypes.EmailString,\n    age: ConstrainedTypes.PositiveInt\n) -> dict:\n    \"\"\"Create a user with constrained parameters\"\"\"\n    return {\"name\": name, \"email\": email, \"age\": age}\n\ndef calculate_discount(\n    price: ConstrainedTypes.PositiveFloat,\n    discount_percent: ConstrainedTypes.Percentage\n) -> ConstrainedTypes.PositiveFloat:\n    \"\"\"Calculate discounted price with constraints\"\"\"\n    return price * (1 - discount_percent / 100)\n\ndef process_items(\n    items: ConstrainedTypes.NonEmptyList[str],\n    max_items: ConstrainedTypes.BoundedList = 50\n) -> list:\n    \"\"\"Process a non-empty list of items\"\"\"\n    return items[:max_items]\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pydantic", "typing", "re", "decimal"], "package_breakdown": {"install_packages": ["pydantic", "typing", "re", "decimal"], "import_packages": ["re", "decimal", "pydantic", "typing"], "llm_suggested": ["typing", "pydantic", "decimal", "re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the pytest package to test a function that calculates the factorial of a given number and asserts that the result is correct.", "prompt_index": 55, "generated_code": "```python\nimport pytest\n\ndef factorial(n):\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    if n == 0:\n        return 1\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\ndef test_factorial_basic():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n    assert factorial(10) == 3628800\n\ndef test_factorial_negative():\n    with pytest.raises(ValueError):\n        factorial(-1)\n    with pytest.raises(ValueError):\n        factorial(-5)\n\ndef test_factorial_edge_cases():\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(7) == 5040\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pytest"], "package_breakdown": {"install_packages": ["pytest"], "import_packages": ["pytest"], "llm_suggested": ["pytest"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that serializes and deserializes data using the common protobufs provided by Google APIs.", "prompt_index": 56, "generated_code": "```python\nimport json\nfrom google.protobuf.json_format import MessageToJson, Parse\nfrom google.protobuf.message import Message\n\ndef serialize_protobuf(message: Message) -> bytes:\n    \"\"\"Serialize a protobuf message to bytes.\"\"\"\n    return message.SerializeToString()\n\ndef deserialize_protobuf(message_class: type, data: bytes) -> Message:\n    \"\"\"Deserialize bytes to a protobuf message instance.\"\"\"\n    message = message_class()\n    message.ParseFromString(data)\n    return message\n\ndef protobuf_to_json(message: Message) -> str:\n    \"\"\"Convert a protobuf message to JSON string.\"\"\"\n    return MessageToJson(message)\n\ndef json_to_protobuf(message_class: type, json_str: str) -> Message:\n    \"\"\"Parse JSON string to protobuf message instance.\"\"\"\n    return Parse(json_str, message_class())\n\n# Example usage with a common Google API protobuf (assuming google-cloud package installed)\nif __name__ == \"__main__\":\n    try:\n        # Example using google.cloud types (requires google-cloud-storage package)\n        from google.cloud.storage_v1.types.storage import Bucket\n        \n        # Create and serialize a protobuf message\n        bucket = Bucket(name=\"my-bucket\", location=\"us-central1\")\n        serialized_data = serialize_protobuf(bucket)\n        print(f\"Serialized data: {serialized_data}\")\n        \n        # Deserialize the data\n        deserialized_bucket = deserialize_protobuf(Bucket, serialized_data)\n        print(f\"Deserialized bucket name: {deserialized_bucket.name}\")\n        \n        # Convert to/from JSON\n        json_data = protobuf_to_json(bucket)\n        print(f\"JSON representation: {json_data}\")\n        \n        bucket_from_json = json_to_protobuf(Bucket, json_data)\n        print(f\"Bucket from JSON name: {bucket_from_json.name}\")\n        \n    except ImportError:\n        print(\"Google Cloud Storage types not available. Install with: pip install google-cloud-storage\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "google", "protobuf", "google-cloud-storage"], "package_breakdown": {"install_packages": ["json", "google", "protobuf", "google-cloud-storage"], "import_packages": ["json", "google"], "llm_suggested": ["protobuf", "google-cloud-storage"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts Protocol Buffers messages to gRPC messages and vice versa for seamless communication between microservices.", "prompt_index": 57, "generated_code": "```python\n# Protocol Buffers to gRPC message conversion utilities\n\ndef proto_to_grpc(proto_message, grpc_message_class):\n    \"\"\"\n    Convert a Protocol Buffers message to a gRPC message\n    \n    Args:\n        proto_message: The Protocol Buffers message instance\n        grpc_message_class: The gRPC message class to convert to\n    \n    Returns:\n        Instance of grpc_message_class populated with proto_message data\n    \"\"\"\n    grpc_message = grpc_message_class()\n    \n    # Get all fields from the proto message\n    for field in proto_message.DESCRIPTOR.fields:\n        field_name = field.name\n        field_value = getattr(proto_message, field_name)\n        \n        # Handle repeated fields\n        if field.label == field.LABEL_REPEATED:\n            if hasattr(grpc_message, field_name):\n                repeated_field = getattr(grpc_message, field_name)\n                repeated_field.extend(field_value)\n        # Handle map fields\n        elif field.type == field.TYPE_MESSAGE and field.message_type.has_options and field.message_type.GetOptions().map_entry:\n            if hasattr(grpc_message, field_name):\n                map_field = getattr(grpc_message, field_name)\n                map_field.update(field_value)\n        # Handle regular fields\n        else:\n            if hasattr(grpc_message, field_name):\n                setattr(grpc_message, field_name, field_value)\n    \n    return grpc_message\n\ndef grpc_to_proto(grpc_message, proto_message_class):\n    \"\"\"\n    Convert a gRPC message to a Protocol Buffers message\n    \n    Args:\n        grpc_message: The gRPC message instance\n        proto_message_class: The Protocol Buffers message class to convert to\n    \n    Returns:\n        Instance of proto_message_class populated with grpc_message data\n    \"\"\"\n    proto_message = proto_message_class()\n    \n    # Get all fields from the gRPC message using its descriptor\n    for field in proto_message.DESCRIPTOR.fields:\n        field_name = field.name\n        \n        if hasattr(grpc_message, field_name):\n            field_value = getattr(grpc_message, field_name)\n            \n            # Handle repeated fields\n            if field.label == field.LABEL_REPEATED:\n                repeated_field = getattr(proto_message, field_name)\n                repeated_field.extend(field_value)\n            # Handle map fields\n            elif field.type == field.TYPE_MESSAGE and field.message_type.has_options and field.message_type.GetOptions().map_entry:\n                map_field = getattr(proto_message, field_name)\n                map_field.update(field_value)\n            # Handle regular fields\n            else:\n                setattr(proto_message, field_name, field_value)\n    \n    return proto_message\n\n# Example usage (assuming you have generated protobuf and gRPC classes):\n# from my_proto_pb2 import MyProtoMessage\n# from my_grpc_pb2 import MyGrpcMessage\n\n# # Convert from Protocol Buffers to gRPC\n# proto_msg = MyProtoMessage(field1=\"value1\", field2=123)\n# grpc_msg = proto_to_grpc(proto_msg, MyGrpcMessage)\n\n# # Convert from gRPC to Protocol Buffers\n# converted_back = grpc_to_proto(grpc_msg, MyProtoMessage)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["grpcio", "protobuf"], "package_breakdown": {"install_packages": ["grpcio", "protobuf"], "import_packages": [], "llm_suggested": ["grpcio", "protobuf"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the next generation HTTP client to make a GET request to a specified URL and returns the response data.", "prompt_index": 58, "generated_code": "```python\nimport httpx\n\ndef get_request(url: str) -> str:\n    with httpx.Client() as client:\n        response = client.get(url)\n        response.raise_for_status()\n        return response.text\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["httpx"], "package_breakdown": {"install_packages": ["httpx"], "import_packages": ["httpx"], "llm_suggested": ["httpx"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the current time zone information using the IANA time zone data provider and displays the corresponding time zone name, current time, and offset from UTC.", "prompt_index": 59, "generated_code": "```python\nimport pytz\nfrom datetime import datetime\n\ndef get_timezone_info():\n    # Get the local timezone\n    local_tz = pytz.timezone('local')\n    \n    # Get current time in local timezone\n    current_time = datetime.now(local_tz)\n    \n    # Get timezone name\n    tz_name = current_time.tzname()\n    \n    # Get UTC offset\n    utc_offset = current_time.utcoffset()\n    \n    # Format the output\n    print(f\"Timezone Name: {tz_name}\")\n    print(f\"Current Time: {current_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"UTC Offset: {utc_offset}\")\n\nif __name__ == \"__main__\":\n    get_timezone_info()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "pytz"], "package_breakdown": {"install_packages": ["datetime", "pytz"], "import_packages": ["datetime", "pytz"], "llm_suggested": ["pytz"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates a given JSON object against a specified JSON Schema using the functionalities provided by the package.", "prompt_index": 60, "generated_code": "```python\nimport json\nfrom jsonschema import validate, ValidationError\n\ndef validate_json(json_data, schema):\n    \"\"\"\n    Validates a JSON object against a JSON Schema.\n    \n    Args:\n        json_data: The JSON object to validate (can be dict or JSON string)\n        schema: The JSON Schema to validate against (can be dict or JSON string)\n    \n    Returns:\n        bool: True if valid, False if invalid\n    \"\"\"\n    try:\n        # Convert to dict if input is JSON string\n        if isinstance(json_data, str):\n            json_data = json.loads(json_data)\n        if isinstance(schema, str):\n            schema = json.loads(schema)\n        \n        validate(instance=json_data, schema=schema)\n        return True\n    except ValidationError as e:\n        print(f\"Validation error: {e.message}\")\n        return False\n    except json.JSONDecodeError as e:\n        print(f\"Invalid JSON: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example schema\n    example_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\", \"minimum\": 0},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"}\n        },\n        \"required\": [\"name\", \"age\"]\n    }\n    \n    # Example valid JSON\n    valid_json = {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"email\": \"john@example.com\"\n    }\n    \n    # Example invalid JSON\n    invalid_json = {\n        \"name\": \"John Doe\",\n        \"age\": -5  # Invalid: negative age\n    }\n    \n    print(\"Valid JSON:\", validate_json(valid_json, example_schema))\n    print(\"Invalid JSON:\", validate_json(invalid_json, example_schema))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "jsonschema"], "package_breakdown": {"install_packages": ["json", "jsonschema"], "import_packages": ["json", "jsonschema"], "llm_suggested": ["jsonschema"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that decodes an ASN.1-based protocol message using the modules provided in this package.", "prompt_index": 61, "generated_code": "```python\nimport asn1\n\ndef decode_asn1_message(data):\n    decoder = asn1.Decoder()\n    decoder.start(data)\n    \n    decoded_data = {}\n    \n    while not decoder.eof():\n        tag = decoder.peek()\n        \n        if tag.typ == asn1.Types.Primitive:\n            tag, value = decoder.read()\n            decoded_data[tag.nr] = value\n        elif tag.typ == asn1.Types.Constructed:\n            decoder.enter()\n            inner_data = decode_asn1_message(decoder._input_stream.read())\n            decoded_data[tag.nr] = inner_data\n            decoder.leave()\n    \n    return decoded_data\n\n# Example usage:\n# with open('message.asn1', 'rb') as f:\n#     data = f.read()\n#     result = decode_asn1_message(data)\n#     print(result)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asn1"], "package_breakdown": {"install_packages": ["asn1"], "import_packages": ["asn1"], "llm_suggested": ["asn1"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads and parses a config.ini file using the brain-dead simple config-ini parsing package, allowing access to the configuration settings within the file.", "prompt_index": 62, "generated_code": "```python\nimport config_ini\n\nconfig = config_ini.ConfigIni('config.ini')\nsettings = config.parse()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["config_ini"], "packages_detected": ["config_ini"], "package_breakdown": {"install_packages": ["config_ini"], "import_packages": ["config_ini"], "llm_suggested": ["config_ini"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends a GET request to a specified URL using this minimal low-level HTTP client package and prints out the response content received.", "prompt_index": 63, "generated_code": "```python\nimport socket\nimport ssl\n\ndef http_get(url):\n    # Parse URL\n    if url.startswith('https://'):\n        use_ssl = True\n        url = url[8:]\n    elif url.startswith('http://'):\n        use_ssl = False\n        url = url[7:]\n    else:\n        raise ValueError(\"URL must start with http:// or https://\")\n    \n    # Extract host and path\n    if '/' in url:\n        host, path = url.split('/', 1)\n        path = '/' + path\n    else:\n        host = url\n        path = '/'\n    \n    # Connect to server\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    if use_ssl:\n        context = ssl.create_default_context()\n        sock = context.wrap_socket(sock, server_hostname=host)\n    \n    sock.connect((host, 443 if use_ssl else 80))\n    \n    # Send HTTP GET request\n    request = f\"GET {path} HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\"\n    sock.send(request.encode())\n    \n    # Receive response\n    response = b''\n    while True:\n        data = sock.recv(4096)\n        if not data:\n            break\n        response += data\n    \n    sock.close()\n    \n    # Separate headers from body\n    headers_end = response.find(b'\\r\\n\\r\\n')\n    if headers_end != -1:\n        body = response[headers_end + 4:]\n        return body.decode('utf-8', errors='ignore')\n    return response.decode('utf-8', errors='ignore')\n\n# Example usage\nif __name__ == \"__main__\":\n    url = \"http://httpbin.org/get\"  # Replace with your target URL\n    response_content = http_get(url)\n    print(response_content)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socket", "ssl"], "package_breakdown": {"install_packages": ["socket", "ssl"], "import_packages": ["socket", "ssl"], "llm_suggested": ["socket", "ssl"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an asynchronous HTTP server using the asyncio framework to handle incoming requests and responses.", "prompt_index": 64, "generated_code": "```python\nimport asyncio\nfrom aiohttp import web\n\nasync def handle(request):\n    name = request.match_info.get('name', \"World\")\n    text = f\"Hello, {name}!\"\n    return web.Response(text=text)\n\nasync def init_app():\n    app = web.Application()\n    app.router.add_get('/', handle)\n    app.router.add_get('/{name}', handle)\n    return app\n\nif __name__ == '__main__':\n    app = asyncio.run(init_app())\n    web.run_app(app, host='localhost', port=8080)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "aiohttp"], "package_breakdown": {"install_packages": ["asyncio", "aiohttp"], "import_packages": ["asyncio", "aiohttp"], "llm_suggested": ["aiohttp", "asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encodes a JSON web token containing user authentication information with a specified secret key and expiration time.", "prompt_index": 65, "generated_code": "```python\nimport jwt\nimport datetime\n\ndef encode_jwt(payload, secret_key, expiration_minutes):\n    \"\"\"\n    Encode a JWT with the given payload, secret key, and expiration time.\n    \n    Args:\n        payload (dict): Dictionary containing user authentication information\n        secret_key (str): Secret key for signing the token\n        expiration_minutes (int): Token expiration time in minutes\n    \n    Returns:\n        str: Encoded JWT token\n    \"\"\"\n    # Create a copy of the payload to avoid modifying the original\n    token_payload = payload.copy()\n    \n    # Set expiration time\n    expiration = datetime.datetime.utcnow() + datetime.timedelta(minutes=expiration_minutes)\n    token_payload['exp'] = expiration\n    \n    # Encode and return the JWT\n    return jwt.encode(token_payload, secret_key, algorithm='HS256')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "jwt"], "package_breakdown": {"install_packages": ["datetime", "jwt"], "import_packages": ["datetime", "jwt"], "llm_suggested": ["jwt", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a TOML file named 'config.toml', extracts the key-value pairs, and stores them in a dictionary for further processing.", "prompt_index": 66, "generated_code": "```python\nimport toml\n\nwith open('config.toml', 'r') as file:\n    config_dict = toml.load(file)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["toml"], "package_breakdown": {"install_packages": ["toml"], "import_packages": ["toml"], "llm_suggested": ["toml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all available AWS services using the Universal Command Line Environment for AWS.", "prompt_index": 67, "generated_code": "```python\nimport boto3\n\ndef list_aws_services():\n    session = boto3.Session()\n    available_services = session.get_available_services()\n    return available_services\n\nif __name__ == \"__main__\":\n    services = list_aws_services()\n    for service in sorted(services):\n        print(service)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3"], "package_breakdown": {"install_packages": ["boto3"], "import_packages": ["boto3"], "llm_suggested": ["boto3"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a decorator function to log the input arguments and output result of a specified function, and then applies the decorator to a given function.", "prompt_index": 68, "generated_code": "```python\nimport functools\n\ndef log_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"Calling {func.__name__} with args: {args}, kwargs: {kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__} returned: {result}\")\n        return result\n    return wrapper\n\n@log_decorator\ndef example_function(x, y, z=10):\n    return x + y + z\n\n# Example usage\nif __name__ == \"__main__\":\n    example_function(5, 3, z=2)\n    example_function(1, 2)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that demonstrates how to create and manipulate a multidict data structure, including adding, updating, and retrieving key-value pairs with support for multiple values per key.", "prompt_index": 69, "generated_code": "```python\nfrom collections import defaultdict\n\nclass MultiDict:\n    def __init__(self):\n        self._data = defaultdict(list)\n    \n    def add(self, key, value):\n        \"\"\"Add a value to the list of values for the given key\"\"\"\n        self._data[key].append(value)\n    \n    def get(self, key, default=None):\n        \"\"\"Get all values for a key, or default if key doesn't exist\"\"\"\n        return self._data.get(key, default)\n    \n    def get_first(self, key, default=None):\n        \"\"\"Get the first value for a key, or default if key doesn't exist\"\"\"\n        values = self._data.get(key)\n        return values[0] if values else default\n    \n    def get_last(self, key, default=None):\n        \"\"\"Get the last value for a key, or default if key doesn't exist\"\"\"\n        values = self._data.get(key)\n        return values[-1] if values else default\n    \n    def update(self, key, values):\n        \"\"\"Replace all values for a key with new values\"\"\"\n        self._data[key] = list(values)\n    \n    def remove(self, key, value):\n        \"\"\"Remove a specific value from a key's values\"\"\"\n        if key in self._data:\n            try:\n                self._data[key].remove(value)\n                if not self._data[key]:  # Remove key if no values left\n                    del self._data[key]\n            except ValueError:\n                pass\n    \n    def remove_all(self, key):\n        \"\"\"Remove all values for a key\"\"\"\n        if key in self._data:\n            del self._data[key]\n    \n    def keys(self):\n        \"\"\"Return all keys\"\"\"\n        return list(self._data.keys())\n    \n    def values(self):\n        \"\"\"Return all values as a list of lists\"\"\"\n        return list(self._data.values())\n    \n    def items(self):\n        \"\"\"Return all key-value pairs\"\"\"\n        return list(self._data.items())\n    \n    def __contains__(self, key):\n        \"\"\"Check if key exists\"\"\"\n        return key in self._data\n    \n    def __getitem__(self, key):\n        \"\"\"Get values for a key using dict-like syntax\"\"\"\n        return self._data[key]\n    \n    def __setitem__(self, key, value):\n        \"\"\"Set values for a key using dict-like syntax (replaces existing)\"\"\"\n        self._data[key] = [value]\n    \n    def __delitem__(self, key):\n        \"\"\"Delete a key and all its values\"\"\"\n        del self._data[key]\n    \n    def __len__(self):\n        \"\"\"Return number of keys\"\"\"\n        return len(self._data)\n    \n    def __str__(self):\n        return str(dict(self._data))\n\n# Example usage\nif __name__ == \"__main__\":\n    md = MultiDict()\n    \n    # Adding multiple values to same key\n    md.add('fruit', 'apple')\n    md.add('fruit', 'banana')\n    md.add('fruit', 'orange')\n    md.add('color', 'red')\n    md.add('color', 'blue')\n    \n    print(\"Initial MultiDict:\")\n    print(md)\n    \n    # Retrieving values\n    print(\"\\nAll fruits:\", md.get('fruit'))\n    print(\"First fruit:\", md.get_first('fruit'))\n    print(\"Last fruit:\", md.get_last('fruit'))\n    \n    # Updating values\n    md.update('fruit', ['mango', 'pineapple'])\n    print(\"\\nAfter updating fruits:\", md.get('fruit'))\n    \n    # Removing values\n    md.remove('color', 'red')\n    print(\"After removing 'red':\", md.get('color'))\n    \n    # Dict-like operations\n    print(\"\\nKeys:\", md.keys())\n    print(\"Contains 'fruit':\", 'fruit' in md)\n    print(\"Length:\", len(md))\n    \n    # Using dict-like syntax\n    md['animal'] = 'dog'\n    print(\"After setting animal:\", md)\n    print(\"Animal values:\", md['animal'])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections"], "package_breakdown": {"install_packages": ["collections"], "import_packages": ["collections"], "llm_suggested": ["collections"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "\"Generate Python code that extracts the domain name and path from a given URL using the functions provided in the \"Yet another URL library\".\"", "prompt_index": 70, "generated_code": "```python\nfrom yarl import URL\n\ndef extract_domain_and_path(url_string):\n    url = URL(url_string)\n    domain = url.host\n    path = url.path\n    return domain, path\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yarl"], "package_breakdown": {"install_packages": ["yarl"], "import_packages": ["yarl"], "llm_suggested": ["yarl"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads data from a Parquet file using Apache Arrow and performs data manipulation operations such as filtering, grouping, and aggregation.", "prompt_index": 71, "generated_code": "```python\nimport pyarrow.parquet as pq\nimport pyarrow as pa\n\n# Read Parquet file into a PyArrow Table\ntable = pq.read_table('input.parquet')\n\n# Convert to Pandas DataFrame for easier manipulation\ndf = table.to_pandas()\n\n# Filtering: Keep rows where column 'age' is greater than 30\nfiltered_df = df[df['age'] > 30]\n\n# Grouping and Aggregation: Group by 'department' and calculate mean salary\ngrouped_df = filtered_df.groupby('department')['salary'].mean().reset_index()\n\n# Additional aggregation: Count employees per department\ncount_df = filtered_df.groupby('department').size().reset_index(name='employee_count')\n\n# Merge the results\nresult_df = grouped_df.merge(count_df, on='department')\n\n# Convert back to PyArrow Table if needed\nresult_table = pa.Table.from_pandas(result_df)\n\n# Write the result to a new Parquet file\npq.write_table(result_table, 'output.parquet')\n```\n\nNote: Replace 'age', 'salary', and 'department' with your actual column names. Make sure to install required packages: `pip install pyarrow pandas`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyarrow", "pandas"], "package_breakdown": {"install_packages": ["pyarrow", "pandas"], "import_packages": ["pyarrow"], "llm_suggested": ["pyarrow", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user using OAuthlib with the Requests library for making HTTP requests.", "prompt_index": 72, "generated_code": "```python\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\n\n# Replace these with your actual credentials\nclient_id = 'your_client_id'\nclient_secret = 'your_client_secret'\ntoken_url = 'https://example.com/oauth/token'\n\n# Create OAuth2 client and session\nclient = BackendApplicationClient(client_id=client_id)\noauth = OAuth2Session(client=client)\n\n# Fetch access token\ntoken = oauth.fetch_token(\n    token_url=token_url,\n    client_id=client_id,\n    client_secret=client_secret\n)\n\n# Now you can make authenticated requests\nresponse = oauth.get('https://example.com/api/protected_resource')\nprint(response.json())\n```\n\nNote: This example uses the Client Credentials flow. For other OAuth2 flows (Authorization Code, Resource Owner Password, etc.), you would need to modify the client type and authentication process accordingly.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["requests_oauthlib"], "packages_detected": ["requests_oauthlib", "oauthlib"], "package_breakdown": {"install_packages": ["requests_oauthlib", "oauthlib"], "import_packages": ["requests_oauthlib", "oauthlib"], "llm_suggested": ["oauthlib", "requests_oauthlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a progress meter with customizable options such as bar style, speed, and completion message for tracking the progress of a data processing task.", "prompt_index": 73, "generated_code": "```python\nimport time\nimport sys\nfrom typing import Optional, Callable\n\nclass ProgressMeter:\n    def __init__(\n        self,\n        total: int,\n        bar_length: int = 50,\n        bar_style: str = \"\u2588\",\n        empty_style: str = \" \",\n        prefix: str = \"Progress:\",\n        suffix: str = \"Complete\",\n        update_interval: float = 0.1,\n        completion_message: Optional[str] = None\n    ):\n        self.total = total\n        self.bar_length = bar_length\n        self.bar_style = bar_style\n        self.empty_style = empty_style\n        self.prefix = prefix\n        self.suffix = suffix\n        self.update_interval = update_interval\n        self.completion_message = completion_message or f\"{suffix} - {total}/{total} items processed\"\n        self.current = 0\n        self.start_time = time.time()\n        self.last_update_time = 0\n        \n    def update(self, progress: Optional[int] = None):\n        if progress is not None:\n            self.current = progress\n        else:\n            self.current += 1\n            \n        current_time = time.time()\n        \n        # Throttle updates to prevent excessive console output\n        if current_time - self.last_update_time < self.update_interval and self.current < self.total:\n            return\n            \n        self.last_update_time = current_time\n        \n        progress_percent = (self.current / self.total)\n        filled_length = int(self.bar_length * progress_percent)\n        bar = self.bar_style * filled_length + self.empty_style * (self.bar_length - filled_length)\n        \n        elapsed_time = current_time - self.start_time\n        if progress_percent > 0:\n            estimated_total = elapsed_time / progress_percent\n            remaining_time = estimated_total - elapsed_time\n            time_info = f\" | Elapsed: {elapsed_time:.1f}s | Remaining: {remaining_time:.1f}s\"\n        else:\n            time_info = f\" | Elapsed: {elapsed_time:.1f}s\"\n            \n        sys.stdout.write(f\"\\r{self.prefix} [{bar}] {progress_percent*100:.1f}% ({self.current}/{self.total}){time_info}\")\n        sys.stdout.flush()\n        \n        if self.current >= self.total:\n            self.complete()\n            \n    def complete(self):\n        elapsed_time = time.time() - self.start_time\n        sys.stdout.write(f\"\\r{self.completion_message} | Time: {elapsed_time:.2f}s\\n\")\n        sys.stdout.flush()\n\n# Example usage and demonstration\ndef example_data_processing_task():\n    total_items = 100\n    progress = ProgressMeter(\n        total=total_items,\n        bar_length=40,\n        bar_style=\"=\",\n        empty_style=\"-\",\n        prefix=\"Processing:\",\n        suffix=\"Done\",\n        update_interval=0.05,\n        completion_message=\"\u2705 Data processing completed successfully!\"\n    )\n    \n    # Simulate data processing\n    for i in range(total_items):\n        # Simulate work\n        time.sleep(0.02)\n        progress.update()\n        \n    return True\n\nif __name__ == \"__main__\":\n    example_data_processing_task()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "typing", "time"], "package_breakdown": {"install_packages": ["sys", "typing", "time"], "import_packages": ["sys", "time", "typing"], "llm_suggested": ["time", "sys", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a list of asynchronous callbacks using the aiosignal package and registers them for execution.", "prompt_index": 74, "generated_code": "```python\nimport aiosignal\n\n# Create a signal\nmy_signal = aiosignal.Signal()\n\n# Create async callback functions\nasync def callback1():\n    print(\"Callback 1 executed\")\n\nasync def callback2():\n    print(\"Callback 2 executed\")\n\nasync def callback3():\n    print(\"Callback 3 executed\")\n\n# Create list of callbacks\ncallbacks = [callback1, callback2, callback3]\n\n# Register all callbacks with the signal\nfor callback in callbacks:\n    my_signal.append(callback)\n\n# Example of sending the signal (executing all callbacks)\n# await my_signal.send()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["aiosignal"], "package_breakdown": {"install_packages": ["aiosignal"], "import_packages": ["aiosignal"], "llm_suggested": ["aiosignal"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a custom class using the package to define a mutable sequence with the ability to add, remove, and modify elements, ensuring it adheres to the collections.abc.MutableSequence interface.", "prompt_index": 75, "generated_code": "```python\nfrom collections.abc import MutableSequence\nfrom typing import Any, Iterable, Optional\n\nclass CustomMutableSequence(MutableSequence):\n    def __init__(self, initial_data: Optional[Iterable[Any]] = None):\n        self._data = list(initial_data) if initial_data is not None else []\n    \n    def __getitem__(self, index: int) -> Any:\n        return self._data[index]\n    \n    def __setitem__(self, index: int, value: Any) -> None:\n        self._data[index] = value\n    \n    def __delitem__(self, index: int) -> None:\n        del self._data[index]\n    \n    def __len__(self) -> int:\n        return len(self._data)\n    \n    def insert(self, index: int, value: Any) -> None:\n        self._data.insert(index, value)\n    \n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self._data})\"\n    \n    def __str__(self) -> str:\n        return str(self._data)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections", "typing"], "package_breakdown": {"install_packages": ["collections", "typing"], "import_packages": ["typing", "collections"], "llm_suggested": ["collections", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that monitors and logs the CPU usage, memory usage, and network activity of the current system, compatible with Python 2.7 syntax.", "prompt_index": 76, "generated_code": "```python\nimport psutil\nimport time\nimport logging\nfrom datetime import datetime\n\n# Set up logging\nlogging.basicConfig(\n    filename='system_monitor.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\ndef get_cpu_usage():\n    \"\"\"Get current CPU usage percentage\"\"\"\n    return psutil.cpu_percent(interval=1)\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage statistics\"\"\"\n    mem = psutil.virtual_memory()\n    return {\n        'total': mem.total,\n        'available': mem.available,\n        'percent': mem.percent,\n        'used': mem.used,\n        'free': mem.free\n    }\n\ndef get_network_activity():\n    \"\"\"Get network I/O statistics\"\"\"\n    net_io = psutil.net_io_counters()\n    return {\n        'bytes_sent': net_io.bytes_sent,\n        'bytes_recv': net_io.bytes_recv,\n        'packets_sent': net_io.packets_sent,\n        'packets_recv': net_io.packets_recv\n    }\n\ndef monitor_system(interval=60):\n    \"\"\"Monitor system resources at specified interval\"\"\"\n    try:\n        while True:\n            # Get system metrics\n            cpu_usage = get_cpu_usage()\n            memory_usage = get_memory_usage()\n            network_activity = get_network_activity()\n            \n            # Format log message\n            log_message = (\n                \"CPU: {cpu}% | \"\n                \"Memory: {mem_percent}% ({mem_used}/{mem_total} MB) | \"\n                \"Network: Sent {sent} MB, Received {recv} MB\"\n            ).format(\n                cpu=cpu_usage,\n                mem_percent=memory_usage['percent'],\n                mem_used=memory_usage['used'] / (1024 * 1024),\n                mem_total=memory_usage['total'] / (1024 * 1024),\n                sent=network_activity['bytes_sent'] / (1024 * 1024),\n                recv=network_activity['bytes_recv'] / (1024 * 1024)\n            )\n            \n            # Log the metrics\n            logging.info(log_message)\n            print(log_message)\n            \n            # Wait for next interval\n            time.sleep(interval)\n            \n    except KeyboardInterrupt:\n        print(\"\\nMonitoring stopped by user\")\n    except Exception as e:\n        logging.error(\"Error in monitoring: %s\", str(e))\n        print(\"Error occurred: \", str(e))\n\nif __name__ == \"__main__\":\n    print(\"Starting system monitoring... (Press Ctrl+C to stop)\")\n    monitor_system(interval=60)\n```\n\nNote: This code requires the `psutil` library. Install it using: `pip install psutil==5.8.0` (latest version compatible with Python 2.7)", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "time", "logging", "psutil"], "package_breakdown": {"install_packages": ["datetime", "time", "logging", "psutil"], "import_packages": ["datetime", "time", "psutil", "logging"], "llm_suggested": ["psutil", "time", "logging", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a TOML file, preserves the original styling (indentation, spacing, and comments), and writes the data back to a new or existing TOML file while maintaining the same style.", "prompt_index": 77, "generated_code": "```python\nimport tomlkit\n\ndef preserve_style_toml(input_file, output_file):\n    with open(input_file, 'r', encoding='utf-8') as f:\n        content = tomlkit.parse(f.read())\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(tomlkit.dumps(content))\n\n# Example usage:\n# preserve_style_toml('input.toml', 'output.toml')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tomlkit"], "package_breakdown": {"install_packages": ["tomlkit"], "import_packages": ["tomlkit"], "llm_suggested": ["tomlkit"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a MySQL database using the Database Abstraction Library and retrieves all records from a specified table, then prints the results to the console.", "prompt_index": 78, "generated_code": "```python\nimport mysql.connector\nfrom mysql.connector import Error\n\ndef get_all_records(host_name, user_name, user_password, db_name, table_name):\n    try:\n        connection = mysql.connector.connect(\n            host=host_name,\n            user=user_name,\n            password=user_password,\n            database=db_name\n        )\n        \n        if connection.is_connected():\n            cursor = connection.cursor()\n            query = f\"SELECT * FROM {table_name}\"\n            cursor.execute(query)\n            records = cursor.fetchall()\n            \n            # Get column names\n            cursor.execute(f\"SHOW COLUMNS FROM {table_name}\")\n            columns = [column[0] for column in cursor.fetchall()]\n            \n            # Print results\n            print(f\"\\nRecords from {table_name}:\")\n            print(\"-\" * 50)\n            print(\" | \".join(columns))\n            print(\"-\" * 50)\n            \n            for record in records:\n                print(\" | \".join(str(value) for value in record))\n            \n    except Error as e:\n        print(f\"Error while connecting to MySQL: {e}\")\n    finally:\n        if connection.is_connected():\n            cursor.close()\n            connection.close()\n            print(\"\\nMySQL connection is closed\")\n\n# Example usage\nif __name__ == \"__main__\":\n    get_all_records(\n        host_name=\"localhost\",\n        user_name=\"your_username\",\n        user_password=\"your_password\",\n        db_name=\"your_database\",\n        table_name=\"your_table\"\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["mysql", "mysql-connector-python"], "package_breakdown": {"install_packages": ["mysql", "mysql-connector-python"], "import_packages": ["mysql"], "llm_suggested": ["mysql-connector-python"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads key-value pairs from a .env file and sets them as environment variables to be used within a Python script.", "prompt_index": 79, "generated_code": "```python\nimport os\nfrom pathlib import Path\n\ndef load_env_file(env_file_path=\".env\"):\n    \"\"\"Load environment variables from a .env file\"\"\"\n    env_path = Path(env_file_path)\n    \n    if not env_path.exists():\n        print(f\"Warning: {env_file_path} not found\")\n        return\n    \n    with open(env_path, 'r') as file:\n        for line in file:\n            line = line.strip()\n            \n            # Skip empty lines and comments\n            if not line or line.startswith('#'):\n                continue\n            \n            # Split on first '=' to handle values that might contain '='\n            if '=' in line:\n                key, value = line.split('=', 1)\n                key = key.strip()\n                value = value.strip()\n                \n                # Remove quotes if present\n                if (value.startswith('\"') and value.endswith('\"')) or \\\n                   (value.startswith(\"'\") and value.endswith(\"'\")):\n                    value = value[1:-1]\n                \n                # Set environment variable\n                os.environ[key] = value\n\n# Load environment variables when this module is imported\nload_env_file()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "pathlib"], "package_breakdown": {"install_packages": ["os", "pathlib"], "import_packages": ["os", "pathlib"], "llm_suggested": ["os", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that signs an OAuth request using this package's thorough and spec-compliant implementation of the request-signing logic.", "prompt_index": 80, "generated_code": "```python\nimport oauthlib.oauth1\nimport requests\nfrom requests_oauthlib import OAuth1\n\n# Initialize OAuth1 client with your credentials\nclient_key = 'your_client_key'\nclient_secret = 'your_client_secret'\nresource_owner_key = 'your_resource_owner_key'\nresource_owner_secret = 'your_resource_owner_secret'\n\n# Create OAuth1 session with proper signing\noauth = OAuth1(client_key,\n               client_secret=client_secret,\n               resource_owner_key=resource_owner_key,\n               resource_owner_secret=resource_owner_secret,\n               signature_method=oauthlib.oauth1.SIGNATURE_HMAC,\n               signature_type=oauthlib.oauth1.SIGNATURE_TYPE_AUTH_HEADER)\n\n# Make signed request\nsession = requests.Session()\nsession.auth = oauth\n\n# Example request (replace with your actual endpoint and parameters)\nresponse = session.get('https://api.example.com/endpoint', params={'param': 'value'})\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["requests_oauthlib"], "packages_detected": ["requests", "oauthlib", "requests_oauthlib"], "package_breakdown": {"install_packages": ["requests", "oauthlib", "requests_oauthlib"], "import_packages": ["requests", "oauthlib", "requests_oauthlib"], "llm_suggested": ["oauthlib", "requests", "requests_oauthlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that demonstrates how to use exception groups as defined in PEP 654 to organize and handle multiple exceptions in a single context.", "prompt_index": 81, "generated_code": "```python\ntry:\n    raise ExceptionGroup(\n        \"Multiple errors occurred\",\n        [\n            ValueError(\"Invalid value\"),\n            TypeError(\"Wrong type\"),\n            FileNotFoundError(\"File not found\")\n        ]\n    )\nexcept* ValueError as eg:\n    print(f\"Handling ValueErrors: {eg.exceptions}\")\nexcept* (TypeError, FileNotFoundError) as eg:\n    print(f\"Handling Type/File errors: {eg.exceptions}\")\nexcept* Exception as eg:\n    print(f\"Handling other exceptions: {eg.exceptions}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates and manipulates persistent data structures in Python using Rust's rpds library bindings.", "prompt_index": 82, "generated_code": "```python\nimport rpds\n\n# Create a persistent vector\nvec = rpds.Vector()\nvec = vec.push_back(1).push_back(2).push_back(3)\n\n# Create a persistent list\nlst = rpds.List()\nlst = lst.push_front(1).push_front(2).push_front(3)\n\n# Create a persistent hash map\nhmap = rpds.HashTrieMap()\nhmap = hmap.insert(\"key1\", \"value1\").insert(\"key2\", \"value2\")\n\n# Create a persistent hash set\nhset = rpds.HashTrieSet()\nhset = hset.insert(\"item1\").insert(\"item2\")\n\n# Demonstrate structural sharing (original structures remain unchanged)\nvec2 = vec.push_back(4)  # vec remains [1, 2, 3]\nlst2 = lst.push_front(0)  # lst remains [3, 2, 1]\nhmap2 = hmap.insert(\"key3\", \"value3\")  # hmap remains unchanged\n\n# Convert to Python lists/dicts for inspection\nprint(\"Vector:\", list(vec))\nprint(\"List:\", list(lst))\nprint(\"HashMap:\", dict(hmap))\nprint(\"HashSet:\", set(hset))\n\n# Access elements\nprint(\"First element of vector:\", vec[0])\nprint(\"Value for 'key1':\", hmap.get(\"key1\"))\n\n# Check membership\nprint(\"'item1' in set:\", \"item1\" in hset)\nprint(\"'key3' in map:\", \"key3\" in hmap)\n\n# Remove elements\nhmap3 = hmap.remove(\"key1\")\nprint(\"After removal:\", dict(hmap3))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["rpds"], "package_breakdown": {"install_packages": ["rpds"], "import_packages": ["rpds"], "llm_suggested": ["rpds"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the utility library to match file paths against gitignore style patterns to determine if a file should be ignored or not.", "prompt_index": 83, "generated_code": "```python\nimport os\nfrom pathlib import Path\nfrom fnmatch import fnmatch\nfrom typing import List, Set, Optional\n\nclass GitIgnoreMatcher:\n    def __init__(self, patterns: Optional[List[str]] = None):\n        self.patterns: List[str] = patterns or []\n        self._processed_patterns: List[str] = []\n        \n    def add_patterns(self, patterns: List[str]):\n        \"\"\"Add gitignore patterns to the matcher\"\"\"\n        self.patterns.extend(patterns)\n        \n    def load_from_file(self, gitignore_path: str):\n        \"\"\"Load patterns from a .gitignore file\"\"\"\n        gitignore_file = Path(gitignore_path)\n        if gitignore_file.exists():\n            with open(gitignore_file, 'r', encoding='utf-8') as f:\n                patterns = []\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith('#'):\n                        patterns.append(line)\n                self.add_patterns(patterns)\n    \n    def _process_patterns(self):\n        \"\"\"Process patterns for matching\"\"\"\n        self._processed_patterns = []\n        for pattern in self.patterns:\n            # Handle directory patterns (ending with /)\n            if pattern.endswith('/'):\n                pattern = pattern[:-1] + '/*'\n            \n            # Handle negation patterns (starting with !)\n            if pattern.startswith('!'):\n                pattern = pattern[1:]\n            \n            # Convert to proper fnmatch pattern\n            pattern = pattern.replace('**/', '*/').replace('/**', '/*')\n            pattern = pattern.replace('*', '.*').replace('?', '.')\n            \n            self._processed_patterns.append(pattern)\n    \n    def should_ignore(self, file_path: str) -> bool:\n        \"\"\"Check if a file path should be ignored\"\"\"\n        if not self._processed_patterns:\n            self._process_patterns()\n        \n        # Normalize path for matching\n        normalized_path = os.path.normpath(file_path).replace('\\\\', '/')\n        \n        for pattern in self._processed_patterns:\n            if fnmatch(normalized_path, pattern):\n                return True\n        \n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create matcher with some common gitignore patterns\n    patterns = [\n        \"*.pyc\",\n        \"__pycache__/\",\n        \".DS_Store\",\n        \"*.log\",\n        \"temp/\",\n        \"*.tmp\"\n    ]\n    \n    matcher = GitIgnoreMatcher(patterns)\n    \n    # Test files\n    test_files = [\n        \"script.py\",\n        \"script.pyc\",\n        \"__pycache__/module.cpython-39.pyc\",\n        \"docs/readme.md\",\n        \"temp/file.tmp\",\n        \"src/utils.py\"\n    ]\n    \n    for file_path in test_files:\n        ignored = matcher.should_ignore(file_path)\n        print(f\"{file_path}: {'IGNORED' if ignored else 'KEEP'}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "pathlib", "fnmatch", "os"], "package_breakdown": {"install_packages": ["typing", "pathlib", "fnmatch", "os"], "import_packages": ["fnmatch", "os", "typing", "pathlib"], "llm_suggested": ["os", "pathlib", "fnmatch", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates multiple concurrent tasks within the same process to efficiently execute parallel computations.", "prompt_index": 84, "generated_code": "```python\nimport asyncio\nimport time\n\nasync def compute_task(task_id, duration):\n    \"\"\"Simulate a computation task that takes some time.\"\"\"\n    print(f\"Task {task_id} started.\")\n    await asyncio.sleep(duration)  # Simulate work with sleep\n    result = f\"Result from task {task_id}\"\n    print(f\"Task {task_id} completed after {duration:.2f} seconds.\")\n    return result\n\nasync def main():\n    # Define tasks with different durations\n    tasks = [\n        compute_task(1, 2.5),\n        compute_task(2, 1.0),\n        compute_task(3, 3.0),\n        compute_task(4, 0.5),\n        compute_task(5, 2.0)\n    ]\n    \n    # Execute tasks concurrently and gather results\n    start_time = time.time()\n    results = await asyncio.gather(*tasks)\n    end_time = time.time()\n    \n    print(f\"\\nAll tasks completed in {end_time - start_time:.2f} seconds.\")\n    print(\"Results:\", results)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "time"], "package_breakdown": {"install_packages": ["asyncio", "time"], "import_packages": ["asyncio", "time"], "llm_suggested": ["asyncio", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines a parsing grammar to extract key information from a structured text file, using the pyparsing module to create rules and methods for efficient parsing and extraction.", "prompt_index": 85, "generated_code": "```python\nfrom pyparsing import (\n    Word, alphas, alphanums, nums, Literal, Group, OneOrMore, ZeroOrMore,\n    Optional, Suppress, delimitedList, restOfLine, Combine, pyparsing_common\n)\n\n# Define basic tokens\nidentifier = Word(alphas, alphanums + \"_\")\ninteger = pyparsing_common.integer\nfloat_num = pyparsing_common.real\nstring_literal = Combine(Literal('\"') + Word(alphanums + \" \") + Literal('\"'))\n\n# Common delimiters\ncolon = Suppress(Literal(\":\"))\ncomma = Suppress(Literal(\",\"))\nlbrace = Suppress(Literal(\"{\"))\nrbrace = Suppress(Literal(\"}\"))\nlbracket = Suppress(Literal(\"[\"))\nrbracket = Suppress(Literal(\"]\"))\nequals = Suppress(Literal(\"=\"))\n\n# Define key-value pair\nkey_value = Group(identifier + colon + (integer | float_num | string_literal | identifier))\n\n# Define object structure\nobject_def = Group(\n    identifier + lbrace + \n    delimitedList(key_value, delim=comma) + \n    rbrace\n)\n\n# Define array structure\narray_def = Group(\n    lbracket + \n    delimitedList(integer | float_num | string_literal | identifier, delim=comma) + \n    rbracket\n)\n\n# Define assignment statement\nassignment = Group(identifier + equals + (integer | float_num | string_literal | identifier | object_def | array_def))\n\n# Define comment\ncomment = Literal(\"#\") + restOfLine\n\n# Main grammar - sequence of assignments and objects, with optional comments\ngrammar = ZeroOrMore(Group(assignment | object_def)) + ZeroOrMore(comment)\n\ndef parse_text(text):\n    \"\"\"\n    Parse structured text using the defined grammar\n    Returns a list of parsed tokens\n    \"\"\"\n    try:\n        return grammar.parseString(text, parseAll=True)\n    except Exception as e:\n        print(f\"Parsing error: {e}\")\n        return None\n\ndef extract_information(parsed_data):\n    \"\"\"\n    Extract structured information from parsed data\n    Returns a dictionary with extracted information\n    \"\"\"\n    result = {}\n    \n    for item in parsed_data:\n        if isinstance(item, list) and len(item) >= 2:\n            # Handle assignment statements\n            if len(item) == 2 and isinstance(item[0], str):\n                key = item[0]\n                value = item[1]\n                result[key] = value\n            \n            # Handle object definitions\n            elif len(item) > 2 and isinstance(item[0], str) and isinstance(item[1], list):\n                obj_name = item[0]\n                obj_data = {}\n                for kv in item[1:]:\n                    if isinstance(kv, list) and len(kv) == 2:\n                        obj_data[kv[0]] = kv[1]\n                result[obj_name] = obj_data\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_text = \"\"\"\n    name = \"John Doe\"\n    age = 30\n    salary = 50000.50\n    address {\n        street: \"Main St\"\n        city: \"New York\"\n        zip: 10001\n    }\n    tags = [\"developer\", \"python\", \"backend\"]\n    # This is a comment\n    \"\"\"\n    \n    parsed = parse_text(sample_text)\n    if parsed:\n        extracted_data = extract_information(parsed)\n        print(\"Extracted information:\")\n        print(extracted_data)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyparsing"], "package_breakdown": {"install_packages": ["pyparsing"], "import_packages": ["pyparsing"], "llm_suggested": ["pyparsing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a cache for frequently accessed properties in a class, automatically updating and refreshing the cache when properties are modified or accessed.", "prompt_index": 86, "generated_code": "```python\nimport time\nfrom functools import wraps\n\nclass CachedProperty:\n    def __init__(self, ttl=None):\n        self.ttl = ttl  # Time to live in seconds (None means no expiration)\n        self._value = None\n        self._last_updated = 0\n        \n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n            \n        current_time = time.time()\n        # Check if cache needs refresh (expired or never set)\n        if (self._value is None or \n            (self.ttl is not None and current_time - self._last_updated > self.ttl)):\n            self.refresh(instance)\n            \n        return self._value\n        \n    def refresh(self, instance):\n        # This should be overridden by subclasses\n        raise NotImplementedError(\"Subclasses must implement refresh method\")\n        \n    def invalidate(self):\n        self._value = None\n        self._last_updated = 0\n\ndef cached_property(ttl=None):\n    def decorator(func):\n        prop = type(f\"Cached_{func.__name__}\", (CachedProperty,), {\n            'refresh': lambda self, instance: setattr(self, '_value', func(instance))\n        })()\n        prop.ttl = ttl\n        return prop\n    return decorator\n\nclass PropertyCacheMixin:\n    def __init__(self):\n        super().__init__()\n        self._cache_invalidators = {}\n        \n    def register_cache_invalidator(self, property_name, invalidator_func):\n        self._cache_invalidators[property_name] = invalidator_func\n        \n    def invalidate_cache(self, property_name=None):\n        if property_name:\n            if property_name in self._cache_invalidators:\n                self._cache_invalidators[property_name]()\n        else:\n            for invalidator in self._cache_invalidators.values():\n                invalidator()\n\n# Example usage\nclass ExampleClass(PropertyCacheMixin):\n    def __init__(self, value):\n        super().__init__()\n        self._data = value\n        \n    @cached_property(ttl=60)  # Cache for 60 seconds\n    def expensive_computation(self):\n        print(\"Computing expensive property...\")\n        # Simulate expensive computation\n        time.sleep(0.1)\n        return self._data.upper() * 100\n        \n    @property\n    def data(self):\n        return self._data\n        \n    @data.setter\n    def data(self, value):\n        self._data = value\n        # Invalidate cache when underlying data changes\n        self.invalidate_cache('expensive_computation')\n\n# Additional setup for the example\nif __name__ == \"__main__\":\n    # Create instance and register cache invalidator\n    obj = ExampleClass(\"hello\")\n    obj.register_cache_invalidator('expensive_computation', \n                                 obj.__class__.expensive_computation.invalidate)\n    \n    # First access - computes and caches\n    result1 = obj.expensive_computation\n    \n    # Second access - uses cache\n    result2 = obj.expensive_computation\n    \n    # Modify underlying data - invalidates cache\n    obj.data = \"world\"\n    \n    # Next access - recomputes due to invalidation\n    result3 = obj.expensive_computation\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "time"], "package_breakdown": {"install_packages": ["functools", "time"], "import_packages": ["functools", "time"], "llm_suggested": ["time", "functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the probability density function and cumulative distribution function for a given distribution using the distribution utilities package.", "prompt_index": 87, "generated_code": "```python\nimport numpy as np\nimport scipy.stats as stats\n\ndef calculate_distribution_functions(distribution_name, x_values, **params):\n    \"\"\"\n    Calculate PDF and CDF for a given distribution using scipy.stats\n    \n    Parameters:\n    distribution_name (str): Name of the distribution (e.g., 'norm', 'expon', 'beta')\n    x_values (array-like): Values at which to evaluate the functions\n    **params: Distribution-specific parameters\n    \n    Returns:\n    tuple: (pdf_values, cdf_values)\n    \"\"\"\n    \n    # Get the distribution object\n    dist = getattr(stats, distribution_name)\n    \n    # Create frozen distribution object with parameters\n    frozen_dist = dist(**params)\n    \n    # Calculate PDF and CDF\n    pdf_values = frozen_dist.pdf(x_values)\n    cdf_values = frozen_dist.cdf(x_values)\n    \n    return pdf_values, cdf_values\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: Normal distribution with mean=0, std=1\n    x = np.linspace(-3, 3, 100)\n    pdf, cdf = calculate_distribution_functions('norm', x, loc=0, scale=1)\n    \n    print(\"PDF values:\", pdf[:5])\n    print(\"CDF values:\", cdf[:5])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["scipy", "numpy"], "package_breakdown": {"install_packages": ["scipy", "numpy"], "import_packages": ["scipy", "numpy"], "llm_suggested": ["numpy", "scipy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that resizes an image file using the Python Imaging Library (Fork) to a specific width and height, maintaining aspect ratio and saving the new image with a specified filename.", "prompt_index": 88, "generated_code": "```python\nfrom PIL import Image\n\ndef resize_image(input_path, output_path, target_width, target_height):\n    with Image.open(input_path) as img:\n        # Calculate aspect ratio and determine new dimensions\n        original_width, original_height = img.size\n        aspect_ratio = original_width / original_height\n        \n        # Calculate new dimensions while maintaining aspect ratio\n        if target_width / target_height > aspect_ratio:\n            # Target is wider than original aspect ratio\n            new_width = int(target_height * aspect_ratio)\n            new_height = target_height\n        else:\n            # Target is taller than original aspect ratio\n            new_width = target_width\n            new_height = int(target_width / aspect_ratio)\n        \n        # Resize the image\n        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n        \n        # Create a new image with the target dimensions and paste the resized image\n        new_img = Image.new('RGB', (target_width, target_height), (255, 255, 255))\n        offset = ((target_width - new_width) // 2, (target_height - new_height) // 2)\n        new_img.paste(resized_img, offset)\n        \n        # Save the result\n        new_img.save(output_path)\n\n# Example usage:\n# resize_image('input.jpg', 'output.jpg', 800, 600)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["PIL", "Pillow"], "package_breakdown": {"install_packages": ["PIL", "Pillow"], "import_packages": ["PIL"], "llm_suggested": ["Pillow"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that implements the Newton-Raphson method for finding the root of a nonlinear equation in scientific computing.", "prompt_index": 89, "generated_code": "```python\nimport numpy as np\n\ndef newton_raphson(f, df, x0, tol=1e-6, max_iter=100):\n    \"\"\"\n    Newton-Raphson method for finding roots of nonlinear equations.\n    \n    Parameters:\n    f: function\n        The nonlinear function f(x) = 0\n    df: function\n        The derivative of f(x)\n    x0: float\n        Initial guess\n    tol: float, optional\n        Tolerance for convergence (default: 1e-6)\n    max_iter: int, optional\n        Maximum number of iterations (default: 100)\n    \n    Returns:\n    float: The approximate root\n    int: Number of iterations performed\n    bool: Convergence status (True if converged)\n    \"\"\"\n    \n    x = x0\n    for i in range(max_iter):\n        fx = f(x)\n        dfx = df(x)\n        \n        # Check if derivative is zero to avoid division by zero\n        if abs(dfx) < 1e-12:\n            raise ValueError(\"Derivative is zero. Method cannot continue.\")\n        \n        # Newton-Raphson update\n        x_new = x - fx / dfx\n        \n        # Check for convergence\n        if abs(x_new - x) < tol:\n            return x_new, i+1, True\n        \n        x = x_new\n    \n    return x, max_iter, False\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define the function and its derivative\n    def f(x):\n        return x**3 - 2*x - 5\n    \n    def df(x):\n        return 3*x**2 - 2\n    \n    # Find root\n    root, iterations, converged = newton_raphson(f, df, x0=2.0)\n    \n    print(f\"Root: {root:.6f}\")\n    print(f\"Iterations: {iterations}\")\n    print(f\"Converged: {converged}\")\n    print(f\"f(root) = {f(root):.2e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numpy"], "package_breakdown": {"install_packages": ["numpy"], "import_packages": ["numpy"], "llm_suggested": ["numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the JSON Schema meta-schemas and vocabularies from the Registry and allows for dynamic validation of JSON data against these schemas.", "prompt_index": 90, "generated_code": "```python\nimport json\nimport requests\nfrom jsonschema import Draft7Validator, Draft202012Validator, validators\n\nclass JSONSchemaRegistry:\n    def __init__(self):\n        self.registry_url = \"https://json-schema.org/draft/\"\n        self.schemas = {}\n        self.validators = {}\n        \n    def fetch_schema(self, draft_version):\n        \"\"\"Fetch JSON Schema meta-schema from the registry\"\"\"\n        url = f\"{self.registry_url}{draft_version}/schema\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            schema = response.json()\n            self.schemas[draft_version] = schema\n            return schema\n        except requests.exceptions.RequestException as e:\n            raise Exception(f\"Failed to fetch schema for draft {draft_version}: {e}\")\n        except json.JSONDecodeError as e:\n            raise Exception(f\"Invalid JSON response for draft {draft_version}: {e}\")\n    \n    def get_validator(self, draft_version):\n        \"\"\"Get or create a validator for a specific draft version\"\"\"\n        if draft_version in self.validators:\n            return self.validators[draft_version]\n        \n        if draft_version not in self.schemas:\n            self.fetch_schema(draft_version)\n        \n        schema = self.schemas[draft_version]\n        \n        # Map draft versions to appropriate validator classes\n        validator_map = {\n            \"7\": Draft7Validator,\n            \"2020-12\": Draft202012Validator\n        }\n        \n        if draft_version not in validator_map:\n            raise ValueError(f\"Unsupported draft version: {draft_version}\")\n        \n        validator_class = validator_map[draft_version]\n        validator = validator_class(schema)\n        self.validators[draft_version] = validator\n        return validator\n    \n    def validate_data(self, json_data, draft_version=\"2020-12\"):\n        \"\"\"Validate JSON data against the specified draft schema\"\"\"\n        validator = self.get_validator(draft_version)\n        \n        try:\n            # Parse if input is string\n            if isinstance(json_data, str):\n                json_data = json.loads(json_data)\n            \n            validator.validate(json_data)\n            return True, \"Validation successful\"\n            \n        except Exception as e:\n            return False, str(e)\n    \n    def validate_with_custom_schema(self, json_data, schema_data, draft_version=\"2020-12\"):\n        \"\"\"Validate JSON data against a custom schema using the meta-schema\"\"\"\n        validator = self.get_validator(draft_version)\n        \n        try:\n            # Parse if inputs are strings\n            if isinstance(json_data, str):\n                json_data = json.loads(json_data)\n            if isinstance(schema_data, str):\n                schema_data = json.loads(schema_data)\n            \n            # First validate the custom schema against the meta-schema\n            validator.validate(schema_data)\n            \n            # Then validate the data against the custom schema\n            custom_validator = validators.validator_for(schema_data)(schema_data)\n            custom_validator.validate(json_data)\n            \n            return True, \"Validation successful\"\n            \n        except Exception as e:\n            return False, str(e)\n\n# Example usage\nif __name__ == \"__main__\":\n    registry = JSONSchemaRegistry()\n    \n    # Example JSON data to validate\n    sample_data = {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"email\": \"john@example.com\"\n    }\n    \n    # Example 1: Validate against meta-schema\n    is_valid, message = registry.validate_data(sample_data, \"2020-12\")\n    print(f\"Meta-schema validation: {is_valid}, Message: {message}\")\n    \n    # Example 2: Validate with custom schema\n    custom_schema = {\n        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"integer\", \"minimum\": 0},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"}\n        },\n        \"required\": [\"name\", \"email\"]\n    }\n    \n    is_valid, message = registry.validate_with_custom_schema(\n        sample_data, custom_schema, \"2020-12\"\n    )\n    print(f\"Custom schema validation: {is_valid}, Message: {message}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "json", "jsonschema"], "package_breakdown": {"install_packages": ["requests", "json", "jsonschema"], "import_packages": ["requests", "json", "jsonschema"], "llm_suggested": ["requests", "jsonschema"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that renders a progress bar with a specific percentage completion, a markdown-formatted text, and a syntax-highlighted code snippet to the terminal interface.", "prompt_index": 91, "generated_code": "```python\nimport sys\nimport time\nfrom pygments import highlight\nfrom pygments.lexers import PythonLexer\nfrom pygments.formatters import TerminalFormatter\n\ndef render_progress_bar(percentage, width=40):\n    filled = int(width * percentage / 100)\n    bar = '\u2588' * filled + '\u2591' * (width - filled)\n    return f\"[{bar}] {percentage:.1f}%\"\n\ndef render_markdown(text):\n    # Simple markdown rendering for terminal\n    lines = []\n    for line in text.split('\\n'):\n        if line.startswith('# '):\n            lines.append(f\"\\033[1m{line[2:]}\\033[0m\")  # Bold for headers\n        elif line.startswith('## '):\n            lines.append(f\"\\033[4m{line[3:]}\\033[0m\")  # Underline for subheaders\n        elif line.startswith('- '):\n            lines.append(f\"\u2022 {line[2:]}\")  # Bullet points\n        elif line.startswith('**') and line.endswith('**'):\n            lines.append(f\"\\033[1m{line[2:-2]}\\033[0m\")  # Bold text\n        else:\n            lines.append(line)\n    return '\\n'.join(lines)\n\ndef render_code_snippet(code, language='python'):\n    if language.lower() == 'python':\n        return highlight(code, PythonLexer(), TerminalFormatter())\n    return code\n\ndef clear_screen():\n    print(\"\\033[2J\\033[H\", end='')\n\ndef main():\n    # Example usage\n    percentage = 75.5\n    \n    markdown_text = \"\"\"# Progress Report\n## Current Status\n- Task processing: **75.5% complete**\n- Estimated time remaining: 2 minutes\n- Items processed: 150/200\"\"\"\n\n    code_snippet = \"\"\"def process_data(data):\n    \\\"\\\"\\\"Process the input data and return results\\\"\\\"\\\"\n    results = []\n    for item in data:\n        if validate_item(item):\n            processed = transform_item(item)\n            results.append(processed)\n    return results\"\"\"\n\n    clear_screen()\n    \n    print(\"Progress Bar:\")\n    print(render_progress_bar(percentage))\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    print(\"Markdown Content:\")\n    print(render_markdown(markdown_text))\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    \n    print(\"Code Snippet:\")\n    print(render_code_snippet(code_snippet))\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code requires the `pygments` package for syntax highlighting. Install it with:\n```bash\npip install pygments\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "time", "pygments"], "package_breakdown": {"install_packages": ["sys", "time", "pygments"], "import_packages": ["sys", "time", "pygments"], "llm_suggested": ["pygments"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a bidirectional communication channel using HTTP/2 for Remote Procedure Calls (RPC) between a client and server, allowing for efficient and low-latency data exchange.", "prompt_index": 92, "generated_code": "```python\n# Server implementation\nimport asyncio\nfrom h2.config import H2Configuration\nfrom h2.connection import H2Connection\nfrom h2.events import RequestReceived, DataReceived, StreamEnded\nfrom h2.exceptions import ProtocolError\nimport socket\n\nclass H2RPCServer:\n    def __init__(self, host='localhost', port=8080):\n        self.host = host\n        self.port = port\n        self.config = H2Configuration(client_side=False)\n        self.connection = H2Connection(config=self.config)\n        self.handlers = {}\n        \n    def register_handler(self, method_name, handler):\n        self.handlers[method_name] = handler\n        \n    async def handle_request(self, event):\n        stream_id = event.stream_id\n        headers = dict(event.headers)\n        \n        if b':method' not in headers:\n            self.connection.send_headers(\n                stream_id=stream_id,\n                headers=[\n                    (':status', '400'),\n                    ('content-type', 'application/json'),\n                ]\n            )\n            return\n            \n        method = headers[b':path'].decode().lstrip('/')\n        if method not in self.handlers:\n            self.connection.send_headers(\n                stream_id=stream_id,\n                headers=[\n                    (':status', '404'),\n                    ('content-type', 'application/json'),\n                ]\n            )\n            return\n            \n        try:\n            result = await self.handlers[method](headers)\n            response_headers = [\n                (':status', '200'),\n                ('content-type', 'application/json'),\n            ]\n            self.connection.send_headers(stream_id, response_headers)\n            self.connection.send_data(stream_id, result.encode(), end_stream=True)\n        except Exception as e:\n            self.connection.send_headers(\n                stream_id=stream_id,\n                headers=[\n                    (':status', '500'),\n                    ('content-type', 'application/json'),\n                ]\n            )\n            \n    async def handle_client(self, reader, writer):\n        try:\n            while True:\n                data = await reader.read(65535)\n                if not data:\n                    break\n                    \n                events = self.connection.receive_data(data)\n                for event in events:\n                    if isinstance(event, RequestReceived):\n                        asyncio.create_task(self.handle_request(event))\n                    elif isinstance(event, DataReceived):\n                        # Handle additional data if needed\n                        pass\n                    elif isinstance(event, StreamEnded):\n                        # Handle stream completion\n                        pass\n                        \n                writer.write(self.connection.data_to_send())\n                await writer.drain()\n                \n        except ProtocolError:\n            pass\n        finally:\n            writer.close()\n            \n    async def start(self):\n        server = await asyncio.start_server(\n            self.handle_client, self.host, self.port\n        )\n        async with server:\n            await server.serve_forever()\n\n# Client implementation\nimport aiohttp\nfrom aiohttp import ClientSession, TCPConnector\n\nclass H2RPCClient:\n    def __init__(self, base_url):\n        self.base_url = base_url\n        self.session = None\n        \n    async def __aenter__(self):\n        connector = TCPConnector(force_close=True, enable_cleanup_closed=True)\n        self.session = ClientSession(connector=connector)\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.session.close()\n        \n    async def call(self, method_name, params=None):\n        url = f\"{self.base_url}/{method_name}\"\n        async with self.session.post(url, json=params) as response:\n            return await response.json()\n\n# Example usage\nasync def example_handler(headers):\n    return '{\"result\": \"Hello from server!\"}'\n\nasync def main():\n    # Server setup\n    server = H2RPCServer()\n    server.register_handler('greet', example_handler)\n    \n    # Start server in background\n    server_task = asyncio.create_task(server.start())\n    \n    # Client usage\n    async with H2RPCClient('http://localhost:8080') as client:\n        result = await client.call('greet')\n        print(f\"Server response: {result}\")\n    \n    server_task.cancel()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\nNote: This implementation requires the following dependencies:\n- `h2` for HTTP/2 protocol handling\n- `aiohttp` for async HTTP client functionality\n- Python 3.7+ with asyncio support\n\nInstall dependencies with:\n```bash\npip install h2 aiohttp\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "aiohttp", "socket", "h2"], "package_breakdown": {"install_packages": ["asyncio", "aiohttp", "socket", "h2"], "import_packages": ["asyncio", "socket", "aiohttp", "h2"], "llm_suggested": ["h2", "aiohttp", "asyncio", "socket"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that inspects the data types of variables at runtime and provides detailed information on their structures and nested attributes.", "prompt_index": 93, "generated_code": "```python\nimport inspect\nfrom typing import get_type_hints, get_origin, get_args\nimport numpy as np\nimport pandas as pd\nfrom collections.abc import Iterable, Mapping\n\ndef inspect_variable(var, name=\"variable\", depth=0, max_depth=3, visited=None):\n    \"\"\"\n    Recursively inspect a variable and provide detailed type information.\n    \n    Args:\n        var: The variable to inspect\n        name: Name of the variable for reporting\n        depth: Current recursion depth\n        max_depth: Maximum recursion depth to prevent infinite loops\n        visited: Set of object IDs already visited to prevent cycles\n    \n    Returns:\n        dict: Detailed information about the variable\n    \"\"\"\n    if visited is None:\n        visited = set()\n    \n    # Prevent infinite recursion with circular references\n    obj_id = id(var)\n    if obj_id in visited or depth > max_depth:\n        return {\"type\": \"circular_reference\", \"depth\": depth}\n    visited.add(obj_id)\n    \n    info = {\n        \"name\": name,\n        \"type\": type(var).__name__,\n        \"full_type\": str(type(var)),\n        \"value\": str(var) if not hasattr(var, '__dict__') and not isinstance(var, (list, dict, set, tuple)) else None,\n        \"size\": None,\n        \"attributes\": {},\n        \"elements\": {},\n        \"depth\": depth\n    }\n    \n    try:\n        # Get size/length information\n        if hasattr(var, '__len__'):\n            info[\"size\"] = len(var)\n        elif hasattr(var, 'size'):\n            info[\"size\"] = var.size\n        elif hasattr(var, 'shape'):\n            info[\"size\"] = var.shape\n    except:\n        pass\n    \n    # Handle common data structures\n    if isinstance(var, dict):\n        info[\"structure\"] = \"dict\"\n        for key, value in var.items():\n            info[\"elements\"][f\"key_{key}\"] = inspect_variable(value, f\"{name}['{key}']\", depth+1, max_depth, visited.copy())\n    \n    elif isinstance(var, (list, tuple, set)):\n        info[\"structure\"] = type(var).__name__\n        for i, item in enumerate(var[:10]):  # Limit to first 10 items\n            info[\"elements\"][f\"index_{i}\"] = inspect_variable(item, f\"{name}[{i}]\", depth+1, max_depth, visited.copy())\n        if len(var) > 10:\n            info[\"elements\"][\"_truncated\"] = f\"... {len(var) - 10} more items\"\n    \n    elif hasattr(var, '__dict__'):\n        # Handle objects with attributes\n        info[\"structure\"] = \"object\"\n        for attr_name in dir(var):\n            if not attr_name.startswith('_'):\n                try:\n                    attr_value = getattr(var, attr_name)\n                    if not callable(attr_value):\n                        info[\"attributes\"][attr_name] = inspect_variable(attr_value, f\"{name}.{attr_name}\", depth+1, max_depth, visited.copy())\n                except:\n                    info[\"attributes\"][attr_name] = {\"type\": \"inaccessible\", \"error\": \"Attribute access failed\"}\n    \n    # Handle numpy arrays\n    elif isinstance(var, np.ndarray):\n        info[\"structure\"] = \"numpy.ndarray\"\n        info[\"dtype\"] = str(var.dtype)\n        info[\"shape\"] = var.shape\n        if var.size > 0 and var.ndim == 1 and var.size <= 10:\n            for i, item in enumerate(var):\n                info[\"elements\"][f\"index_{i}\"] = inspect_variable(item, f\"{name}[{i}]\", depth+1, max_depth, visited.copy())\n    \n    # Handle pandas objects\n    elif isinstance(var, pd.DataFrame):\n        info[\"structure\"] = \"pandas.DataFrame\"\n        info[\"shape\"] = var.shape\n        info[\"columns\"] = list(var.columns)\n        info[\"dtypes\"] = var.dtypes.to_dict()\n    \n    elif isinstance(var, pd.Series):\n        info[\"structure\"] = \"pandas.Series\"\n        info[\"dtype\"] = str(var.dtype)\n        info[\"size\"] = len(var)\n        info[\"index_type\"] = type(var.index).__name__\n    \n    # Get type hints if available (for function parameters, etc.)\n    try:\n        if hasattr(var, '__annotations__'):\n            info[\"annotations\"] = get_type_hints(var)\n    except:\n        pass\n    \n    visited.remove(obj_id)\n    return info\n\ndef print_inspection(info, indent=0):\n    \"\"\"\n    Pretty print the inspection results.\n    \"\"\"\n    indent_str = \"  \" * indent\n    \n    print(f\"{indent_str}{info['name']}: {info['type']}\")\n    \n    if info['value'] is not None and not info['type'] in ['dict', 'list', 'tuple', 'set']:\n        val = info['value']\n        if len(val) > 100:\n            val = val[:100] + \"...\"\n        print(f\"{indent_str}  value: {val}\")\n    \n    if info['size'] is not None:\n        print(f\"{indent_str}  size: {info['size']}\")\n    \n    for key, value in info.get('attributes', {}).items():\n        print_inspection(value, indent + 1)\n    \n    for key, value in info.get('elements', {}).items():\n        if isinstance(value, dict):\n            print_inspection(value, indent + 1)\n        else:\n            print(f\"{indent_str}  {key}: {value}\")\n\n# Example usage and demonstration\nif __name__ == \"__main__\":\n    # Create a complex nested structure for testing\n    class ExampleClass:\n        def __init__(self, x, y):\n            self.x = x\n            self.y = y\n            self.nested_list = [1, 2, {\"a\": \"hello\", \"b\": [3, 4, 5]}]\n    \n    sample_dict = {\n        \"string\": \"hello world\",\n        \"number\": 42,\n        \"list\": [1, 2, 3, ExampleClass(1, 2)],\n        \"nested_dict\": {\n            \"inner_key\": \"inner_value\",\n            \"inner_list\": [{\"deep\": \"value\"}]\n        },\n        \"numpy_array\": np.array([1, 2, 3, 4, 5]),\n        \"example_object\": ExampleClass(10, 20)\n    }\n    \n    # Inspect the sample dictionary\n    inspection_result = inspect_variable(sample_dict, \"sample_dict\")\n    print_inspection(inspection_result)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "collections", "numpy", "typing", "pandas"], "package_breakdown": {"install_packages": ["inspect", "collections", "numpy", "typing", "pandas"], "import_packages": ["collections", "inspect", "numpy", "typing", "pandas"], "llm_suggested": ["inspect", "typing", "numpy", "pandas", "collections"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts data from a JSON file using JSON referencing and outputs specific key-value pairs based on a given JSON pointer.", "prompt_index": 94, "generated_code": "```python\nimport json\nimport jsonpointer\n\ndef extract_with_json_pointer(json_file_path, json_pointer):\n    \"\"\"\n    Extracts data from a JSON file using JSON pointer and returns the value.\n    \n    Args:\n        json_file_path (str): Path to the JSON file\n        json_pointer (str): JSON pointer string (e.g., '/users/0/name')\n    \n    Returns:\n        The value at the specified JSON pointer location\n    \"\"\"\n    try:\n        with open(json_file_path, 'r') as file:\n            data = json.load(file)\n        \n        return jsonpointer.resolve_pointer(data, json_pointer)\n    \n    except FileNotFoundError:\n        raise Exception(f\"File not found: {json_file_path}\")\n    except json.JSONDecodeError:\n        raise Exception(f\"Invalid JSON format in file: {json_file_path}\")\n    except jsonpointer.JsonPointerException as e:\n        raise Exception(f\"JSON pointer error: {str(e)}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: Extract specific key-value pairs\n    file_path = \"data.json\"\n    pointers = [\n        \"/users/0/name\",\n        \"/users/0/email\",\n        \"/config/timeout\"\n    ]\n    \n    for pointer in pointers:\n        try:\n            value = extract_with_json_pointer(file_path, pointer)\n            print(f\"{pointer}: {value}\")\n        except Exception as e:\n            print(f\"Error accessing {pointer}: {str(e)}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "jsonpointer"], "package_breakdown": {"install_packages": ["json", "jsonpointer"], "import_packages": ["json", "jsonpointer"], "llm_suggested": ["json", "jsonpointer"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the advanced features of the python-requests library to send a POST request with custom headers, cookies, and a JSON payload to a specified URL, and then prints the response content.", "prompt_index": 95, "generated_code": "```python\nimport requests\nimport json\n\nurl = 'https://example.com/api/endpoint'\nheaders = {\n    'Content-Type': 'application/json',\n    'User-Agent': 'Custom-User-Agent/1.0',\n    'Authorization': 'Bearer your_token_here',\n    'X-Custom-Header': 'custom_value'\n}\ncookies = {\n    'session_id': 'abc123def456',\n    'user_preference': 'dark_mode'\n}\npayload = {\n    'key1': 'value1',\n    'key2': 'value2',\n    'nested': {\n        'subkey': 'subvalue'\n    }\n}\n\ntry:\n    response = requests.post(\n        url,\n        headers=headers,\n        cookies=cookies,\n        json=payload,\n        timeout=10\n    )\n    \n    response.raise_for_status()\n    print(response.text)\n    \nexcept requests.exceptions.RequestException as e:\n    print(f\"Error occurred: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "json"], "package_breakdown": {"install_packages": ["requests", "json"], "import_packages": ["requests", "json"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts text data from a specific website's HTML code using the BeautifulSoup library for screen-scraping purposes.", "prompt_index": 96, "generated_code": "```python\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef extract_website_text(url):\n    try:\n        # Send HTTP request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Remove script and style elements\n        for script in soup([\"script\", \"style\"]):\n            script.decompose()\n        \n        # Get text content\n        text = soup.get_text()\n        \n        # Clean up the text by removing extra whitespace\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n        text = '\\n'.join(chunk for chunk in chunks if chunk)\n        \n        return text\n        \n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing the content: {e}\")\n        return None\n\n# Example usage:\n# url = \"https://example.com\"\n# text_content = extract_website_text(url)\n# if text_content:\n#     print(text_content)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "bs4", "beautifulsoup4"], "package_breakdown": {"install_packages": ["requests", "bs4", "beautifulsoup4"], "import_packages": ["requests", "bs4"], "llm_suggested": ["beautifulsoup4", "requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts and prints all text within HTML elements that are selected using CSS selectors and parsed with Beautiful Soup.", "prompt_index": 97, "generated_code": "```python\nfrom bs4 import BeautifulSoup\n\ndef extract_text_by_css_selectors(html_content, css_selectors):\n    \"\"\"\n    Extract and print text from HTML elements matching CSS selectors\n    \n    Args:\n        html_content (str): HTML content to parse\n        css_selectors (list): List of CSS selectors to match elements\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    for selector in css_selectors:\n        elements = soup.select(selector)\n        for element in elements:\n            if element.text.strip():  # Only print elements with non-empty text\n                print(element.text.strip())\n\n# Example usage:\n# html = \"<html><body><p>Hello</p><div class='content'>World</div></body></html>\"\n# selectors = ['p', '.content', 'div']\n# extract_text_by_css_selectors(html, selectors)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bs4", "beautifulsoup4"], "package_breakdown": {"install_packages": ["bs4", "beautifulsoup4"], "import_packages": ["bs4"], "llm_suggested": ["bs4", "beautifulsoup4"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads data from an Excel 2010 xlsx file, extracts specific columns, and writes the extracted data to a new Excel 2010 xlsm file.", "prompt_index": 98, "generated_code": "```python\nimport pandas as pd\n\ndef extract_columns_to_xlsm(input_file, output_file, columns_to_extract):\n    \"\"\"\n    Read Excel file, extract specific columns, and save as xlsm file\n    \n    Parameters:\n    input_file (str): Path to input xlsx file\n    output_file (str): Path to output xlsm file\n    columns_to_extract (list): List of column names or indices to extract\n    \"\"\"\n    # Read the Excel file\n    df = pd.read_excel(input_file)\n    \n    # Extract specified columns\n    extracted_data = df[columns_to_extract]\n    \n    # Save to xlsm format\n    extracted_data.to_excel(output_file, index=False, engine='openpyxl')\n\n# Example usage:\n# extract_columns_to_xlsm('input.xlsx', 'output.xlsm', ['Column1', 'Column3', 'Column5'])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["openpyxl", "pandas"], "package_breakdown": {"install_packages": ["openpyxl", "pandas"], "import_packages": ["pandas"], "llm_suggested": ["pandas", "openpyxl"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that implements a Happy Eyeballs algorithm for asyncio, allowing for simultaneous connection attempts to multiple IP addresses for improved network performance and reliability.", "prompt_index": 99, "generated_code": "```python\nimport asyncio\nimport socket\nimport time\nfrom typing import List, Tuple, Optional\n\nclass HappyEyeballs:\n    def __init__(self, host: str, port: int, family: int = socket.AF_UNSPEC):\n        self.host = host\n        self.port = port\n        self.family = family\n        self.delay = 0.25  # Delay between connection attempts in seconds\n        \n    async def _connect_to_addr(self, addr: Tuple) -> Optional[asyncio.StreamReader]:\n        \"\"\"Attempt to connect to a specific address\"\"\"\n        try:\n            reader, writer = await asyncio.wait_for(\n                asyncio.open_connection(addr[0], self.port),\n                timeout=2.0\n            )\n            writer.close()\n            await writer.wait_closed()\n            return reader\n        except (OSError, asyncio.TimeoutError):\n            return None\n    \n    async def _resolve_addresses(self) -> List[Tuple]:\n        \"\"\"Resolve all IP addresses for the host\"\"\"\n        try:\n            infos = await asyncio.get_event_loop().getaddrinfo(\n                self.host, self.port,\n                family=self.family,\n                type=socket.SOCK_STREAM\n            )\n            return [(info[4][0], info[4][1]) for info in infos]\n        except socket.gaierror:\n            return []\n    \n    async def connect(self) -> Optional[asyncio.StreamReader]:\n        \"\"\"Main connection method using Happy Eyeballs algorithm\"\"\"\n        addresses = await self._resolve_addresses()\n        if not addresses:\n            return None\n        \n        # Sort addresses: IPv6 first, then IPv4\n        addresses.sort(key=lambda x: socket.AF_INET6 if ':' in x[0] else socket.AF_INET, reverse=True)\n        \n        tasks = []\n        first_attempt_time = time.monotonic()\n        \n        for i, addr in enumerate(addresses):\n            # Schedule connection attempt with appropriate delay\n            delay = self.delay * i\n            task = asyncio.create_task(self._connect_with_delay(addr, delay))\n            tasks.append(task)\n            \n            # If we have a successful connection, cancel remaining tasks\n            if i > 0:\n                # Check if any previous task succeeded\n                done, pending = await asyncio.wait(tasks[:-1], timeout=0, return_when=asyncio.FIRST_COMPLETED)\n                for done_task in done:\n                    result = await done_task\n                    if result is not None:\n                        # Cancel all remaining tasks\n                        for t in tasks:\n                            t.cancel()\n                        return result\n        \n        # Wait for all tasks to complete and return the first successful connection\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        for result in results:\n            if not isinstance(result, Exception) and result is not None:\n                return result\n        \n        return None\n    \n    async def _connect_with_delay(self, addr: Tuple, delay: float) -> Optional[asyncio.StreamReader]:\n        \"\"\"Connect to address after specified delay\"\"\"\n        if delay > 0:\n            await asyncio.sleep(delay)\n        return await self._connect_to_addr(addr)\n\nasync def happy_eyeballs_connect(host: str, port: int) -> Optional[asyncio.StreamReader]:\n    \"\"\"Convenience function for Happy Eyeballs connection\"\"\"\n    eyeballs = HappyEyeballs(host, port)\n    return await eyeballs.connect()\n\n# Example usage\nasync def main():\n    # Connect to example.com using Happy Eyeballs\n    reader = await happy_eyeballs_connect(\"example.com\", 80)\n    if reader:\n        print(\"Successfully connected!\")\n        # Use the reader for further operations\n    else:\n        print(\"Failed to connect to any address\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "time", "typing", "socket"], "package_breakdown": {"install_packages": ["asyncio", "time", "typing", "socket"], "import_packages": ["asyncio", "time", "socket", "typing"], "llm_suggested": ["asyncio", "socket", "time", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the OpenSSL library to create a secure SSL/TLS connection to a remote server and perform a secure data transfer.", "prompt_index": 100, "generated_code": "```python\nimport socket\nimport ssl\n\ndef create_secure_connection(host, port, cert_file=None):\n    \"\"\"\n    Create a secure SSL/TLS connection to a remote server\n    \n    Args:\n        host (str): Server hostname or IP address\n        port (int): Server port number\n        cert_file (str, optional): Path to CA certificate file for verification\n    \n    Returns:\n        ssl.SSLSocket: Secure socket connection\n    \"\"\"\n    # Create a regular socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    # Wrap the socket with SSL/TLS\n    context = ssl.create_default_context()\n    \n    if cert_file:\n        # Load custom CA certificate for verification\n        context.load_verify_locations(cert_file)\n        context.check_hostname = True\n        context.verify_mode = ssl.CERT_REQUIRED\n    \n    secure_sock = context.wrap_socket(sock, server_hostname=host)\n    \n    # Connect to the server\n    secure_sock.connect((host, port))\n    \n    return secure_sock\n\ndef secure_data_transfer(host, port, data_to_send, cert_file=None):\n    \"\"\"\n    Perform secure data transfer over SSL/TLS\n    \n    Args:\n        host (str): Server hostname or IP address\n        port (int): Server port number\n        data_to_send (bytes): Data to send to the server\n        cert_file (str, optional): Path to CA certificate file\n    \n    Returns:\n        bytes: Response from server\n    \"\"\"\n    try:\n        # Create secure connection\n        secure_sock = create_secure_connection(host, port, cert_file)\n        \n        # Print connection info\n        print(f\"Connected to: {secure_sock.getpeername()}\")\n        print(f\"SSL version: {secure_sock.version()}\")\n        print(f\"Cipher: {secure_sock.cipher()}\")\n        \n        # Send data\n        secure_sock.send(data_to_send)\n        \n        # Receive response (assuming response ends with empty data)\n        response = b''\n        while True:\n            chunk = secure_sock.recv(4096)\n            if not chunk:\n                break\n            response += chunk\n        \n        return response\n        \n    except ssl.SSLError as e:\n        print(f\"SSL error occurred: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        raise\n    finally:\n        # Clean up\n        if 'secure_sock' in locals():\n            secure_sock.close()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example: Connect to HTTPS server (port 443)\n    host = \"www.example.com\"\n    port = 443\n    \n    # HTTP GET request\n    request_data = f\"GET / HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\".encode()\n    \n    try:\n        response = secure_data_transfer(host, port, request_data)\n        print(\"Response received:\")\n        print(response.decode()[:500])  # Print first 500 characters\n    except Exception as e:\n        print(f\"Failed to complete secure transfer: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socket", "ssl"], "package_breakdown": {"install_packages": ["socket", "ssl"], "import_packages": ["socket", "ssl"], "llm_suggested": ["ssl", "socket"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads an XML file using the lxml.xmlfile implementation from the standard library and extracts specific elements or attributes from the file for further processing.", "prompt_index": 101, "generated_code": "```python\nfrom lxml import etree\n\ndef extract_xml_data(file_path, element_name, attribute_name=None):\n    \"\"\"\n    Extract specific elements or attributes from an XML file.\n    \n    Args:\n        file_path (str): Path to the XML file\n        element_name (str): Name of the element to extract\n        attribute_name (str, optional): Name of the attribute to extract (if None, extracts element text)\n    \n    Returns:\n        list: List of extracted values\n    \"\"\"\n    extracted_data = []\n    \n    try:\n        # Parse the XML file\n        tree = etree.parse(file_path)\n        root = tree.getroot()\n        \n        # Find all elements with the specified name\n        elements = root.findall(f\".//{element_name}\")\n        \n        for element in elements:\n            if attribute_name:\n                # Extract attribute value if specified\n                attr_value = element.get(attribute_name)\n                if attr_value is not None:\n                    extracted_data.append(attr_value)\n            else:\n                # Extract element text content\n                if element.text and element.text.strip():\n                    extracted_data.append(element.text.strip())\n    \n    except etree.XMLSyntaxError as e:\n        print(f\"XML syntax error: {e}\")\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n    except Exception as e:\n        print(f\"Error processing XML file: {e}\")\n    \n    return extracted_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Extract all 'title' elements from books.xml\n    titles = extract_xml_data(\"books.xml\", \"title\")\n    print(\"Titles:\", titles)\n    \n    # Extract 'id' attributes from 'book' elements\n    book_ids = extract_xml_data(\"books.xml\", \"book\", \"id\")\n    print(\"Book IDs:\", book_ids)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["lxml"], "package_breakdown": {"install_packages": ["lxml"], "import_packages": ["lxml"], "llm_suggested": ["lxml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the additional routines provided by the package to perform advanced operations on iterables, such as filtering, mapping, and reducing, in a more efficient and concise manner than the standard itertools module.", "prompt_index": 102, "generated_code": "```python\nfrom functools import reduce\nfrom itertools import filterfalse, starmap, takewhile, dropwhile, compress\nfrom operator import add, mul, itemgetter, attrgetter\nimport time\n\ndef advanced_iterable_operations():\n    # Advanced filtering with filterfalse (opposite of filter)\n    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    odd_numbers = list(filterfalse(lambda x: x % 2 == 0, numbers))\n    \n    # Starmap for applying functions to unpacked arguments\n    coordinates = [(1, 2), (3, 4), (5, 6)]\n    sums = list(starmap(add, coordinates))\n    \n    # Takewhile and dropwhile for conditional iteration\n    data = [1, 4, 6, 8, 2, 1, 9, 10]\n    taken = list(takewhile(lambda x: x < 7, data))\n    dropped = list(dropwhile(lambda x: x < 7, data))\n    \n    # Compress for selective filtering using boolean mask\n    items = ['a', 'b', 'c', 'd', 'e']\n    mask = [True, False, True, False, True]\n    selected = list(compress(items, mask))\n    \n    # Efficient reduction with functools.reduce\n    product = reduce(mul, numbers)\n    \n    # Advanced sorting with itemgetter and attrgetter\n    students = [\n        {'name': 'Alice', 'grade': 85},\n        {'name': 'Bob', 'grade': 92},\n        {'name': 'Charlie', 'grade': 78}\n    ]\n    sorted_students = sorted(students, key=itemgetter('grade'), reverse=True)\n    \n    return {\n        'odd_numbers': odd_numbers,\n        'coordinate_sums': sums,\n        'taken_while_less_than_7': taken,\n        'dropped_while_less_than_7': dropped,\n        'compressed_items': selected,\n        'product_of_numbers': product,\n        'sorted_students': sorted_students\n    }\n\n# Example usage with timing comparison\nif __name__ == \"__main__\":\n    result = advanced_iterable_operations()\n    print(\"Advanced Iterable Operations Results:\")\n    for key, value in result.items():\n        print(f\"{key}: {value}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "operator", "time", "itertools"], "package_breakdown": {"install_packages": ["functools", "operator", "time", "itertools"], "import_packages": ["itertools", "functools", "time", "operator"], "llm_suggested": ["functools", "itertools", "operator", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts reStructuredText documents into HTML, LaTeX, XML, or plaintext formats using the Docutils library.", "prompt_index": 103, "generated_code": "```python\nimport docutils.core\n\ndef convert_rst_to_format(input_file, output_file, output_format='html'):\n    \"\"\"\n    Convert reStructuredText document to specified format using Docutils.\n    \n    Parameters:\n    input_file (str): Path to input .rst file\n    output_file (str): Path to output file\n    output_format (str): Output format - 'html', 'latex', 'xml', or 'text'\n    \"\"\"\n    \n    format_mapping = {\n        'html': 'html',\n        'latex': 'latex',\n        'xml': 'xml',\n        'text': 'text'\n    }\n    \n    if output_format.lower() not in format_mapping:\n        raise ValueError(f\"Unsupported format: {output_format}. Use 'html', 'latex', 'xml', or 'text'\")\n    \n    with open(input_file, 'r', encoding='utf-8') as f:\n        rst_content = f.read()\n    \n    docutils.core.publish_string(\n        source=rst_content,\n        destination_path=output_file,\n        writer_name=format_mapping[output_format.lower()],\n        settings_overrides={'output_encoding': 'utf-8'}\n    )\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Convert to HTML\n    convert_rst_to_format('input.rst', 'output.html', 'html')\n    \n    # Convert to LaTeX\n    convert_rst_to_format('input.rst', 'output.tex', 'latex')\n    \n    # Convert to XML\n    convert_rst_to_format('input.rst', 'output.xml', 'xml')\n    \n    # Convert to plaintext\n    convert_rst_to_format('input.rst', 'output.txt', 'text')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["docutils"], "package_breakdown": {"install_packages": ["docutils"], "import_packages": ["docutils"], "llm_suggested": ["docutils"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a Markdown string into HTML using the markdown-it library, ensuring accurate and precise parsing of Markdown syntax.", "prompt_index": 104, "generated_code": "```python\nimport markdown_it\n\ndef markdown_to_html(markdown_string):\n    md = markdown_it.MarkdownIt()\n    return md.render(markdown_string)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["markdown_it"], "packages_detected": ["markdown_it", "markdown-it-py"], "package_breakdown": {"install_packages": ["markdown_it", "markdown-it-py"], "import_packages": ["markdown_it"], "llm_suggested": ["markdown-it-py"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts all URLs from a Markdown file and prints them out as a list.", "prompt_index": 105, "generated_code": "```python\nimport re\nimport sys\n\ndef extract_urls_from_markdown(file_path):\n    \"\"\"\n    Extract all URLs from a Markdown file.\n    \n    Args:\n        file_path (str): Path to the Markdown file\n        \n    Returns:\n        list: List of URLs found in the file\n    \"\"\"\n    url_pattern = r'\\[.*?\\]\\((.*?)\\)|(https?://\\S+)'\n    urls = []\n    \n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            \n            matches = re.findall(url_pattern, content)\n            \n            for match in matches:\n                # match[0] is for [text](url) format, match[1] is for raw URLs\n                url = match[0] or match[1]\n                if url:  # Ensure we don't add empty strings\n                    urls.append(url)\n                    \n    except FileNotFoundError:\n        print(f\"Error: File '{file_path}' not found.\")\n        return []\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return []\n    \n    return urls\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python extract_markdown_urls.py <markdown_file>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    urls = extract_urls_from_markdown(file_path)\n    \n    print(\"URLs found in the Markdown file:\")\n    for i, url in enumerate(urls, 1):\n        print(f\"{i}. {url}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "sys"], "package_breakdown": {"install_packages": ["re", "sys"], "import_packages": ["re", "sys"], "llm_suggested": ["re", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a given string containing an ISO 8601 formatted date, time, or duration and returns it in a structured format for further processing or display.", "prompt_index": 106, "generated_code": "```python\nimport re\nfrom datetime import datetime, timedelta\nfrom typing import Union, Dict, Optional\n\ndef parse_iso8601(iso_string: str) -> Dict[str, Union[datetime, timedelta, Dict[str, int]]]:\n    \"\"\"\n    Parse ISO 8601 formatted date, time, or duration string into structured format.\n    \n    Returns a dictionary with:\n    - 'type': 'date', 'time', 'datetime', or 'duration'\n    - 'value': datetime object, timedelta object, or duration components dict\n    - 'components': detailed breakdown of parsed components\n    \"\"\"\n    \n    # Remove timezone offset for simpler parsing (will handle separately)\n    clean_string = re.sub(r'[+-]\\d{2}:?\\d{2}$', '', iso_string)\n    \n    # Duration pattern (P[nY][nM][nD][T[nH][nM][nS]])\n    duration_pattern = r'^P(?:(\\d+)Y)?(?:(\\d+)M)?(?:(\\d+)D)?(?:T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?)?$'\n    \n    # Date patterns\n    date_patterns = [\n        r'^(\\d{4})-(\\d{2})-(\\d{2})$',  # YYYY-MM-DD\n        r'^(\\d{4})(\\d{2})(\\d{2})$',    # YYYYMMDD\n        r'^(\\d{4})-(\\d{2})$',          # YYYY-MM\n        r'^(\\d{4})$'                   # YYYY\n    ]\n    \n    # Time patterns\n    time_patterns = [\n        r'^(\\d{2}):(\\d{2}):(\\d{2})(?:\\.(\\d+))?$',  # HH:MM:SS.sss\n        r'^(\\d{2})(\\d{2})(\\d{2})(?:\\.(\\d+))?$',    # HHMMSS.sss\n        r'^(\\d{2}):(\\d{2})$',                      # HH:MM\n        r'^(\\d{2})(\\d{2})$'                        # HHMM\n    ]\n    \n    # DateTime patterns\n    datetime_patterns = [\n        r'^(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})(?:\\.(\\d+))?$',\n        r'^(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})(\\d{2})(?:\\.(\\d+))?$'\n    ]\n    \n    # Check for duration first\n    duration_match = re.match(duration_pattern, clean_string)\n    if duration_match:\n        years, months, days, hours, minutes, seconds = duration_match.groups()\n        \n        duration_components = {\n            'years': int(years) if years else 0,\n            'months': int(months) if months else 0,\n            'days': int(days) if days else 0,\n            'hours': int(hours) if hours else 0,\n            'minutes': int(minutes) if minutes else 0,\n            'seconds': int(seconds) if seconds else 0\n        }\n        \n        # Calculate total approximate duration in seconds\n        total_seconds = (\n            duration_components['years'] * 31536000 +  # Approximate year length\n            duration_components['months'] * 2592000 +  # Approximate month length\n            duration_components['days'] * 86400 +\n            duration_components['hours'] * 3600 +\n            duration_components['minutes'] * 60 +\n            duration_components['seconds']\n        )\n        \n        return {\n            'type': 'duration',\n            'value': timedelta(seconds=total_seconds),\n            'components': duration_components\n        }\n    \n    # Check for datetime\n    for pattern in datetime_patterns:\n        match = re.match(pattern, clean_string)\n        if match:\n            groups = match.groups()\n            year, month, day = int(groups[0]), int(groups[1]), int(groups[2])\n            hour, minute, second = int(groups[3]), int(groups[4]), int(groups[5])\n            microsecond = int(groups[6].ljust(6, '0')[:6]) if groups[6] else 0\n            \n            dt = datetime(year, month, day, hour, minute, second, microsecond)\n            return {\n                'type': 'datetime',\n                'value': dt,\n                'components': {\n                    'year': year, 'month': month, 'day': day,\n                    'hour': hour, 'minute': minute, 'second': second,\n                    'microsecond': microsecond\n                }\n            }\n    \n    # Check for date\n    for pattern in date_patterns:\n        match = re.match(pattern, clean_string)\n        if match:\n            groups = match.groups()\n            year = int(groups[0])\n            month = int(groups[1]) if len(groups) > 1 and groups[1] else 1\n            day = int(groups[2]) if len(groups) > 2 and groups[2] else 1\n            \n            dt = datetime(year, month, day)\n            return {\n                'type': 'date',\n                'value': dt,\n                'components': {\n                    'year': year, 'month': month, 'day': day\n                }\n            }\n    \n    # Check for time\n    for pattern in time_patterns:\n        match = re.match(pattern, clean_string)\n        if match:\n            groups = match.groups()\n            hour = int(groups[0])\n            minute = int(groups[1])\n            second = int(groups[2]) if len(groups) > 2 and groups[2] else 0\n            microsecond = int(groups[3].ljust(6, '0')[:6]) if len(groups) > 3 and groups[3] else 0\n            \n            # Create a datetime with today's date for time-only values\n            today = datetime.now().date()\n            dt = datetime(today.year, today.month, today.day, hour, minute, second, microsecond)\n            \n            return {\n                'type': 'time',\n                'value': dt,\n                'components': {\n                    'hour': hour, 'minute': minute, 'second': second, 'microsecond': microsecond\n                }\n            }\n    \n    raise ValueError(f\"Invalid ISO 8601 format: {iso_string}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    examples = [\n        \"2023-12-25\",\n        \"15:30:45\",\n        \"2023-12-25T15:30:45.123456\",\n        \"P1Y2M3DT4H5M6S\",\n        \"20231225\",\n        \"153045\"\n    ]\n    \n    for example in examples:\n        try:\n            result = parse_iso8601(example)\n            print(f\"Input: {example}\")\n            print(f\"Type: {result['type']}\")\n            print(f\"Value: {result['value']}\")\n            print(f\"Components: {result['components']}\")\n            print(\"-\" * 40)\n        except ValueError as e:\n            print(f\"Error parsing {example}: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "re", "datetime"], "package_breakdown": {"install_packages": ["typing", "re", "datetime"], "import_packages": ["re", "datetime", "typing"], "llm_suggested": ["re", "datetime", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses an XML file using the ElementTree API in combination with libxml2/libxslt to extract specific data and manipulate the XML structure.", "prompt_index": 107, "generated_code": "```python\nimport xml.etree.ElementTree as ET\nimport lxml.etree as lxml_etree\n\ndef parse_and_manipulate_xml(xml_file_path, xpath_query, transformations=None):\n    \"\"\"\n    Parse XML file using ElementTree with libxml2 backend and manipulate structure\n    \n    Args:\n        xml_file_path (str): Path to the XML file\n        xpath_query (str): XPath query to extract specific data\n        transformations (dict): Dictionary of XPath:value pairs for manipulation\n    \n    Returns:\n        tuple: (extracted_data, modified_xml_string)\n    \"\"\"\n    \n    # Parse with standard ElementTree (uses libxml2 if available)\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n    \n    # Use lxml for better XPath support and manipulation\n    lxml_tree = lxml_etree.parse(xml_file_path)\n    \n    # Extract data using XPath\n    extracted_data = []\n    elements = lxml_tree.xpath(xpath_query)\n    \n    for element in elements:\n        if hasattr(element, 'text'):\n            extracted_data.append(element.text)\n        else:\n            extracted_data.append(str(element))\n    \n    # Apply transformations if specified\n    if transformations:\n        for xpath, new_value in transformations.items():\n            target_elements = lxml_tree.xpath(xpath)\n            for element in target_elements:\n                if hasattr(element, 'text'):\n                    element.text = new_value\n                # For attributes\n                elif '@' in xpath:\n                    attr_name = xpath.split('@')[-1]\n                    element.set(attr_name, new_value)\n    \n    # Convert modified tree back to string\n    modified_xml = lxml_etree.tostring(lxml_tree, \n                                      encoding='unicode', \n                                      pretty_print=True)\n    \n    return extracted_data, modified_xml\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example XML file path\n    xml_file = \"example.xml\"\n    \n    # XPath query to extract data\n    xpath_query = \"//book/title\"\n    \n    # Transformations to apply\n    transformations = {\n        \"//book[@category='WEB']/price\": \"29.99\",\n        \"//book[1]/@category\": \"TECH\"\n    }\n    \n    try:\n        data, modified_xml = parse_and_manipulate_xml(\n            xml_file, xpath_query, transformations\n        )\n        \n        print(\"Extracted data:\", data)\n        print(\"\\nModified XML:\")\n        print(modified_xml)\n        \n    except ET.ParseError as e:\n        print(f\"XML parsing error: {e}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["xml"], "packages_detected": ["lxml", "xml"], "package_breakdown": {"install_packages": ["lxml", "xml"], "import_packages": ["lxml", "xml"], "llm_suggested": ["lxml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uploads a local file named 'example.txt' to a specific bucket in Google Cloud Storage using the Google Cloud Storage API client library.", "prompt_index": 108, "generated_code": "```python\nfrom google.cloud import storage\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_filename(source_file_name)\n\n    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n\n# Usage\nupload_blob(\"your-bucket-name\", \"example.txt\", \"example.txt\")\n```\n\nNote: Replace \"your-bucket-name\" with your actual bucket name and ensure you have:\n1. Google Cloud Storage client library installed (`pip install google-cloud-storage`)\n2. Proper authentication set up (GOOGLE_APPLICATION_CREDENTIALS environment variable)\n3. The file 'example.txt' exists in the current directory", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-storage"], "package_breakdown": {"install_packages": ["google", "google-cloud-storage"], "import_packages": ["google"], "llm_suggested": ["google-cloud-storage"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses OpenTelemetry Semantic Conventions to instrument a Flask application for distributed tracing with automatic context propagation and correlation of traces across services.", "prompt_index": 109, "generated_code": "```python\nfrom flask import Flask, request\nimport requests\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.semconv.trace import SpanAttributes\n\n# Set up tracer provider\nresource = Resource.create({\n    \"service.name\": \"flask-app\",\n    \"service.version\": \"1.0.0\"\n})\ntrace.set_tracer_provider(TracerProvider(resource=resource))\n\n# Configure OTLP exporter (adjust endpoint as needed)\notlp_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\nspan_processor = BatchSpanProcessor(otlp_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Create Flask app\napp = Flask(__name__)\n\n# Instrument Flask and Requests\nFlaskInstrumentor().instrument_app(app)\nRequestsInstrumentor().instrument()\n\ntracer = trace.get_tracer(__name__)\n\n@app.route('/')\ndef hello_world():\n    with tracer.start_as_current_span(\"hello-operation\") as span:\n        # Add semantic attributes\n        span.set_attribute(SpanAttributes.HTTP_METHOD, request.method)\n        span.set_attribute(SpanAttributes.HTTP_URL, request.url)\n        span.set_attribute(SpanAttributes.HTTP_SCHEME, request.scheme)\n        span.set_attribute(SpanAttributes.HTTP_HOST, request.host)\n        span.set_attribute(SpanAttributes.HTTP_TARGET, request.path)\n        \n        # Example: Make external HTTP call (context automatically propagated)\n        try:\n            response = requests.get('http://external-service:8080/api/data')\n            span.set_attribute(SpanAttributes.HTTP_STATUS_CODE, response.status_code)\n            return f\"Hello World! External service returned: {response.status_code}\"\n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return \"Error occurred\", 500\n\n@app.route('/api/users/<user_id>')\ndef get_user(user_id):\n    with tracer.start_as_current_span(\"get-user\") as span:\n        span.set_attribute(SpanAttributes.HTTP_METHOD, request.method)\n        span.set_attribute(SpanAttributes.HTTP_URL, request.url)\n        span.set_attribute(\"user.id\", user_id)  # Custom attribute\n        \n        # Business logic here\n        return f\"User {user_id} details\"\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-sdk", "flask", "requests", "opentelemetry-semantic-conventions", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry"], "package_breakdown": {"install_packages": ["opentelemetry-sdk", "flask", "requests", "opentelemetry-semantic-conventions", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry"], "import_packages": ["requests", "opentelemetry", "flask"], "llm_suggested": ["flask", "requests", "opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-requests", "opentelemetry-semantic-conventions"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a WSGI web application using the features and utilities provided by the comprehensive WSGI web application library.", "prompt_index": 110, "generated_code": "```python\nfrom wsgiref.simple_server import make_server\nfrom wsgiref.util import setup_testing_defaults\n\ndef simple_app(environ, start_response):\n    setup_testing_defaults(environ)\n    \n    status = '200 OK'\n    headers = [('Content-type', 'text/plain; charset=utf-8')]\n    \n    start_response(status, headers)\n    \n    response_body = [\n        f\"{key}: {value}\" for key, value in sorted(environ.items())\n        if key.startswith('wsgi.') or key in ['REQUEST_METHOD', 'PATH_INFO']\n    ]\n    \n    return [\"\\n\".join(response_body).encode(\"utf-8\")]\n\nif __name__ == '__main__':\n    with make_server('', 8000, simple_app) as httpd:\n        print(\"Serving on port 8000...\")\n        httpd.serve_forever()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["wsgiref"], "package_breakdown": {"install_packages": ["wsgiref"], "import_packages": ["wsgiref"], "llm_suggested": ["wsgiref"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the decorators provided by this package to add logging functionality to a function, printing the function name, its arguments, and return value every time the function is called.", "prompt_index": 111, "generated_code": "```python\nimport functools\n\ndef log_function_call(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Log function name and arguments\n        args_str = ', '.join([repr(arg) for arg in args])\n        kwargs_str = ', '.join([f\"{key}={repr(value)}\" for key, value in kwargs.items()])\n        all_args = ', '.join(filter(None, [args_str, kwargs_str]))\n        \n        print(f\"Calling {func.__name__}({all_args})\")\n        \n        # Call the original function\n        result = func(*args, **kwargs)\n        \n        # Log return value\n        print(f\"{func.__name__} returned: {repr(result)}\")\n        \n        return result\n    return wrapper\n\n# Example usage:\n@log_function_call\ndef add_numbers(a, b):\n    return a + b\n\n@log_function_call  \ndef greet(name, greeting=\"Hello\"):\n    return f\"{greeting}, {name}!\"\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that serializes a Python dictionary containing various data types into a MessagePack format for efficient and compact storage or transmission.", "prompt_index": 112, "generated_code": "```python\nimport msgpack\n\ndef serialize_to_msgpack(data_dict):\n    \"\"\"\n    Serializes a Python dictionary to MessagePack format.\n    \n    Args:\n        data_dict (dict): Dictionary containing various data types\n        \n    Returns:\n        bytes: MessagePack serialized data\n    \"\"\"\n    return msgpack.packb(data_dict, use_bin_type=True)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dictionary with various data types\n    sample_data = {\n        \"string\": \"hello world\",\n        \"integer\": 42,\n        \"float\": 3.14159,\n        \"boolean\": True,\n        \"list\": [1, 2, 3, \"items\"],\n        \"nested_dict\": {\"key\": \"value\", \"number\": 123},\n        \"none_value\": None\n    }\n    \n    # Serialize to MessagePack\n    msgpack_data = serialize_to_msgpack(sample_data)\n    print(f\"Original size: {len(str(sample_data))} bytes\")\n    print(f\"MessagePack size: {len(msgpack_data)} bytes\")\n    print(f\"Compression ratio: {len(msgpack_data)/len(str(sample_data)):.2f}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["msgpack"], "package_breakdown": {"install_packages": ["msgpack"], "import_packages": ["msgpack"], "llm_suggested": ["msgpack"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines custom type annotations using the type system extensions provided by the mypy type checker for ensuring type safety and consistency in your program.", "prompt_index": 113, "generated_code": "```python\nfrom typing import List, Dict, Optional, Union, Tuple, Callable, Any\nfrom typing_extensions import TypedDict, Literal, Protocol\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\n\n# Custom type aliases\nUserId = int\nEmail = str\nPercentage = float\nCoordinates = Tuple[float, float]\n\n# TypedDict for structured data\nclass UserProfile(TypedDict):\n    user_id: UserId\n    email: Email\n    name: str\n    age: Optional[int]\n    is_active: bool\n\n# Protocol for duck typing\nclass Renderable(Protocol):\n    def render(self) -> str: ...\n\n# Abstract base class with type annotations\nclass DataProcessor(ABC):\n    @abstractmethod\n    def process(self, data: List[Dict[str, Any]]) -> Dict[str, Union[int, float, str]]: ...\n\n# Dataclass with type annotations\n@dataclass\nclass Point:\n    x: float\n    y: float\n    label: Optional[str] = None\n\n# Function with detailed type annotations\ndef process_user_data(\n    users: List[UserProfile],\n    processor: DataProcessor,\n    strict_mode: bool = False\n) -> Dict[UserId, Union[UserProfile, None]]:\n    \"\"\"Process user data and return mapped results.\"\"\"\n    result: Dict[UserId, Union[UserProfile, None]] = {}\n    \n    for user in users:\n        if user['is_active'] or not strict_mode:\n            result[user['user_id']] = user\n        else:\n            result[user['user_id']] = None\n    \n    return result\n\n# Generic function with type variables\nfrom typing import TypeVar, Generic\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nclass Result(Generic[T, U]):\n    def __init__(self, value: T, error: Optional[U] = None):\n        self.value = value\n        self.error = error\n\n# Literal types for specific values\nStatus = Literal['pending', 'processing', 'completed', 'failed']\n\ndef update_status(status: Status) -> None:\n    \"\"\"Update system status with validated literal values.\"\"\"\n    print(f\"Status updated to: {status}\")\n\n# Callable type with specific signature\nMathOperation = Callable[[float, float], float]\n\ndef apply_operation(a: float, b: float, op: MathOperation) -> float:\n    return op(a, b)\n\n# Union types for flexible parameters\ndef format_value(value: Union[str, int, float, None]) -> str:\n    if value is None:\n        return \"N/A\"\n    return str(value)\n\n# Optional type (equivalent to Union[T, None])\ndef find_user(user_id: UserId) -> Optional[UserProfile]:\n    \"\"\"Find user by ID, returns None if not found.\"\"\"\n    # Implementation would go here\n    return None\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["typing_extensions"], "packages_detected": ["typing", "typing_extensions", "abc", "dataclasses"], "package_breakdown": {"install_packages": ["typing", "typing_extensions", "abc", "dataclasses"], "import_packages": ["typing_extensions", "abc", "dataclasses", "typing"], "llm_suggested": ["typing_extensions", "dataclasses", "abc"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an instance of the Pickler class from the package, extends the standard pickle.Pickler functionality by adding custom serialization methods, and then pickles an object using the extended Pickler instance.", "prompt_index": 114, "generated_code": "```python\nimport pickle\n\nclass ExtendedPickler(pickle.Pickler):\n    def __init__(self, file, protocol=None, fix_imports=True, buffer_callback=None):\n        super().__init__(file, protocol, fix_imports, buffer_callback)\n    \n    def persistent_id(self, obj):\n        # Add custom serialization logic here\n        # Return None for default pickling, or a persistent ID for custom handling\n        return None\n    \n    def reducer_override(self, obj):\n        # Alternative method for custom serialization (Python 3.8+)\n        # Return a tuple (callable, args) for custom reduction, or NotImplemented for default\n        return NotImplemented\n\n# Example usage\nclass CustomObject:\n    def __init__(self, data):\n        self.data = data\n\n# Create object to pickle\nobj_to_pickle = CustomObject(\"test data\")\n\n# Pickle using extended pickler\nwith open('pickled_object.pkl', 'wb') as f:\n    pickler = ExtendedPickler(f)\n    pickler.dump(obj_to_pickle)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pickle"], "package_breakdown": {"install_packages": ["pickle"], "import_packages": ["pickle"], "llm_suggested": ["pickle"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that serializes and deserializes data using protocol buffers in a visually appealing and pythonic manner.", "prompt_index": 115, "generated_code": "```python\nimport json\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport protobuf_pb2\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    email: str\n    phone_numbers: List[str]\n    is_active: bool = True\n\n    def to_protobuf(self) -> protobuf_pb2.User:\n        \"\"\"Convert User instance to protobuf message.\"\"\"\n        return protobuf_pb2.User(\n            id=self.id,\n            name=self.name,\n            email=self.email,\n            phone_numbers=self.phone_numbers,\n            is_active=self.is_active\n        )\n\n    @classmethod\n    def from_protobuf(cls, proto_user: protobuf_pb2.User) -> 'User':\n        \"\"\"Create User instance from protobuf message.\"\"\"\n        return cls(\n            id=proto_user.id,\n            name=proto_user.name,\n            email=proto_user.email,\n            phone_numbers=list(proto_user.phone_numbers),\n            is_active=proto_user.is_active\n        )\n\n    def to_json(self) -> str:\n        \"\"\"Convert User instance to JSON string.\"\"\"\n        return json.dumps({\n            'id': self.id,\n            'name': self.name,\n            'email': self.email,\n            'phone_numbers': self.phone_numbers,\n            'is_active': self.is_active\n        }, indent=2)\n\n    @classmethod\n    def from_json(cls, json_str: str) -> 'User':\n        \"\"\"Create User instance from JSON string.\"\"\"\n        data = json.loads(json_str)\n        return cls(**data)\n\ndef serialize_user(user: User) -> bytes:\n    \"\"\"Serialize User instance to protobuf binary format.\"\"\"\n    return user.to_protobuf().SerializeToString()\n\ndef deserialize_user(data: bytes) -> User:\n    \"\"\"Deserialize protobuf binary data to User instance.\"\"\"\n    proto_user = protobuf_pb2.User()\n    proto_user.ParseFromString(data)\n    return User.from_protobuf(proto_user)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample user\n    user = User(\n        id=123,\n        name=\"Alice Smith\",\n        email=\"alice@example.com\",\n        phone_numbers=[\"+1234567890\", \"+0987654321\"],\n        is_active=True\n    )\n\n    print(\"Original User:\")\n    print(user.to_json())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Serialize to protobuf\n    serialized_data = serialize_user(user)\n    print(f\"Serialized data (length: {len(serialized_data)} bytes):\")\n    print(serialized_data.hex())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Deserialize from protobuf\n    deserialized_user = deserialize_user(serialized_data)\n    print(\"Deserialized User:\")\n    print(deserialized_user.to_json())\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n    # Verify data integrity\n    print(\"Data integrity check:\")\n    print(f\"Original == Deserialized: {user == deserialized_user}\")\n    print(f\"JSON representations match: {user.to_json() == deserialized_user.to_json()}\")\n```\n\n**Note**: This code assumes you have a protobuf schema file named `protobuf.proto` that has been compiled to generate `protobuf_pb2.py`. The `.proto` file should contain:\n\n```protobuf\nsyntax = \"proto3\";\n\nmessage User {\n    int32 id = 1;\n    string name = 2;\n    string email = 3;\n    repeated string phone_numbers = 4;\n    bool is_active = 5;\n}\n```\n\nTo compile the protobuf schema, use:\n```bash\nprotoc --python_out=. protobuf.proto\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["protobuf_pb2"], "packages_detected": ["dataclasses", "protobuf", "protobuf_pb2", "typing", "json"], "package_breakdown": {"install_packages": ["dataclasses", "protobuf", "protobuf_pb2", "typing", "json"], "import_packages": ["json", "protobuf_pb2", "dataclasses", "typing"], "llm_suggested": ["protobuf", "json", "typing", "dataclasses"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves a list of classifiers available on PyPI (pypi.org) using the canonical source for classifiers package.", "prompt_index": 116, "generated_code": "```python\nimport requests\nfrom xml.etree import ElementTree as ET\n\ndef get_pypi_classifiers():\n    url = \"https://pypi.org/rss/packages.xml\"\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    root = ET.fromstring(response.content)\n    classifiers = set()\n    \n    for item in root.findall('.//item'):\n        classifier_elements = item.findall('classifier')\n        for classifier in classifier_elements:\n            if classifier.text:\n                classifiers.add(classifier.text.strip())\n    \n    return sorted(classifiers)\n\nif __name__ == \"__main__\":\n    classifiers = get_pypi_classifiers()\n    for classifier in classifiers:\n        print(classifier)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["xml"], "packages_detected": ["requests", "xml"], "package_breakdown": {"install_packages": ["requests", "xml"], "import_packages": ["requests", "xml"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that scans the system environment to detect any active shell processes and provides a detailed report on their surrounding context and activities.", "prompt_index": 117, "generated_code": "```python\nimport os\nimport subprocess\nimport psutil\nimport json\nfrom datetime import datetime\n\ndef get_shell_processes():\n    \"\"\"Identify active shell processes\"\"\"\n    shell_keywords = ['bash', 'zsh', 'fish', 'sh', 'csh', 'tcsh', 'ksh', 'powershell', 'cmd']\n    shell_processes = []\n    \n    for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'username', 'create_time']):\n        try:\n            proc_name = proc.info['name'].lower()\n            cmdline = ' '.join(proc.info['cmdline']).lower() if proc.info['cmdline'] else ''\n            \n            # Check if process is a shell\n            if any(keyword in proc_name or keyword in cmdline for keyword in shell_keywords):\n                shell_processes.append(proc)\n        except (psutil.NoSuchProcess, psutil.AccessDenied):\n            continue\n    \n    return shell_processes\n\ndef get_process_details(proc):\n    \"\"\"Get detailed information about a process\"\"\"\n    try:\n        details = {\n            'pid': proc.info['pid'],\n            'name': proc.info['name'],\n            'cmdline': proc.info['cmdline'],\n            'username': proc.info['username'],\n            'create_time': datetime.fromtimestamp(proc.info['create_time']).strftime('%Y-%m-%d %H:%M:%S'),\n            'status': proc.status(),\n            'cpu_percent': proc.cpu_percent(interval=0.1),\n            'memory_info': proc.memory_info()._asdict(),\n            'cwd': proc.cwd() if hasattr(proc, 'cwd') else 'N/A',\n            'connections': [conn._asdict() for conn in proc.connections()] if hasattr(proc, 'connections') else []\n        }\n        \n        # Get parent process info\n        try:\n            parent = proc.parent()\n            details['parent'] = {\n                'pid': parent.pid,\n                'name': parent.name()\n            }\n        except:\n            details['parent'] = None\n            \n        return details\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n        return None\n\ndef get_environment_info():\n    \"\"\"Get system environment information\"\"\"\n    return {\n        'hostname': os.uname().nodename,\n        'system': os.uname().sysname,\n        'release': os.uname().release,\n        'current_user': os.getlogin(),\n        'environment_variables': dict(os.environ),\n        'scan_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    }\n\ndef generate_shell_report():\n    \"\"\"Generate comprehensive shell process report\"\"\"\n    report = {\n        'system_info': get_environment_info(),\n        'shell_processes': []\n    }\n    \n    shell_processes = get_shell_processes()\n    \n    for proc in shell_processes:\n        process_details = get_process_details(proc)\n        if process_details:\n            report['shell_processes'].append(process_details)\n    \n    return report\n\ndef print_report(report):\n    \"\"\"Print the report in a readable format\"\"\"\n    print(\"=\" * 60)\n    print(\"SHELL PROCESS SCAN REPORT\")\n    print(\"=\" * 60)\n    print(f\"Scan Time: {report['system_info']['scan_time']}\")\n    print(f\"Hostname: {report['system_info']['hostname']}\")\n    print(f\"System: {report['system_info']['system']}\")\n    print(f\"Current User: {report['system_info']['current_user']}\")\n    print(f\"Found {len(report['shell_processes'])} shell processes\")\n    print(\"=\" * 60)\n    \n    for i, proc in enumerate(report['shell_processes'], 1):\n        print(f\"\\n{i}. Process: {proc['name']} (PID: {proc['pid']})\")\n        print(f\"   User: {proc['username']}\")\n        print(f\"   Command: {' '.join(proc['cmdline'])}\")\n        print(f\"   Started: {proc['create_time']}\")\n        print(f\"   Status: {proc['status']}\")\n        print(f\"   CPU: {proc['cpu_percent']}%\")\n        print(f\"   Memory: {proc['memory_info']['rss'] / 1024 / 1024:.2f} MB\")\n        print(f\"   Working Directory: {proc['cwd']}\")\n        \n        if proc['parent']:\n            print(f\"   Parent: {proc['parent']['name']} (PID: {proc['parent']['pid']})\")\n        \n        if proc['connections']:\n            print(f\"   Network Connections: {len(proc['connections'])}\")\n\nif __name__ == \"__main__\":\n    try:\n        report = generate_shell_report()\n        print_report(report)\n        \n        # Optionally save to JSON file\n        with open('shell_scan_report.json', 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n            \n    except ImportError as e:\n        print(f\"Required package missing: {e}\")\n        print(\"Install required packages: pip install psutil\")\n    except Exception as e:\n        print(f\"Error during scan: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "datetime", "psutil", "json", "os"], "package_breakdown": {"install_packages": ["subprocess", "datetime", "psutil", "json", "os"], "import_packages": ["subprocess", "datetime", "psutil", "json", "os"], "llm_suggested": ["psutil", "json", "datetime", "os", "subprocess"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a WebSocket server using low level API options and sends a message with custom headers and payload.", "prompt_index": 118, "generated_code": "```python\nimport socket\nimport ssl\nimport base64\nimport hashlib\nimport struct\nimport random\nimport string\n\ndef generate_websocket_key():\n    \"\"\"Generate a random WebSocket key for the handshake\"\"\"\n    return base64.b64encode(''.join(random.choice(string.ascii_letters + string.digits) for _ in range(16)).encode()).decode()\n\ndef websocket_handshake(sock, host, port, path, custom_headers):\n    \"\"\"Perform WebSocket handshake\"\"\"\n    key = generate_websocket_key()\n    \n    # Build handshake request\n    request = f\"GET {path} HTTP/1.1\\r\\n\"\n    request += f\"Host: {host}:{port}\\r\\n\"\n    request += \"Upgrade: websocket\\r\\n\"\n    request += \"Connection: Upgrade\\r\\n\"\n    request += f\"Sec-WebSocket-Key: {key}\\r\\n\"\n    request += \"Sec-WebSocket-Version: 13\\r\\n\"\n    \n    # Add custom headers\n    for header_name, header_value in custom_headers.items():\n        request += f\"{header_name}: {header_value}\\r\\n\"\n    \n    request += \"\\r\\n\"\n    \n    # Send handshake request\n    sock.send(request.encode())\n    \n    # Read handshake response\n    response = b\"\"\n    while b\"\\r\\n\\r\\n\" not in response:\n        response += sock.recv(1024)\n    \n    # Verify handshake (simplified)\n    if b\"101 Switching Protocols\" not in response:\n        raise Exception(\"WebSocket handshake failed\")\n    \n    return True\n\ndef create_websocket_frame(payload, opcode=0x1):\n    \"\"\"Create WebSocket frame with payload\"\"\"\n    frame = bytearray()\n    \n    # FIN bit set, opcode for text frame\n    frame.append(0x80 | opcode)\n    \n    # Payload length\n    payload_length = len(payload)\n    if payload_length <= 125:\n        frame.append(payload_length)\n    elif payload_length <= 65535:\n        frame.append(126)\n        frame.extend(struct.pack(\"!H\", payload_length))\n    else:\n        frame.append(127)\n        frame.extend(struct.pack(\"!Q\", payload_length))\n    \n    # Add payload\n    frame.extend(payload.encode())\n    \n    return bytes(frame)\n\ndef connect_websocket(host, port, path, use_ssl=False, custom_headers=None):\n    \"\"\"Connect to WebSocket server\"\"\"\n    if custom_headers is None:\n        custom_headers = {}\n    \n    # Create socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    if use_ssl:\n        context = ssl.create_default_context()\n        sock = context.wrap_socket(sock, server_hostname=host)\n    \n    # Connect\n    sock.connect((host, port))\n    \n    # Perform handshake\n    websocket_handshake(sock, host, port, path, custom_headers)\n    \n    return sock\n\ndef send_websocket_message(sock, message):\n    \"\"\"Send message over WebSocket\"\"\"\n    frame = create_websocket_frame(message)\n    sock.send(frame)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Configuration\n    HOST = \"echo.websocket.org\"\n    PORT = 80\n    PATH = \"/\"\n    USE_SSL = False\n    \n    # Custom headers\n    CUSTOM_HEADERS = {\n        \"X-Custom-Header\": \"custom-value\",\n        \"Authorization\": \"Bearer your-token-here\",\n        \"User-Agent\": \"CustomWebSocketClient/1.0\"\n    }\n    \n    try:\n        # Connect to WebSocket\n        ws_socket = connect_websocket(HOST, PORT, PATH, USE_SSL, CUSTOM_HEADERS)\n        print(\"Connected to WebSocket server\")\n        \n        # Send message\n        message = \"Hello, WebSocket!\"\n        send_websocket_message(ws_socket, message)\n        print(f\"Sent message: {message}\")\n        \n        # Close connection\n        ws_socket.close()\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["base64", "random", "string", "ssl", "struct", "hashlib", "socket"], "package_breakdown": {"install_packages": ["base64", "random", "string", "ssl", "struct", "hashlib", "socket"], "import_packages": ["base64", "random", "string", "ssl", "struct", "hashlib", "socket"], "llm_suggested": ["socket", "ssl", "base64", "hashlib", "struct", "random", "string"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates and displays the code coverage percentage of a Python script using the package for code coverage measurement.", "prompt_index": 119, "generated_code": "```python\nimport coverage\nimport sys\nimport os\n\ndef calculate_coverage(script_path):\n    # Initialize coverage object\n    cov = coverage.Coverage()\n    \n    # Start measuring coverage\n    cov.start()\n    \n    try:\n        # Execute the target script\n        with open(script_path, 'r') as f:\n            script_code = f.read()\n        exec(script_code, {})\n    except Exception as e:\n        print(f\"Error executing script: {e}\")\n        return None\n    finally:\n        # Stop measuring coverage\n        cov.stop()\n    \n    # Get coverage data\n    cov.save()\n    cov_data = cov.get_data()\n    \n    # Calculate coverage percentage\n    analyzed = cov_data.measured_files()\n    if not analyzed:\n        print(\"No files were analyzed for coverage\")\n        return None\n    \n    # Get coverage for the main script\n    script_abs_path = os.path.abspath(script_path)\n    if script_abs_path not in analyzed:\n        print(f\"Script {script_path} was not analyzed\")\n        return None\n    \n    line_numbers = cov_data.lines(script_abs_path)\n    executed_lines = cov_data.arcs(script_abs_path) or []\n    \n    if not line_numbers:\n        print(\"No executable lines found in the script\")\n        return None\n    \n    total_lines = len(line_numbers)\n    covered_lines = len(set(executed_lines))\n    \n    coverage_percentage = (covered_lines / total_lines) * 100\n    \n    # Display results\n    print(f\"Total lines: {total_lines}\")\n    print(f\"Covered lines: {covered_lines}\")\n    print(f\"Coverage percentage: {coverage_percentage:.2f}%\")\n    \n    return coverage_percentage\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python coverage_calculator.py <script_path>\")\n        sys.exit(1)\n    \n    script_path = sys.argv[1]\n    calculate_coverage(script_path)\n```\n\nTo use this code:\n1. Install the coverage package: `pip install coverage`\n2. Save the code to a file (e.g., `coverage_calculator.py`)\n3. Run it with: `python coverage_calculator.py path/to/your/script.py`\n\nThe script will execute the target script, measure its code coverage, and display the coverage percentage.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["coverage", "sys", "os"], "package_breakdown": {"install_packages": ["coverage", "sys", "os"], "import_packages": ["coverage", "sys", "os"], "llm_suggested": ["coverage"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a ZeroMQ socket, binds it to a specific port for publishing messages, and sends a series of JSON-encoded data packets to a specified endpoint using ZeroMQ bindings.", "prompt_index": 120, "generated_code": "```python\nimport zmq\nimport json\n\ndef create_publisher_socket(port):\n    context = zmq.Context()\n    socket = context.socket(zmq.PUB)\n    socket.bind(f\"tcp://*:{port}\")\n    return socket\n\ndef send_json_data(socket, data_packets):\n    for packet in data_packets:\n        json_data = json.dumps(packet)\n        socket.send_string(json_data)\n\n# Example usage\nif __name__ == \"__main__\":\n    port = 5555\n    publisher_socket = create_publisher_socket(port)\n    \n    # Example data packets to send\n    data_packets = [\n        {\"type\": \"temperature\", \"value\": 23.5, \"timestamp\": \"2023-10-01T10:00:00\"},\n        {\"type\": \"humidity\", \"value\": 65.2, \"timestamp\": \"2023-10-01T10:00:01\"},\n        {\"type\": \"pressure\", \"value\": 1013.25, \"timestamp\": \"2023-10-01T10:00:02\"}\n    ]\n    \n    send_json_data(publisher_socket, data_packets)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "zmq"], "package_breakdown": {"install_packages": ["json", "zmq"], "import_packages": ["json", "zmq"], "llm_suggested": ["zmq", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encrypts a message using the NaCl library's cryptography functions and then decrypts the encrypted message to recover the original plaintext.", "prompt_index": 121, "generated_code": "```python\nimport nacl.secret\nimport nacl.utils\n\n# Generate a random key for symmetric encryption\nkey = nacl.utils.random(nacl.secret.SecretBox.KEY_SIZE)\n\n# Create a SecretBox instance with the key\nbox = nacl.secret.SecretBox(key)\n\n# Message to encrypt\nmessage = b\"Secret message to encrypt\"\n\n# Encrypt the message\nencrypted = box.encrypt(message)\n\n# Decrypt the encrypted message\ndecrypted = box.decrypt(encrypted)\n\n# Verify the decrypted message matches the original\nassert decrypted == message\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pynacl", "nacl"], "package_breakdown": {"install_packages": ["pynacl", "nacl"], "import_packages": ["nacl"], "llm_suggested": ["pynacl"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retries calling a specific function until it succeeds, with a maximum number of attempts and a delay between each attempt.", "prompt_index": 122, "generated_code": "```python\nimport time\nfrom functools import wraps\n\ndef retry(max_attempts=3, delay=1):\n    \"\"\"\n    Decorator that retries calling a function until it succeeds.\n    \n    Args:\n        max_attempts (int): Maximum number of attempts before giving up\n        delay (float): Delay in seconds between attempts\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            attempts = 0\n            while attempts < max_attempts:\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    attempts += 1\n                    if attempts == max_attempts:\n                        raise Exception(f\"Function {func.__name__} failed after {max_attempts} attempts: {e}\")\n                    time.sleep(delay)\n            return None\n        return wrapper\n    return decorator\n\n# Example usage:\n@retry(max_attempts=5, delay=2)\ndef unreliable_function():\n    # Your function logic here\n    # This will be retried if it raises an exception\n    pass\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "time"], "package_breakdown": {"install_packages": ["functools", "time"], "import_packages": ["functools", "time"], "llm_suggested": ["functools", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Poetry PEP 517 Build Backend package to programmatically build a Python project with its dependencies and generate a distributable package in a specific format, such as a wheel or a source distribution.", "prompt_index": 123, "generated_code": "```python\nimport tempfile\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef build_project(project_dir, formats=None, output_dir=None):\n    \"\"\"\n    Build a Python project using Poetry PEP 517 backend\n    \n    Args:\n        project_dir (str/Path): Path to the project directory containing pyproject.toml\n        formats (list): List of formats to build ('wheel', 'sdist', or both)\n        output_dir (str/Path): Directory to output built packages (defaults to dist/)\n    \n    Returns:\n        list: Paths to the built packages\n    \"\"\"\n    if formats is None:\n        formats = ['wheel', 'sdist']\n    \n    project_dir = Path(project_dir)\n    if output_dir:\n        output_dir = Path(output_dir)\n    else:\n        output_dir = project_dir / 'dist'\n    \n    # Ensure output directory exists\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Build command\n    cmd = [\n        sys.executable, '-m', 'build',\n        '--outdir', str(output_dir)\n    ]\n    \n    # Add format flags\n    if 'wheel' in formats:\n        cmd.append('--wheel')\n    if 'sdist' in formats:\n        cmd.append('--sdist')\n    \n    # Run the build process\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=project_dir,\n            check=True,\n            capture_output=True,\n            text=True\n        )\n        \n        # Get list of built packages\n        built_packages = list(output_dir.glob('*'))\n        return built_packages\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Build failed with error: {e.stderr}\")\n        raise\n    except FileNotFoundError:\n        raise RuntimeError(\"The 'build' package is not installed. Install it with: pip install build\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Build both wheel and source distribution\n    packages = build_project(\n        project_dir='.',  # Current directory\n        formats=['wheel', 'sdist'],\n        output_dir='dist'\n    )\n    \n    print(\"Built packages:\")\n    for package in packages:\n        print(f\"  - {package.name}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "build", "pathlib", "sys", "tempfile"], "package_breakdown": {"install_packages": ["subprocess", "build", "pathlib", "sys", "tempfile"], "import_packages": ["pathlib", "sys", "subprocess", "tempfile"], "llm_suggested": ["pathlib", "subprocess", "sys", "tempfile", "build"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the provided package to create a context manager that sets a timeout for executing asynchronous functions within an asyncio program.", "prompt_index": 124, "generated_code": "```python\nimport asyncio\nimport contextlib\nfrom typing import Optional, Type\n\n@contextlib.asynccontextmanager\nasync def async_timeout(seconds: Optional[float] = None):\n    \"\"\"Context manager that sets a timeout for async operations.\"\"\"\n    if seconds is None:\n        yield\n        return\n        \n    try:\n        async with asyncio.timeout(seconds):\n            yield\n    except asyncio.TimeoutError:\n        raise TimeoutError(f\"Operation timed out after {seconds} seconds\")\n\n# Example usage:\nasync def example_usage():\n    try:\n        async with async_timeout(2.5):  # 2.5 second timeout\n            await asyncio.sleep(3)      # This will timeout\n    except TimeoutError as e:\n        print(f\"Caught timeout: {e}\")\n\n# To run the example:\n# asyncio.run(example_usage())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "contextlib", "typing"], "package_breakdown": {"install_packages": ["asyncio", "contextlib", "typing"], "import_packages": ["asyncio", "contextlib", "typing"], "llm_suggested": ["asyncio", "contextlib", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a Sorted List, populates it with integers in descending order, and then prints the list after adding a new integer at a specific index.", "prompt_index": 125, "generated_code": "```python\nimport bisect\n\nclass SortedList:\n    def __init__(self):\n        self.data = []\n    \n    def add(self, item):\n        bisect.insort(self.data, item)\n    \n    def add_at_index(self, index, item):\n        self.data.insert(index, item)\n        self.data.sort()\n    \n    def __str__(self):\n        return str(self.data)\n\n# Create and populate sorted list in descending order\nsorted_list = SortedList()\nfor num in range(10, 0, -1):\n    sorted_list.add(num)\n\nprint(\"Original sorted list:\", sorted_list)\n\n# Add new integer at specific index\nsorted_list.add_at_index(3, 25)\nprint(\"After adding 25 at index 3:\", sorted_list)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bisect"], "package_breakdown": {"install_packages": ["bisect"], "import_packages": ["bisect"], "llm_suggested": ["bisect"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with Microsoft Azure using the Azure Core Library for Python and retrieves a list of resources in a specific Azure subscription.", "prompt_index": 126, "generated_code": "```python\nfrom azure.identity import DefaultAzureCredential\nfrom azure.mgmt.resource import ResourceManagementClient\nfrom azure.core.exceptions import ClientAuthenticationError\n\ndef list_azure_resources(subscription_id):\n    try:\n        # Authenticate using DefaultAzureCredential\n        credential = DefaultAzureCredential()\n        \n        # Create Resource Management Client\n        resource_client = ResourceManagementClient(credential, subscription_id)\n        \n        # List all resources in the subscription\n        resources = resource_client.resources.list()\n        \n        # Print resource details\n        print(f\"Resources in subscription {subscription_id}:\")\n        for resource in resources:\n            print(f\"Name: {resource.name}, Type: {resource.type}, Location: {resource.location}\")\n            \n        return resources\n        \n    except ClientAuthenticationError as e:\n        print(f\"Authentication failed: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Error retrieving resources: {e}\")\n        raise\n\n# Example usage\nif __name__ == \"__main__\":\n    subscription_id = \"your-subscription-id-here\"  # Replace with your Azure subscription ID\n    list_azure_resources(subscription_id)\n```\n\n**Note:** Before running this code, you need to:\n1. Install required packages: `pip install azure-identity azure-mgmt-resource`\n2. Set up authentication using one of these methods:\n   - Azure CLI: Run `az login`\n   - Environment variables: Set `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`\n   - Managed Identity (for Azure-hosted applications)\n3. Replace `\"your-subscription-id-here\"` with your actual Azure subscription ID", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure-mgmt-resource", "azure", "azure-identity"], "package_breakdown": {"install_packages": ["azure-mgmt-resource", "azure", "azure-identity"], "import_packages": ["azure"], "llm_suggested": ["azure-identity", "azure-mgmt-resource"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that decorates a specific function named 'old_function' with the @deprecated decorator to mark it as deprecated and provide a custom warning message when the function is called.", "prompt_index": 127, "generated_code": "```python\nimport warnings\nfrom functools import wraps\n\ndef deprecated(message=None):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warning_msg = message or f\"Call to deprecated function {func.__name__}.\"\n            warnings.warn(warning_msg, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@deprecated(\"This function is deprecated. Use 'new_function' instead.\")\ndef old_function():\n    # Original function implementation\n    pass\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["warnings", "functools"], "package_breakdown": {"install_packages": ["warnings", "functools"], "import_packages": ["warnings", "functools"], "llm_suggested": ["warnings", "functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that instruments a Python application to collect distributed traces and metrics using the OpenTelemetry Python API.", "prompt_index": 128, "generated_code": "```python\nimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom opentelemetry.sdk.resources import Resource\n\n# Configure resource with service name\nresource = Resource.create({\"service.name\": \"your-service-name\"})\n\n# Initialize tracing\ntrace_provider = TracerProvider(resource=resource)\ntrace.set_tracer_provider(trace_provider)\n\n# Configure OTLP exporter for traces\notlp_trace_exporter = OTLPSpanExporter(\n    endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\", \"http://localhost:4317\"),\n    insecure=bool(os.getenv(\"OTEL_EXPORTER_OTLP_TRACES_INSECURE\", True)),\n)\ntrace_provider.add_span_processor(BatchSpanProcessor(otlp_trace_exporter))\n\n# Initialize metrics\nmetric_reader = PeriodicExportingMetricReader(\n    OTLPMetricExporter(\n        endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_METRICS_ENDPOINT\", \"http://localhost:4317\"),\n        insecure=bool(os.getenv(\"OTEL_EXPORTER_OTLP_METRICS_INSECURE\", True)),\n    ),\n    export_interval_millis=5000,\n)\nmeter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])\nmetrics.set_meter_provider(meter_provider)\n\n# Get tracer and meter\ntracer = trace.get_tracer(__name__)\nmeter = metrics.get_meter(__name__)\n\n# Example: Create a counter metric\nrequest_counter = meter.create_counter(\n    \"requests_total\",\n    description=\"Total number of requests\",\n    unit=\"1\",\n)\n\n# Example usage in your application\ndef handle_request():\n    with tracer.start_as_current_span(\"handle_request\") as span:\n        # Add attributes to the span\n        span.set_attribute(\"http.method\", \"GET\")\n        span.set_attribute(\"http.route\", \"/api/endpoint\")\n        \n        # Record metrics\n        request_counter.add(1, {\"endpoint\": \"/api/endpoint\"})\n        \n        # Your application logic here\n        try:\n            # Simulate some work\n            result = process_data()\n            span.set_status(trace.Status(trace.StatusCode.OK))\n            return result\n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            raise\n\ndef process_data():\n    # Nested span example\n    with tracer.start_as_current_span(\"process_data\") as span:\n        span.set_attribute(\"processing.stage\", \"data_transformation\")\n        # Your processing logic\n        return {\"result\": \"success\"}\n\n# Example of creating a histogram metric\nrequest_duration = meter.create_histogram(\n    \"request_duration_seconds\",\n    description=\"Duration of requests in seconds\",\n    unit=\"s\",\n)\n\n# Example of using histogram\nimport time\n\ndef timed_request():\n    start_time = time.time()\n    with tracer.start_as_current_span(\"timed_request\"):\n        # Your request handling\n        time.sleep(0.1)  # Simulate work\n        duration = time.time() - start_time\n        request_duration.record(duration, {\"endpoint\": \"/timed\"})\n        return duration\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-sdk", "opentelemetry", "time", "os", "opentelemetry-exporter-otlp-proto-grpc"], "package_breakdown": {"install_packages": ["opentelemetry-sdk", "opentelemetry", "time", "os", "opentelemetry-exporter-otlp-proto-grpc"], "import_packages": ["opentelemetry", "time", "os"], "llm_suggested": ["opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the Standard Health Checking Service for gRPC to continuously monitor the health status of a gRPC server and handle any necessary health check requests.", "prompt_index": 129, "generated_code": "```python\nimport grpc\nfrom concurrent import futures\nimport time\nimport threading\nfrom grpc_health.v1 import health_pb2\nfrom grpc_health.v1 import health_pb2_grpc\nfrom grpc_health.v1.health import HealthServicer\n\nclass HealthCheckMonitor:\n    def __init__(self, server_address, service_name=''):\n        self.server_address = server_address\n        self.service_name = service_name\n        self.channel = None\n        self.health_stub = None\n        self.is_monitoring = False\n        \n    def connect(self):\n        \"\"\"Establish connection to the gRPC server\"\"\"\n        try:\n            self.channel = grpc.insecure_channel(self.server_address)\n            self.health_stub = health_pb2_grpc.HealthStub(self.channel)\n            return True\n        except Exception as e:\n            print(f\"Connection failed: {e}\")\n            return False\n    \n    def check_health(self):\n        \"\"\"Perform a single health check\"\"\"\n        try:\n            request = health_pb2.HealthCheckRequest(service=self.service_name)\n            response = self.health_stub.Check(request, timeout=5)\n            return response.status == health_pb2.HealthCheckResponse.SERVING\n        except grpc.RpcError as e:\n            print(f\"Health check failed: {e}\")\n            return False\n    \n    def start_monitoring(self, interval_seconds=10):\n        \"\"\"Start continuous health monitoring\"\"\"\n        if not self.connect():\n            print(\"Failed to connect to server\")\n            return\n        \n        self.is_monitoring = True\n        \n        def monitor_loop():\n            while self.is_monitoring:\n                is_healthy = self.check_health()\n                status = \"HEALTHY\" if is_healthy else \"UNHEALTHY\"\n                print(f\"Health status at {time.strftime('%Y-%m-%d %H:%M:%S')}: {status}\")\n                \n                if not is_healthy:\n                    # Handle unhealthy state (e.g., alert, reconnect, etc.)\n                    self.handle_unhealthy_server()\n                \n                time.sleep(interval_seconds)\n        \n        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)\n        self.monitor_thread.start()\n        print(f\"Started health monitoring for {self.server_address}\")\n    \n    def stop_monitoring(self):\n        \"\"\"Stop the health monitoring\"\"\"\n        self.is_monitoring = False\n        if self.channel:\n            self.channel.close()\n        print(\"Stopped health monitoring\")\n    \n    def handle_unhealthy_server(self):\n        \"\"\"Custom handler for unhealthy server state\"\"\"\n        print(\"Server is unhealthy! Taking appropriate action...\")\n        # Implement your custom logic here (e.g., reconnect, alert, failover)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create health check servicer (for server-side implementation)\n    health_servicer = HealthServicer()\n    \n    # For client-side monitoring\n    monitor = HealthCheckMonitor('localhost:50051', 'your_service_name')\n    monitor.start_monitoring(interval_seconds=5)\n    \n    try:\n        # Keep the main thread alive\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        monitor.stop_monitoring()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 2, "hallucinations": ["concurrent", "grpc_health"], "packages_detected": ["grpcio-health-checking", "concurrent", "time", "grpc", "threading", "grpc_health"], "package_breakdown": {"install_packages": ["grpcio-health-checking", "concurrent", "time", "grpc", "threading", "grpc_health"], "import_packages": ["concurrent", "time", "grpc", "threading", "grpc_health"], "llm_suggested": ["grpc", "grpcio-health-checking", "concurrent", "time", "threading"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that imports the alternative regular expression module and uses it to search a given string for all occurrences of a specific pattern, then prints the results.", "prompt_index": 130, "generated_code": "```python\nimport regex\n\npattern = r'your_pattern_here'\ntext = 'your_text_here'\n\nmatches = regex.findall(pattern, text)\nfor match in matches:\n    print(match)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["regex"], "package_breakdown": {"install_packages": ["regex"], "import_packages": ["regex"], "llm_suggested": ["regex"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a PostgreSQL database using psycopg2, executes a query to retrieve data from a specified table, and then iterates through the results to print each row to the console.", "prompt_index": 131, "generated_code": "```python\nimport psycopg2\nfrom psycopg2 import sql\n\n# Database connection parameters\ndb_params = {\n    'host': 'localhost',\n    'database': 'your_database',\n    'user': 'your_username',\n    'password': 'your_password',\n    'port': '5432'\n}\n\n# Table name and query\ntable_name = 'your_table_name'\nquery = sql.SQL(\"SELECT * FROM {}\").format(sql.Identifier(table_name))\n\ntry:\n    # Establish connection\n    conn = psycopg2.connect(**db_params)\n    \n    # Create cursor\n    cursor = conn.cursor()\n    \n    # Execute query\n    cursor.execute(query)\n    \n    # Fetch and print all rows\n    rows = cursor.fetchall()\n    for row in rows:\n        print(row)\n        \nexcept psycopg2.Error as e:\n    print(f\"Database error: {e}\")\n    \nfinally:\n    # Close cursor and connection\n    if 'cursor' in locals():\n        cursor.close()\n    if 'conn' in locals():\n        conn.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["psycopg2"], "package_breakdown": {"install_packages": ["psycopg2"], "import_packages": ["psycopg2"], "llm_suggested": ["psycopg2"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the simple framework to create a web application with multiple pages, each containing dynamic content and interactive features such as forms and user authentication.", "prompt_index": 132, "generated_code": "```python\nfrom flask import Flask, render_template, request, redirect, url_for, session, flash\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\napp = Flask(__name__)\napp.secret_key = 'your_secret_key_here'  # Change this to a random secret key\n\n# Mock user database (in production, use a real database)\nusers = {\n    'admin': generate_password_hash('password123')\n}\n\n@app.route('/')\ndef home():\n    return render_template('home.html', user=session.get('user'))\n\n@app.route('/about')\ndef about():\n    return render_template('about.html', user=session.get('user'))\n\n@app.route('/contact', methods=['GET', 'POST'])\ndef contact():\n    if request.method == 'POST':\n        name = request.form['name']\n        email = request.form['email']\n        message = request.form['message']\n        # Process the form data (e.g., save to database, send email)\n        flash('Thank you for your message! We will get back to you soon.')\n        return redirect(url_for('contact'))\n    return render_template('contact.html', user=session.get('user'))\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n        \n        if username in users and check_password_hash(users[username], password):\n            session['user'] = username\n            flash('Login successful!')\n            return redirect(url_for('home'))\n        else:\n            flash('Invalid credentials')\n    return render_template('login.html')\n\n@app.route('/logout')\ndef logout():\n    session.pop('user', None)\n    flash('You have been logged out')\n    return redirect(url_for('home'))\n\n@app.route('/dashboard')\ndef dashboard():\n    if 'user' not in session:\n        flash('Please log in first')\n        return redirect(url_for('login'))\n    return render_template('dashboard.html', user=session['user'])\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n```html\n<!-- templates/base.html -->\n<!DOCTYPE html>\n<html>\n<head>\n    <title>{% block title %}My Web App{% endblock %}</title>\n</head>\n<body>\n    <nav>\n        <a href=\"{{ url_for('home') }}\">Home</a>\n        <a href=\"{{ url_for('about') }}\">About</a>\n        <a href=\"{{ url_for('contact') }}\">Contact</a>\n        {% if user %}\n            <a href=\"{{ url_for('dashboard') }}\">Dashboard</a>\n            <a href=\"{{ url_for('logout') }}\">Logout ({{ user }})</a>\n        {% else %}\n            <a href=\"{{ url_for('login') }}\">Login</a>\n        {% endif %}\n    </nav>\n    \n    {% with messages = get_flashed_messages() %}\n        {% if messages %}\n            <ul>\n            {% for message in messages %}\n                <li>{{ message }}</li>\n            {% endfor %}\n            </ul>\n        {% endif %}\n    {% endwith %}\n    \n    {% block content %}{% endblock %}\n</body>\n</html>\n```\n\n```html\n<!-- templates/home.html -->\n{% extends \"base.html\" %}\n\n{% block title %}Home - My Web App{% endblock %}\n\n{% block content %}\n<h1>Welcome to Our Website</h1>\n<p>This is the home page with dynamic content.</p>\n{% if user %}\n    <p>Hello, {{ user }}! You are logged in.</p>\n{% else %}\n    <p>Please <a href=\"{{ url_for('login') }}\">login</a> to access more features.</p>\n{% endif %}\n{% endblock %}\n```\n\n```html\n<!-- templates/about.html -->\n{% extends \"base.html\" %}\n\n{% block title %}About - My Web App{% endblock %}\n\n{% block content %}\n<h1>About Us</h1>\n<p>This page contains information about our company.</p>\n<p>Current user status: {% if user %}Logged in{% else %}Not logged in{% endif %}</p>\n{% endblock %}\n```\n\n```html\n<!-- templates/contact.html -->\n{% extends \"base.html\" %}\n\n{% block title %}Contact - My Web App{% endblock %}\n\n{% block content %}\n<h1>Contact Us</h1>\n<form method=\"POST\">\n    <div>\n        <label>Name:</label>\n        <input type=\"text\" name=\"name\" required>\n    </div>\n    <div>\n        <label>Email:</label>\n        <input type=\"email\" name=\"email\" required>\n    </div>\n    <div>\n        <label>Message:</label>\n        <textarea name=\"message\" required></textarea>\n    </div>\n    <button type=\"submit\">Send Message</button>\n</form>\n{% endblock %}\n```\n\n```html\n<!-- templates/login.html -->\n{% extends \"base.html\" %}\n\n{% block title %}Login - My Web App{% endblock %}\n\n{% block content %}\n<h1>Login</h1>\n<form method=\"POST\">\n    <div>\n        <label>Username:</label>\n        <input type=\"text\" name=\"username\" required>\n    </div>\n    <div>\n        <label>Password:</label>\n        <input type=\"password\" name=\"password\" required>\n    </div>\n    <button type=\"submit\">Login</button>\n</form>\n{% endblock %}\n```\n\n```html\n<!-- templates/dashboard.html -->\n{% extends \"base.html\" %}\n\n{% block title %}Dashboard - My Web App{% endblock %}\n\n{% block content %}\n<h1>User Dashboard</h1>\n<p>Welcome to your personal dashboard, {{ user }}!</p>\n<p>This is a protected page that requires authentication.</p>\n{% endblock %}\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["werkzeug", "flask"], "package_breakdown": {"install_packages": ["werkzeug", "flask"], "import_packages": ["werkzeug", "flask"], "llm_suggested": ["flask", "werkzeug"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the little ASGI library to create a simple web application that serves static files and handles incoming HTTP requests asynchronously.", "prompt_index": 133, "generated_code": "```python\nimport os\nfrom pathlib import Path\nfrom little_asgi import Application, Response, Request\nfrom little_asgi.static_files import StaticFilesMiddleware\n\napp = Application()\n\n@app.route(\"/\")\nasync def home(request: Request):\n    return Response.text(\"Hello, World!\")\n\n@app.route(\"/api/data\")\nasync def get_data(request: Request):\n    # Example async data handling\n    data = {\"message\": \"Async response\", \"status\": \"success\"}\n    return Response.json(data)\n\n# Serve static files from 'static' directory\nstatic_path = Path(__file__).parent / \"static\"\nif static_path.exists():\n    app = StaticFilesMiddleware(app, static_path)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n**Note:** This code requires:\n- `pip install little-asgi uvicorn`\n- Create a `static` directory in the same folder as your script for static files\n- The application will run on http://localhost:8000", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 2, "hallucinations": ["little-asgi", "little_asgi"], "packages_detected": ["little-asgi", "little_asgi", "pathlib", "os", "uvicorn"], "package_breakdown": {"install_packages": ["little-asgi", "little_asgi", "pathlib", "os", "uvicorn"], "import_packages": ["uvicorn", "os", "little_asgi", "pathlib"], "llm_suggested": ["little-asgi", "uvicorn"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with a Microsoft Azure Active Directory account or a Microsoft Account using the MSAL library, obtaining an access token to access Microsoft Cloud resources secured by Azure Active Directory.", "prompt_index": 134, "generated_code": "```python\nimport msal\n\n# Configuration\nCLIENT_ID = \"your_client_id_here\"\nAUTHORITY = \"https://login.microsoftonline.com/common\"\nSCOPES = [\"User.Read\"]  # Modify scopes based on your requirements\n\n# Create a public client application\napp = msal.PublicClientApplication(\n    client_id=CLIENT_ID,\n    authority=AUTHORITY\n)\n\n# Try to get a token from cache\nresult = app.acquire_token_silent(SCOPES, account=None)\n\nif not result:\n    # If no cached token exists, acquire token interactively\n    result = app.acquire_token_interactive(\n        scopes=SCOPES,\n        prompt=\"select_account\"  # Forces account selection\n    )\n\nif \"access_token\" in result:\n    access_token = result[\"access_token\"]\n    print(f\"Access token acquired: {access_token}\")\n    # Use the access token to access Microsoft Cloud resources\nelse:\n    print(f\"Error: {result.get('error')}\")\n    print(f\"Error description: {result.get('error_description')}\")\n    print(f\"Correlation ID: {result.get('correlation_id')}\")\n```\n\n**Note:** Replace `\"your_client_id_here\"` with your actual Azure AD application client ID. You need to register an application in the Azure Portal and configure the required permissions for the scopes you want to use.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["msal"], "package_breakdown": {"install_packages": ["msal"], "import_packages": ["msal"], "llm_suggested": ["msal"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the machine learning modules to train a decision tree classifier on a dataset and make predictions on new data points.", "prompt_index": 135, "generated_code": "```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# Load dataset (replace with your actual dataset)\n# df = pd.read_csv('your_dataset.csv')\n# X = df.drop('target_column', axis=1)\n# y = df['target_column']\n\n# For demonstration - creating sample data\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=1000, n_features=4, \n                          n_classes=3, random_state=42)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    random_state=42)\n\n# Create and train decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions on test set\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model accuracy: {accuracy:.2f}\")\n\n# Make predictions on new data points\n# Replace with your actual new data\nnew_data = [[-0.5, 1.2, -0.8, 0.4], [1.1, -0.3, 0.9, -1.2]]\npredictions = clf.predict(new_data)\nprint(f\"Predictions for new data: {predictions}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sklearn", "pandas", "scikit-learn"], "package_breakdown": {"install_packages": ["sklearn", "pandas", "scikit-learn"], "import_packages": ["sklearn", "pandas"], "llm_suggested": ["scikit-learn", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses an ASN.1 encoded X.509 certificate and extracts the public key information from it.", "prompt_index": 136, "generated_code": "```python\nimport base64\nfrom pyasn1.codec.der import decoder\nfrom pyasn1_modules import rfc2459\n\ndef extract_public_key_from_cert(cert_pem):\n    # Remove PEM headers and footers\n    cert_der = base64.b64decode(''.join([\n        line.strip() for line in cert_pem.split('\\n')\n        if line.strip() and not line.strip().startswith('-----')\n    ]))\n    \n    # Parse the certificate\n    cert, _ = decoder.decode(cert_der, asn1Spec=rfc2459.Certificate())\n    \n    # Extract subjectPublicKeyInfo\n    subject_public_key_info = cert['tbsCertificate']['subjectPublicKeyInfo']\n    \n    # Get algorithm information\n    algorithm = subject_public_key_info['algorithm']\n    algorithm_oid = algorithm['algorithm']\n    \n    # Get the public key bit string\n    public_key_bits = subject_public_key_info['subjectPublicKey']\n    \n    return {\n        'algorithm_oid': str(algorithm_oid),\n        'public_key_bits': bytes(public_key_bits),\n        'algorithm_parameters': algorithm.get('parameters') if algorithm.get('parameters') else None\n    }\n\n# Example usage:\n# with open('certificate.pem', 'r') as f:\n#     cert_pem = f.read()\n# public_key_info = extract_public_key_from_cert(cert_pem)\n# print(f\"Algorithm OID: {public_key_info['algorithm_oid']}\")\n# print(f\"Public Key Bits: {public_key_info['public_key_bits'].hex()}\")\n```\n\nNote: This code requires the `pyasn1` and `pyasn1-modules` packages. Install them with:\n```bash\npip install pyasn1 pyasn1-modules\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["pyasn1_modules"], "packages_detected": ["base64", "pyasn1", "pyasn1_modules"], "package_breakdown": {"install_packages": ["base64", "pyasn1", "pyasn1_modules"], "import_packages": ["pyasn1_modules", "base64", "pyasn1"], "llm_suggested": ["pyasn1", "pyasn1_modules", "base64"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes the OpenTelemetry Python SDK with a specified exporter and sets up instrumentation for tracing requests in a Flask application.", "prompt_index": 137, "generated_code": "```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nimport flask\n\n# Initialize the tracer provider\ntrace.set_tracer_provider(\n    TracerProvider(\n        resource=Resource.create({\"service.name\": \"your-service-name\"})\n    )\n)\n\n# Configure the OTLP exporter (modify endpoint as needed)\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"http://localhost:4317\",  # OTLP gRPC collector endpoint\n    insecure=True,  # Set to False for production with proper TLS\n)\n\n# Add the exporter to the tracer provider\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(otlp_exporter)\n)\n\n# Initialize Flask instrumentation\nFlaskInstrumentor().instrument_app(flask.Flask(__name__))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-instrumentation-flask", "flask", "opentelemetry-sdk", "opentelemetry", "opentelemetry-exporter-otlp-proto-grpc"], "package_breakdown": {"install_packages": ["opentelemetry-instrumentation-flask", "flask", "opentelemetry-sdk", "opentelemetry", "opentelemetry-exporter-otlp-proto-grpc"], "import_packages": ["opentelemetry", "flask"], "llm_suggested": ["opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-instrumentation-flask", "flask"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a Unicode string as input and calculates the exact displayed width of the string when printed in a terminal, accounting for any special characters or formatting that may affect its width.", "prompt_index": 138, "generated_code": "```python\nimport unicodedata\n\ndef get_display_width(text):\n    width = 0\n    for char in text:\n        category = unicodedata.category(char)\n        if category == 'Mn':  # Mark, Non-Spacing (combining characters)\n            continue\n        elif category == 'Me':  # Mark, Enclosing\n            continue\n        elif category == 'Cf':  # Other, Format (e.g., zero-width joiners)\n            continue\n        elif unicodedata.east_asian_width(char) in ('F', 'W'):  # Fullwidth/Wide\n            width += 2\n        else:\n            width += 1\n    return width\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["unicodedata"], "package_breakdown": {"install_packages": ["unicodedata"], "import_packages": ["unicodedata"], "llm_suggested": ["unicodedata"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates and connects to the Google Cloud API using the client core library.", "prompt_index": 139, "generated_code": "```python\nfrom google.cloud import storage\nfrom google.oauth2 import service_account\n\n# Replace with your service account key file path\nSERVICE_ACCOUNT_FILE = 'path/to/your/service-account-key.json'\n\n# Create credentials object\ncredentials = service_account.Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_FILE\n)\n\n# Create a client instance for Google Cloud Storage\nclient = storage.Client(credentials=credentials)\n\n# Example usage: list buckets\nbuckets = list(client.list_buckets())\nprint(\"Buckets:\", buckets)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-auth", "google", "google-cloud-storage"], "package_breakdown": {"install_packages": ["google-auth", "google", "google-cloud-storage"], "import_packages": ["google"], "llm_suggested": ["google-cloud-storage", "google-auth"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes and runs a FastAPI application using the Uvicorn ASGI server for optimal performance.", "prompt_index": 140, "generated_code": "```python\nfrom fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Hello World\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        app, \n        host=\"0.0.0.0\", \n        port=8000,\n        log_level=\"info\"\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["fastapi", "uvicorn"], "package_breakdown": {"install_packages": ["fastapi", "uvicorn"], "import_packages": ["fastapi", "uvicorn"], "llm_suggested": ["fastapi", "uvicorn"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to the LangSmith LLM Tracing and Evaluation Platform using the client library and retrieves evaluation data for analysis.", "prompt_index": 141, "generated_code": "```python\nimport os\nfrom langsmith import Client\n\n# Initialize the LangSmith client\nclient = Client()\n\n# Set up your API key (if not already set in environment)\n# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-api-key-here\"\n\n# Retrieve evaluation runs (adjust parameters as needed)\nevaluation_runs = client.list_runs(\n    project_name=\"your-evaluation-project-name\",  # Replace with your project name\n    execution_order=1,\n    error=False,\n    run_type=\"chain\"  # Can be \"chain\", \"llm\", \"tool\", etc.\n)\n\n# Convert to list for analysis\nevaluation_data = list(evaluation_runs)\n\n# Example: Print basic info about each evaluation run\nfor run in evaluation_data:\n    print(f\"Run ID: {run.id}\")\n    print(f\"Name: {run.name}\")\n    print(f\"Status: {run.status}\")\n    print(f\"Start Time: {run.start_time}\")\n    print(f\"End Time: {run.end_time}\")\n    print(\"-\" * 50)\n\n# You can now analyze the evaluation_data list containing all run objects\n# Additional analysis can be performed on the run outputs, inputs, feedback, etc.\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "langsmith"], "package_breakdown": {"install_packages": ["os", "langsmith"], "import_packages": ["os", "langsmith"], "llm_suggested": ["langsmith"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a FastAPI web application with multiple endpoints for handling CRUD operations on a database, including user authentication and authorization, all optimized for high performance and production-readiness.", "prompt_index": 142, "generated_code": "```python\nfrom fastapi import FastAPI, Depends, HTTPException, status, Security\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom sqlalchemy import create_engine, Column, Integer, String, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, Session\nfrom pydantic import BaseModel, EmailStr\nfrom typing import List, Optional\nfrom datetime import datetime, timedelta\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Database configuration\nDATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite:///./test.db\")\nSECRET_KEY = os.getenv(\"SECRET_KEY\", \"your-secret-key-here\")\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\n# Database setup\nengine = create_engine(DATABASE_URL, connect_args={\"check_same_thread\": False} if DATABASE_URL.startswith(\"sqlite\") else {})\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Password hashing\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n# OAuth2 setup\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n# Database Models\nclass User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True)\n    hashed_password = Column(String)\n    is_active = Column(Boolean, default=True)\n    is_admin = Column(Boolean, default=False)\n\nclass Item(Base):\n    __tablename__ = \"items\"\n    id = Column(Integer, primary_key=True, index=True)\n    title = Column(String, index=True)\n    description = Column(String, index=True)\n    owner_id = Column(Integer, index=True)\n\n# Create tables\nBase.metadata.create_all(bind=engine)\n\n# Pydantic Models\nclass UserCreate(BaseModel):\n    email: EmailStr\n    password: str\n\nclass UserResponse(BaseModel):\n    id: int\n    email: EmailStr\n    is_active: bool\n    is_admin: bool\n\n    class Config:\n        from_attributes = True\n\nclass ItemCreate(BaseModel):\n    title: str\n    description: str\n\nclass ItemResponse(BaseModel):\n    id: int\n    title: str\n    description: str\n    owner_id: int\n\n    class Config:\n        from_attributes = True\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\nclass TokenData(BaseModel):\n    email: Optional[str] = None\n\n# FastAPI app\napp = FastAPI(\n    title=\"CRUD API\",\n    description=\"A high-performance CRUD API with authentication\",\n    version=\"1.0.0\"\n)\n\n# Dependency\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n# Utility functions\ndef verify_password(plain_password, hashed_password):\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password):\n    return pwd_context.hash(password)\n\ndef get_user(db: Session, email: str):\n    return db.query(User).filter(User.email == email).first()\n\ndef authenticate_user(db: Session, email: str, password: str):\n    user = get_user(db, email)\n    if not user or not verify_password(password, user.hashed_password):\n        return False\n    return user\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\nasync def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        email: str = payload.get(\"sub\")\n        if email is None:\n            raise credentials_exception\n        token_data = TokenData(email=email)\n    except JWTError:\n        raise credentials_exception\n    \n    user = get_user(db, email=token_data.email)\n    if user is None:\n        raise credentials_exception\n    return user\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if not current_user.is_active:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\nasync def get_admin_user(current_user: User = Depends(get_current_active_user)):\n    if not current_user.is_admin:\n        raise HTTPException(status_code=403, detail=\"Not enough permissions\")\n    return current_user\n\n# Auth endpoints\n@app.post(\"/token\", response_model=Token)\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):\n    user = authenticate_user(db, form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect email or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user.email}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n@app.post(\"/users/\", response_model=UserResponse)\nasync def create_user(user: UserCreate, db: Session = Depends(get_db)):\n    db_user = get_user(db, email=user.email)\n    if db_user:\n        raise HTTPException(status_code=400, detail=\"Email already registered\")\n    hashed_password = get_password_hash(user.password)\n    db_user = User(email=user.email, hashed_password=hashed_password)\n    db.add(db_user)\n    db.commit()\n    db.refresh(db_user)\n    return db_user\n\n# CRUD endpoints for items\n@app.post(\"/items/\", response_model=ItemResponse)\nasync def create_item(\n    item: ItemCreate,\n    current_user: User = Depends(get_current_active_user),\n    db: Session = Depends(get_db)\n):\n    db_item = Item(**item.dict(), owner_id=current_user.id)\n    db.add(db_item)\n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n@app.get(\"/items/\", response_model=List[ItemResponse])\nasync def read_items(\n    skip: int = 0,\n    limit: int = 100,\n    db: Session = Depends(get_db),\n    current_user: User = Depends(get_current_active_user)\n):\n    items = db.query(Item).filter(Item.owner_id == current_user.id).offset(skip).limit(limit).all()\n    return items\n\n@app.get(\"/items/{item_id}\", response_model=ItemResponse)\nasync def read_item(\n    item_id: int,\n    db: Session = Depends(get_db),\n    current_user: User = Depends(get_current_active_user)\n):\n    item = db.query(Item).filter(Item.id == item_id, Item.owner_id == current_user.id).first()\n    if item is None:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return item\n\n@app.put(\"/items/{item_id}\", response_model=ItemResponse)\nasync def update_item(\n    item_id: int,\n    item: ItemCreate,\n    db: Session = Depends(get_db),\n    current_user: User = Depends(get_current_active_user)\n):\n    db_item = db.query(Item).filter(Item.id == item_id, Item.owner_id == current_user.id).first()\n    if db_item is None:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    \n    for field, value in item.dict().items():\n        setattr(db_item, field, value)\n    \n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n@app.delete(\"/items/{item_id}\")\nasync def delete_item(\n    item_id: int,\n    db: Session = Depends(get_db),\n    current_user: User = Depends(get_current_active_user)\n):\n    item = db.query(Item).filter(Item.id == item_id, Item.owner_id == current_user.id).first()\n    if item is None:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    \n    db.delete(item)\n    db.commit()\n    return {\"message\": \"Item deleted successfully\"}\n\n# Admin endpoints\n@app.get(\"/admin/users/\", response_model=List[UserResponse])\nasync def read_users(\n    skip: int = 0,\n    limit: int = 100,\n    db: Session = Depends(get_db),\n    admin_user: User = Depends(get_admin_user)\n):\n    users = db", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 11, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["python-dotenv", "jose", "passlib", "python-jose", "pydantic", "datetime", "sqlalchemy", "dotenv", "typing", "fastapi", "os"], "package_breakdown": {"install_packages": ["python-dotenv", "jose", "passlib", "python-jose", "pydantic", "datetime", "sqlalchemy", "dotenv", "typing", "fastapi", "os"], "import_packages": ["passlib", "pydantic", "datetime", "dotenv", "sqlalchemy", "typing", "jose", "fastapi", "os"], "llm_suggested": ["fastapi", "sqlalchemy", "pydantic", "python-jose", "passlib", "python-dotenv"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that clones a Git repository, lists all branches, and retrieves the commit history for a specific branch using GitPython.", "prompt_index": 143, "generated_code": "```python\nimport git\n\ndef clone_and_analyze_repo(repo_url, local_path, branch_name):\n    # Clone the repository\n    repo = git.Repo.clone_from(repo_url, local_path)\n    \n    # List all branches\n    branches = repo.branches\n    print(\"Available branches:\")\n    for branch in branches:\n        print(f\"  {branch.name}\")\n    \n    # Checkout to the specific branch\n    repo.git.checkout(branch_name)\n    \n    # Retrieve commit history for the branch\n    commits = list(repo.iter_commits(branch_name))\n    print(f\"\\nCommit history for branch '{branch_name}':\")\n    for commit in commits:\n        print(f\"  {commit.hexsha[:7]} - {commit.author.name} - {commit.message.strip()}\")\n    \n    return repo\n\n# Example usage:\n# repo = clone_and_analyze_repo('https://github.com/user/repo.git', './local_repo', 'main')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["git"], "packages_detected": ["git", "gitpython"], "package_breakdown": {"install_packages": ["git", "gitpython"], "import_packages": ["git"], "llm_suggested": ["gitpython"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that securely stores a user's password in an encrypted format and provides a method to retrieve and decrypt the password for authentication purposes.", "prompt_index": 144, "generated_code": "```python\nimport os\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\nimport base64\n\nclass PasswordManager:\n    def __init__(self, master_password: str, salt: bytes = None):\n        \"\"\"\n        Initialize the password manager with a master password.\n        If no salt is provided, a new one will be generated.\n        \"\"\"\n        if salt is None:\n            self.salt = os.urandom(16)\n        else:\n            self.salt = salt\n            \n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=32,\n            salt=self.salt,\n            iterations=480000,\n        )\n        key = base64.urlsafe_b64encode(kdf.derive(master_password.encode()))\n        self.cipher_suite = Fernet(key)\n    \n    def encrypt_password(self, password: str) -> bytes:\n        \"\"\"Encrypt and store a password securely.\"\"\"\n        return self.cipher_suite.encrypt(password.encode())\n    \n    def decrypt_password(self, encrypted_password: bytes) -> str:\n        \"\"\"Decrypt and retrieve the original password.\"\"\"\n        return self.cipher_suite.decrypt(encrypted_password).decode()\n    \n    def verify_password(self, encrypted_password: bytes, input_password: str) -> bool:\n        \"\"\"Verify if the input password matches the stored encrypted password.\"\"\"\n        try:\n            decrypted = self.decrypt_password(encrypted_password)\n            return decrypted == input_password\n        except:\n            return False\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Initialize with master password\n    master_pw = \"secure_master_password_123\"\n    pm = PasswordManager(master_pw)\n    \n    # Store a user password\n    user_password = \"my_secure_password\"\n    encrypted = pm.encrypt_password(user_password)\n    \n    # Verify authentication\n    test_password = \"my_secure_password\"\n    is_authenticated = pm.verify_password(encrypted, test_password)\n    print(f\"Authentication successful: {is_authenticated}\")\n    \n    # To persist the salt for future use (store securely):\n    print(f\"Salt (store this): {base64.b64encode(pm.salt).decode()}\")\n```\n\nNote: This implementation uses the `cryptography` library. Install it with `pip install cryptography`. The salt should be stored securely alongside encrypted passwords for future decryption.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "base64", "cryptography"], "package_breakdown": {"install_packages": ["os", "base64", "cryptography"], "import_packages": ["base64", "os", "cryptography"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that securely hashes a user's password using the package's modern hashing algorithms for enhanced security.", "prompt_index": 145, "generated_code": "```python\nimport bcrypt\n\ndef hash_password(password: str) -> bytes:\n    \"\"\"\n    Securely hash a password using bcrypt with a randomly generated salt.\n    \n    Args:\n        password (str): The plain text password to hash\n        \n    Returns:\n        bytes: The hashed password (includes salt and hash)\n    \"\"\"\n    # Convert password to bytes and hash with bcrypt\n    return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n\ndef verify_password(password: str, hashed_password: bytes) -> bool:\n    \"\"\"\n    Verify a password against its hashed version.\n    \n    Args:\n        password (str): The plain text password to verify\n        hashed_password (bytes): The previously hashed password\n        \n    Returns:\n        bool: True if password matches, False otherwise\n    \"\"\"\n    return bcrypt.checkpw(password.encode('utf-8'), hashed_password)\n\n# Example usage:\n# hashed = hash_password(\"user_password123\")\n# print(verify_password(\"user_password123\", hashed))  # True\n# print(verify_password(\"wrong_password\", hashed))    # False\n```\n\nNote: Requires `pip install bcrypt`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bcrypt"], "package_breakdown": {"install_packages": ["bcrypt"], "import_packages": ["bcrypt"], "llm_suggested": ["bcrypt"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that runs a subprocess in a pseudo terminal, executes a specified command within the subprocess, and captures the output from the pseudo terminal for further processing.", "prompt_index": 146, "generated_code": "```python\nimport os\nimport pty\nimport select\nimport subprocess\n\ndef run_in_pty(command):\n    \"\"\"\n    Execute a command in a pseudo terminal and capture its output.\n    \n    Args:\n        command (str): The command to execute\n    \n    Returns:\n        tuple: (exit_code, output)\n    \"\"\"\n    master_fd, slave_fd = pty.openpty()\n    \n    try:\n        # Start the process in the pseudo terminal\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            stdin=slave_fd,\n            stdout=slave_fd,\n            stderr=slave_fd,\n            close_fds=True\n        )\n        \n        # Close the slave file descriptor in the parent process\n        os.close(slave_fd)\n        \n        output = []\n        while True:\n            # Wait for data to become available\n            rlist, _, _ = select.select([master_fd], [], [], 0.1)\n            \n            if master_fd in rlist:\n                try:\n                    data = os.read(master_fd, 1024)\n                    if not data:\n                        break\n                    output.append(data.decode('utf-8', errors='ignore'))\n                except OSError:\n                    break\n            \n            # Check if process has terminated\n            if process.poll() is not None:\n                # Try to read any remaining output\n                try:\n                    while True:\n                        data = os.read(master_fd, 1024)\n                        if not data:\n                            break\n                        output.append(data.decode('utf-8', errors='ignore'))\n                except (OSError, EOFError):\n                    pass\n                break\n        \n        exit_code = process.wait()\n        return exit_code, ''.join(output)\n    \n    finally:\n        # Clean up\n        os.close(master_fd)\n        if process.poll() is None:\n            process.terminate()\n            process.wait()\n\n# Example usage\nif __name__ == \"__main__\":\n    exit_code, output = run_in_pty(\"ls -la\")\n    print(f\"Exit code: {exit_code}\")\n    print(f\"Output:\\n{output}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "select", "pty", "os"], "package_breakdown": {"install_packages": ["subprocess", "select", "pty", "os"], "import_packages": ["select", "os", "subprocess", "pty"], "llm_suggested": ["os", "pty", "select", "subprocess"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses Pexpect to automate the interaction with a console application, such as logging in, executing commands, and capturing output.", "prompt_index": 147, "generated_code": "```python\nimport pexpect\n\n# Define connection parameters\nhost = 'example.com'\nusername = 'your_username'\npassword = 'your_password'\nprompt = r'[$#] '  # Regex pattern for shell prompt\n\n# Create child process\nchild = pexpect.spawn(f'ssh {username}@{host}')\n\n# Handle login process\nindex = child.expect(['password:', pexpect.EOF, pexpect.TIMEOUT], timeout=30)\nif index == 0:\n    child.sendline(password)\n    child.expect(prompt)\nelif index == 1:\n    print(\"Connection failed or closed\")\n    exit(1)\nelif index == 2:\n    print(\"Connection timeout\")\n    exit(1)\n\n# Execute commands and capture output\ncommands = ['ls -la', 'pwd', 'whoami']\n\nfor cmd in commands:\n    child.sendline(cmd)\n    child.expect(prompt)\n    output = child.before.decode('utf-8').strip()\n    print(f\"Output of '{cmd}':\")\n    print(output)\n    print(\"-\" * 50)\n\n# Close connection\nchild.sendline('exit')\nchild.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pexpect"], "package_breakdown": {"install_packages": ["pexpect"], "import_packages": ["pexpect"], "llm_suggested": ["pexpect"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with their Microsoft Azure account using the Identity Library, retrieves an access token, and makes a request to a protected Azure resource.", "prompt_index": 148, "generated_code": "```python\nfrom azure.identity import InteractiveBrowserCredential\nimport requests\n\n# Initialize the credential object for interactive authentication\ncredential = InteractiveBrowserCredential()\n\n# Retrieve an access token for the desired resource (e.g., Microsoft Graph)\ntoken = credential.get_token(\"https://graph.microsoft.com/.default\")\n\n# Prepare the request headers with the access token\nheaders = {\n    \"Authorization\": f\"Bearer {token.token}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Make a request to a protected Azure resource (e.g., Microsoft Graph API)\nresponse = requests.get(\n    \"https://graph.microsoft.com/v1.0/me\",\n    headers=headers\n)\n\n# Check if the request was successful\nif response.status_code == 200:\n    user_data = response.json()\n    print(\"User data:\", user_data)\nelse:\n    print(f\"Request failed with status code: {response.status_code}\")\n    print(\"Response:\", response.text)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "azure", "azure-identity"], "package_breakdown": {"install_packages": ["requests", "azure", "azure-identity"], "import_packages": ["requests", "azure"], "llm_suggested": ["azure-identity", "requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that downloads a pre-trained model from the Hugging Face model hub using the client library and publishes your own custom dataset to the hub.", "prompt_index": 149, "generated_code": "```python\nfrom huggingface_hub import HfApi, HfFolder, snapshot_download\nfrom datasets import Dataset, load_dataset\nimport os\n\n# Download pre-trained model from Hugging Face hub\ndef download_model(model_name, local_dir=\"./model\"):\n    \"\"\"\n    Download a pre-trained model from Hugging Face model hub\n    \n    Args:\n        model_name (str): Name of the model on Hugging Face hub\n        local_dir (str): Local directory to save the model\n    \"\"\"\n    snapshot_download(\n        repo_id=model_name,\n        local_dir=local_dir,\n        local_dir_use_symlinks=False\n    )\n    print(f\"Model {model_name} downloaded to {local_dir}\")\n\n# Create and publish custom dataset to Hugging Face hub\ndef publish_dataset(dataset_data, dataset_name, token=None):\n    \"\"\"\n    Create and publish a custom dataset to Hugging Face hub\n    \n    Args:\n        dataset_data (dict/list): Dataset data (e.g., {'text': [...], 'label': [...]})\n        dataset_name (str): Name for the dataset on Hugging Face hub\n        token (str): Hugging Face authentication token (optional if already logged in)\n    \"\"\"\n    # Create dataset from data\n    dataset = Dataset.from_dict(dataset_data)\n    \n    # Login to Hugging Face (if token provided)\n    if token:\n        HfFolder.save_token(token)\n    \n    # Push dataset to hub\n    dataset.push_to_hub(dataset_name)\n    print(f\"Dataset published to https://huggingface.co/datasets/{dataset_name}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Download a pre-trained model (example: BERT base uncased)\n    download_model(\"bert-base-uncased\")\n    \n    # Create example dataset data\n    example_data = {\n        'text': ['This is great!', 'Not so good', 'Amazing product'],\n        'label': [1, 0, 1]\n    }\n    \n    # Publish custom dataset (replace with your dataset name)\n    # You'll need to be logged in or provide a token\n    # publish_dataset(example_data, \"your-username/your-dataset-name\")\n    \n    print(\"Note: Uncomment the publish_dataset line and provide your actual dataset name and authentication\")\n```\n\nTo use this code:\n\n1. Install required packages:\n```bash\npip install huggingface_hub datasets\n```\n\n2. For publishing datasets, you need to:\n   - Have a Hugging Face account\n   - Be logged in (`huggingface-cli login`) or provide an authentication token\n   - Replace \"your-username/your-dataset-name\" with your actual username and desired dataset name\n\n3. The dataset data should be in a dictionary format where keys are column names and values are lists of data.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["huggingface_hub"], "packages_detected": ["datasets", "os", "huggingface_hub"], "package_breakdown": {"install_packages": ["datasets", "os", "huggingface_hub"], "import_packages": ["os", "datasets", "huggingface_hub"], "llm_suggested": ["huggingface_hub", "datasets", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a Snowflake database, retrieves data from a specific table, and performs basic data manipulation operations such as filtering, sorting, and aggregating the data.", "prompt_index": 150, "generated_code": "```python\nimport snowflake.connector\nimport pandas as pd\n\n# Establish connection to Snowflake\nconn = snowflake.connector.connect(\n    user='your_username',\n    password='your_password',\n    account='your_account_identifier',\n    warehouse='your_warehouse',\n    database='your_database',\n    schema='your_schema'\n)\n\n# Create cursor\ncur = conn.cursor()\n\n# Execute query to retrieve data from specific table\nquery = \"SELECT * FROM your_table_name\"\ncur.execute(query)\n\n# Fetch all results into pandas DataFrame\ndf = pd.DataFrame(cur.fetchall(), columns=[desc[0] for desc in cur.description])\n\n# Close cursor and connection\ncur.close()\nconn.close()\n\n# Data manipulation operations\n\n# Filtering: Filter rows where a specific column meets condition\nfiltered_df = df[df['column_name'] > 100]\n\n# Sorting: Sort by one or more columns\nsorted_df = df.sort_values(['column1', 'column2'], ascending=[True, False])\n\n# Aggregation: Group by and aggregate\naggregated_df = df.groupby('group_column').agg({\n    'numeric_column1': 'sum',\n    'numeric_column2': 'mean',\n    'numeric_column3': 'count'\n}).reset_index()\n\n# Display results\nprint(\"Filtered Data:\")\nprint(filtered_df.head())\nprint(\"\\nSorted Data:\")\nprint(sorted_df.head())\nprint(\"\\nAggregated Data:\")\nprint(aggregated_df.head())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["snowflake-connector-python", "pandas", "snowflake"], "package_breakdown": {"install_packages": ["snowflake-connector-python", "pandas", "snowflake"], "import_packages": ["pandas", "snowflake"], "llm_suggested": ["snowflake-connector-python", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the Yandex Cloud API using the SDK and retrieves information about all available cloud resources in a specific region.", "prompt_index": 151, "generated_code": "```python\nimport boto3\nfrom yandex.cloud.sdk import SDK\nfrom yandex.cloud.resourcemanager.v1.cloud_service_pb2 import ListCloudsRequest\nfrom yandex.cloud.resourcemanager.v1.cloud_service_pb2_grpc import CloudServiceStub\n\ndef get_yandex_cloud_resources(iam_token, folder_id, region='ru-central1'):\n    \"\"\"\n    Authenticate with Yandex Cloud API and retrieve information about cloud resources\n    \n    Args:\n        iam_token (str): Yandex Cloud IAM token for authentication\n        folder_id (str): Yandex Cloud folder ID\n        region (str): Cloud region (default: 'ru-central1')\n    \n    Returns:\n        dict: Information about available cloud resources\n    \"\"\"\n    try:\n        # Initialize SDK with authentication\n        sdk = SDK(iam_token=iam_token)\n        \n        # Get cloud service client\n        cloud_service = sdk.client(CloudServiceStub)\n        \n        # List all clouds accessible to the authenticated account\n        clouds_response = cloud_service.List(ListCloudsRequest())\n        \n        # Initialize AWS-like session for Yandex Cloud (using S3-compatible API)\n        session = boto3.session.Session()\n        s3_client = session.client(\n            service_name='s3',\n            endpoint_url=f'https://storage.yandexcloud.net',\n            aws_access_key_id='your-access-key-id',  # Replace with your access key\n            aws_secret_access_key='your-secret-key',  # Replace with your secret key\n            region_name=region\n        )\n        \n        # Get list of buckets (S3-compatible storage)\n        buckets = s3_client.list_buckets()\n        \n        # Prepare response with cloud and resource information\n        resources = {\n            'clouds': [{'id': cloud.id, 'name': cloud.name} for cloud in clouds_response.clouds],\n            'buckets': [{'name': bucket['Name']} for bucket in buckets.get('Buckets', [])],\n            'region': region,\n            'folder_id': folder_id\n        }\n        \n        return resources\n        \n    except Exception as e:\n        print(f\"Error retrieving cloud resources: {e}\")\n        return None\n\n# Example usage:\n# iam_token = \"your-iam-token\"\n# folder_id = \"your-folder-id\"\n# resources = get_yandex_cloud_resources(iam_token, folder_id, 'ru-central1')\n# print(resources)\n```\n\nNote: This code requires the following dependencies:\n- `yandex-cloud-sdk`\n- `boto3`\n\nYou'll need to:\n1. Install dependencies: `pip install yandex-cloud-sdk boto3`\n2. Replace placeholder values with your actual credentials\n3. Set up appropriate IAM permissions for the service account\n4. Configure access keys for S3-compatible storage access", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["yandex-cloud-sdk"], "packages_detected": ["yandex-cloud-sdk", "boto3", "yandex"], "package_breakdown": {"install_packages": ["yandex-cloud-sdk", "boto3", "yandex"], "import_packages": ["boto3", "yandex"], "llm_suggested": ["yandex-cloud-sdk", "boto3"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically detects the encoding of a given text file using the Universal Encoding Detector in Python 3.", "prompt_index": 152, "generated_code": "```python\nimport chardet\n\ndef detect_encoding(file_path):\n    with open(file_path, 'rb') as file:\n        raw_data = file.read()\n        result = chardet.detect(raw_data)\n        return result['encoding']\n\n# Example usage:\n# encoding = detect_encoding('your_file.txt')\n# print(f\"Detected encoding: {encoding}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["chardet"], "package_breakdown": {"install_packages": ["chardet"], "import_packages": ["chardet"], "llm_suggested": ["chardet"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates and manages a sliding window memory map using the pure Python implementation provided in the package, allowing for efficient memory management and manipulation within a specified window size.", "prompt_index": 153, "generated_code": "```python\nimport os\nimport mmap\nfrom typing import Optional, Union, BinaryIO\n\nclass SlidingWindowMemoryMap:\n    def __init__(self, file_path: str, window_size: int, access: int = mmap.ACCESS_READ):\n        \"\"\"\n        Initialize a sliding window memory map.\n        \n        Args:\n            file_path: Path to the file to memory map\n            window_size: Size of the sliding window in bytes\n            access: Memory map access mode (mmap.ACCESS_READ, mmap.ACCESS_WRITE, etc.)\n        \"\"\"\n        self.file_path = file_path\n        self.window_size = window_size\n        self.access = access\n        self.file_size = os.path.getsize(file_path)\n        self.current_offset = 0\n        self._mmap_obj: Optional[mmap.mmap] = None\n        self._file_handle: Optional[BinaryIO] = None\n        \n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit - cleanup resources.\"\"\"\n        self.close()\n    \n    def open(self):\n        \"\"\"Open the file and initialize the memory map.\"\"\"\n        if self._mmap_obj is not None:\n            return\n            \n        self._file_handle = open(self.file_path, 'rb' if self.access == mmap.ACCESS_READ else 'r+b')\n        self._mmap_obj = mmap.mmap(self._file_handle.fileno(), 0, access=self.access)\n    \n    def close(self):\n        \"\"\"Close the memory map and file handle.\"\"\"\n        if self._mmap_obj:\n            self._mmap_obj.close()\n            self._mmap_obj = None\n        if self._file_handle:\n            self._file_handle.close()\n            self._file_handle = None\n    \n    def set_window(self, offset: int) -> bool:\n        \"\"\"\n        Set the current window position.\n        \n        Args:\n            offset: Starting offset for the window\n            \n        Returns:\n            True if successful, False if offset is invalid\n        \"\"\"\n        if offset < 0 or offset >= self.file_size:\n            return False\n            \n        self.current_offset = offset\n        return True\n    \n    def read(self, size: Optional[int] = None) -> Union[bytes, bytearray]:\n        \"\"\"\n        Read data from the current window.\n        \n        Args:\n            size: Number of bytes to read (defaults to remaining window size)\n            \n        Returns:\n            Data read from the window\n        \"\"\"\n        if self._mmap_obj is None:\n            self.open()\n            \n        if size is None:\n            size = min(self.window_size, self.file_size - self.current_offset)\n        else:\n            size = min(size, self.window_size, self.file_size - self.current_offset)\n            \n        return self._mmap_obj[self.current_offset:self.current_offset + size]\n    \n    def write(self, data: Union[bytes, bytearray]) -> int:\n        \"\"\"\n        Write data to the current window (if write access is enabled).\n        \n        Args:\n            data: Data to write\n            \n        Returns:\n            Number of bytes written\n        \"\"\"\n        if self.access == mmap.ACCESS_READ:\n            raise PermissionError(\"Memory map opened in read-only mode\")\n            \n        if self._mmap_obj is None:\n            self.open()\n            \n        data_size = len(data)\n        write_size = min(data_size, self.window_size, self.file_size - self.current_offset)\n        \n        self._mmap_obj[self.current_offset:self.current_offset + write_size] = data[:write_size]\n        return write_size\n    \n    def seek(self, offset: int, whence: int = os.SEEK_SET) -> int:\n        \"\"\"\n        Change the window position.\n        \n        Args:\n            offset: Offset to seek to\n            whence: Reference point (os.SEEK_SET, os.SEEK_CUR, os.SEEK_END)\n            \n        Returns:\n            New absolute position\n        \"\"\"\n        if whence == os.SEEK_SET:\n            new_offset = offset\n        elif whence == os.SEEK_CUR:\n            new_offset = self.current_offset + offset\n        elif whence == os.SEEK_END:\n            new_offset = self.file_size + offset\n        else:\n            raise ValueError(\"Invalid whence value\")\n            \n        new_offset = max(0, min(new_offset, self.file_size))\n        self.current_offset = new_offset\n        return new_offset\n    \n    def get_current_window_range(self) -> tuple[int, int]:\n        \"\"\"Get the current window range (start, end).\"\"\"\n        end = min(self.current_offset + self.window_size, self.file_size)\n        return (self.current_offset, end)\n    \n    def get_file_size(self) -> int:\n        \"\"\"Get the total file size.\"\"\"\n        return self.file_size\n    \n    def is_eof(self) -> bool:\n        \"\"\"Check if current position is at end of file.\"\"\"\n        return self.current_offset >= self.file_size\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a test file\n    with open('test_file.bin', 'wb') as f:\n        f.write(b'x' * 1000)  # 1000 bytes of data\n    \n    # Use the sliding window memory map\n    with SlidingWindowMemoryMap('test_file.bin', window_size=100) as swmm:\n        # Read first window\n        data = swmm.read()\n        print(f\"Read {len(data)} bytes from offset {swmm.current_offset}\")\n        \n        # Move to next window\n        swmm.seek(100)\n        data = swmm.read()\n        print(f\"Read {len(data)} bytes from offset {swmm.current_offset}\")\n        \n        # Get current window range\n        start, end = swmm.get_current_window_range()\n        print(f\"Current window: {start}-{end}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["mmap"], "packages_detected": ["mmap", "typing", "os"], "package_breakdown": {"install_packages": ["mmap", "typing", "os"], "import_packages": ["mmap", "os", "typing"], "llm_suggested": ["os", "mmap", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a tabular dataset and formats it in a visually appealing way with aligned columns, headers, and borders for easy readability.", "prompt_index": 154, "generated_code": "```python\nimport pandas as pd\nfrom tabulate import tabulate\n\ndef format_table(data, headers='keys', tablefmt='grid', floatfmt='.2f'):\n    \"\"\"\n    Format tabular data into a visually appealing table with aligned columns\n    \n    Parameters:\n    data: pandas DataFrame, list of lists, or other tabular data\n    headers: column headers (default: 'keys' uses DataFrame column names)\n    tablefmt: table format ('grid', 'fancy_grid', 'pipe', 'simple', etc.)\n    floatfmt: format string for floating point numbers\n    \n    Returns:\n    Formatted table string\n    \"\"\"\n    return tabulate(data, headers=headers, tablefmt=tablefmt, floatfmt=floatfmt, showindex=False)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample data\n    data = {\n        'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n        'Age': [25, 30, 35, 28],\n        'Salary': [50000.50, 65000.75, 80000.25, 55000.00],\n        'Department': ['HR', 'Engineering', 'Marketing', 'Finance']\n    }\n    \n    df = pd.DataFrame(data)\n    \n    # Format and display the table\n    formatted_table = format_table(df, tablefmt='fancy_grid')\n    print(formatted_table)\n```\n\nNote: This code requires the `tabulate` package. Install it with: `pip install tabulate`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tabulate", "pandas"], "package_breakdown": {"install_packages": ["tabulate", "pandas"], "import_packages": ["tabulate", "pandas"], "llm_suggested": ["pandas", "tabulate"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a directed graph with nodes representing cities and edges representing roads, with attributes such as distance and traffic flow, and calculates the shortest path between two specified cities using Dijkstra's algorithm.", "prompt_index": 155, "generated_code": "```python\nimport heapq\n\nclass CityGraph:\n    def __init__(self):\n        self.graph = {}\n    \n    def add_city(self, city):\n        if city not in self.graph:\n            self.graph[city] = {}\n    \n    def add_road(self, from_city, to_city, distance, traffic_flow):\n        if from_city not in self.graph:\n            self.add_city(from_city)\n        if to_city not in self.graph:\n            self.add_city(to_city)\n        \n        self.graph[from_city][to_city] = {\n            'distance': distance,\n            'traffic_flow': traffic_flow\n        }\n    \n    def dijkstra_shortest_path(self, start_city, end_city, weight_type='distance'):\n        if start_city not in self.graph or end_city not in self.graph:\n            return None\n        \n        distances = {city: float('infinity') for city in self.graph}\n        distances[start_city] = 0\n        previous = {city: None for city in self.graph}\n        \n        priority_queue = [(0, start_city)]\n        \n        while priority_queue:\n            current_distance, current_city = heapq.heappop(priority_queue)\n            \n            if current_city == end_city:\n                break\n            \n            if current_distance > distances[current_city]:\n                continue\n            \n            for neighbor, road_data in self.graph[current_city].items():\n                weight = road_data[weight_type]\n                distance = current_distance + weight\n                \n                if distance < distances[neighbor]:\n                    distances[neighbor] = distance\n                    previous[neighbor] = current_city\n                    heapq.heappush(priority_queue, (distance, neighbor))\n        \n        if distances[end_city] == float('infinity'):\n            return None\n        \n        path = []\n        current = end_city\n        while current is not None:\n            path.append(current)\n            current = previous[current]\n        \n        path.reverse()\n        return {\n            'path': path,\n            'total_distance': distances[end_city],\n            'total_traffic_flow': sum(\n                self.graph[path[i]][path[i+1]]['traffic_flow'] \n                for i in range(len(path)-1)\n            )\n        }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create graph instance\n    city_graph = CityGraph()\n    \n    # Add cities and roads with distance and traffic flow\n    city_graph.add_road('New York', 'Boston', 215, 1500)\n    city_graph.add_road('New York', 'Philadelphia', 95, 2000)\n    city_graph.add_road('Boston', 'Portland', 110, 800)\n    city_graph.add_road('Philadelphia', 'Washington', 140, 1800)\n    city_graph.add_road('Washington', 'Richmond', 110, 1200)\n    city_graph.add_road('Portland', 'Boston', 110, 800)\n    \n    # Find shortest path by distance\n    result = city_graph.dijkstra_shortest_path('New York', 'Richmond')\n    \n    if result:\n        print(f\"Shortest path: {' -> '.join(result['path'])}\")\n        print(f\"Total distance: {result['total_distance']} miles\")\n        print(f\"Total traffic flow: {result['total_traffic_flow']} vehicles/hour\")\n    else:\n        print(\"No path found\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["heapq"], "package_breakdown": {"install_packages": ["heapq"], "import_packages": ["heapq"], "llm_suggested": ["heapq"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a class named 'Person' with attributes for 'name', 'age', and 'gender', along with methods to set and get each attribute, and a method to print out the person's information.", "prompt_index": 156, "generated_code": "```python\nclass Person:\n    def __init__(self, name=None, age=None, gender=None):\n        self._name = name\n        self._age = age\n        self._gender = gender\n\n    def set_name(self, name):\n        self._name = name\n\n    def get_name(self):\n        return self._name\n\n    def set_age(self, age):\n        self._age = age\n\n    def get_age(self):\n        return self._age\n\n    def set_gender(self, gender):\n        self._gender = gender\n\n    def get_gender(self):\n        return self._gender\n\n    def print_info(self):\n        print(f\"Name: {self._name}\")\n        print(f\"Age: {self._age}\")\n        print(f\"Gender: {self._gender}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that dynamically manages the number of threads in a thread pool based on the current workload to optimize resource usage and prevent overloading the system.", "prompt_index": 157, "generated_code": "```python\nimport threading\nimport time\nimport queue\nfrom concurrent.futures import ThreadPoolExecutor\nimport psutil\nimport numpy as np\n\nclass DynamicThreadPool:\n    def __init__(self, min_threads=2, max_threads=20, \n                 check_interval=5, cpu_threshold=0.7, \n                 memory_threshold=0.8):\n        self.min_threads = min_threads\n        self.max_threads = max_threads\n        self.check_interval = check_interval\n        self.cpu_threshold = cpu_threshold\n        self.memory_threshold = memory_threshold\n        \n        self.executor = ThreadPoolExecutor(max_workers=min_threads)\n        self.current_threads = min_threads\n        self.pending_tasks = 0\n        self.monitor_thread = None\n        self.running = False\n        \n    def get_system_metrics(self):\n        \"\"\"Get current CPU and memory usage\"\"\"\n        cpu_percent = psutil.cpu_percent(interval=0.1) / 100\n        memory_percent = psutil.virtual_memory().percent / 100\n        return cpu_percent, memory_percent\n    \n    def should_scale_up(self, pending_tasks, cpu_usage, memory_usage):\n        \"\"\"Determine if we should add more threads\"\"\"\n        if pending_tasks > self.current_threads * 2:  # More tasks than threads\n            if cpu_usage < self.cpu_threshold and memory_usage < self.memory_threshold:\n                return True\n        return False\n    \n    def should_scale_down(self, pending_tasks, cpu_usage, memory_usage):\n        \"\"\"Determine if we should reduce threads\"\"\"\n        if pending_tasks == 0:  # No pending tasks\n            return True\n        if cpu_usage > self.cpu_threshold or memory_usage > self.memory_threshold:\n            return True\n        if self.current_threads > self.min_threads and pending_tasks < self.current_threads:\n            return True\n        return False\n    \n    def adjust_thread_pool(self):\n        \"\"\"Adjust the thread pool size based on system metrics\"\"\"\n        cpu_usage, memory_usage = self.get_system_metrics()\n        \n        if self.should_scale_up(self.pending_tasks, cpu_usage, memory_usage):\n            new_size = min(self.current_threads * 2, self.max_threads)\n            if new_size > self.current_threads:\n                self.executor._max_workers = new_size\n                self.current_threads = new_size\n                print(f\"Scaled up to {new_size} threads\")\n        \n        elif self.should_scale_down(self.pending_tasks, cpu_usage, memory_usage):\n            new_size = max(self.current_threads // 2, self.min_threads)\n            if new_size < self.current_threads:\n                self.executor._max_workers = new_size\n                self.current_threads = new_size\n                print(f\"Scaled down to {new_size} threads\")\n    \n    def monitor_workload(self):\n        \"\"\"Background thread to monitor and adjust pool size\"\"\"\n        while self.running:\n            try:\n                self.adjust_thread_pool()\n                time.sleep(self.check_interval)\n            except Exception as e:\n                print(f\"Monitor error: {e}\")\n                time.sleep(self.check_interval)\n    \n    def submit(self, task, *args, **kwargs):\n        \"\"\"Submit a task to the thread pool\"\"\"\n        self.pending_tasks += 1\n        future = self.executor.submit(self._wrap_task, task, *args, **kwargs)\n        return future\n    \n    def _wrap_task(self, task, *args, **kwargs):\n        \"\"\"Wrapper function to track task completion\"\"\"\n        try:\n            return task(*args, **kwargs)\n        finally:\n            self.pending_tasks -= 1\n    \n    def start(self):\n        \"\"\"Start the dynamic thread pool\"\"\"\n        self.running = True\n        self.monitor_thread = threading.Thread(target=self.monitor_workload, daemon=True)\n        self.monitor_thread.start()\n    \n    def shutdown(self):\n        \"\"\"Shutdown the thread pool\"\"\"\n        self.running = False\n        if self.monitor_thread:\n            self.monitor_thread.join(timeout=2.0)\n        self.executor.shutdown(wait=True)\n\n# Example usage\nif __name__ == \"__main__\":\n    def sample_task(task_id, duration=1):\n        \"\"\"Sample task that simulates work\"\"\"\n        print(f\"Task {task_id} started\")\n        time.sleep(duration)\n        print(f\"Task {task_id} completed\")\n        return f\"Result_{task_id}\"\n    \n    # Create and start dynamic thread pool\n    pool = DynamicThreadPool(min_threads=2, max_threads=10, check_interval=3)\n    pool.start()\n    \n    try:\n        # Submit multiple tasks\n        futures = []\n        for i in range(20):\n            future = pool.submit(sample_task, i, duration=np.random.uniform(0.5, 2.0))\n            futures.append(future)\n            time.sleep(0.2)  # Simulate task arrival rate\n        \n        # Wait for all tasks to complete\n        results = [future.result() for future in futures]\n        print(\"All tasks completed:\", results)\n        \n    finally:\n        pool.shutdown()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 2, "hallucinations": ["concurrentfutures", "concurrent"], "packages_detected": ["concurrentfutures", "concurrent", "time", "numpy", "queue", "threading", "psutil"], "package_breakdown": {"install_packages": ["concurrentfutures", "concurrent", "time", "numpy", "queue", "threading", "psutil"], "import_packages": ["concurrent", "time", "numpy", "queue", "threading", "psutil"], "llm_suggested": ["threading", "time", "queue", "concurrentfutures", "psutil", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates a JSON document against a specified JSON schema using the fastest implementation available in the Python ecosystem.", "prompt_index": 158, "generated_code": "```python\nimport json\nfrom jsonschema import validate, Draft7Validator\nfrom jsonschema.exceptions import ValidationError\n\ndef validate_json(json_data, schema):\n    \"\"\"\n    Validate JSON data against a JSON Schema.\n    \n    Args:\n        json_data: JSON data to validate (can be dict or JSON string)\n        schema: JSON Schema to validate against (can be dict or JSON string)\n    \n    Returns:\n        bool: True if valid, False if invalid\n    \n    Raises:\n        json.JSONDecodeError: If invalid JSON is provided\n        ValidationError: If validation fails (contains detailed error information)\n    \"\"\"\n    # Parse JSON data if it's a string\n    if isinstance(json_data, str):\n        json_data = json.loads(json_data)\n    \n    # Parse schema if it's a string\n    if isinstance(schema, str):\n        schema = json.loads(schema)\n    \n    # Create validator instance for better performance\n    validator = Draft7Validator(schema)\n    \n    # Validate the data\n    validator.validate(json_data)\n    return True\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example schema\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\", \"minimum\": 0},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"}\n        },\n        \"required\": [\"name\", \"age\"]\n    }\n    \n    # Example valid JSON\n    valid_json = {\n        \"name\": \"John Doe\",\n        \"age\": 30,\n        \"email\": \"john@example.com\"\n    }\n    \n    # Example invalid JSON\n    invalid_json = {\n        \"name\": \"John Doe\",\n        \"age\": -5  # Invalid: negative age\n    }\n    \n    try:\n        result = validate_json(valid_json, schema)\n        print(\"Valid JSON:\", result)\n    except ValidationError as e:\n        print(\"Validation error:\", e.message)\n    \n    try:\n        result = validate_json(invalid_json, schema)\n        print(\"Valid JSON:\", result)\n    except ValidationError as e:\n        print(\"Validation error:\", e.message)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "jsonschema"], "package_breakdown": {"install_packages": ["json", "jsonschema"], "import_packages": ["json", "jsonschema"], "llm_suggested": ["jsonschema"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates and serializes OpenTelemetry traces and metrics into Protocol Buffers format using the OpenTelemetry Python Proto package.", "prompt_index": 159, "generated_code": "```python\nfrom opentelemetry.proto.collector.trace.v1.trace_service_pb2 import ExportTraceServiceRequest\nfrom opentelemetry.proto.collector.metrics.v1.metrics_service_pb2 import ExportMetricsServiceRequest\nfrom opentelemetry.proto.trace.v1.trace_pb2 import (\n    TracesData,\n    ResourceSpans,\n    ScopeSpans,\n    Span,\n    Status,\n)\nfrom opentelemetry.proto.metrics.v1.metrics_pb2 import (\n    MetricsData,\n    ResourceMetrics,\n    ScopeMetrics,\n    Metric,\n    Gauge,\n    NumberDataPoint,\n)\nfrom opentelemetry.proto.common.v1.common_pb2 import (\n    AnyValue,\n    ArrayValue,\n    KeyValue,\n    KeyValueList,\n)\nfrom opentelemetry.proto.resource.v1.resource_pb2 import Resource\nfrom google.protobuf.timestamp_pb2 import Timestamp\nimport time\n\ndef create_timestamp() -> Timestamp:\n    \"\"\"Create a current timestamp in protobuf format\"\"\"\n    timestamp = Timestamp()\n    timestamp.GetCurrentTime()\n    return timestamp\n\ndef create_span(name: str, trace_id: bytes, span_id: bytes, parent_span_id: bytes = None) -> Span:\n    \"\"\"Create a span with basic attributes\"\"\"\n    span = Span(\n        trace_id=trace_id,\n        span_id=span_id,\n        name=name,\n        start_time_unix_nano=int(time.time_ns()),\n        end_time_unix_nano=int(time.time_ns() + 100000000),  # 100ms duration\n        kind=Span.SPAN_KIND_INTERNAL,\n        status=Status(code=Status.STATUS_CODE_OK),\n    )\n    \n    if parent_span_id:\n        span.parent_span_id = parent_span_id\n    \n    # Add some attributes\n    span.attributes.extend([\n        KeyValue(\n            key=\"service.name\",\n            value=AnyValue(string_value=\"example-service\")\n        ),\n        KeyValue(\n            key=\"http.method\",\n            value=AnyValue(string_value=\"GET\")\n        )\n    ])\n    \n    return span\n\ndef create_metric(name: str, value: float) -> Metric:\n    \"\"\"Create a simple gauge metric\"\"\"\n    timestamp = create_timestamp()\n    \n    return Metric(\n        name=name,\n        description=\"Example metric\",\n        unit=\"ms\",\n        gauge=Gauge(\n            data_points=[\n                NumberDataPoint(\n                    time_unix_nano=timestamp.ToNanoseconds(),\n                    as_double=value,\n                    attributes=[\n                        KeyValue(\n                            key=\"environment\",\n                            value=AnyValue(string_value=\"production\")\n                        )\n                    ]\n                )\n            ]\n        )\n    )\n\ndef serialize_traces(trace_id: bytes, span_id: bytes) -> bytes:\n    \"\"\"Create and serialize traces to protobuf bytes\"\"\"\n    # Create spans\n    span = create_span(\"example-operation\", trace_id, span_id)\n    \n    # Build the trace data structure\n    traces_data = TracesData(\n        resource_spans=[\n            ResourceSpans(\n                resource=Resource(attributes=[\n                    KeyValue(\n                        key=\"service.name\",\n                        value=AnyValue(string_value=\"example-service\")\n                    )\n                ]),\n                scope_spans=[\n                    ScopeSpans(\n                        scope=ScopeSpans.InstrumentationScope(\n                            name=\"example-instrumentation\",\n                            version=\"1.0.0\"\n                        ),\n                        spans=[span]\n                    )\n                ]\n            )\n        ]\n    )\n    \n    # Create export request\n    export_request = ExportTraceServiceRequest(resource_spans=traces_data.resource_spans)\n    \n    # Serialize to bytes\n    return export_request.SerializeToString()\n\ndef serialize_metrics() -> bytes:\n    \"\"\"Create and serialize metrics to protobuf bytes\"\"\"\n    # Create metrics\n    metric = create_metric(\"response_time\", 150.5)\n    \n    # Build the metrics data structure\n    metrics_data = MetricsData(\n        resource_metrics=[\n            ResourceMetrics(\n                resource=Resource(attributes=[\n                    KeyValue(\n                        key=\"service.name\",\n                        value=AnyValue(string_value=\"example-service\")\n                    )\n                ]),\n                scope_metrics=[\n                    ScopeMetrics(\n                        scope=ScopeMetrics.InstrumentationScope(\n                            name=\"example-instrumentation\",\n                            version=\"1.0.0\"\n                        ),\n                        metrics=[metric]\n                    )\n                ]\n            )\n        ]\n    )\n    \n    # Create export request\n    export_request = ExportMetricsServiceRequest(resource_metrics=metrics_data.resource_metrics)\n    \n    # Serialize to bytes\n    return export_request.SerializeToString()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random IDs (in real usage, these would come from OpenTelemetry context)\n    trace_id = b'\\x00' * 16  # 16 bytes for trace ID\n    span_id = b'\\x00' * 8    # 8 bytes for span ID\n    \n    # Serialize traces\n    traces_proto = serialize_traces(trace_id, span_id)\n    print(f\"Traces protobuf size: {len(traces_proto)} bytes\")\n    \n    # Serialize metrics\n    metrics_proto = serialize_metrics()\n    print(f\"Metrics protobuf size: {len(metrics_proto)} bytes\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["google", "protobuf", "opentelemetry", "time"], "package_breakdown": {"install_packages": ["google", "protobuf", "opentelemetry", "time"], "import_packages": ["opentelemetry", "google", "time"], "llm_suggested": ["opentelemetry", "protobuf"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads and executes build backend hooks specified in a pyproject.toml file for automated build processes and project management.", "prompt_index": 160, "generated_code": "```python\nimport os\nimport sys\nimport subprocess\nimport toml\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\n\ndef get_build_backend(pyproject_path: str = \"pyproject.toml\") -> Optional[str]:\n    \"\"\"Extract build backend from pyproject.toml\"\"\"\n    try:\n        config = toml.load(pyproject_path)\n        build_system = config.get(\"build-system\", {})\n        return build_system.get(\"build-backend\")\n    except (FileNotFoundError, toml.TomlDecodeError, KeyError):\n        return None\n\ndef get_requires(pyproject_path: str = \"pyproject.toml\") -> List[str]:\n    \"\"\"Extract build requirements from pyproject.toml\"\"\"\n    try:\n        config = toml.load(pyproject_path)\n        build_system = config.get(\"build-system\", {})\n        return build_system.get(\"requires\", [])\n    except (FileNotFoundError, toml.TomlDecodeError, KeyError):\n        return []\n\ndef execute_hook(hook_name: str, backend: str, args: List[str] = None) -> int:\n    \"\"\"Execute a build backend hook using subprocess\"\"\"\n    if args is None:\n        args = []\n    \n    cmd = [sys.executable, \"-c\", f\"import {backend.split('.')[0]}; {backend}.{hook_name}(*{args})\"]\n    \n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        print(result.stdout)\n        if result.stderr:\n            print(result.stderr, file=sys.stderr)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Hook '{hook_name}' failed with return code {e.returncode}\", file=sys.stderr)\n        print(e.stderr, file=sys.stderr)\n        return e.returncode\n    except Exception as e:\n        print(f\"Error executing hook '{hook_name}': {e}\", file=sys.stderr)\n        return 1\n\ndef install_build_dependencies(requires: List[str]) -> int:\n    \"\"\"Install build dependencies using pip\"\"\"\n    if not requires:\n        return 0\n    \n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"pip\", \"install\"] + requires,\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        print(result.stdout)\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to install build dependencies: {e}\", file=sys.stderr)\n        return e.returncode\n\ndef main():\n    \"\"\"Main function to read and execute build backend hooks\"\"\"\n    pyproject_path = \"pyproject.toml\"\n    \n    if not os.path.exists(pyproject_path):\n        print(f\"Error: {pyproject_path} not found\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Get build backend and requirements\n    backend = get_build_backend(pyproject_path)\n    requires = get_requires(pyproject_path)\n    \n    if not backend:\n        print(\"Error: No build-backend specified in pyproject.toml\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Install build dependencies\n    if requires:\n        print(\"Installing build dependencies...\")\n        if install_build_dependencies(requires) != 0:\n            sys.exit(1)\n    \n    # Example: Execute common build hooks\n    hooks_to_execute = [\n        (\"build_sdist\", [\"dist\"]),\n        (\"build_wheel\", [\"dist\"]),\n        (\"get_requires_for_build_sdist\", []),\n        (\"get_requires_for_build_wheel\", [])\n    ]\n    \n    for hook_name, args in hooks_to_execute:\n        print(f\"Executing hook: {hook_name}\")\n        return_code = execute_hook(hook_name, backend, args)\n        if return_code != 0:\n            print(f\"Hook '{hook_name}' failed\", file=sys.stderr)\n            sys.exit(return_code)\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "toml", "pathlib", "sys", "typing", "os"], "package_breakdown": {"install_packages": ["subprocess", "toml", "pathlib", "sys", "typing", "os"], "import_packages": ["subprocess", "toml", "sys", "pathlib", "typing", "os"], "llm_suggested": ["toml", "pathlib", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves a specific commit object from a Git repository using the Git Object Database package and prints out its details.", "prompt_index": 161, "generated_code": "```python\nimport gitdb\n\ndef get_commit_details(repo_path, commit_sha):\n    # Open the repository\n    repo = gitdb.Repo(repo_path)\n    \n    # Retrieve the commit object using its SHA\n    commit = repo.commit(commit_sha)\n    \n    # Print commit details\n    print(f\"Commit SHA: {commit.hexsha}\")\n    print(f\"Author: {commit.author.name} <{commit.author.email}>\")\n    print(f\"Date: {commit.authored_datetime}\")\n    print(f\"Message: {commit.message.strip()}\")\n    print(f\"Parent commits: {[p.hexsha for p in commit.parents]}\")\n\n# Example usage:\n# get_commit_details('/path/to/your/repo', 'abc123def456...')\n```\n\nNote: This code requires the `gitdb` package to be installed (`pip install gitdb`). Replace `/path/to/your/repo` with the actual path to your Git repository and `'abc123def456...'` with the specific commit SHA you want to retrieve.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["gitdb"], "package_breakdown": {"install_packages": ["gitdb"], "import_packages": ["gitdb"], "llm_suggested": ["gitdb"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that securely serializes and deserializes sensitive data to be passed between trusted and untrusted environments, ensuring data integrity and preventing potential security vulnerabilities.", "prompt_index": 162, "generated_code": "```python\nimport json\nimport hmac\nimport hashlib\nimport base64\nfrom typing import Any, Dict, Optional\n\nclass SecureSerializer:\n    def __init__(self, secret_key: bytes):\n        \"\"\"\n        Initialize the secure serializer with a secret key for HMAC\n        \n        Args:\n            secret_key: Cryptographic key for HMAC signing (should be kept secret)\n        \"\"\"\n        if not isinstance(secret_key, bytes):\n            raise ValueError(\"Secret key must be bytes\")\n        self.secret_key = secret_key\n    \n    def serialize(self, data: Dict[str, Any]) -> str:\n        \"\"\"\n        Serialize and sign sensitive data\n        \n        Args:\n            data: Dictionary containing sensitive data\n            \n        Returns:\n            Base64 encoded string containing JSON data and HMAC signature\n        \"\"\"\n        # Convert data to JSON string\n        json_data = json.dumps(data, separators=(',', ':'))\n        \n        # Create HMAC signature\n        signature = hmac.new(\n            self.secret_key,\n            json_data.encode('utf-8'),\n            hashlib.sha256\n        ).digest()\n        \n        # Combine data and signature\n        payload = {\n            'data': json_data,\n            'signature': base64.b64encode(signature).decode('utf-8')\n        }\n        \n        # Return base64 encoded payload\n        return base64.b64encode(json.dumps(payload).encode('utf-8')).decode('utf-8')\n    \n    def deserialize(self, encoded_data: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Deserialize and verify signed data\n        \n        Args:\n            encoded_data: Base64 encoded string from serialize()\n            \n        Returns:\n            Original data dictionary if verification succeeds, None otherwise\n        \"\"\"\n        try:\n            # Decode base64\n            decoded = base64.b64decode(encoded_data.encode('utf-8')).decode('utf-8')\n            payload = json.loads(decoded)\n            \n            # Extract components\n            json_data = payload['data']\n            received_signature = base64.b64decode(payload['signature'])\n            \n            # Verify HMAC signature\n            expected_signature = hmac.new(\n                self.secret_key,\n                json_data.encode('utf-8'),\n                hashlib.sha256\n            ).digest()\n            \n            # Use constant-time comparison to prevent timing attacks\n            if not hmac.compare_digest(received_signature, expected_signature):\n                return None\n            \n            # Return original data\n            return json.loads(json_data)\n            \n        except (ValueError, KeyError, json.JSONDecodeError, base64.binascii.Error):\n            # Handle any parsing or decoding errors\n            return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate a secure key (in practice, use a proper key derivation function)\n    secret_key = b'your-secure-random-key-32-bytes-long'\n    \n    serializer = SecureSerializer(secret_key)\n    \n    # Sensitive data to serialize\n    sensitive_data = {\n        'user_id': 12345,\n        'email': 'user@example.com',\n        'permissions': ['read', 'write']\n    }\n    \n    # Serialize with integrity protection\n    serialized = serializer.serialize(sensitive_data)\n    print(f\"Serialized: {serialized}\")\n    \n    # Deserialize and verify integrity\n    deserialized = serializer.deserialize(serialized)\n    print(f\"Deserialized: {deserialized}\")\n    \n    # Test tampering detection\n    tampered = serialized[:-1] + 'A'  # Modify the last character\n    result = serializer.deserialize(tampered)\n    print(f\"Tampered result: {result}\")  # Should be None\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["base64", "hmac", "typing", "json", "hashlib"], "package_breakdown": {"install_packages": ["base64", "hmac", "typing", "json", "hashlib"], "import_packages": ["base64", "hmac", "typing", "json", "hashlib"], "llm_suggested": ["json", "hmac", "hashlib", "base64", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a D-Bus service using pure Python, sends a method call to the service, and receives the response.", "prompt_index": 163, "generated_code": "```python\nimport dbus\n\ntry:\n    # Connect to the system bus (use session_bus for user session)\n    bus = dbus.SystemBus()\n    \n    # Get a proxy object for the service (example: NetworkManager)\n    proxy = bus.get_object('org.freedesktop.NetworkManager',\n                         '/org/freedesktop/NetworkManager')\n    \n    # Create an interface object for method calls\n    interface = dbus.Interface(proxy, 'org.freedesktop.NetworkManager')\n    \n    # Call a method (example: GetDevices)\n    result = interface.GetDevices()\n    \n    # Print the result\n    print(\"Method call result:\", result)\n    \nexcept dbus.exceptions.DBusException as e:\n    print(f\"D-Bus error: {e}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["dbus"], "packages_detected": ["dbus", "dbus-python"], "package_breakdown": {"install_packages": ["dbus", "dbus-python"], "import_packages": ["dbus"], "llm_suggested": ["dbus-python"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that queries a specified domain's DNS records and prints out the results, including the IP addresses associated with the domain, using the DNS toolkit package.", "prompt_index": 164, "generated_code": "```python\nimport dns.resolver\n\ndef query_dns_records(domain):\n    try:\n        # Query A records (IPv4 addresses)\n        a_records = dns.resolver.resolve(domain, 'A')\n        print(f\"A Records for {domain}:\")\n        for record in a_records:\n            print(f\"  {record.address}\")\n        \n        # Query AAAA records (IPv6 addresses)\n        aaaa_records = dns.resolver.resolve(domain, 'AAAA')\n        print(f\"AAAA Records for {domain}:\")\n        for record in aaaa_records:\n            print(f\"  {record.address}\")\n            \n    except dns.resolver.NoAnswer:\n        print(f\"No DNS records found for {domain}\")\n    except dns.resolver.NXDOMAIN:\n        print(f\"Domain {domain} does not exist\")\n    except Exception as e:\n        print(f\"Error querying DNS records: {e}\")\n\nif __name__ == \"__main__\":\n    domain = input(\"Enter domain to query: \")\n    query_dns_records(domain)\n```\n\nNote: This code requires the `dnspython` package. Install it using:\n```bash\npip install dnspython\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dns", "dnspython"], "package_breakdown": {"install_packages": ["dns", "dnspython"], "import_packages": ["dns"], "llm_suggested": ["dnspython"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that downloads a large media file from Google using resumable uploads to ensure uninterrupted downloading in case of network disruptions.", "prompt_index": 165, "generated_code": "```python\nimport os\nimport requests\nimport time\nfrom pathlib import Path\n\ndef download_file_with_resume(url, destination_path, chunk_size=8192):\n    \"\"\"\n    Download a file from a URL with resumable capability.\n    \n    Args:\n        url (str): The URL of the file to download\n        destination_path (str): Local path where file should be saved\n        chunk_size (int): Size of chunks to download at a time (default: 8KB)\n    \"\"\"\n    # Create destination directory if it doesn't exist\n    Path(destination_path).parent.mkdir(parents=True, exist_ok=True)\n    \n    # Get file size if resuming\n    initial_byte = 0\n    if os.path.exists(destination_path):\n        initial_byte = os.path.getsize(destination_path)\n    \n    # Set up headers for resumable download\n    headers = {}\n    if initial_byte > 0:\n        headers['Range'] = f'bytes={initial_byte}-'\n    \n    try:\n        with requests.get(url, headers=headers, stream=True, timeout=30) as response:\n            response.raise_for_status()\n            \n            # Check if server supports range requests\n            if initial_byte > 0 and response.status_code != 206:\n                print(\"Server doesn't support resumable downloads. Starting from beginning.\")\n                initial_byte = 0\n                # Re-open connection without range header\n                with requests.get(url, stream=True, timeout=30) as new_response:\n                    new_response.raise_for_status()\n                    return _download_stream(new_response, destination_path, 0, chunk_size)\n            \n            total_size = int(response.headers.get('content-length', 0)) + initial_byte\n            \n            return _download_stream(response, destination_path, initial_byte, chunk_size, total_size)\n            \n    except requests.exceptions.RequestException as e:\n        print(f\"Download failed: {e}\")\n        return False\n\ndef _download_stream(response, destination_path, initial_byte, chunk_size, total_size=None):\n    \"\"\"\n    Helper function to handle the actual downloading of chunks.\n    \"\"\"\n    mode = 'ab' if initial_byte > 0 else 'wb'\n    \n    try:\n        with open(destination_path, mode) as file:\n            for chunk in response.iter_content(chunk_size=chunk_size):\n                if chunk:\n                    file.write(chunk)\n                    initial_byte += len(chunk)\n                    \n                    if total_size:\n                        progress = (initial_byte / total_size) * 100\n                        print(f\"Downloaded: {initial_byte}/{total_size} bytes ({progress:.1f}%)\", end='\\r')\n        \n        print(\"\\nDownload completed successfully!\")\n        return True\n        \n    except IOError as e:\n        print(f\"File write error: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error during download: {e}\")\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    file_url = \"https://example.com/large_media_file.mp4\"  # Replace with actual Google Drive/Docs URL\n    output_file = \"downloaded_file.mp4\"\n    \n    # For Google Drive files, you might need to use the export URL format:\n    # file_url = \"https://docs.google.com/uc?export=download&id=FILE_ID\"\n    \n    success = download_file_with_resume(file_url, output_file)\n    if success:\n        print(f\"File downloaded successfully to {output_file}\")\n    else:\n        print(\"Download failed\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "time", "pathlib", "os"], "package_breakdown": {"install_packages": ["requests", "time", "pathlib", "os"], "import_packages": ["requests", "time", "os", "pathlib"], "llm_suggested": ["requests", "os", "pathlib", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the package to compile multiple Python source files into a single executable file with all dependencies included for easy distribution.", "prompt_index": 166, "generated_code": "```python\nimport PyInstaller.__main__\nimport os\nimport sys\n\ndef compile_to_executable(main_script_path, output_name=None, additional_files=None, onefile=True, windowed=False):\n    \"\"\"\n    Compile Python scripts into a standalone executable using PyInstaller.\n    \n    Args:\n        main_script_path (str): Path to the main Python script\n        output_name (str, optional): Name for the output executable\n        additional_files (list, optional): List of additional files to include\n        onefile (bool): Whether to create a single executable file\n        windowed (bool): Whether to run without console window (for GUI apps)\n    \"\"\"\n    \n    if not os.path.exists(main_script_path):\n        raise FileNotFoundError(f\"Main script not found: {main_script_path}\")\n    \n    # Build PyInstaller command\n    cmd = [\n        main_script_path,\n        '--onefile' if onefile else '--onedir',\n        '--windowed' if windowed else '--console'\n    ]\n    \n    if output_name:\n        cmd.extend(['--name', output_name])\n    \n    if additional_files:\n        for file in additional_files:\n            if os.path.exists(file):\n                cmd.extend(['--add-data', f'{file}:.'])\n            else:\n                print(f\"Warning: Additional file not found: {file}\")\n    \n    # Run PyInstaller\n    PyInstaller.__main__.run(cmd)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Compile main.py with dependencies\n    compile_to_executable(\n        main_script_path='main.py',\n        output_name='MyApp',\n        additional_files=['config.ini', 'data/file.txt'],\n        onefile=True,\n        windowed=False\n    )\n```\n\n```bash\n# Required installation\npip install pyinstaller\n```\n\nThis code uses PyInstaller to create a standalone executable. The executable will include:\n- All Python dependencies\n- The main script and any additional files\n- Python interpreter\n- Required libraries\n\nThe resulting executable can be distributed without requiring Python installation on the target machine.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "PyInstaller", "os"], "package_breakdown": {"install_packages": ["sys", "PyInstaller", "os"], "import_packages": ["PyInstaller", "sys", "os"], "llm_suggested": ["PyInstaller", "os", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that securely stores and retrieves sensitive information using the FreeDesktop.org Secret Service API.", "prompt_index": 167, "generated_code": "```python\nimport gi\ngi.require_version('Secret', '1')\nfrom gi.repository import Secret\n\nSCHEMA = Secret.Schema.new(\n    \"org.example.MyApp\",\n    Secret.SchemaFlags.NONE,\n    {\n        \"name\": Secret.SchemaAttributeType.STRING,\n        \"service\": Secret.SchemaAttributeType.STRING,\n    }\n)\n\ndef store_secret(name, service, secret_value):\n    attributes = {\n        \"name\": name,\n        \"service\": service\n    }\n    \n    Secret.password_store_sync(\n        SCHEMA,\n        attributes,\n        Secret.COLLECTION_DEFAULT,\n        f\"Secret for {name}@{service}\",\n        secret_value,\n        None\n    )\n\ndef retrieve_secret(name, service):\n    attributes = {\n        \"name\": name,\n        \"service\": service\n    }\n    \n    return Secret.password_lookup_sync(SCHEMA, attributes, None)\n\ndef delete_secret(name, service):\n    attributes = {\n        \"name\": name,\n        \"service\": service\n    }\n    \n    Secret.password_clear_sync(SCHEMA, attributes, None)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Store a secret\n    store_secret(\"my_username\", \"my_service\", \"my_password123\")\n    \n    # Retrieve the secret\n    secret = retrieve_secret(\"my_username\", \"my_service\")\n    if secret:\n        print(f\"Retrieved secret: {secret}\")\n    else:\n        print(\"Secret not found\")\n    \n    # Delete the secret (uncomment to use)\n    # delete_secret(\"my_username\", \"my_service\")\n```\n\nNote: This code requires the `python-gobject` package and a running secret service daemon (like GNOME Keyring or KeePassXC).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["gi", "Secret"], "package_breakdown": {"install_packages": ["gi", "Secret"], "import_packages": ["gi"], "llm_suggested": ["gi", "Secret"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with their Google account and retrieves their calendar events using the Google API Client Library for Python.", "prompt_index": 168, "generated_code": "```python\nimport os.path\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\n\n# If modifying these scopes, delete the file token.json.\nSCOPES = ['https://www.googleapis.com/auth/calendar.readonly']\n\ndef get_calendar_events():\n    creds = None\n    # The file token.json stores the user's access and refresh tokens\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    \n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                'credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n\n    service = build('calendar', 'v3', credentials=creds)\n\n    # Call the Calendar API\n    events_result = service.events().list(\n        calendarId='primary', \n        maxResults=10, \n        singleEvents=True,\n        orderBy='startTime'\n    ).execute()\n    events = events_result.get('items', [])\n    \n    return events\n\nif __name__ == '__main__':\n    events = get_calendar_events()\n    if not events:\n        print('No upcoming events found.')\n    for event in events:\n        start = event['start'].get('dateTime', event['start'].get('date'))\n        print(f\"{start} - {event['summary']}\")\n```\n\n**Prerequisites:**\n1. Install required packages: `pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib`\n2. Create a project in Google Cloud Console and enable the Calendar API\n3. Download the OAuth 2.0 Client credentials as `credentials.json` in the same directory\n4. On first run, you'll be redirected to a browser to authenticate with Google", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google_auth_oauthlib", "google", "googleapiclient", "google-api-python-client", "google-auth-httplib2", "os", "google-auth-oauthlib"], "package_breakdown": {"install_packages": ["google_auth_oauthlib", "google", "googleapiclient", "google-api-python-client", "google-auth-httplib2", "os", "google-auth-oauthlib"], "import_packages": ["google_auth_oauthlib", "google", "os", "googleapiclient"], "llm_suggested": ["google-api-python-client", "google-auth-httplib2", "google-auth-oauthlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates type hints for functions that utilize the requests library, ensuring accurate and comprehensive static type checking for HTTP requests and responses.", "prompt_index": 169, "generated_code": "```python\nfrom typing import Dict, Any, Optional, Union, List, Tuple\nfrom requests import Response, Session\nfrom requests.models import PreparedRequest\nimport requests\n\ndef get_request(\n    url: str,\n    params: Optional[Dict[str, Any]] = None,\n    headers: Optional[Dict[str, str]] = None,\n    timeout: Union[int, float, Tuple[int, int], None] = None,\n    **kwargs: Any\n) -> Response:\n    \"\"\"\n    Type-hinted GET request function.\n    \n    Args:\n        url: Target URL for the request\n        params: URL parameters to send\n        headers: HTTP headers to send\n        timeout: Request timeout in seconds\n        **kwargs: Additional arguments passed to requests.get\n        \n    Returns:\n        Response object containing server's response\n    \"\"\"\n    return requests.get(\n        url=url,\n        params=params,\n        headers=headers,\n        timeout=timeout,\n        **kwargs\n    )\n\ndef post_request(\n    url: str,\n    data: Optional[Union[Dict[str, Any], str, bytes]] = None,\n    json: Optional[Any] = None,\n    headers: Optional[Dict[str, str]] = None,\n    timeout: Union[int, float, Tuple[int, int], None] = None,\n    **kwargs: Any\n) -> Response:\n    \"\"\"\n    Type-hinted POST request function.\n    \n    Args:\n        url: Target URL for the request\n        data: Dictionary, bytes, or file-like object to send\n        json: JSON data to send in the body\n        headers: HTTP headers to send\n        timeout: Request timeout in seconds\n        **kwargs: Additional arguments passed to requests.post\n        \n    Returns:\n        Response object containing server's response\n    \"\"\"\n    return requests.post(\n        url=url,\n        data=data,\n        json=json,\n        headers=headers,\n        timeout=timeout,\n        **kwargs\n    )\n\ndef put_request(\n    url: str,\n    data: Optional[Union[Dict[str, Any], str, bytes]] = None,\n    json: Optional[Any] = None,\n    headers: Optional[Dict[str, str]] = None,\n    timeout: Union[int, float, Tuple[int, int], None] = None,\n    **kwargs: Any\n) -> Response:\n    \"\"\"\n    Type-hinted PUT request function.\n    \n    Args:\n        url: Target URL for the request\n        data: Dictionary, bytes, or file-like object to send\n        json: JSON data to send in the body\n        headers: HTTP headers to send\n        timeout: Request timeout in seconds\n        **kwargs: Additional arguments passed to requests.put\n        \n    Returns:\n        Response object containing server's response\n    \"\"\"\n    return requests.put(\n        url=url,\n        data=data,\n        json=json,\n        headers=headers,\n        timeout=timeout,\n        **kwargs\n    )\n\ndef delete_request(\n    url: str,\n    headers: Optional[Dict[str, str]] = None,\n    timeout: Union[int, float, Tuple[int, int], None] = None,\n    **kwargs: Any\n) -> Response:\n    \"\"\"\n    Type-hinted DELETE request function.\n    \n    Args:\n        url: Target URL for the request\n        headers: HTTP headers to send\n        timeout: Request timeout in seconds\n        **kwargs: Additional arguments passed to requests.delete\n        \n    Returns:\n        Response object containing server's response\n    \"\"\"\n    return requests.delete(\n        url=url,\n        headers=headers,\n        timeout=timeout,\n        **kwargs\n    )\n\ndef session_request(\n    session: Session,\n    method: str,\n    url: str,\n    params: Optional[Dict[str, Any]] = None,\n    data: Optional[Union[Dict[str, Any], str, bytes]] = None,\n    json: Optional[Any] = None,\n    headers: Optional[Dict[str, str]] = None,\n    timeout: Union[int, float, Tuple[int, int], None] = None,\n    **kwargs: Any\n) -> Response:\n    \"\"\"\n    Type-hinted session-based request function.\n    \n    Args:\n        session: Requests Session object\n        method: HTTP method (GET, POST, PUT, DELETE, etc.)\n        url: Target URL for the request\n        params: URL parameters to send\n        data: Dictionary, bytes, or file-like object to send\n        json: JSON data to send in the body\n        headers: HTTP headers to send\n        timeout: Request timeout in seconds\n        **kwargs: Additional arguments passed to session.request\n        \n    Returns:\n        Response object containing server's response\n    \"\"\"\n    return session.request(\n        method=method.upper(),\n        url=url,\n        params=params,\n        data=data,\n        json=json,\n        headers=headers,\n        timeout=timeout,\n        **kwargs\n    )\n\ndef parse_response_json(response: Response) -> Union[Dict[str, Any], List[Any]]:\n    \"\"\"\n    Type-hinted function to parse JSON response.\n    \n    Args:\n        response: Response object from requests\n        \n    Returns:\n        Parsed JSON data as dictionary or list\n    \"\"\"\n    return response.json()\n\ndef get_response_headers(response: Response) -> Dict[str, str]:\n    \"\"\"\n    Type-hinted function to get response headers.\n    \n    Args:\n        response: Response object from requests\n        \n    Returns:\n        Dictionary of response headers\n    \"\"\"\n    return dict(response.headers)\n\ndef check_status_code(response: Response, expected_code: int = 200) -> bool:\n    \"\"\"\n    Type-hinted function to check response status code.\n    \n    Args:\n        response: Response object from requests\n        expected_code: Expected HTTP status code\n        \n    Returns:\n        Boolean indicating if status code matches expected\n    \"\"\"\n    return response.status_code == expected_code\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "typing"], "package_breakdown": {"install_packages": ["requests", "typing"], "import_packages": ["requests", "typing"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a scatter plot using the matplotlib library to visualize the relationship between two sets of data.", "prompt_index": 170, "generated_code": "```python\nimport matplotlib.pyplot as plt\n\n# Sample data (replace with your actual data)\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n\n# Create scatter plot\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, color='blue', alpha=0.7, edgecolors='black')\n\n# Add labels and title\nplt.xlabel('X Values')\nplt.ylabel('Y Values')\nplt.title('Scatter Plot of X vs Y')\n\n# Add grid for better readability\nplt.grid(True, alpha=0.3)\n\n# Show the plot\nplt.show()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib"], "package_breakdown": {"install_packages": ["matplotlib"], "import_packages": ["matplotlib"], "llm_suggested": ["matplotlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a secure SSH2 connection to a remote server, authenticates using a username and password, executes a command on the server, and retrieves the command output.", "prompt_index": 171, "generated_code": "```python\nimport paramiko\n\ndef ssh_command_execute(hostname, port, username, password, command):\n    client = paramiko.SSHClient()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    \n    try:\n        client.connect(hostname, port=port, username=username, password=password)\n        stdin, stdout, stderr = client.exec_command(command)\n        output = stdout.read().decode()\n        error = stderr.read().decode()\n        return output, error\n    except Exception as e:\n        return None, str(e)\n    finally:\n        client.close()\n\n# Example usage:\n# output, error = ssh_command_execute('example.com', 22, 'user', 'password', 'ls -la')\n# if output is not None:\n#     print(\"Output:\", output)\n# if error:\n#     print(\"Error:\", error)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["paramiko"], "package_breakdown": {"install_packages": ["paramiko"], "import_packages": ["paramiko"], "llm_suggested": ["paramiko"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that recursively traverses a Python object, serializes its contents, and saves it to a file in a format such as JSON or Pickle for easy storage and retrieval.", "prompt_index": 172, "generated_code": "```python\nimport json\nimport pickle\nfrom typing import Any\n\ndef serialize_object(obj: Any, filename: str, format_type: str = 'json') -> None:\n    \"\"\"\n    Recursively traverse and serialize a Python object to a file.\n    \n    Args:\n        obj: The Python object to serialize\n        filename: Output filename\n        format_type: Serialization format ('json' or 'pickle')\n    \"\"\"\n    def _serialize_recursive(item: Any) -> Any:\n        \"\"\"Helper function to recursively handle different data types\"\"\"\n        if isinstance(item, (str, int, float, bool, type(None))):\n            return item\n        elif isinstance(item, dict):\n            return {str(k): _serialize_recursive(v) for k, v in item.items()}\n        elif isinstance(item, (list, tuple, set)):\n            return [_serialize_recursive(i) for i in item]\n        else:\n            # For custom objects, convert to dict representation\n            try:\n                return _serialize_recursive(item.__dict__)\n            except AttributeError:\n                # Fallback: convert to string representation\n                return str(item)\n    \n    serialized_data = _serialize_recursive(obj)\n    \n    if format_type.lower() == 'json':\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(serialized_data, f, indent=2, ensure_ascii=False)\n    elif format_type.lower() == 'pickle':\n        with open(filename, 'wb') as f:\n            pickle.dump(obj, f)\n    else:\n        raise ValueError(\"Unsupported format. Use 'json' or 'pickle'\")\n\ndef deserialize_object(filename: str, format_type: str = 'json') -> Any:\n    \"\"\"\n    Deserialize an object from a file.\n    \n    Args:\n        filename: Input filename\n        format_type: Serialization format ('json' or 'pickle')\n    \n    Returns:\n        The deserialized Python object\n    \"\"\"\n    if format_type.lower() == 'json':\n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    elif format_type.lower() == 'pickle':\n        with open(filename, 'rb') as f:\n            return pickle.load(f)\n    else:\n        raise ValueError(\"Unsupported format. Use 'json' or 'pickle'\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample complex object\n    sample_data = {\n        'name': 'Test',\n        'numbers': [1, 2, 3],\n        'nested': {\n            'key': 'value',\n            'items': {'a', 'b', 'c'}\n        },\n        'none_value': None\n    }\n    \n    # Serialize to JSON\n    serialize_object(sample_data, 'data.json', 'json')\n    \n    # Serialize to Pickle\n    serialize_object(sample_data, 'data.pickle', 'pickle')\n    \n    # Deserialize examples\n    json_data = deserialize_object('data.json', 'json')\n    pickle_data = deserialize_object('data.pickle', 'pickle')\n    \n    print(\"JSON deserialized:\", json_data)\n    print(\"Pickle deserialized:\", pickle_data)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pickle", "typing", "json"], "package_breakdown": {"install_packages": ["pickle", "typing", "json"], "import_packages": ["json", "pickle", "typing"], "llm_suggested": ["json", "pickle"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the package to create a signal system for broadcasting messages between objects, allowing for efficient and straightforward communication.", "prompt_index": 173, "generated_code": "```python\nimport weakref\nfrom typing import Callable, Any, List\n\nclass Signal:\n    def __init__(self):\n        self._receivers = []\n    \n    def connect(self, receiver: Callable, sender: Any = None) -> None:\n        \"\"\"Connect a receiver function to this signal.\"\"\"\n        receiver_ref = weakref.ref(receiver)\n        self._receivers.append((receiver_ref, sender))\n    \n    def disconnect(self, receiver: Callable, sender: Any = None) -> None:\n        \"\"\"Disconnect a receiver function from this signal.\"\"\"\n        for i, (ref, s) in enumerate(self._receivers):\n            if ref() is receiver and s == sender:\n                del self._receivers[i]\n                return\n    \n    def send(self, sender: Any, **kwargs) -> List[Any]:\n        \"\"\"Send signal from sender with given arguments.\"\"\"\n        responses = []\n        for receiver_ref, signal_sender in self._receivers:\n            if signal_sender is None or signal_sender is sender:\n                receiver = receiver_ref()\n                if receiver is not None:\n                    responses.append(receiver(sender=sender, **kwargs))\n        return responses\n\nclass MessageBroadcaster:\n    def __init__(self):\n        self.message_received = Signal()\n    \n    def broadcast(self, message: str, **kwargs) -> List[Any]:\n        \"\"\"Broadcast a message to all connected receivers.\"\"\"\n        return self.message_received.send(sender=self, message=message, **kwargs)\n\n# Example usage\nif __name__ == \"__main__\":\n    broadcaster = MessageBroadcaster()\n    \n    def receiver1(sender, message, **kwargs):\n        print(f\"Receiver 1 got: {message} from {sender}\")\n        return \"processed_1\"\n    \n    def receiver2(sender, message, **kwargs):\n        print(f\"Receiver 2 got: {message} from {sender}\")\n        return \"processed_2\"\n    \n    # Connect receivers\n    broadcaster.message_received.connect(receiver1)\n    broadcaster.message_received.connect(receiver2)\n    \n    # Broadcast message\n    responses = broadcaster.broadcast(\"Hello World!\", extra_data=\"test\")\n    print(f\"Responses: {responses}\")\n    \n    # Disconnect one receiver\n    broadcaster.message_received.disconnect(receiver1)\n    \n    # Broadcast another message\n    responses = broadcaster.broadcast(\"Second message\")\n    print(f\"Responses: {responses}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["weakref", "typing"], "package_breakdown": {"install_packages": ["weakref", "typing"], "import_packages": ["weakref", "typing"], "llm_suggested": ["weakref", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the CRC32C checksum for a given input data using the Google CRC32C C library wrapper.", "prompt_index": 174, "generated_code": "```python\nimport crc32c\n\ndef calculate_crc32c(data):\n    \"\"\"\n    Calculate CRC32C checksum for input data using Google's crc32c library.\n    \n    Args:\n        data: Input data (bytes, bytearray, or string)\n    \n    Returns:\n        int: CRC32C checksum value\n    \"\"\"\n    if isinstance(data, str):\n        data = data.encode('utf-8')\n    \n    return crc32c.crc32c(data)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example with string\n    test_string = \"Hello, World!\"\n    checksum = calculate_crc32c(test_string)\n    print(f\"CRC32C checksum for '{test_string}': {checksum:08x}\")\n    \n    # Example with bytes\n    test_bytes = b\"Hello, World!\"\n    checksum_bytes = calculate_crc32c(test_bytes)\n    print(f\"CRC32C checksum for bytes: {checksum_bytes:08x}\")\n```\n\nNote: This code requires the `crc32c` package to be installed. You can install it using:\n```bash\npip install crc32c\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["crc32c"], "package_breakdown": {"install_packages": ["crc32c"], "import_packages": ["crc32c"], "llm_suggested": ["crc32c"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a YAML file, preserves any comments within the file, maintains the original sequence and flow style of mappings, and ensures that the order of keys in the map is preserved during parsing and emission.", "prompt_index": 175, "generated_code": "```python\nimport yaml\nfrom yaml import Loader, Dumper\nfrom yaml.composer import Composer\nfrom yaml.constructor import Constructor\nfrom yaml.resolver import Resolver\nfrom yaml.representer import Representer\nfrom yaml.emitter import Emitter\nfrom yaml.serializer import Serializer\nfrom yaml.parser import Parser\nfrom yaml.reader import Reader\nfrom yaml.scanner import Scanner\nfrom yaml.nodes import *\n\nclass CommentedLoader(Loader):\n    def __init__(self, stream):\n        Loader.__init__(self, stream)\n        self.comments = []\n\n    def compose_node(self, parent, index):\n        line = self.line\n        column = self.column\n        node = Loader.compose_node(self, parent, index)\n        node.__line__ = line + 1\n        node.__column__ = column + 1\n        return node\n\n    def construct_mapping(self, node, deep=False):\n        mapping = {}\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            value = self.construct_object(value_node, deep=deep)\n            mapping[key] = value\n        return mapping\n\nclass CommentedDumper(Dumper):\n    def represent_mapping(self, tag, mapping, flow_style=None):\n        value = []\n        node = MappingNode(tag, value, flow_style=flow_style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        best_style = True\n        if hasattr(mapping, 'items'):\n            mapping = list(mapping.items())\n        for item_key, item_value in mapping:\n            node_key = self.represent_data(item_key)\n            node_value = self.represent_data(item_value)\n            value.append((node_key, node_value))\n        return node\n\ndef load_yaml_with_comments(stream):\n    loader = CommentedLoader(stream)\n    try:\n        return loader.get_single_data()\n    finally:\n        loader.dispose()\n\ndef dump_yaml_with_comments(data, stream=None, **kwds):\n    dumper = CommentedDumper(stream, **kwds)\n    try:\n        dumper.open()\n        dumper.represent(data)\n        dumper.close()\n    finally:\n        dumper.dispose()\n\ndef read_yaml_preserve_comments(filename):\n    with open(filename, 'r') as file:\n        data = load_yaml_with_comments(file)\n    return data\n\ndef write_yaml_preserve_comments(data, filename, **kwds):\n    with open(filename, 'w') as file:\n        dump_yaml_with_comments(data, file, **kwds)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml"], "package_breakdown": {"install_packages": ["yaml"], "import_packages": ["yaml"], "llm_suggested": ["yaml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a pipeline of Python functions to process a list of data, where each function in the pipeline takes the output of the previous function as input and performs a specific operation on it.", "prompt_index": 176, "generated_code": "```python\ndef pipeline(data, *functions):\n    \"\"\"\n    Applies a series of functions to data in sequence.\n    \n    Args:\n        data: Initial input data\n        *functions: Variable number of functions to apply sequentially\n        \n    Returns:\n        Result after applying all functions\n    \"\"\"\n    result = data\n    for func in functions:\n        result = func(result)\n    return result\n\n# Example usage functions\ndef double_numbers(data):\n    \"\"\"Doubles each number in a list\"\"\"\n    return [x * 2 for x in data]\n\ndef filter_even(data):\n    \"\"\"Filters out odd numbers, keeping only even numbers\"\"\"\n    return [x for x in data if x % 2 == 0]\n\ndef sum_numbers(data):\n    \"\"\"Sums all numbers in a list\"\"\"\n    return sum(data)\n\ndef square_numbers(data):\n    \"\"\"Squares each number in a list\"\"\"\n    return [x ** 2 for x in data]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    # Create and execute pipeline\n    result = pipeline(\n        numbers,\n        double_numbers,    # [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n        filter_even,       # [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] (all even after doubling)\n        square_numbers,    # [4, 16, 36, 64, 100, 144, 196, 256, 324, 400]\n        sum_numbers        # 1540\n    )\n    \n    print(f\"Final result: {result}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a command line interface with interactive features, such as user input prompts, dynamic output updates, and customizable command options.", "prompt_index": 177, "generated_code": "```python\nimport cmd\nimport argparse\nimport sys\nfrom typing import Optional, List, Dict, Any\n\nclass InteractiveCLI(cmd.Cmd):\n    intro = \"Welcome to the Interactive CLI. Type 'help' for commands or 'exit' to quit.\"\n    prompt = \">>> \"\n    \n    def __init__(self, custom_options: Optional[Dict[str, Any]] = None):\n        super().__init__()\n        self.custom_options = custom_options or {}\n        self.running = True\n    \n    def do_echo(self, arg: str) -> None:\n        \"\"\"Echo the input text: ECHO [text]\"\"\"\n        print(f\"Echo: {arg}\")\n    \n    def do_calculate(self, arg: str) -> None:\n        \"\"\"Perform simple calculation: CALCULATE [expression]\"\"\"\n        try:\n            result = eval(arg)\n            print(f\"Result: {result}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    \n    def do_set_option(self, arg: str) -> None:\n        \"\"\"Set a custom option: SET_OPTION [key] [value]\"\"\"\n        args = arg.split(maxsplit=1)\n        if len(args) < 2:\n            print(\"Usage: SET_OPTION [key] [value]\")\n            return\n        \n        key, value = args\n        self.custom_options[key] = value\n        print(f\"Option '{key}' set to '{value}'\")\n    \n    def do_show_options(self, _: str) -> None:\n        \"\"\"Show all custom options\"\"\"\n        if not self.custom_options:\n            print(\"No custom options set\")\n            return\n        \n        print(\"Custom Options:\")\n        for key, value in self.custom_options.items():\n            print(f\"  {key}: {value}\")\n    \n    def do_exit(self, _: str) -> bool:\n        \"\"\"Exit the CLI\"\"\"\n        print(\"Goodbye!\")\n        self.running = False\n        return True\n    \n    def default(self, line: str) -> None:\n        print(f\"Unknown command: {line}. Type 'help' for available commands.\")\n    \n    def emptyline(self) -> None:\n        pass\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=\"Interactive Command Line Interface\")\n    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose output')\n    parser.add_argument('--config', '-c', type=str, help='Path to configuration file')\n    return parser.parse_args()\n\ndef main():\n    args = parse_arguments()\n    \n    if args.verbose:\n        print(\"Verbose mode enabled\")\n    \n    if args.config:\n        print(f\"Loading config from: {args.config}\")\n    \n    cli = InteractiveCLI()\n    \n    try:\n        cli.cmdloop()\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted by user\")\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cmd", "argparse", "sys", "typing"], "package_breakdown": {"install_packages": ["cmd", "argparse", "sys", "typing"], "import_packages": ["typing", "argparse", "sys", "cmd"], "llm_suggested": ["cmd", "argparse", "sys", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the rapid fuzzy string matching package to efficiently compare a list of strings against a target string and return the most similar strings along with their similarity scores.", "prompt_index": 178, "generated_code": "```python\nimport rapidfuzz\n\ndef find_most_similar_strings(target_string, string_list, limit=5, scorer=rapidfuzz.fuzz.ratio):\n    \"\"\"\n    Find the most similar strings to a target string from a list of strings.\n    \n    Args:\n        target_string (str): The string to compare against\n        string_list (list): List of strings to search through\n        limit (int): Maximum number of results to return\n        scorer (callable): RapidFuzz scoring function (default: fuzz.ratio)\n    \n    Returns:\n        list: List of tuples containing (string, similarity_score)\n    \"\"\"\n    # Use process.extract to get the most similar strings with scores\n    results = rapidfuzz.process.extract(\n        target_string,\n        string_list,\n        scorer=scorer,\n        limit=limit\n    )\n    \n    return results\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    target = \"hello world\"\n    strings_to_search = [\n        \"hello world\",\n        \"hello there\",\n        \"world hello\",\n        \"goodbye world\",\n        \"hell world\",\n        \"hello word\",\n        \"hola mundo\"\n    ]\n    \n    # Find the 3 most similar strings\n    similar_strings = find_most_similar_strings(target, strings_to_search, limit=3)\n    \n    print(f\"Target string: '{target}'\")\n    print(\"Most similar strings:\")\n    for string, score, index in similar_strings:\n        print(f\"  '{string}' - Score: {score}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["rapidfuzz"], "package_breakdown": {"install_packages": ["rapidfuzz"], "import_packages": ["rapidfuzz"], "llm_suggested": ["rapidfuzz"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that decorates a function with a backoff and retry mechanism, allowing the function to automatically retry a specified number of times with increasing delays between attempts in case of failures.", "prompt_index": 179, "generated_code": "```python\nimport time\nimport functools\nfrom typing import Callable, Any, Optional\n\n\ndef backoff_retry(\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    backoff_factor: float = 2.0,\n    exceptions: tuple = (Exception,)\n) -> Callable:\n    \"\"\"\n    Decorator that adds backoff and retry mechanism to a function.\n    \n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay in seconds before first retry\n        backoff_factor: Multiplier for delay between retries\n        exceptions: Tuple of exceptions that should trigger retries\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs) -> Any:\n            delay = initial_delay\n            for attempt in range(max_retries + 1):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    if attempt == max_retries:\n                        raise\n                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s...\")\n                    time.sleep(delay)\n                    delay *= backoff_factor\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\n# Example usage:\n@backoff_retry(max_retries=5, initial_delay=0.5, backoff_factor=1.5)\ndef unreliable_function():\n    import random\n    if random.random() < 0.7:  # 70% chance of failure\n        raise ConnectionError(\"Connection failed\")\n    return \"Success!\"\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "random", "typing", "time"], "package_breakdown": {"install_packages": ["functools", "random", "typing", "time"], "import_packages": ["functools", "time", "random", "typing"], "llm_suggested": ["time", "functools", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically instruments a Python application with OpenTelemetry for monitoring and tracing purposes.", "prompt_index": 180, "generated_code": "```python\nimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.auto_instrumentation import (\n    run_with_auto_instrumentation,\n)\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.django import DjangoInstrumentor\nfrom opentelemetry.instrumentation.redis import RedisInstrumentor\nfrom opentelemetry.instrumentation.logging import LoggingInstrumentor\n\ndef setup_opentelemetry():\n    # Configure resource\n    resource = Resource.create({\n        \"service.name\": os.getenv(\"OTEL_SERVICE_NAME\", \"python-app\"),\n        \"service.version\": os.getenv(\"OTEL_SERVICE_VERSION\", \"1.0.0\"),\n    })\n    \n    # Set up tracer provider\n    tracer_provider = TracerProvider(resource=resource)\n    trace.set_tracer_provider(tracer_provider)\n    \n    # Configure OTLP exporter\n    otlp_exporter = OTLPSpanExporter(\n        endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4317\"),\n        insecure=bool(os.getenv(\"OTEL_EXPORTER_OTLP_INSECURE\", \"True\")),\n    )\n    \n    # Add span processor\n    span_processor = BatchSpanProcessor(otlp_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    \n    # Auto-instrument common libraries\n    RequestsInstrumentor().instrument()\n    Psycopg2Instrumentor().instrument()\n    RedisInstrumentor().instrument()\n    LoggingInstrumentor().instrument()\n    \n    # Framework-specific instrumentation (choose one based on your app)\n    framework = os.getenv(\"APP_FRAMEWORK\", \"\").lower()\n    if framework == \"flask\":\n        FlaskInstrumentor().instrument()\n    elif framework == \"django\":\n        DjangoInstrumentor().instrument_apps()\n    \n    return tracer_provider\n\ndef instrument_application():\n    \"\"\"Main function to instrument the application\"\"\"\n    setup_opentelemetry()\n    print(\"OpenTelemetry instrumentation configured successfully\")\n\n# Environment variables needed:\n# OTEL_SERVICE_NAME - Your service name\n# OTEL_SERVICE_VERSION - Service version\n# OTEL_EXPORTER_OTLP_ENDPOINT - OTLP collector endpoint\n# APP_FRAMEWORK - \"flask\" or \"django\" (optional)\n\n# Usage: Call instrument_application() at the start of your application\n# or use: run_with_auto_instrumentation(your_main_function)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 10, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-instrumentation-logging", "opentelemetry-exporter-otlp", "opentelemetry-sdk", "opentelemetry-instrumentation-django", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-redis", "opentelemetry-instrumentation-flask", "opentelemetry", "opentelemetry-instrumentation-psycopg2", "os"], "package_breakdown": {"install_packages": ["opentelemetry-instrumentation-logging", "opentelemetry-exporter-otlp", "opentelemetry-sdk", "opentelemetry-instrumentation-django", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-redis", "opentelemetry-instrumentation-flask", "opentelemetry", "opentelemetry-instrumentation-psycopg2", "os"], "import_packages": ["opentelemetry", "os"], "llm_suggested": ["opentelemetry-sdk", "opentelemetry-exporter-otlp", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-psycopg2", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-django", "opentelemetry-instrumentation-redis", "opentelemetry-instrumentation-logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts metadata information such as font name, style, and character set from a given font file.", "prompt_index": 181, "generated_code": "```python\nimport os\nimport sys\nfrom fontTools.ttLib import TTFont\n\ndef extract_font_metadata(font_path):\n    \"\"\"\n    Extract metadata from a font file.\n    \n    Args:\n        font_path (str): Path to the font file\n    \n    Returns:\n        dict: Dictionary containing font metadata\n    \"\"\"\n    try:\n        if not os.path.exists(font_path):\n            raise FileNotFoundError(f\"Font file not found: {font_path}\")\n        \n        font = TTFont(font_path)\n        name_table = font['name']\n        \n        metadata = {\n            'font_family': get_name_record(name_table, 1),  # Font Family name\n            'font_subfamily': get_name_record(name_table, 2),  # Font Subfamily name\n            'full_font_name': get_name_record(name_table, 4),  # Full font name\n            'version': get_name_record(name_table, 5),  # Version string\n            'postscript_name': get_name_record(name_table, 6),  # PostScript name\n            'trademark': get_name_record(name_table, 7),  # Trademark notice\n            'manufacturer': get_name_record(name_table, 8),  # Manufacturer name\n            'designer': get_name_record(name_table, 9),  # Designer\n            'description': get_name_record(name_table, 10),  # Description\n            'vendor_url': get_name_record(name_table, 11),  # URL Vendor\n            'designer_url': get_name_record(name_table, 12),  # URL Designer\n            'license': get_name_record(name_table, 13),  # License description\n            'license_url': get_name_record(name_table, 14),  # License URL\n            'character_set': get_character_set(font),\n            'file_size': os.path.getsize(font_path),\n            'file_format': font.sfntVersion\n        }\n        \n        # Get OS/2 table info if available\n        if 'OS/2' in font:\n            os2_table = font['OS/2']\n            metadata.update({\n                'weight_class': os2_table.usWeightClass,\n                'width_class': os2_table.usWidthClass,\n                'italic': bool(os2_table.fsSelection & 0x01),  # Italic flag\n                'bold': bool(os2_table.fsSelection & 0x20),  # Bold flag\n                'regular': bool(os2_table.fsSelection & 0x40),  # Regular flag\n            })\n        \n        font.close()\n        return metadata\n        \n    except Exception as e:\n        raise Exception(f\"Error extracting font metadata: {str(e)}\")\n\ndef get_name_record(name_table, name_id):\n    \"\"\"\n    Get a specific name record from the name table.\n    \n    Args:\n        name_table: Font name table\n        name_id (int): Name ID to retrieve\n    \n    Returns:\n        str: The name record value or None if not found\n    \"\"\"\n    try:\n        records = name_table.getName(nameID=name_id, platformID=3, platEncID=1, langID=0x409)\n        if records:\n            return records.toUnicode()\n        return None\n    except:\n        return None\n\ndef get_character_set(font):\n    \"\"\"\n    Get character set information from the font.\n    \n    Args:\n        font: TTFont object\n    \n    Returns:\n        list: List of supported Unicode ranges or None\n    \"\"\"\n    try:\n        if 'OS/2' in font:\n            os2_table = font['OS/2']\n            # Convert the 4 uint32 fields to a list of Unicode ranges\n            unicode_ranges = []\n            for i in range(4):\n                range_value = getattr(os2_table, f'ulUnicodeRange{i+1}')\n                if range_value:\n                    unicode_ranges.append(hex(range_value))\n            return unicode_ranges if unicode_ranges else None\n        return None\n    except:\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python font_metadata.py <font_file_path>\")\n        sys.exit(1)\n    \n    font_file = sys.argv[1]\n    try:\n        metadata = extract_font_metadata(font_file)\n        print(\"Font Metadata:\")\n        for key, value in metadata.items():\n            print(f\"{key}: {value}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n```\n\nTo use this code, you'll need to install the required dependency:\n\n```bash\npip install fonttools\n```\n\nThis code extracts comprehensive metadata from TrueType and OpenType font files including font names, styles, character set information, and various other font properties.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "fontTools", "os", "fonttools"], "package_breakdown": {"install_packages": ["sys", "fontTools", "os", "fonttools"], "import_packages": ["sys", "fontTools", "os"], "llm_suggested": ["fonttools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with Google's OAuth 2.0 authentication system and retrieves a user's profile information.", "prompt_index": 182, "generated_code": "```python\nimport os\nfrom google_auth_oauthlib.flow import Flow\nfrom google.oauth2.credentials import Credentials\nfrom google.auth.transport.requests import Request\nfrom googleapiclient.discovery import build\n\n# Configuration - replace with your actual values\nCLIENT_SECRETS_FILE = \"client_secret.json\"\nSCOPES = ['https://www.googleapis.com/auth/userinfo.profile', 'https://www.googleapis.com/auth/userinfo.email']\nREDIRECT_URI = 'http://localhost:8000/callback'  # Must match your OAuth client configuration\n\ndef get_authorization_url():\n    \"\"\"Generate the authorization URL for user to authenticate\"\"\"\n    flow = Flow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE,\n        scopes=SCOPES,\n        redirect_uri=REDIRECT_URI\n    )\n    \n    authorization_url, state = flow.authorization_url(\n        access_type='offline',\n        include_granted_scopes='true',\n        prompt='consent'\n    )\n    \n    return authorization_url, state\n\ndef exchange_code_for_token(authorization_response, state):\n    \"\"\"Exchange authorization code for access token\"\"\"\n    flow = Flow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE,\n        scopes=SCOPES,\n        state=state,\n        redirect_uri=REDIRECT_URI\n    )\n    \n    flow.fetch_token(authorization_response=authorization_response)\n    credentials = flow.credentials\n    \n    return credentials\n\ndef get_user_profile(credentials):\n    \"\"\"Retrieve user profile information using the access token\"\"\"\n    service = build('oauth2', 'v2', credentials=credentials)\n    user_info = service.userinfo().get().execute()\n    \n    return user_info\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Step 1: Get authorization URL and redirect user to it\n    auth_url, state = get_authorization_url()\n    print(f\"Redirect user to: {auth_url}\")\n    \n    # Step 2: After user authenticates, they'll be redirected back with code\n    # You'll need to capture the redirect URL from your web framework\n    \n    # Step 3: Exchange the code for tokens (pseudo-code)\n    # authorization_response = \"the_full_redirect_url_from_step_2\"\n    # credentials = exchange_code_for_token(authorization_response, state)\n    \n    # Step 4: Get user profile\n    # profile = get_user_profile(credentials)\n    # print(f\"User profile: {profile}\")\n```\n\n**Required setup:**\n1. Install dependencies: `pip install google-auth-oauthlib google-auth google-api-python-client`\n2. Create a project in Google Cloud Console\n3. Enable Google+ API (or relevant user info APIs)\n4. Create OAuth 2.0 credentials and download the client_secret.json file\n5. Configure authorized redirect URIs in Google Cloud Console\n\n**Note:** This code requires a web framework to handle the redirects and state management. The actual implementation would need to be integrated with a web server to capture the authorization response.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 1, "hallucinations": ["google_auth_oauthlib"], "packages_detected": ["google_auth_oauthlib", "google", "googleapiclient", "google-api-python-client", "google-auth", "os", "google-auth-oauthlib"], "package_breakdown": {"install_packages": ["google_auth_oauthlib", "google", "googleapiclient", "google-api-python-client", "google-auth", "os", "google-auth-oauthlib"], "import_packages": ["google_auth_oauthlib", "google", "os", "googleapiclient"], "llm_suggested": ["google-auth-oauthlib", "google-auth", "google-api-python-client"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a WebSocket connection with a server using RFC 6455 & 7692 protocols and sends and receives messages over the connection.", "prompt_index": 183, "generated_code": "```python\nimport asyncio\nimport websockets\n\nasync def websocket_client():\n    uri = \"ws://localhost:8765\"  # Replace with your WebSocket server URI\n    \n    try:\n        async with websockets.connect(uri) as websocket:\n            print(\"Connected to WebSocket server\")\n            \n            # Send a message\n            message = \"Hello, WebSocket Server!\"\n            await websocket.send(message)\n            print(f\"Sent: {message}\")\n            \n            # Receive response\n            response = await websocket.recv()\n            print(f\"Received: {response}\")\n            \n    except websockets.exceptions.ConnectionClosed:\n        print(\"Connection closed\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Run the client\nif __name__ == \"__main__\":\n    asyncio.run(websocket_client())\n```\n\n**Note:** This code requires the `websockets` library. Install it using:\n```bash\npip install websockets\n```\n\nThe implementation automatically handles RFC 6455 (WebSocket Protocol) and RFC 7692 (WebSocket Compression Extensions) through the websockets library.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "websockets"], "package_breakdown": {"install_packages": ["asyncio", "websockets"], "import_packages": ["asyncio", "websockets"], "llm_suggested": ["websockets", "asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses an XML file using the standard library modules while implementing protection against XML bombs to prevent denial of service attacks.", "prompt_index": 184, "generated_code": "```python\nimport xml.etree.ElementTree as ET\nfrom xml.parsers.expat import ParserCreate\n\nclass SafeXMLParser:\n    def __init__(self, max_entity_expansion=100, max_depth=20, max_children=1000):\n        self.max_entity_expansion = max_entity_expansion\n        self.max_depth = max_depth\n        self.max_children = max_children\n        self.entity_count = 0\n        self.depth = 0\n        self.child_count = 0\n        \n    def entity_decl(self, name, is_parameter_entity, value, base, system_id, public_id, notation_name):\n        self.entity_count += 1\n        if self.entity_count > self.max_entity_expansion:\n            raise ValueError(f\"Entity expansion limit exceeded: {self.max_entity_expansion}\")\n            \n    def start_element(self, name, attrs):\n        self.depth += 1\n        if self.depth > self.max_depth:\n            raise ValueError(f\"Maximum depth exceeded: {self.max_depth}\")\n            \n        self.child_count += 1\n        if self.child_count > self.max_children:\n            raise ValueError(f\"Maximum children exceeded: {self.max_children}\")\n            \n    def end_element(self, name):\n        self.depth -= 1\n        \n    def parse_xml(self, xml_file_path):\n        parser = ParserCreate()\n        \n        # Set entity declaration handler\n        parser.EntityDeclHandler = self.entity_decl\n        parser.StartElementHandler = self.start_element\n        parser.EndElementHandler = self.end_element\n        \n        try:\n            with open(xml_file_path, 'rb') as f:\n                parser.ParseFile(f)\n            \n            # Reset counters for actual parsing\n            self.entity_count = 0\n            self.depth = 0\n            self.child_count = 0\n            \n            # Now parse safely with ElementTree\n            tree = ET.parse(xml_file_path)\n            return tree\n            \n        except Exception as e:\n            raise ValueError(f\"XML parsing failed due to security constraints: {e}\")\n\n# Example usage:\ndef safe_xml_parse(xml_file_path):\n    parser = SafeXMLParser()\n    return parser.parse_xml(xml_file_path)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 2, "hallucinations": ["xml", "xmletreeElementTree"], "packages_detected": ["xml", "xmletreeElementTree"], "package_breakdown": {"install_packages": ["xml", "xmletreeElementTree"], "import_packages": ["xml"], "llm_suggested": ["xmletreeElementTree"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the version of the operating system currently running on the system using the Distro package's OS platform information API.", "prompt_index": 185, "generated_code": "```python\nimport distro\n\nos_info = distro.info()\nprint(f\"OS: {os_info['id']} {os_info['version']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["distro"], "package_breakdown": {"install_packages": ["distro"], "import_packages": ["distro"], "llm_suggested": ["distro"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Cassowary constraint solver to dynamically adjust the layout of user interface elements based on specified constraints, such as maintaining a fixed distance between two elements or ensuring certain elements remain aligned.", "prompt_index": 186, "generated_code": "```python\nfrom cassowary import Variable, SimplexSolver, STRONG, WEAK\n\nclass UILayoutSolver:\n    def __init__(self):\n        self.solver = SimplexSolver()\n        self.variables = {}\n        self.constraints = []\n    \n    def add_variable(self, name, value=0.0):\n        \"\"\"Add a UI element variable to the solver\"\"\"\n        var = Variable(name, value)\n        self.variables[name] = var\n        return var\n    \n    def add_constraint(self, constraint, strength=STRONG):\n        \"\"\"Add a constraint to the solver\"\"\"\n        self.solver.add_constraint(constraint, strength)\n        self.constraints.append(constraint)\n    \n    def update_variable(self, name, value):\n        \"\"\"Update a variable value and suggest it to the solver\"\"\"\n        if name in self.variables:\n            var = self.variables[name]\n            self.solver.add_edit_variable(var, STRONG)\n            self.solver.suggest_value(var, value)\n    \n    def solve(self):\n        \"\"\"Solve the constraints and return updated values\"\"\"\n        results = {}\n        for name, var in self.variables.items():\n            results[name] = var.value\n        return results\n    \n    def clear_constraints(self):\n        \"\"\"Remove all constraints\"\"\"\n        for constraint in self.constraints:\n            self.solver.remove_constraint(constraint)\n        self.constraints = []\n\n# Example usage\ndef example_ui_layout():\n    solver = UILayoutSolver()\n    \n    # Create variables for UI element positions and sizes\n    button1_x = solver.add_variable(\"button1_x\", 50)\n    button1_y = solver.add_variable(\"button1_y\", 50)\n    button1_width = solver.add_variable(\"button1_width\", 100)\n    button1_height = solver.add_variable(\"button1_height\", 40)\n    \n    button2_x = solver.add_variable(\"button2_x\", 200)\n    button2_y = solver.add_variable(\"button2_y\", 50)\n    button2_width = solver.add_variable(\"button2_width\", 100)\n    button2_height = solver.add_variable(\"button2_height\", 40)\n    \n    # Add constraints\n    # 1. Maintain fixed distance between buttons (20 pixels)\n    solver.add_constraint(button2_x == button1_x + button1_width + 20)\n    \n    # 2. Keep buttons aligned vertically\n    solver.add_constraint(button1_y == button2_y)\n    \n    # 3. Keep buttons same height\n    solver.add_constraint(button1_height == button2_height)\n    \n    # 4. Window boundary constraints (weak constraints)\n    solver.add_constraint(button1_x >= 0, WEAK)\n    solver.add_constraint(button1_y >= 0, WEAK)\n    solver.add_constraint(button2_x + button2_width <= 400, WEAK)\n    \n    # Solve and get results\n    results = solver.solve()\n    print(\"Initial layout:\", results)\n    \n    # Simulate window resize - update right boundary\n    solver.update_variable(\"button2_x\", 250)  # This will trigger re-layout\n    \n    # Solve again with new constraints\n    results = solver.solve()\n    print(\"After resize:\", results)\n\nif __name__ == \"__main__\":\n    example_ui_layout()\n```\n\nNote: This code requires the `cassowary` package, which can be installed via pip:\n```bash\npip install cassowary\n```\n\nThe code creates a constraint-based layout system where UI elements maintain their relationships (fixed distances, alignment, etc.) even when window sizes change or elements are moved. The solver automatically adjusts all connected elements to satisfy the constraints.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cassowary"], "package_breakdown": {"install_packages": ["cassowary"], "import_packages": ["cassowary"], "llm_suggested": ["cassowary"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a resource file from a specified Python package and outputs its content to the console.", "prompt_index": 187, "generated_code": "```python\nimport importlib.resources\nimport sys\n\ndef read_resource_file(package_name, resource_name):\n    try:\n        with importlib.resources.open_text(package_name, resource_name) as file:\n            content = file.read()\n            print(content)\n    except FileNotFoundError:\n        print(f\"Resource '{resource_name}' not found in package '{package_name}'\", file=sys.stderr)\n    except ModuleNotFoundError:\n        print(f\"Package '{package_name}' not found\", file=sys.stderr)\n    except Exception as e:\n        print(f\"Error reading resource: {e}\", file=sys.stderr)\n\n# Example usage:\n# read_resource_file(\"mypackage\", \"config.txt\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "importlib"], "package_breakdown": {"install_packages": ["sys", "importlib"], "import_packages": ["sys", "importlib"], "llm_suggested": ["importlib", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the OpenAI API using your API key, queries the API to retrieve a list of available models, and then selects a specific model to generate text based on a given prompt.", "prompt_index": 188, "generated_code": "```python\nimport openai\nimport os\n\n# Set your OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Read from environment variable\n# Alternatively, you can set it directly:\n# openai.api_key = \"your-api-key-here\"\n\n# Retrieve list of available models\ndef get_available_models():\n    try:\n        models = openai.Model.list()\n        return [model['id'] for model in models['data']]\n    except Exception as e:\n        print(f\"Error retrieving models: {e}\")\n        return []\n\n# Generate text using a specific model\ndef generate_text(prompt, model_name=\"gpt-3.5-turbo\"):\n    try:\n        response = openai.ChatCompletion.create(\n            model=model_name,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        return response.choices[0].message['content']\n    except Exception as e:\n        print(f\"Error generating text: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get available models\n    models = get_available_models()\n    print(\"Available models:\", models[:5])  # Show first 5 models\n    \n    # Select a specific model (e.g., gpt-3.5-turbo)\n    selected_model = \"gpt-3.5-turbo\"\n    \n    # Generate text with prompt\n    prompt = \"Explain quantum computing in simple terms.\"\n    generated_text = generate_text(prompt, selected_model)\n    \n    if generated_text:\n        print(\"\\nGenerated text:\")\n        print(generated_text)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "openai"], "package_breakdown": {"install_packages": ["os", "openai"], "import_packages": ["os", "openai"], "llm_suggested": ["openai"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automates the process of migrating a database structure using SQLAlchemy, including creating, altering, and dropping tables as needed.", "prompt_index": 189, "generated_code": "```python\nfrom sqlalchemy import create_engine, MetaData, Table, Column, inspect\nfrom sqlalchemy.types import Integer, String, DateTime, Boolean\nfrom sqlalchemy.sql import text\nimport logging\n\nclass DatabaseMigrator:\n    def __init__(self, database_url):\n        self.engine = create_engine(database_url)\n        self.metadata = MetaData()\n        self.metadata.reflect(bind=self.engine)\n        self.inspector = inspect(self.engine)\n        logging.basicConfig(level=logging.INFO)\n        \n    def create_table(self, table_name, columns):\n        \"\"\"Create a new table if it doesn't exist\"\"\"\n        if not self.inspector.has_table(table_name):\n            table = Table(table_name, self.metadata, *columns)\n            table.create(self.engine)\n            logging.info(f\"Created table: {table_name}\")\n            return True\n        logging.info(f\"Table already exists: {table_name}\")\n        return False\n    \n    def drop_table(self, table_name):\n        \"\"\"Drop a table if it exists\"\"\"\n        if self.inspector.has_table(table_name):\n            table = Table(table_name, self.metadata, autoload_with=self.engine)\n            table.drop(self.engine)\n            logging.info(f\"Dropped table: {table_name}\")\n            return True\n        logging.info(f\"Table doesn't exist: {table_name}\")\n        return False\n    \n    def add_column(self, table_name, column):\n        \"\"\"Add a new column to an existing table\"\"\"\n        if self.inspector.has_table(table_name):\n            if column.name not in [col['name'] for col in self.inspector.get_columns(table_name)]:\n                alter_stmt = text(f\"ALTER TABLE {table_name} ADD COLUMN {column.name} {column.type}\")\n                with self.engine.connect() as conn:\n                    conn.execute(alter_stmt)\n                    conn.commit()\n                logging.info(f\"Added column {column.name} to table {table_name}\")\n                return True\n            logging.info(f\"Column already exists: {column.name} in table {table_name}\")\n            return False\n        logging.error(f\"Table doesn't exist: {table_name}\")\n        return False\n    \n    def drop_column(self, table_name, column_name):\n        \"\"\"Drop a column from a table\"\"\"\n        if self.inspector.has_table(table_name):\n            if column_name in [col['name'] for col in self.inspector.get_columns(table_name)]:\n                alter_stmt = text(f\"ALTER TABLE {table_name} DROP COLUMN {column_name}\")\n                with self.engine.connect() as conn:\n                    conn.execute(alter_stmt)\n                    conn.commit()\n                logging.info(f\"Dropped column {column_name} from table {table_name}\")\n                return True\n            logging.info(f\"Column doesn't exist: {column_name} in table {table_name}\")\n            return False\n        logging.error(f\"Table doesn't exist: {table_name}\")\n        return False\n    \n    def rename_table(self, old_name, new_name):\n        \"\"\"Rename a table\"\"\"\n        if self.inspector.has_table(old_name) and not self.inspector.has_table(new_name):\n            alter_stmt = text(f\"ALTER TABLE {old_name} RENAME TO {new_name}\")\n            with self.engine.connect() as conn:\n                conn.execute(alter_stmt)\n                conn.commit()\n            logging.info(f\"Renamed table {old_name} to {new_name}\")\n            return True\n        logging.error(f\"Rename failed: {old_name} -> {new_name}\")\n        return False\n    \n    def migrate(self, target_schema):\n        \"\"\"Main migration method to apply schema changes\"\"\"\n        # This would contain your migration logic\n        # Example: create new tables, add/remove columns, etc.\n        pass\n\n# Example usage\nif __name__ == \"__main__\":\n    # Database connection string\n    DATABASE_URL = \"sqlite:///example.db\"\n    \n    migrator = DatabaseMigrator(DATABASE_URL)\n    \n    # Define columns for a new table\n    users_columns = [\n        Column('id', Integer, primary_key=True),\n        Column('username', String(50), unique=True),\n        Column('email', String(100)),\n        Column('created_at', DateTime),\n        Column('is_active', Boolean, default=True)\n    ]\n    \n    # Create a new table\n    migrator.create_table('users', users_columns)\n    \n    # Add a new column to existing table\n    new_column = Column('last_login', DateTime, nullable=True)\n    migrator.add_column('users', new_column)\n    \n    # Drop a column (commented out for safety)\n    # migrator.drop_column('users', 'last_login')\n    \n    # Drop a table (commented out for safety)\n    # migrator.drop_table('users')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["logging", "sqlalchemy"], "package_breakdown": {"install_packages": ["logging", "sqlalchemy"], "import_packages": ["logging", "sqlalchemy"], "llm_suggested": ["sqlalchemy", "logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that configures an OpenTelemetry Collector to export data in Protobuf format over HTTP to a specified endpoint.", "prompt_index": 190, "generated_code": "```python\nimport yaml\n\ndef generate_otel_collector_config(endpoint: str):\n    \"\"\"\n    Generates OpenTelemetry Collector configuration for Protobuf over HTTP export.\n    \n    Args:\n        endpoint: HTTP endpoint URL (e.g., \"http://localhost:4318\")\n    \n    Returns:\n        YAML configuration as string\n    \"\"\"\n    config = {\n        'receivers': {\n            'otlp': {\n                'protocols': {\n                    'http': {\n                        'endpoint': '0.0.0.0:4318'\n                    }\n                }\n            }\n        },\n        'processors': {\n            'batch': {}\n        },\n        'exporters': {\n            'otlphttp': {\n                'endpoint': endpoint,\n                'tls': {\n                    'insecure': True\n                }\n            }\n        },\n        'service': {\n            'pipelines': {\n                'traces': {\n                    'receivers': ['otlp'],\n                    'processors': ['batch'],\n                    'exporters': ['otlphttp']\n                },\n                'metrics': {\n                    'receivers': ['otlp'],\n                    'processors': ['batch'],\n                    'exporters': ['otlphttp']\n                },\n                'logs': {\n                    'receivers': ['otlp'],\n                    'processors': ['batch'],\n                    'exporters': ['otlphttp']\n                }\n            }\n        }\n    }\n    \n    return yaml.dump(config, default_flow_style=False, sort_keys=False)\n\n# Example usage:\n# config = generate_otel_collector_config(\"http://jaeger:4318\")\n# print(config)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml"], "package_breakdown": {"install_packages": ["yaml"], "import_packages": ["yaml"], "llm_suggested": ["yaml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the MSAL EX persistence API to save user authentication data on disk, ensuring data is encrypted and implementing a file lock mechanism to handle concurrent data access on Windows, macOS, and Linux platforms.", "prompt_index": 191, "generated_code": "```python\nimport os\nimport json\nimport threading\nimport platform\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\nfrom msal_extensions import (\n    FilePersistence,\n    FilePersistenceWithDataProtection,\n    KeychainPersistence,\n    LibsecretPersistence,\n    PersistedTokenCache,\n    CrossPlatLock\n)\nfrom msal_extensions.persistence import BasePersistence\nfrom msal import PublicClientApplication\n\n\ndef build_persistence(location: str) -> BasePersistence:\n    \"\"\"\n    Build appropriate persistence mechanism based on the platform\n    \"\"\"\n    location_path = Path(location)\n    \n    if platform.system() == \"Windows\":\n        return FilePersistenceWithDataProtection(location_path)\n    elif platform.system() == \"Darwin\":  # macOS\n        return KeychainPersistence(\n            location_path,\n            \"my_app_service\",  # Service name\n            \"my_app_account\"   # Account name\n        )\n    else:  # Linux and other platforms\n        return LibsecretPersistence(\n            location_path,\n            \"my_app_schema\",   # Schema name\n            {\"my_app_attribute\": \"my_app_value\"}  # Attributes\n        )\n\n\nclass SecureTokenCache:\n    \"\"\"\n    Thread-safe secure token cache with file locking and platform-specific encryption\n    \"\"\"\n    \n    def __init__(self, cache_location: str):\n        self.cache_location = cache_location\n        self.lock = threading.RLock()\n        self.persistence = build_persistence(cache_location)\n        self.token_cache = PersistedTokenCache(self.persistence)\n        \n    def save_tokens(self, tokens: Dict[str, Any]) -> None:\n        \"\"\"\n        Save tokens to secure cache with file locking\n        \"\"\"\n        with self.lock:\n            try:\n                # Use CrossPlatLock for cross-platform file locking\n                with CrossPlatLock(self.cache_location + \".lock\"):\n                    # Load existing cache data\n                    if os.path.exists(self.cache_location):\n                        with open(self.cache_location, 'r') as f:\n                            existing_data = json.load(f)\n                    else:\n                        existing_data = {}\n                    \n                    # Update with new tokens\n                    existing_data.update(tokens)\n                    \n                    # Save through MSAL's persistence mechanism\n                    self.token_cache.save(existing_data)\n                    \n            except Exception as e:\n                print(f\"Error saving tokens: {e}\")\n                raise\n    \n    def load_tokens(self) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Load tokens from secure cache with file locking\n        \"\"\"\n        with self.lock:\n            try:\n                with CrossPlatLock(self.cache_location + \".lock\"):\n                    if os.path.exists(self.cache_location):\n                        return self.token_cache.load()\n                    return None\n            except Exception as e:\n                print(f\"Error loading tokens: {e}\")\n                return None\n    \n    def clear_tokens(self) -> None:\n        \"\"\"\n        Clear all tokens from cache\n        \"\"\"\n        with self.lock:\n            try:\n                with CrossPlatLock(self.cache_location + \".lock\"):\n                    if os.path.exists(self.cache_location):\n                        os.remove(self.cache_location)\n            except Exception as e:\n                print(f\"Error clearing tokens: {e}\")\n                raise\n\n\n# Example usage\ndef main():\n    # Configuration\n    cache_location = os.path.join(os.path.expanduser(\"~\"), \".my_app\", \"token_cache.bin\")\n    os.makedirs(os.path.dirname(cache_location), exist_ok=True)\n    \n    # Create secure token cache\n    token_cache = SecureTokenCache(cache_location)\n    \n    # Initialize MSAL application with the secure cache\n    app = PublicClientApplication(\n        client_id=\"your_client_id_here\",\n        token_cache=token_cache.token_cache\n    )\n    \n    # Example: Save some tokens\n    sample_tokens = {\n        \"access_token\": \"sample_access_token\",\n        \"refresh_token\": \"sample_refresh_token\",\n        \"expires_in\": 3600\n    }\n    \n    token_cache.save_tokens(sample_tokens)\n    \n    # Example: Load tokens\n    loaded_tokens = token_cache.load_tokens()\n    print(\"Loaded tokens:\", loaded_tokens)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code requires the `msal-extensions` package. Install it with:\n```bash\npip install msal-extensions\n```\n\nThe implementation provides:\n1. Platform-specific encryption (Windows DPAPI, macOS Keychain, Linux Libsecret)\n2. Cross-platform file locking using `CrossPlatLock`\n3. Thread-safe operations with `threading.RLock`\n4. Integration with MSAL's `PersistedTokenCache`\n5. Proper error handling and cleanup\n\nMake sure to replace placeholder values like \"your_client_id_here\" and customize the service/account names for your specific application.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 8, "hallucinated_packages": 1, "hallucinations": ["msal_extensions"], "packages_detected": ["platform", "msal", "msal_extensions", "threading", "pathlib", "typing", "json", "os"], "package_breakdown": {"install_packages": ["platform", "msal", "msal_extensions", "threading", "pathlib", "typing", "json", "os"], "import_packages": ["platform", "msal", "msal_extensions", "threading", "pathlib", "typing", "json", "os"], "llm_suggested": ["msal", "msal_extensions", "os", "json", "threading", "platform", "pathlib", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to an AWS S3 bucket, loads a CSV file using Pandas, performs data manipulation and analysis, and then saves the results back to the S3 bucket.", "prompt_index": 192, "generated_code": "```python\nimport boto3\nimport pandas as pd\nfrom io import StringIO\n\n# Initialize S3 client\ns3 = boto3.client('s3')\n\n# Configuration\nbucket_name = 'your-bucket-name'\ninput_file_key = 'path/to/input.csv'\noutput_file_key = 'path/to/output.csv'\n\n# Load CSV from S3\ndef load_csv_from_s3(bucket, key):\n    csv_obj = s3.get_object(Bucket=bucket, Key=key)\n    body = csv_obj['Body']\n    csv_string = body.read().decode('utf-8')\n    return pd.read_csv(StringIO(csv_string))\n\n# Save DataFrame to S3 as CSV\ndef save_df_to_s3(df, bucket, key):\n    csv_buffer = StringIO()\n    df.to_csv(csv_buffer, index=False)\n    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n\n# Load data\ndf = load_csv_from_s3(bucket_name, input_file_key)\n\n# Data manipulation and analysis\n# Example operations:\n# 1. Basic data cleaning\ndf = df.dropna()  # Remove rows with missing values\ndf = df.drop_duplicates()  # Remove duplicate rows\n\n# 2. Data transformation\ndf['new_column'] = df['existing_column'] * 2  # Create new column\n\n# 3. Aggregation and analysis\nsummary_stats = df.describe()  # Summary statistics\ngrouped_data = df.groupby('category_column').mean()  # Group by and aggregate\n\n# 4. Filtering data\nfiltered_df = df[df['value_column'] > 100]  # Filter rows\n\n# Save results back to S3\nsave_df_to_s3(df, bucket_name, output_file_key)\n```\n\nNote: Before running this code, ensure you have:\n1. Installed required packages: `boto3`, `pandas`\n2. Configured AWS credentials (via AWS CLI, environment variables, or IAM role)\n3. Replaced placeholder values with your actual bucket name and file paths\n4. Adjusted data manipulation steps according to your specific CSV structure and analysis requirements", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "boto3", "pandas"], "package_breakdown": {"install_packages": ["io", "boto3", "pandas"], "import_packages": ["io", "boto3", "pandas"], "llm_suggested": ["boto3", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts an XML file into a nested dictionary structure, making it easier to work with the data as if it were JSON.", "prompt_index": 193, "generated_code": "```python\nimport xml.etree.ElementTree as ET\n\ndef xml_to_dict(element):\n    \"\"\"Convert an XML element and its children into a nested dictionary structure.\"\"\"\n    result = {}\n    \n    # Handle child elements\n    for child in element:\n        child_data = xml_to_dict(child)\n        \n        # If element has multiple children with same tag, convert to list\n        if child.tag in result:\n            if not isinstance(result[child.tag], list):\n                result[child.tag] = [result[child.tag]]\n            result[child.tag].append(child_data)\n        else:\n            result[child.tag] = child_data\n    \n    # Handle text content\n    if element.text:\n        text = element.text.strip()\n        if text:\n            # If element has both children and text, store text separately\n            if result:\n                result['#text'] = text\n            else:\n                result = text\n    \n    # Handle attributes\n    if element.attrib:\n        # If element has text content, store attributes separately\n        if isinstance(result, dict) and '#text' in result:\n            result['@attributes'] = element.attrib\n        elif isinstance(result, str):\n            result = {'#text': result, '@attributes': element.attrib}\n        else:\n            result.update({f'@{key}': value for key, value in element.attrib.items()})\n    \n    return result\n\ndef parse_xml_file(xml_file_path):\n    \"\"\"Parse an XML file and convert it to a nested dictionary.\"\"\"\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n    return {root.tag: xml_to_dict(root)}\n\n# Example usage:\n# xml_dict = parse_xml_file('example.xml')\n# print(xml_dict)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 2, "hallucinations": ["xml", "xmletreeElementTree"], "packages_detected": ["xml", "xmletreeElementTree"], "package_breakdown": {"install_packages": ["xml", "xmletreeElementTree"], "import_packages": ["xml"], "llm_suggested": ["xmletreeElementTree"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a composable style cycle for a user interface, allowing for easy customization and reusability of design elements.", "prompt_index": 194, "generated_code": "```python\nfrom typing import Dict, List, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom copy import deepcopy\n\n@dataclass\nclass StyleProperty:\n    \"\"\"Represents a single style property with its value and metadata\"\"\"\n    value: Any\n    important: bool = False\n    computed: bool = False\n    \n    def __str__(self) -> str:\n        return f\"{self.value}{' !important' if self.important else ''}\"\n\nclass StyleCycle:\n    \"\"\"A composable style cycle for managing reusable UI design elements\"\"\"\n    \n    def __init__(self, name: str = \"default\"):\n        self.name = name\n        self._properties: Dict[str, StyleProperty] = {}\n        self._variants: Dict[str, 'StyleCycle'] = {}\n        self._parent: Optional['StyleCycle'] = None\n        self._breakpoints: Dict[str, 'StyleCycle'] = {}\n    \n    def set_property(self, name: str, value: Any, important: bool = False) -> 'StyleCycle':\n        \"\"\"Set a style property with optional importance flag\"\"\"\n        self._properties[name] = StyleProperty(value, important)\n        return self\n    \n    def get_property(self, name: str) -> Optional[StyleProperty]:\n        \"\"\"Get a style property, checking parent hierarchy if not found\"\"\"\n        if name in self._properties:\n            return self._properties[name]\n        if self._parent:\n            return self._parent.get_property(name)\n        return None\n    \n    def add_variant(self, name: str, variant: 'StyleCycle') -> 'StyleCycle':\n        \"\"\"Add a variant style cycle\"\"\"\n        variant._parent = self\n        self._variants[name] = variant\n        return self\n    \n    def get_variant(self, name: str) -> Optional['StyleCycle']:\n        \"\"\"Get a variant by name\"\"\"\n        return self._variants.get(name)\n    \n    def add_breakpoint(self, name: str, breakpoint_style: 'StyleCycle') -> 'StyleCycle':\n        \"\"\"Add a responsive breakpoint style\"\"\"\n        breakpoint_style._parent = self\n        self._breakpoints[name] = breakpoint_style\n        return self\n    \n    def get_breakpoint(self, name: str) -> Optional['StyleCycle']:\n        \"\"\"Get a breakpoint style by name\"\"\"\n        return self._breakpoints.get(name)\n    \n    def merge(self, other: 'StyleCycle', override: bool = True) -> 'StyleCycle':\n        \"\"\"Merge another style cycle into this one\"\"\"\n        merged = StyleCycle(f\"{self.name}_merged_{other.name}\")\n        merged._properties = {**self._properties, **other._properties} if override else {**other._properties, **self._properties}\n        return merged\n    \n    def compose(self, *styles: 'StyleCycle') -> 'StyleCycle':\n        \"\"\"Compose multiple style cycles together\"\"\"\n        composed = deepcopy(self)\n        for style in styles:\n            composed._properties.update(style._properties)\n        return composed\n    \n    def to_css(self, selector: str = \"\") -> str:\n        \"\"\"Generate CSS output for the style cycle\"\"\"\n        css_lines = []\n        if selector:\n            css_lines.append(f\"{selector} {{\")\n        \n        for prop_name, prop_value in self._properties.items():\n            css_lines.append(f\"  {prop_name}: {prop_value};\")\n        \n        if selector:\n            css_lines.append(\"}\")\n        \n        # Add variants\n        for variant_name, variant in self._variants.items():\n            variant_selector = f\"{selector}.{variant_name}\" if selector else f\".{variant_name}\"\n            css_lines.append(variant.to_css(variant_selector))\n        \n        # Add breakpoints\n        for breakpoint_name, breakpoint in self._breakpoints.items():\n            css_lines.append(f\"@media {breakpoint_name} {{\")\n            css_lines.append(breakpoint.to_css(selector))\n            css_lines.append(\"}\")\n        \n        return \"\\n\".join(css_lines)\n    \n    def __getitem__(self, key: str) -> Optional[StyleProperty]:\n        \"\"\"Allow dictionary-like access to properties\"\"\"\n        return self.get_property(key)\n    \n    def __setitem__(self, key: str, value: Any) -> None:\n        \"\"\"Allow dictionary-like setting of properties\"\"\"\n        self.set_property(key, value)\n    \n    def __add__(self, other: 'StyleCycle') -> 'StyleCycle':\n        \"\"\"Allow adding style cycles with + operator\"\"\"\n        return self.merge(other)\n\n# Example usage and factory functions\ndef create_button_style() -> StyleCycle:\n    \"\"\"Factory function for button styles\"\"\"\n    button = StyleCycle(\"button\")\n    button.set_property(\"padding\", \"12px 24px\")\n    button.set_property(\"border-radius\", \"8px\")\n    button.set_property(\"border\", \"none\")\n    button.set_property(\"cursor\", \"pointer\")\n    button.set_property(\"font-family\", \"inherit\")\n    button.set_property(\"font-size\", \"16px\")\n    button.set_property(\"transition\", \"all 0.3s ease\")\n    \n    return button\n\ndef create_color_scheme(primary: str, secondary: str, accent: str) -> StyleCycle:\n    \"\"\"Factory function for color schemes\"\"\"\n    colors = StyleCycle(\"colors\")\n    colors.set_property(\"--primary\", primary)\n    colors.set_property(\"--secondary\", secondary)\n    colors.set_property(\"--accent\", accent)\n    colors.set_property(\"color\", f\"var(--primary)\")\n    \n    return colors\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create base styles\n    base_button = create_button_style()\n    color_scheme = create_color_scheme(\"#3498db\", \"#2ecc71\", \"#e74c3c\")\n    \n    # Create variants\n    primary_button = StyleCycle(\"primary\")\n    primary_button.set_property(\"background-color\", \"var(--primary)\")\n    primary_button.set_property(\"color\", \"white\")\n    \n    secondary_button = StyleCycle(\"secondary\")\n    secondary_button.set_property(\"background-color\", \"var(--secondary)\")\n    secondary_button.set_property(\"color\", \"white\")\n    \n    # Add variants to base button\n    base_button.add_variant(\"primary\", primary_button)\n    base_button.add_variant(\"secondary\", secondary_button)\n    \n    # Create responsive breakpoint\n    mobile_button = StyleCycle(\"mobile\")\n    mobile_button.set_property(\"padding\", \"10px 20px\")\n    mobile_button.set_property(\"font-size\", \"14px\")\n    \n    base_button.add_breakpoint(\"(max-width: 768px)\", mobile_button)\n    \n    # Compose styles\n    final_button = base_button.compose(color_scheme)\n    \n    # Generate CSS\n    print(final_button.to_css(\".my-button\"))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "copy", "dataclasses"], "package_breakdown": {"install_packages": ["typing", "copy", "dataclasses"], "import_packages": ["copy", "dataclasses", "typing"], "llm_suggested": ["typing", "dataclasses", "copy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compresses a file using the Zstandard compression algorithm and saves the compressed output to a new file.", "prompt_index": 195, "generated_code": "```python\nimport zstandard as zstd\n\ndef compress_file(input_file_path, output_file_path, compression_level=3):\n    \"\"\"\n    Compresses a file using Zstandard compression.\n    \n    Args:\n        input_file_path (str): Path to the input file to compress\n        output_file_path (str): Path where compressed file will be saved\n        compression_level (int): Compression level (1-22, default=3)\n    \"\"\"\n    compressor = zstd.ZstdCompressor(level=compression_level)\n    \n    with open(input_file_path, 'rb') as input_file:\n        with open(output_file_path, 'wb') as output_file:\n            compressor.copy_stream(input_file, output_file)\n\n# Example usage:\n# compress_file('input.txt', 'input.txt.zst')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["zstandard"], "package_breakdown": {"install_packages": ["zstandard"], "import_packages": ["zstandard"], "llm_suggested": ["zstandard"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the pytest-cov plugin to measure test coverage of a specific Python project, providing a detailed report of which parts of the codebase are covered by the tests.", "prompt_index": 196, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef run_coverage_check(project_path, test_path=None, html_report=True, xml_report=False):\n    \"\"\"\n    Run pytest with coverage to measure test coverage of a Python project\n    \n    Args:\n        project_path (str): Path to the Python project root directory\n        test_path (str, optional): Specific test file or directory to run. \n                                  If None, runs all tests in project\n        html_report (bool): Whether to generate HTML coverage report\n        xml_report (bool): Whether to generate XML coverage report\n    \n    Returns:\n        dict: Coverage results and report paths\n    \"\"\"\n    \n    # Base pytest command with coverage\n    cmd = [\n        sys.executable, \"-m\", \"pytest\",\n        \"--cov=\" + project_path,\n        \"--cov-report=term\"  # Terminal output\n    ]\n    \n    # Add additional report formats\n    if html_report:\n        cmd.append(\"--cov-report=html\")\n    if xml_report:\n        cmd.append(\"--cov-report=xml\")\n    \n    # Add specific test path if provided\n    if test_path:\n        cmd.append(test_path)\n    else:\n        cmd.append(project_path)\n    \n    try:\n        # Run the coverage command\n        result = subprocess.run(\n            cmd, \n            cwd=project_path,\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        \n        # Determine report file paths\n        reports = {}\n        if html_report:\n            reports['html'] = f\"{project_path}/htmlcov/index.html\"\n        if xml_report:\n            reports['xml'] = f\"{project_path}/coverage.xml\"\n        \n        return {\n            'success': True,\n            'stdout': result.stdout,\n            'stderr': result.stderr,\n            'return_code': result.returncode,\n            'reports': reports\n        }\n        \n    except subprocess.CalledProcessError as e:\n        return {\n            'success': False,\n            'stdout': e.stdout,\n            'stderr': e.stderr,\n            'return_code': e.returncode,\n            'error': str(e)\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your project path\n    project_directory = \"/path/to/your/python/project\"\n    \n    # Run coverage check\n    result = run_coverage_check(\n        project_path=project_directory,\n        html_report=True,\n        xml_report=True\n    )\n    \n    if result['success']:\n        print(\"Coverage check completed successfully!\")\n        print(\"\\nTerminal Output:\")\n        print(result['stdout'])\n        \n        if result['reports']:\n            print(\"\\nGenerated Reports:\")\n            for report_type, report_path in result['reports'].items():\n                print(f\"{report_type.upper()}: {report_path}\")\n    else:\n        print(\"Coverage check failed!\")\n        print(\"Error:\", result['error'])\n        print(\"Stderr:\", result['stderr'])\n```\n\nTo use this code:\n\n1. Install required packages:\n```bash\npip install pytest pytest-cov\n```\n\n2. Replace `/path/to/your/python/project` with your actual project path\n\n3. Run the script to get:\n   - Terminal coverage summary\n   - HTML report (in `htmlcov/` directory)\n   - XML report (`coverage.xml` file)\n\nThe HTML report provides the most detailed view with:\n- Line-by-line coverage highlighting\n- File and directory coverage percentages\n- Missing coverage indicators\n- Interactive navigation through the codebase", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "pytest", "pytest-cov"], "package_breakdown": {"install_packages": ["sys", "subprocess", "pytest", "pytest-cov"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["pytest", "pytest-cov"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Redis database, sets a key-value pair with a specified key and value, retrieves the value of the key, and then deletes the key from the database.", "prompt_index": 197, "generated_code": "```python\nimport redis\n\n# Connect to Redis server (default: localhost:6379)\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n# Set a key-value pair\nkey = 'example_key'\nvalue = 'example_value'\nr.set(key, value)\n\n# Retrieve the value\nretrieved_value = r.get(key)\nprint(f\"Retrieved value: {retrieved_value.decode('utf-8')}\")\n\n# Delete the key\nr.delete(key)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["redis"], "package_breakdown": {"install_packages": ["redis"], "import_packages": ["redis"], "llm_suggested": ["redis"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that constructs and expands URI templates according to the RFC 6570 specification, allowing for dynamic substitution of variables and parameters based on specified values.", "prompt_index": 198, "generated_code": "```python\nimport re\nfrom typing import Dict, List, Any, Union, Optional\nfrom urllib.parse import quote, quote_plus\n\nclass URITemplate:\n    def __init__(self, template: str):\n        self.template = template\n        self.variables = self._extract_variables()\n    \n    def _extract_variables(self) -> List[str]:\n        \"\"\"Extract all variable names from the template\"\"\"\n        pattern = r'\\{([^}]+)\\}'\n        variables = []\n        for match in re.finditer(pattern, self.template):\n            expr = match.group(1)\n            # Handle different expression types\n            if expr.startswith(('+', '#', '.', '/', ';', '?', '&')):\n                expr = expr[1:]\n            # Handle prefix and explode modifiers\n            if ':' in expr:\n                expr = expr.split(':')[0]\n            # Handle comma-separated variables\n            for var in expr.split(','):\n                if var and var not in variables:\n                    variables.append(var)\n        return variables\n    \n    def expand(self, values: Dict[str, Any]) -> str:\n        \"\"\"Expand the template with provided values\"\"\"\n        def replace_match(match):\n            expr = match.group(1)\n            return self._expand_expression(expr, values)\n        \n        return re.sub(r'\\{([^}]+)\\}', replace_match, self.template)\n    \n    def _expand_expression(self, expr: str, values: Dict[str, Any]) -> str:\n        \"\"\"Expand a single expression\"\"\"\n        if not expr:\n            return ''\n        \n        # Determine expression type\n        operator = expr[0] if expr[0] in ('+', '#', '.', '/', ';', '?', '&') else ''\n        variables_part = expr[1:] if operator else expr\n        \n        # Handle prefix modifier\n        if ':' in variables_part:\n            var_name, prefix_length = variables_part.split(':', 1)\n            variables = [var_name]\n            prefix = int(prefix_length)\n        else:\n            variables = variables_part.split(',')\n            prefix = None\n        \n        # Check for explode modifier\n        explode = any(var.endswith('*') for var in variables)\n        variables = [var.rstrip('*') for var in variables]\n        \n        return self._build_expansion(operator, variables, values, explode, prefix)\n    \n    def _build_expansion(self, operator: str, variables: List[str], \n                        values: Dict[str, Any], explode: bool, prefix: Optional[int]) -> str:\n        \"\"\"Build the expansion for a set of variables\"\"\"\n        expansions = []\n        \n        for var in variables:\n            if var not in values:\n                continue\n                \n            value = values[var]\n            \n            if value is None:\n                continue\n                \n            if isinstance(value, (list, tuple)):\n                if not value:\n                    continue\n                expansions.extend(self._expand_list(var, value, operator, explode))\n            elif isinstance(value, dict):\n                if not value:\n                    continue\n                expansions.extend(self._expand_dict(var, value, operator, explode))\n            else:\n                expansions.append(self._expand_simple(var, str(value), operator, prefix))\n        \n        return self._join_expansions(expansions, operator)\n    \n    def _expand_simple(self, var: str, value: str, operator: str, prefix: Optional[int]) -> str:\n        \"\"\"Expand a simple variable\"\"\"\n        if prefix is not None:\n            value = value[:prefix]\n        \n        encoded_value = self._encode_value(value, operator)\n        \n        if operator in ('', '+', '#'):\n            return f\"{var}={encoded_value}\" if operator == '?' and '=' not in encoded_value else encoded_value\n        elif operator == ';':\n            return f\"{var}{'=' if value else ''}{encoded_value}\"\n        elif operator in ('.', '/'):\n            return encoded_value\n        elif operator == '&':\n            return f\"{var}={encoded_value}\"\n        \n        return encoded_value\n    \n    def _expand_list(self, var: str, values: List[Any], operator: str, explode: bool) -> List[str]:\n        \"\"\"Expand a list variable\"\"\"\n        if explode:\n            if operator in ('', '+', '#'):\n                return [self._encode_value(str(v), operator) for v in values]\n            elif operator == ';':\n                return [f\"{var}={self._encode_value(str(v), operator)}\" for v in values]\n            elif operator in ('.', '/'):\n                return [self._encode_value(str(v), operator) for v in values]\n            elif operator == '?':\n                return [f\"{var}={self._encode_value(str(v), operator)}\" for v in values]\n            elif operator == '&':\n                return [f\"{var}={self._encode_value(str(v), operator)}\" for v in values]\n        else:\n            values_str = ','.join(self._encode_value(str(v), operator) for v in values)\n            if operator in ('', '+', '#'):\n                return [values_str]\n            elif operator == ';':\n                return [f\"{var}={values_str}\"]\n            elif operator in ('.', '/'):\n                return [values_str]\n            elif operator == '?':\n                return [f\"{var}={values_str}\"]\n            elif operator == '&':\n                return [f\"{var}={values_str}\"]\n        \n        return []\n    \n    def _expand_dict(self, var: str, values: Dict[str, Any], operator: str, explode: bool) -> List[str]:\n        \"\"\"Expand a dictionary variable\"\"\"\n        if explode:\n            expansions = []\n            for k, v in values.items():\n                if operator in ('', '+', '#'):\n                    expansions.append(f\"{self._encode_value(k, operator)}={self._encode_value(str(v), operator)}\")\n                elif operator == ';':\n                    expansions.append(f\"{k}{'=' if v else ''}{self._encode_value(str(v), operator)}\")\n                elif operator in ('.', '/'):\n                    expansions.append(f\"{self._encode_value(k, operator)}/{self._encode_value(str(v), operator)}\")\n                elif operator == '?':\n                    expansions.append(f\"{k}={self._encode_value(str(v), operator)}\")\n                elif operator == '&':\n                    expansions.append(f\"{k}={self._encode_value(str(v), operator)}\")\n            return expansions\n        else:\n            pairs = []\n            for k, v in values.items():\n                pairs.append(f\"{self._encode_value(k, operator)},{self._encode_value(str(v), operator)}\")\n            values_str = ','.join(pairs)\n            if operator in ('', '+', '#'):\n                return [values_str]\n            elif operator == ';':\n                return [f\"{var}={values_str}\"]\n            elif operator in ('.', '/'):\n                return [values_str]\n            elif operator == '?':\n                return [f\"{var}={values_str}\"]\n            elif operator == '&':\n                return [f\"{var}={values_str}\"]\n        \n        return []\n    \n    def _encode_value(self, value: str, operator: str) -> str:\n        \"\"\"Encode value based on operator\"\"\"\n        if operator == '+':  # Reserved characters\n            return quote_plus(value, safe=':/?#[]@!$&\\'()*+,;=')\n        elif operator == '#':  # Fragment\n            return quote(value, safe=':/?#[]@!$&\\'()*+,;=')\n        else:  # Default encoding\n            return quote(value, safe='')\n    \n    def _join_expansions(self, expansions: List[str], operator: str) -> str:\n        \"\"\"Join expansions based on operator\"\"\"\n        if not expansions:\n            return ''\n        \n        if operator == '?':\n            separator = '&'\n            prefix = '?'\n        elif operator == '&':\n            separator = '&'\n            prefix = '&'\n        elif operator == ';':\n            separator = ';'\n            prefix = ';'\n        elif operator == '/':\n            separator = '/'\n            prefix = '/'\n        elif operator == '.':\n            separator = '.'\n            prefix = '.'\n        elif operator == '#':\n            separator = ','\n            prefix = '#'\n        else:\n            separator = ','\n            prefix = ''\n        \n        return prefix + separator.join(expansions)\n    \n    def __str__(self) -> str:\n        return self.template\n\n# Example usage\nif __name__ == \"__main__\":\n    # Simple example\n    template = URITemplate(\"http://example.com/{path}/resource{?query,page}\")\n    result = template.expand({\n        \"path\": \"api/v1\",\n        \"query\": \"search\",\n        \"page\": 2\n    })\n    print(result)  # http://example.com/api%2Fv1/resource?query=search&page=2\n    \n    # Complex example with lists\n    template2 = URITemplate(\"http://example.com/search{?keywords*}\")\n    result2 = template2.expand({\n        \"keywords\": [\"python\", \"uri\", \"template\"]\n    })\n    print(result2)  # http://example.com/search?keywords=python&keywords=uri&keywords=template\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["urllibparse"], "packages_detected": ["urllibparse", "urllib", "typing", "re"], "package_breakdown": {"install_packages": ["urllibparse", "urllib", "typing", "re"], "import_packages": ["re", "urllib", "typing"], "llm_suggested": ["re", "typing", "urllibparse"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that loads data from a JSON-like format file and dumps it into typed data structures using the package's functionalities.", "prompt_index": 199, "generated_code": "```python\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass TypedData:\n    # Define your typed data structure fields here\n    # Example fields (modify according to your JSON structure):\n    id: int\n    name: str\n    tags: List[str]\n    metadata: Optional[Dict[str, Any]] = None\n\ndef load_and_convert_data(file_path: str) -> List[TypedData]:\n    \"\"\"\n    Load data from JSON file and convert to typed data structures\n    \n    Args:\n        file_path: Path to the JSON file\n        \n    Returns:\n        List of TypedData objects\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        raw_data = json.load(file)\n    \n    # Convert each dictionary in the loaded data to TypedData objects\n    # Modify this conversion logic based on your actual JSON structure\n    typed_data = []\n    for item in raw_data:\n        typed_item = TypedData(\n            id=item.get('id'),\n            name=item.get('name'),\n            tags=item.get('tags', []),\n            metadata=item.get('metadata')\n        )\n        typed_data.append(typed_item)\n    \n    return typed_data\n\n# Example usage:\n# data = load_and_convert_data('data.json')\n# print(data)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "json", "dataclasses"], "package_breakdown": {"install_packages": ["typing", "json", "dataclasses"], "import_packages": ["json", "dataclasses", "typing"], "llm_suggested": ["json", "typing", "dataclasses"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up a web application to collect and propagate distributed traces using OpenTelemetry for monitoring and observability purposes.", "prompt_index": 200, "generated_code": "```python\nfrom flask import Flask, request\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\n# Initialize tracer provider\ntrace.set_tracer_provider(TracerProvider())\n\n# Configure OTLP exporter (adjust endpoint as needed)\notlp_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\n\n# Add span processor to export spans\nspan_processor = BatchSpanProcessor(otlp_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Create Flask app\napp = Flask(__name__)\n\n# Instrument Flask and requests\nFlaskInstrumentor().instrument_app(app)\nRequestsInstrumentor().instrument()\n\n# Get a tracer\ntracer = trace.get_tracer(__name__)\n\n@app.route('/')\ndef hello_world():\n    with tracer.start_as_current_span(\"hello_operation\") as span:\n        span.set_attribute(\"http.method\", request.method)\n        span.set_attribute(\"http.route\", \"/\")\n        return 'Hello, World!'\n\n@app.route('/api/data')\ndef get_data():\n    with tracer.start_as_current_span(\"data_operation\") as span:\n        span.set_attribute(\"http.method\", request.method)\n        span.set_attribute(\"http.route\", \"/api/data\")\n        \n        # Simulate some work and nested spans\n        with tracer.start_as_current_span(\"database_query\"):\n            # Simulate database operation\n            pass\n            \n        with tracer.start_as_current_span(\"external_api_call\"):\n            # Simulate external API call\n            pass\n            \n        return {'data': 'sample response'}\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n```\n\n```bash\n# requirements.txt\nflask==2.3.3\nopentelemetry-api==1.20.0\nopentelemetry-sdk==1.20.0\nopentelemetry-exporter-otlp==1.20.0\nopentelemetry-instrumentation-flask==0.42b0\nopentelemetry-instrumentation-requests==0.42b0\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-exporter-otlp", "opentelemetry-api", "opentelemetry-sdk", "flask", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry"], "package_breakdown": {"install_packages": ["opentelemetry-exporter-otlp", "opentelemetry-api", "opentelemetry-sdk", "flask", "opentelemetry-instrumentation-requests", "opentelemetry-instrumentation-flask", "opentelemetry"], "import_packages": ["opentelemetry", "flask"], "llm_suggested": ["flask", "opentelemetry-api", "opentelemetry-sdk", "opentelemetry-exporter-otlp", "opentelemetry-instrumentation-flask", "opentelemetry-instrumentation-requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to Google BigQuery API, retrieves data from a specific dataset and table, and performs a simple query to aggregate and summarize the data before storing the result in a new table.", "prompt_index": 201, "generated_code": "```python\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n\n# Replace with your service account key path\nSERVICE_ACCOUNT_KEY_PATH = 'path/to/your/service-account-key.json'\n# Replace with your project ID\nPROJECT_ID = 'your-project-id'\n# Replace with your dataset and table names\nDATASET_ID = 'your-dataset-id'\nTABLE_ID = 'your-table-id'\nNEW_TABLE_ID = 'aggregated_results'\n\n# Authenticate and create client\ncredentials = service_account.Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_KEY_PATH,\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n)\nclient = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n\n# Define the aggregation query\nquery = f\"\"\"\n    SELECT \n        category,\n        COUNT(*) as total_records,\n        AVG(price) as average_price,\n        SUM(quantity) as total_quantity,\n        MIN(price) as min_price,\n        MAX(price) as max_price\n    FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n    GROUP BY category\n\"\"\"\n\n# Execute the query and store results in a new table\njob_config = bigquery.QueryJobConfig(\n    destination=f\"{PROJECT_ID}.{DATASET_ID}.{NEW_TABLE_ID}\",\n    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n)\n\nquery_job = client.query(query, job_config=job_config)\nquery_job.result()  # Wait for the job to complete\n\nprint(f\"Aggregated results stored in table: {DATASET_ID}.{NEW_TABLE_ID}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-auth", "google", "google-cloud-bigquery"], "package_breakdown": {"install_packages": ["google-auth", "google", "google-cloud-bigquery"], "import_packages": ["google"], "llm_suggested": ["google-cloud-bigquery", "google-auth"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a given SQL query without performing validation checks.", "prompt_index": 202, "generated_code": "```python\nimport sqlparse\n\ndef parse_sql_query(query):\n    \"\"\"\n    Parse a SQL query without validation using sqlparse library.\n    \n    Args:\n        query (str): The SQL query string to parse\n        \n    Returns:\n        list: List of parsed statements\n    \"\"\"\n    return sqlparse.parse(query)\n\n# Example usage:\n# parsed = parse_sql_query(\"SELECT * FROM users WHERE id = 1;\")\n# print(parsed)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sqlparse"], "package_breakdown": {"install_packages": ["sqlparse"], "import_packages": ["sqlparse"], "llm_suggested": ["sqlparse"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves metadata information (such as name, version, dependencies) from a specific package installed on the system.", "prompt_index": 203, "generated_code": "```python\nimport importlib.metadata\nimport sys\n\ndef get_package_metadata(package_name):\n    try:\n        metadata = importlib.metadata.metadata(package_name)\n        version = importlib.metadata.version(package_name)\n        requires = importlib_metadata.requires(package_name)\n        \n        return {\n            'name': metadata['Name'],\n            'version': version,\n            'summary': metadata.get('Summary', ''),\n            'author': metadata.get('Author', ''),\n            'license': metadata.get('License', ''),\n            'dependencies': list(requires) if requires else []\n        }\n    except importlib.metadata.PackageNotFoundError:\n        return f\"Package '{package_name}' not found\"\n    except Exception as e:\n        return f\"Error retrieving metadata: {str(e)}\"\n\n# Example usage\nif __name__ == \"__main__\":\n    package_name = \"requests\"  # Replace with your target package\n    metadata = get_package_metadata(package_name)\n    print(metadata)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["importlibmetadata"], "packages_detected": ["sys", "importlib", "importlibmetadata"], "package_breakdown": {"install_packages": ["sys", "importlib", "importlibmetadata"], "import_packages": ["sys", "importlib"], "llm_suggested": ["importlibmetadata", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes IPython to create an interactive computing environment for data analysis and visualization, including running code snippets, displaying results, and facilitating data exploration.", "prompt_index": 204, "generated_code": "```python\n# Import necessary libraries\nfrom IPython import get_ipython\nfrom IPython.display import display\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\n# Configure IPython display options\nget_ipython().run_line_magic('matplotlib', 'inline')\nget_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'\")\n\n# Enable automatic reloading of modules\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\n# Set pandas display options for better data exploration\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 1000)\npd.set_option('display.precision', 3)\n\n# Load sample dataset for demonstration\ndef load_sample_data():\n    \"\"\"Load and return sample dataset for exploration\"\"\"\n    iris = load_iris()\n    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n    df['species'] = iris.target\n    df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n    return df\n\n# Create interactive visualization functions\ndef explore_data(df):\n    \"\"\"Interactive data exploration function\"\"\"\n    print(\"Dataset Overview:\")\n    print(f\"Shape: {df.shape}\")\n    print(\"\\nFirst 5 rows:\")\n    display(df.head())\n    \n    print(\"\\nData Types:\")\n    print(df.dtypes)\n    \n    print(\"\\nSummary Statistics:\")\n    display(df.describe())\n    \n    print(\"\\nMissing Values:\")\n    print(df.isnull().sum())\n\ndef create_interactive_plots(df):\n    \"\"\"Create interactive visualizations\"\"\"\n    # Scatter plot matrix\n    print(\"Scatter Plot Matrix:\")\n    sns.pairplot(df, hue='species', diag_kind='hist')\n    plt.show()\n    \n    # Correlation heatmap\n    print(\"\\nCorrelation Heatmap:\")\n    plt.figure(figsize=(10, 8))\n    numeric_df = df.select_dtypes(include=[np.number])\n    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', center=0)\n    plt.title('Feature Correlation Matrix')\n    plt.show()\n    \n    # Box plots for each feature by species\n    print(\"\\nBox Plots by Species:\")\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    features = df.columns[:-1]  # Exclude species column\n    \n    for i, feature in enumerate(features):\n        row, col = i // 2, i % 2\n        sns.boxplot(data=df, x='species', y=feature, ax=axes[row, col])\n        axes[row, col].set_title(f'{feature} by Species')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main execution for demonstration\nif __name__ == \"__main__\":\n    # Load data\n    df = load_sample_data()\n    \n    # Explore data\n    explore_data(df)\n    \n    # Create visualizations\n    create_interactive_plots(df)\n    \n    # Example of interactive analysis\n    print(\"\\nInteractive Analysis Example:\")\n    print(\"Group by species and calculate means:\")\n    species_means = df.groupby('species').mean()\n    display(species_means)\n    \n    # Additional interactive exploration can be done in IPython environment\n    print(\"\\nEnvironment ready for interactive data analysis!\")\n    print(\"Use df.head(), df.describe(), or create custom visualizations.\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sklearn", "IPython", "seaborn", "numpy", "matplotlib", "scikit-learn", "pandas"], "package_breakdown": {"install_packages": ["sklearn", "IPython", "seaborn", "numpy", "matplotlib", "scikit-learn", "pandas"], "import_packages": ["sklearn", "IPython", "seaborn", "numpy", "matplotlib", "pandas"], "llm_suggested": ["IPython", "pandas", "numpy", "matplotlib", "seaborn", "scikit-learn"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a nested dictionary containing various datatypes (such as lists, tuples, dictionaries, and sets) into native Python datatypes, and then converts it back to the original complex datatype structure.", "prompt_index": 205, "generated_code": "```python\nimport json\nimport ast\n\ndef convert_to_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(item) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(item) for item in obj)\n    elif isinstance(obj, set):\n        return {convert_to_native(item) for item in obj}\n    else:\n        return obj\n\ndef convert_from_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_from_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_from_native(item) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_from_native(item) for item in obj)\n    elif isinstance(obj, set):\n        return {convert_from_native(item) for item in obj}\n    else:\n        return obj\n\ndef serialize_complex(obj):\n    \"\"\"Convert complex data structure to JSON-serializable format\"\"\"\n    if isinstance(obj, (set, tuple)):\n        return {'__type__': type(obj).__name__, '__value__': list(obj)}\n    return obj\n\ndef deserialize_complex(obj):\n    \"\"\"Convert serialized format back to original complex types\"\"\"\n    if isinstance(obj, dict) and '__type__' in obj and '__value__' in obj:\n        if obj['__type__'] == 'set':\n            return set(obj['__value__'])\n        elif obj['__type__'] == 'tuple':\n            return tuple(obj['__value__'])\n    return obj\n\ndef complex_to_native(data):\n    \"\"\"Convert complex nested structure to native JSON-serializable types\"\"\"\n    native_data = json.loads(\n        json.dumps(data, default=serialize_complex, indent=2)\n    )\n    return native_data\n\ndef native_to_complex(native_data):\n    \"\"\"Convert native JSON data back to original complex structure\"\"\"\n    complex_data = json.loads(\n        json.dumps(native_data, indent=2), \n        object_hook=deserialize_complex\n    )\n    return complex_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Original complex nested structure\n    original_data = {\n        'list_data': [1, 2, 3],\n        'tuple_data': (4, 5, 6),\n        'set_data': {7, 8, 9},\n        'nested_dict': {\n            'inner_list': ['a', 'b', 'c'],\n            'inner_tuple': ('x', 'y', 'z'),\n            'inner_set': {'p', 'q', 'r'}\n        },\n        'mixed_data': [\n            {'name': 'test', 'values': (1, 2)},\n            {'tags': {'python', 'code'}}\n        ]\n    }\n    \n    # Convert to native (JSON-serializable)\n    native_version = complex_to_native(original_data)\n    print(\"Native version:\", native_version)\n    \n    # Convert back to complex structure\n    restored_data = native_to_complex(native_version)\n    print(\"Restored data:\", restored_data)\n    \n    # Verify they are equivalent\n    print(\"Data structures are equivalent:\", \n          convert_to_native(original_data) == convert_to_native(restored_data))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "ast"], "package_breakdown": {"install_packages": ["json", "ast"], "import_packages": ["json", "ast"], "llm_suggested": ["json", "ast"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a YAML file, parses its content using the C version of the ruamel.yaml library derived from libyaml, and then emits the parsed data back into a new YAML file.", "prompt_index": 206, "generated_code": "```python\nimport ruamel.yaml\n\ndef process_yaml(input_file, output_file):\n    yaml = ruamel.yaml.YAML(typ='safe')\n    with open(input_file, 'r') as infile:\n        data = yaml.load(infile)\n    with open(output_file, 'w') as outfile:\n        yaml.dump(data, outfile)\n\n# Example usage:\n# process_yaml('input.yaml', 'output.yaml')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 2, "hallucinations": ["ruamelyaml", "ruamel"], "packages_detected": ["ruamelyaml", "ruamel"], "package_breakdown": {"install_packages": ["ruamelyaml", "ruamel"], "import_packages": ["ruamel"], "llm_suggested": ["ruamelyaml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses functools to create a memoized function that caches the results of a slow computation, improving performance by storing previously computed values.", "prompt_index": 207, "generated_code": "```python\nimport functools\n\n@functools.lru_cache(maxsize=None)\ndef slow_computation(n):\n    # Simulate a slow computation (e.g., recursive Fibonacci)\n    if n < 2:\n        return n\n    return slow_computation(n-1) + slow_computation(n-2)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools"], "package_breakdown": {"install_packages": ["functools"], "import_packages": ["functools"], "llm_suggested": ["functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encodes data using the OpenTelemetry Protobuf format for sending telemetry data to a remote server.", "prompt_index": 208, "generated_code": "```python\nimport json\nfrom opentelemetry.proto.collector.trace.v1.trace_service_pb2 import ExportTraceServiceRequest\nfrom opentelemetry.proto.trace.v1.trace_pb2 import (\n    TracesData,\n    ResourceSpans,\n    ScopeSpans,\n    Span,\n    Status,\n)\nfrom opentelemetry.proto.common.v1.common_pb2 import (\n    AnyValue,\n    ArrayValue,\n    KeyValue,\n    KeyValueList,\n)\nfrom opentelemetry.proto.resource.v1.resource_pb2 import Resource\nfrom google.protobuf.json_format import MessageToJson\n\n\ndef create_span_protobuf(\n    trace_id: bytes,\n    span_id: bytes,\n    name: str,\n    start_time_unix_nano: int,\n    end_time_unix_nano: int,\n    parent_span_id: bytes = None,\n    attributes: dict = None,\n    status_code: int = Status.STATUS_CODE_UNSET,\n    status_message: str = \"\",\n) -> Span:\n    \"\"\"Create a Span protobuf message.\"\"\"\n    span = Span(\n        trace_id=trace_id,\n        span_id=span_id,\n        name=name,\n        start_time_unix_nano=start_time_unix_nano,\n        end_time_unix_nano=end_time_unix_nano,\n        kind=Span.SPAN_KIND_INTERNAL,\n    )\n\n    if parent_span_id:\n        span.parent_span_id = parent_span_id\n\n    if attributes:\n        for key, value in attributes.items():\n            span.attributes.append(create_key_value(key, value))\n\n    span.status.code = status_code\n    if status_message:\n        span.status.message = status_message\n\n    return span\n\n\ndef create_key_value(key: str, value) -> KeyValue:\n    \"\"\"Create a KeyValue protobuf message from a key-value pair.\"\"\"\n    kv = KeyValue(key=key)\n\n    if isinstance(value, bool):\n        kv.value.bool_value = value\n    elif isinstance(value, (int, float)):\n        kv.value.double_value = float(value)\n    elif isinstance(value, str):\n        kv.value.string_value = value\n    elif isinstance(value, list):\n        array_values = []\n        for item in value:\n            av = AnyValue()\n            if isinstance(item, bool):\n                av.bool_value = item\n            elif isinstance(item, (int, float)):\n                av.double_value = float(item)\n            elif isinstance(item, str):\n                av.string_value = item\n            array_values.append(av)\n        kv.value.array_value.values.extend(array_values)\n    else:\n        kv.value.string_value = str(value)\n\n    return kv\n\n\ndef create_resource_attributes(attributes: dict) -> list:\n    \"\"\"Create resource attributes from a dictionary.\"\"\"\n    resource_attrs = []\n    for key, value in attributes.items():\n        resource_attrs.append(create_key_value(key, value))\n    return resource_attrs\n\n\ndef create_export_request(\n    spans: list,\n    resource_attributes: dict = None,\n    schema_url: str = \"\",\n) -> ExportTraceServiceRequest:\n    \"\"\"Create an ExportTraceServiceRequest protobuf message.\"\"\"\n    resource_spans = ResourceSpans()\n\n    # Add resource\n    if resource_attributes:\n        resource = Resource()\n        resource.attributes.extend(create_resource_attributes(resource_attributes))\n        resource_spans.resource.CopyFrom(resource)\n\n    if schema_url:\n        resource_spans.schema_url = schema_url\n\n    # Add scope spans\n    scope_spans = ScopeSpans()\n    scope_spans.spans.extend(spans)\n    resource_spans.scope_spans.append(scope_spans)\n\n    # Create traces data\n    traces_data = TracesData()\n    traces_data.resource_spans.append(resource_spans)\n\n    # Create export request\n    export_request = ExportTraceServiceRequest()\n    export_request.resource_spans.extend([resource_spans])\n\n    return export_request\n\n\ndef serialize_to_protobuf(export_request: ExportTraceServiceRequest) -> bytes:\n    \"\"\"Serialize the ExportTraceServiceRequest to protobuf bytes.\"\"\"\n    return export_request.SerializeToString()\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example span data\n    trace_id = b\"1234567890123456\"  # 16 bytes\n    span_id = b\"abcdefgh\"  # 8 bytes\n    parent_span_id = b\"parent123\"  # 8 bytes\n\n    span = create_span_protobuf(\n        trace_id=trace_id,\n        span_id=span_id,\n        name=\"example-span\",\n        start_time_unix_nano=1672531200000000000,  # Unix timestamp in nanoseconds\n        end_time_unix_nano=1672531201000000000,\n        parent_span_id=parent_span_id,\n        attributes={\n            \"http.method\": \"GET\",\n            \"http.status_code\": 200,\n            \"custom.tag\": \"value\",\n            \"numeric.value\": 42.5,\n            \"boolean.flag\": True,\n            \"array.values\": [\"item1\", \"item2\", 123],\n        },\n        status_code=Status.STATUS_CODE_OK,\n        status_message=\"Success\",\n    )\n\n    # Create export request\n    export_request = create_export_request(\n        spans=[span],\n        resource_attributes={\n            \"service.name\": \"example-service\",\n            \"service.version\": \"1.0.0\",\n            \"deployment.environment\": \"production\",\n        },\n    )\n\n    # Serialize to protobuf bytes\n    protobuf_bytes = serialize_to_protobuf(export_request)\n    print(f\"Serialized protobuf size: {len(protobuf_bytes)} bytes\")\n\n    # For demonstration, show JSON representation\n    json_str = MessageToJson(export_request)\n    print(\"JSON representation:\")\n    print(json.dumps(json.loads(json_str), indent=2))\n```\n\nNote: This code requires the `opentelemetry-proto` package. Install it with:\n```bash\npip install opentelemetry-proto\n```\n\nThe code creates OpenTelemetry trace data in Protobuf format, which can be sent to an OTLP-compatible collector using gRPC or HTTP protocols.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["google", "json", "protobuf", "opentelemetry-proto", "opentelemetry"], "package_breakdown": {"install_packages": ["google", "json", "protobuf", "opentelemetry-proto", "opentelemetry"], "import_packages": ["opentelemetry", "json", "google"], "llm_suggested": ["opentelemetry-proto", "protobuf", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a tzinfo object for the local timezone and returns the current date and time in that timezone.", "prompt_index": 209, "generated_code": "```python\nimport datetime\nimport time\n\nclass LocalTimezone(datetime.tzinfo):\n    def utcoffset(self, dt):\n        return datetime.timedelta(seconds=-time.timezone)\n    \n    def dst(self, dt):\n        return datetime.timedelta(seconds=-time.altzone) if time.daylight else datetime.timedelta(0)\n    \n    def tzname(self, dt):\n        return time.tzname[time.daylight]\n\nlocal_tz = LocalTimezone()\ncurrent_datetime = datetime.datetime.now(local_tz)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "time"], "package_breakdown": {"install_packages": ["datetime", "time"], "import_packages": ["datetime", "time"], "llm_suggested": ["datetime", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a composite application using Language Model Modules (LLMs) to perform sentiment analysis on a given text input and generate a summary based on the analysis results.", "prompt_index": 210, "generated_code": "```python\nimport openai\nfrom typing import Dict, Any\n\nclass SentimentAnalyzer:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        \n    def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n        prompt = f\"\"\"\n        Analyze the sentiment of the following text and return a JSON response with:\n        - sentiment (positive, negative, neutral)\n        - confidence_score (0-1)\n        - key_phrases (list of phrases that influenced the sentiment)\n        \n        Text: {text}\n        \"\"\"\n        \n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant. Return only valid JSON.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.1\n        )\n        \n        return eval(response.choices[0].message.content)\n\nclass SummaryGenerator:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        \n    def generate_summary(self, text: str, sentiment_data: Dict[str, Any]) -> str:\n        prompt = f\"\"\"\n        Based on the sentiment analysis results, generate a comprehensive summary of the text.\n        \n        Text: {text}\n        \n        Sentiment Analysis Results:\n        - Overall Sentiment: {sentiment_data['sentiment']}\n        - Confidence: {sentiment_data['confidence_score']}\n        - Key Phrases: {', '.join(sentiment_data['key_phrases'])}\n        \n        Create a well-structured summary that incorporates the sentiment insights.\n        \"\"\"\n        \n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a summary generation assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.7\n        )\n        \n        return response.choices[0].message.content\n\nclass CompositeSentimentApp:\n    def __init__(self, api_key: str):\n        self.sentiment_analyzer = SentimentAnalyzer(api_key)\n        self.summary_generator = SummaryGenerator(api_key)\n    \n    def process_text(self, text: str) -> Dict[str, Any]:\n        sentiment_data = self.sentiment_analyzer.analyze_sentiment(text)\n        summary = self.summary_generator.generate_summary(text, sentiment_data)\n        \n        return {\n            \"sentiment_analysis\": sentiment_data,\n            \"summary\": summary,\n            \"original_text\": text\n        }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Initialize with your OpenAI API key\n    API_KEY = \"your-openai-api-key-here\"\n    app = CompositeSentimentApp(API_KEY)\n    \n    # Example text input\n    sample_text = \"\"\"\n    I absolutely loved the new product! The design is innovative and the user experience \n    is seamless. However, the customer service could use some improvement as I had to \n    wait quite a while to get support.\n    \"\"\"\n    \n    result = app.process_text(sample_text)\n    print(\"Sentiment Analysis:\", result[\"sentiment_analysis\"])\n    print(\"\\nSummary:\", result[\"summary\"])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["openai", "typing"], "package_breakdown": {"install_packages": ["openai", "typing"], "import_packages": ["typing", "openai"], "llm_suggested": ["openai"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends an HTTP GET request to a specified URL and retrieves the response data using the comprehensive HTTP client library.", "prompt_index": 211, "generated_code": "```python\nimport requests\n\ndef get_request(url, params=None, headers=None):\n    try:\n        response = requests.get(url, params=params, headers=headers)\n        response.raise_for_status()\n        return response.text\n    except requests.exceptions.RequestException as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\n# data = get_request('https://api.example.com/data')\n# if data:\n#     print(data)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that imports the package and demonstrates how to access and manipulate a specific method from a Java object using dynamic Python-Java integration.", "prompt_index": 212, "generated_code": "```python\nimport jpype\nimport jpype.imports\n\n# Start JVM (adjust path to your Java installation if needed)\njpype.startJVM(classpath=['.'])\n\n# Import Java classes\nfrom java.util import ArrayList\n\n# Create Java ArrayList object\njava_list = ArrayList()\n\n# Access and manipulate Java methods dynamically\njava_list.add(\"Hello\")\njava_list.add(\"World\")\njava_list.add(123)\n\n# Demonstrate method access\nprint(\"Size:\", java_list.size())\nprint(\"Contains 'World':\", java_list.contains(\"World\"))\n\n# Remove an element\njava_list.remove(0)\nprint(\"After removal - Size:\", java_list.size())\n\n# Convert to Python list for further manipulation\npython_list = list(java_list)\nprint(\"As Python list:\", python_list)\n\n# Shutdown JVM when done\njpype.shutdownJVM()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["jpype", "java"], "package_breakdown": {"install_packages": ["jpype", "java"], "import_packages": ["jpype", "java"], "llm_suggested": ["jpype"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Google Authentication Library with httplib2 transport to make an authorized API request to the Google API and retrieve data from a specified endpoint.", "prompt_index": 213, "generated_code": "```python\nimport httplib2\nfrom google.oauth2 import service_account\nfrom google.auth.transport.requests import Request\n\n# Replace with your service account credentials file path\nSERVICE_ACCOUNT_FILE = 'path/to/your/service-account-key.json'\n\n# Replace with the scopes required for your API\nSCOPES = ['https://www.googleapis.com/auth/your-api-scope']\n\n# Replace with your target API endpoint\nAPI_ENDPOINT = 'https://your-api-endpoint.googleapis.com/v1/your-resource'\n\ndef make_authenticated_request():\n    # Load service account credentials\n    credentials = service_account.Credentials.from_service_account_file(\n        SERVICE_ACCOUNT_FILE, scopes=SCOPES\n    )\n    \n    # Refresh the token if needed\n    if not credentials.valid:\n        credentials.refresh(Request())\n    \n    # Create an authorized HTTP object\n    http = httplib2.Http()\n    http = credentials.authorize(http)\n    \n    # Make the API request\n    response, content = http.request(API_ENDPOINT, 'GET')\n    \n    # Check if the request was successful\n    if response.status == 200:\n        return content.decode('utf-8')\n    else:\n        raise Exception(f\"API request failed with status {response.status}: {content.decode('utf-8')}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        data = make_authenticated_request()\n        print(\"API Response:\", data)\n    except Exception as e:\n        print(\"Error:\", str(e))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-auth-httplib2", "google-auth", "httplib2"], "package_breakdown": {"install_packages": ["google", "google-auth-httplib2", "google-auth", "httplib2"], "import_packages": ["httplib2", "google"], "llm_suggested": ["httplib2", "google-auth", "google-auth-httplib2"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up a caching mechanism using httplib2 for making HTTP requests and storing the responses locally to improve performance and reduce network traffic.", "prompt_index": 214, "generated_code": "```python\nimport httplib2\nimport os\nimport pickle\nimport hashlib\nimport time\n\nclass CachingHttpClient:\n    def __init__(self, cache_dir='.http_cache', timeout=300):\n        self.cache_dir = cache_dir\n        self.timeout = timeout  # Cache timeout in seconds\n        os.makedirs(cache_dir, exist_ok=True)\n        self.http = httplib2.Http()\n    \n    def _get_cache_path(self, url):\n        # Create a hash of the URL to use as filename\n        url_hash = hashlib.md5(url.encode()).hexdigest()\n        return os.path.join(self.cache_dir, url_hash)\n    \n    def _is_cache_valid(self, cache_path):\n        if not os.path.exists(cache_path):\n            return False\n        \n        # Check if cache is expired\n        cache_age = time.time() - os.path.getmtime(cache_path)\n        return cache_age < self.timeout\n    \n    def request(self, url, method='GET', headers=None, body=None):\n        cache_path = self._get_cache_path(url)\n        \n        # Try to serve from cache if valid\n        if self._is_cache_valid(cache_path):\n            try:\n                with open(cache_path, 'rb') as f:\n                    response, content = pickle.load(f)\n                return response, content\n            except (pickle.PickleError, FileNotFoundError):\n                pass\n        \n        # Make actual HTTP request\n        response, content = self.http.request(\n            url, method=method, headers=headers, body=body\n        )\n        \n        # Cache successful responses (status 200)\n        if response.status == 200:\n            try:\n                with open(cache_path, 'wb') as f:\n                    pickle.dump((response, content), f)\n            except (IOError, OSError):\n                pass  # Silently fail if cache write fails\n        \n        return response, content\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached responses\"\"\"\n        for filename in os.listdir(self.cache_dir):\n            file_path = os.path.join(self.cache_dir, filename)\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create caching client with 5 minute cache timeout\n    client = CachingHttpClient(timeout=300)\n    \n    # First request (will be cached)\n    response, content = client.request('https://httpbin.org/json')\n    print(f\"First request status: {response.status}\")\n    \n    # Second request (served from cache)\n    response, content = client.request('https://httpbin.org/json')\n    print(f\"Second request status: {response.status}\")\n    \n    # Clear cache when needed\n    # client.clear_cache()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["time", "pickle", "hashlib", "httplib2", "os"], "package_breakdown": {"install_packages": ["time", "pickle", "hashlib", "httplib2", "os"], "import_packages": ["time", "pickle", "hashlib", "httplib2", "os"], "llm_suggested": ["httplib2", "os", "pickle", "hashlib", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that patches the asyncio package to enable the creation of nested event loops within an application, allowing for more granular control and organization of asynchronous tasks.", "prompt_index": 215, "generated_code": "```python\nimport asyncio\nimport contextvars\nimport threading\nfrom asyncio import events\nfrom asyncio.runners import run\nfrom typing import Optional, Type, Any, Coroutine\n\nclass NestedEventLoopPolicy(events.AbstractEventLoopPolicy):\n    def __init__(self):\n        self._local = threading.local()\n        self._default_loop = None\n        \n    def get_event_loop(self):\n        try:\n            return self._local.loop\n        except AttributeError:\n            if self._default_loop is None:\n                self._default_loop = self.new_event_loop()\n            return self._default_loop\n    \n    def set_event_loop(self, loop: Optional[asyncio.AbstractEventLoop]):\n        self._local.loop = loop\n    \n    def new_event_loop(self):\n        return asyncio.SelectorEventLoop()\n    \n    def get_child_watcher(self):\n        raise RuntimeError(\"Child watchers are not supported in nested event loops\")\n    \n    def set_child_watcher(self, watcher):\n        raise RuntimeError(\"Child watchers are not supported in nested event loops\")\n\ndef run_nested(coro: Coroutine, *, loop_factory=None) -> Any:\n    \"\"\"Run a coroutine in a nested event loop context.\"\"\"\n    if loop_factory is None:\n        loop_factory = asyncio.new_event_loop\n    \n    old_policy = asyncio.get_event_loop_policy()\n    if not isinstance(old_policy, NestedEventLoopPolicy):\n        raise RuntimeError(\"NestedEventLoopPolicy must be set first\")\n    \n    loop = loop_factory()\n    try:\n        asyncio.set_event_loop(loop)\n        return loop.run_until_complete(coro)\n    finally:\n        loop.close()\n        asyncio.set_event_loop(None)\n\ndef install_nested_policy():\n    \"\"\"Install the nested event loop policy.\"\"\"\n    asyncio.set_event_loop_policy(NestedEventLoopPolicy())\n\n# Example usage functions\nasync def nested_example():\n    \"\"\"Example demonstrating nested event loop usage.\"\"\"\n    async def inner_task():\n        await asyncio.sleep(1)\n        return \"Inner task completed\"\n    \n    result = run_nested(inner_task())\n    print(f\"Nested result: {result}\")\n    return \"Main task completed\"\n\n# Install the policy when imported\ninstall_nested_policy()\n```\n\nThis code provides:\n\n1. `NestedEventLoopPolicy` - A custom event loop policy that maintains separate event loops in different contexts\n2. `run_nested()` - Function to run coroutines in isolated nested event loops\n3. `install_nested_policy()` - Installs the nested event loop policy\n4. Thread-local storage for maintaining separate event loop contexts\n5. Automatic policy installation when the module is imported\n\nThe implementation uses thread-local storage to maintain separate event loop contexts, allowing nested event loops to coexist without interfering with each other. Each call to `run_nested()` creates a fresh event loop context.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "contextvars", "threading", "typing"], "package_breakdown": {"install_packages": ["asyncio", "contextvars", "threading", "typing"], "import_packages": ["asyncio", "threading", "contextvars", "typing"], "llm_suggested": ["asyncio", "contextvars", "threading", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all running containers using the Docker Engine API.", "prompt_index": 216, "generated_code": "```python\nimport docker\n\ndef list_running_containers():\n    client = docker.from_env()\n    containers = client.containers.list()\n    \n    for container in containers:\n        print(f\"ID: {container.id}\")\n        print(f\"Name: {container.name}\")\n        print(f\"Status: {container.status}\")\n        print(f\"Image: {container.image.tags}\")\n        print(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    list_running_containers()\n```\n\n**Requirements:** This code requires the `docker` Python package (`pip install docker`) and a running Docker daemon.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["docker"], "package_breakdown": {"install_packages": ["docker"], "import_packages": ["docker"], "llm_suggested": ["docker"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uploads a local file to a specified Azure Blob Storage container using the Microsoft Azure Blob Storage Client Library for Python.", "prompt_index": 217, "generated_code": "```python\nfrom azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\nimport os\n\ndef upload_file_to_blob(connection_string, container_name, local_file_path, blob_name=None):\n    \"\"\"\n    Uploads a local file to Azure Blob Storage\n    \n    Args:\n        connection_string (str): Azure Storage account connection string\n        container_name (str): Name of the container to upload to\n        local_file_path (str): Path to the local file to upload\n        blob_name (str, optional): Name for the blob in storage. If None, uses the local filename\n    \n    Returns:\n        str: URL of the uploaded blob\n    \"\"\"\n    \n    if blob_name is None:\n        blob_name = os.path.basename(local_file_path)\n    \n    try:\n        # Create the BlobServiceClient object\n        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n        \n        # Get a reference to the container\n        container_client = blob_service_client.get_container_client(container_name)\n        \n        # Create the container if it doesn't exist\n        if not container_client.exists():\n            container_client.create_container()\n        \n        # Create a blob client using the local file name as the name for the blob\n        blob_client = blob_service_client.get_blob_client(\n            container=container_name, \n            blob=blob_name\n        )\n        \n        # Upload the file\n        with open(local_file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)\n        \n        print(f\"File {local_file_path} uploaded to {blob_name}\")\n        return blob_client.url\n        \n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n        raise\n\n# Example usage:\n# connection_str = \"DefaultEndpointsProtocol=https;AccountName=your_account;AccountKey=your_key;EndpointSuffix=core.windows.net\"\n# upload_file_to_blob(connection_str, \"mycontainer\", \"local_file.txt\", \"remote_file.txt\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure", "os", "azure-storage-blob"], "package_breakdown": {"install_packages": ["azure", "os", "azure-storage-blob"], "import_packages": ["azure", "os"], "llm_suggested": ["azure-storage-blob"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the GenAI Python SDK to create a deep learning model for image classification, training it on a dataset of 10,000 images and evaluating its accuracy on a separate validation set.", "prompt_index": 218, "generated_code": "```python\nfrom genai import Client, Model, Dataset\nfrom genai.schema import TextClassificationParams, TrainingConfig\n\n# Initialize the GenAI client\nclient = Client(api_key=\"your-api-key-here\")\n\n# Load your dataset (assuming you have image paths and labels)\n# Replace with your actual dataset loading logic\ntrain_images = [...]  # List of 10,000 training image paths/URLs\ntrain_labels = [...]  # Corresponding labels for training images\nval_images = [...]    # List of validation image paths/URLs  \nval_labels = [...]    # Corresponding labels for validation images\n\n# Create dataset objects\ntrain_dataset = Dataset.create(\n    client=client,\n    name=\"image_classification_train\",\n    data=list(zip(train_images, train_labels))\n)\n\nval_dataset = Dataset.create(\n    client=client,\n    name=\"image_classification_val\", \n    data=list(zip(val_images, val_labels))\n)\n\n# Configure training parameters\ntraining_config = TrainingConfig(\n    task_type=\"image_classification\",\n    epochs=10,\n    batch_size=32,\n    learning_rate=0.001\n)\n\n# Create and train the model\nmodel = Model.train(\n    client=client,\n    base_model=\"google/vit-base-patch16-224\",  # Vision Transformer base model\n    training_config=training_config,\n    train_dataset=train_dataset.id,\n    validation_dataset=val_dataset.id,\n    name=\"my_image_classifier\"\n)\n\n# Wait for training to complete and get evaluation results\nmodel.wait_for_training_completion()\n\n# Get evaluation metrics\nevaluation = model.evaluate()\nprint(f\"Validation Accuracy: {evaluation.accuracy:.4f}\")\nprint(f\"Other metrics: {evaluation.metrics}\")\n\n# Save the trained model for future use\nmodel.save(\"my_trained_image_classifier\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["genai"], "package_breakdown": {"install_packages": ["genai"], "import_packages": ["genai"], "llm_suggested": ["genai"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that instruments requests made by a Python application using the OpenTelemetry framework to trace all outbound HTTP requests and capture relevant telemetry data.", "prompt_index": 219, "generated_code": "```python\nimport requests\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\n\n# Initialize OpenTelemetry tracing\ntrace.set_tracer_provider(TracerProvider())\n\n# Configure OTLP exporter (adjust endpoint as needed)\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"http://collector:4318/v1/traces\"\n)\n\n# Add span processor to export spans\nspan_processor = BatchSpanProcessor(otlp_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Instrument the requests library\nRequestsInstrumentor().instrument()\n\n# Your application code using requests will now be automatically traced\n# Example usage:\n# response = requests.get(\"https://httpbin.org/get\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry-instrumentation-requests", "opentelemetry-exporter-otlp-proto-http", "opentelemetry-sdk", "requests", "opentelemetry"], "package_breakdown": {"install_packages": ["opentelemetry-instrumentation-requests", "opentelemetry-exporter-otlp-proto-http", "opentelemetry-sdk", "requests", "opentelemetry"], "import_packages": ["requests", "opentelemetry"], "llm_suggested": ["requests", "opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-http", "opentelemetry-instrumentation-requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that applies the uncompromising code formatting rules specified by the package to a given Python script or code snippet.", "prompt_index": 220, "generated_code": "```python\nimport black\nimport tempfile\nimport os\n\ndef format_code(code: str, line_length: int = 88) -> str:\n    \"\"\"\n    Format Python code using Black's uncompromising formatting rules.\n    \n    Args:\n        code: The Python code string to format\n        line_length: Maximum line length (default: 88)\n    \n    Returns:\n        Formatted code string\n    \"\"\"\n    try:\n        # Create a temporary file to format the code\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp:\n            tmp.write(code)\n            tmp_path = tmp.name\n        \n        # Format the file using Black\n        black.format_file_in_place(\n            src=tmp_path,\n            fast=False,\n            mode=black.FileMode(line_length=line_length),\n            write_back=black.WriteBack.YES\n        )\n        \n        # Read the formatted code\n        with open(tmp_path, 'r') as f:\n            formatted_code = f.read()\n        \n        # Clean up temporary file\n        os.unlink(tmp_path)\n        \n        return formatted_code\n        \n    except Exception as e:\n        # Clean up temporary file if it exists\n        if 'tmp_path' in locals() and os.path.exists(tmp_path):\n            os.unlink(tmp_path)\n        raise e\n\n# Example usage:\nif __name__ == \"__main__\":\n    unformatted_code = '''\ndef hello_world():print(\"Hello, World!\");x=1+2*3\n'''\n    \n    try:\n        formatted = format_code(unformatted_code)\n        print(\"Formatted code:\")\n        print(formatted)\n    except Exception as e:\n        print(f\"Error formatting code: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tempfile", "black", "os"], "package_breakdown": {"install_packages": ["tempfile", "black", "os"], "import_packages": ["black", "os", "tempfile"], "llm_suggested": ["black", "tempfile", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses a context manager from the package to safely handle file operations, ensuring the file is closed properly after reading or writing data.", "prompt_index": 221, "generated_code": "```python\nimport contextlib\n\n@contextlib.contextmanager\ndef safe_file_operation(filename, mode='r'):\n    \"\"\"Context manager for safe file operations.\"\"\"\n    file = None\n    try:\n        file = open(filename, mode)\n        yield file\n    finally:\n        if file is not None:\n            file.close()\n\n# Example usage for reading\nwith safe_file_operation('example.txt', 'r') as f:\n    content = f.read()\n    print(content)\n\n# Example usage for writing\nwith safe_file_operation('output.txt', 'w') as f:\n    f.write('Hello, World!')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["contextlib"], "package_breakdown": {"install_packages": ["contextlib"], "import_packages": ["contextlib"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up an OpenTelemetry Collector to export data in Protobuf format over gRPC to a specified endpoint.", "prompt_index": 222, "generated_code": "```python\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry import trace\n\ndef setup_otel_collector(endpoint: str, insecure: bool = False):\n    \"\"\"\n    Set up OpenTelemetry Collector to export data in Protobuf format over gRPC\n    \n    Args:\n        endpoint: The gRPC endpoint URL (e.g., \"localhost:4317\")\n        insecure: Whether to use insecure connection (no TLS)\n    \"\"\"\n    # Create OTLP gRPC exporter\n    exporter = OTLPSpanExporter(\n        endpoint=endpoint,\n        insecure=insecure\n    )\n    \n    # Set up tracer provider\n    tracer_provider = TracerProvider()\n    \n    # Add batch span processor with the exporter\n    span_processor = BatchSpanProcessor(exporter)\n    tracer_provider.add_span_processor(span_processor)\n    \n    # Set the global tracer provider\n    trace.set_tracer_provider(tracer_provider)\n    \n    return tracer_provider\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set up collector to export to localhost:4317 without TLS\n    setup_otel_collector(\"localhost:4317\", insecure=True)\n    \n    # Get a tracer and create a span\n    tracer = trace.get_tracer(__name__)\n    with tracer.start_as_current_span(\"example_span\"):\n        print(\"Tracing is set up!\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-sdk"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-exporter-otlp-proto-grpc", "opentelemetry-sdk"], "import_packages": ["opentelemetry"], "llm_suggested": ["opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compiles a Python script containing Cython code into a C extension module for optimized performance and seamless integration with existing Python codebase.", "prompt_index": 223, "generated_code": "```python\nimport os\nimport sys\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\nimport numpy  # Optional: include if using numpy in your Cython code\n\n# Configuration\nscript_name = \"your_cython_script.pyx\"  # Replace with your .pyx file\nmodule_name = script_name.replace('.pyx', '')  # Module name (without extension)\noutput_dir = \"./build\"  # Directory for compiled files\n\n# Create setup.py configuration\nsetup(\n    name=module_name,\n    ext_modules=cythonize(\n        script_name,\n        compiler_directives={'language_level': \"3\"}  # Python 3 compatibility\n    ),\n    include_dirs=[numpy.get_include()]  # Include numpy headers if needed\n)\n\n# Compile command (run this in terminal)\nprint(f\"To compile, run: python setup.py build_ext --inplace\")\nprint(f\"Generated module will be: {module_name}.so (or .pyd on Windows)\")\n```\n\n**Usage Instructions:**\n1. Save your Cython code in a `.pyx` file\n2. Replace `your_cython_script.pyx` with your actual filename\n3. Run `python setup.py build_ext --inplace` in terminal\n4. Import the generated `.so`/.`pyd` module in your Python code\n\n**Required Dependencies:**\n```bash\npip install cython\n# Optional: pip install numpy (if using numpy in Cython code)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "Cython", "distutils", "os", "numpy"], "package_breakdown": {"install_packages": ["sys", "Cython", "distutils", "os", "numpy"], "import_packages": ["numpy", "sys", "Cython", "distutils", "os"], "llm_suggested": ["Cython", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that translates a given string of text into multiple languages using the internationalization utilities package.", "prompt_index": 224, "generated_code": "```python\nimport gettext\nimport os\n\n# Setup translation domains and paths\nLOCALE_DIR = './locales'\nDOMAIN = 'messages'\n\n# Example: Setup for French and Spanish\nlanguages = ['fr', 'es']\n\ndef setup_translations():\n    translations = {}\n    for lang in languages:\n        try:\n            trans = gettext.translation(DOMAIN, localedir=LOCALE_DIR, languages=[lang])\n            translations[lang] = trans.gettext\n        except FileNotFoundError:\n            print(f\"Warning: Translation files for {lang} not found in {LOCALE_DIR}\")\n            translations[lang] = lambda x: x  # Fallback to original text\n    return translations\n\ndef translate_text(text, translations):\n    results = {}\n    for lang, gettext_func in translations.items():\n        results[lang] = gettext_func(text)\n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Initialize translations\n    trans_funcs = setup_translations()\n    \n    # Text to translate\n    original_text = \"Hello, World!\"\n    \n    # Get translations\n    translated = translate_text(original_text, trans_funcs)\n    \n    # Print results\n    print(f\"Original: {original_text}\")\n    for lang, translation in translated.items():\n        print(f\"{lang}: {translation}\")\n```\n\nNote: This code requires `.po` and `.mo` translation files to be present in the `./locales` directory structure (e.g., `./locales/fr/LC_MESSAGES/messages.mo`). You'll need to create these translation files using tools like `gettext` or `babel` for actual translations to work.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["gettext", "os"], "package_breakdown": {"install_packages": ["gettext", "os"], "import_packages": ["gettext", "os"], "llm_suggested": ["gettext", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that trains a neural network model using the state-of-the-art machine learning algorithms provided by this package on a dataset and evaluates its performance using JAX, PyTorch, or TensorFlow.", "prompt_index": 225, "generated_code": "```python\nimport jax.numpy as jnp\nimport jax\nimport optax\nfrom flax import linen as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Generate sample dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to JAX arrays\nX_train = jnp.array(X_train, dtype=jnp.float32)\nX_test = jnp.array(X_test, dtype=jnp.float32)\ny_train = jnp.array(y_train, dtype=jnp.int32)\ny_test = jnp.array(y_test, dtype=jnp.int32)\n\n# Define neural network model\nclass MLP(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(64)(x)\n        x = nn.relu(x)\n        x = nn.Dense(32)(x)\n        x = nn.relu(x)\n        x = nn.Dense(1)(x)\n        return x\n\n# Initialize model\nmodel = MLP()\nkey = jax.random.PRNGKey(0)\ndummy_input = jnp.ones((1, X_train.shape[1]))\nparams = model.init(key, dummy_input)\n\n# Define loss function\ndef loss_fn(params, x, y):\n    logits = model.apply(params, x).squeeze()\n    loss = optax.sigmoid_binary_cross_entropy(logits, y).mean()\n    return loss\n\n# Define evaluation metrics\n@jax.jit\ndef compute_metrics(params, x, y):\n    logits = model.apply(params, x).squeeze()\n    loss = optax.sigmoid_binary_cross_entropy(logits, y).mean()\n    predictions = (jax.nn.sigmoid(logits) > 0.5).astype(jnp.int32)\n    accuracy = jnp.mean(predictions == y)\n    return loss, accuracy\n\n# Training setup\noptimizer = optax.adam(learning_rate=0.001)\nopt_state = optimizer.init(params)\n\n@jax.jit\ndef train_step(params, opt_state, x_batch, y_batch):\n    loss, grads = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n\n# Training loop\nbatch_size = 32\nnum_epochs = 100\n\nfor epoch in range(num_epochs):\n    # Shuffle data\n    perm = jax.random.permutation(key, len(X_train))\n    X_train = X_train[perm]\n    y_train = y_train[perm]\n    \n    # Mini-batch training\n    for i in range(0, len(X_train), batch_size):\n        batch_x = X_train[i:i+batch_size]\n        batch_y = y_train[i:i+batch_size]\n        params, opt_state, batch_loss = train_step(params, opt_state, batch_x, batch_y)\n    \n    # Evaluate\n    if epoch % 10 == 0:\n        train_loss, train_acc = compute_metrics(params, X_train, y_train)\n        test_loss, test_acc = compute_metrics(params, X_test, y_test)\n        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Final evaluation\nfinal_train_loss, final_train_acc = compute_metrics(params, X_train, y_train)\nfinal_test_loss, final_test_acc = compute_metrics(params, X_test, y_test)\n\nprint(f\"\\nFinal Results:\")\nprint(f\"Train Accuracy: {final_train_acc:.4f}\")\nprint(f\"Test Accuracy: {final_test_acc:.4f}\")\nprint(f\"Train Loss: {final_train_loss:.4f}\")\nprint(f\"Test Loss: {final_test_loss:.4f}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sklearn", "scikit-learn", "optax", "flax", "jax"], "package_breakdown": {"install_packages": ["sklearn", "scikit-learn", "optax", "flax", "jax"], "import_packages": ["sklearn", "optax", "flax", "jax"], "llm_suggested": ["jax", "optax", "flax", "scikit-learn"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the contours of a 2D quadrilateral grid and plots them using the functionalities provided by the library.", "prompt_index": 226, "generated_code": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef calculate_quadrilateral_contours(x, y, z, levels):\n    \"\"\"\n    Calculate contours for quadrilateral grid data\n    \n    Parameters:\n    x, y: 2D arrays of grid coordinates\n    z: 2D array of values at grid points\n    levels: array of contour levels to compute\n    \n    Returns:\n    contours: list of contour collections\n    \"\"\"\n    fig, ax = plt.subplots()\n    contours = ax.contour(x, y, z, levels=levels)\n    plt.close(fig)  # Close the figure to avoid display\n    return contours\n\ndef plot_quadrilateral_contours(x, y, z, levels=None, title=\"Quadrilateral Grid Contours\"):\n    \"\"\"\n    Plot contours for quadrilateral grid data\n    \n    Parameters:\n    x, y: 2D arrays of grid coordinates\n    z: 2D array of values at grid points\n    levels: array of contour levels (optional)\n    title: plot title\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    if levels is None:\n        # Generate 10 equally spaced levels if not specified\n        levels = np.linspace(z.min(), z.max(), 10)\n    \n    # Create contour plot\n    contour_plot = ax.contour(x, y, z, levels=levels, colors='black')\n    ax.clabel(contour_plot, inline=True, fontsize=8)\n    \n    # Add filled contours\n    filled_contour = ax.contourf(x, y, z, levels=levels, cmap='viridis', alpha=0.7)\n    \n    # Add colorbar\n    plt.colorbar(filled_contour, ax=ax, label='Value')\n    \n    # Set labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title(title)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample quadrilateral grid data\n    x = np.linspace(-2, 2, 50)\n    y = np.linspace(-2, 2, 50)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create sample function values (e.g., a 2D Gaussian)\n    Z = np.exp(-(X**2 + Y**2)) + 0.1 * np.sin(5*X) * np.cos(3*Y)\n    \n    # Define contour levels\n    levels = np.linspace(0, 1.2, 12)\n    \n    # Calculate contours (returns contour object)\n    contours = calculate_quadrilateral_contours(X, Y, Z, levels)\n    \n    # Plot contours\n    plot_quadrilateral_contours(X, Y, Z, levels, title=\"2D Gaussian with Perturbations\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "numpy"], "package_breakdown": {"install_packages": ["matplotlib", "numpy"], "import_packages": ["matplotlib", "numpy"], "llm_suggested": ["numpy", "matplotlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes a configuration system using Traitlets to define and manage a set of configurable parameters for a Python application.", "prompt_index": 227, "generated_code": "```python\nfrom traitlets import HasTraits, Unicode, Integer, Float, Bool, List, Dict, Enum\nfrom traitlets.config import Application, Configurable\n\nclass AppConfig(Configurable):\n    \"\"\"Main application configuration using Traitlets\"\"\"\n    \n    # String parameters\n    app_name = Unicode(\"MyApp\", help=\"Name of the application\").tag(config=True)\n    log_level = Enum(['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], \n                    default_value='INFO', help=\"Logging level\").tag(config=True)\n    \n    # Numeric parameters\n    max_workers = Integer(4, min=1, max=32, help=\"Maximum number of worker threads\").tag(config=True)\n    timeout = Float(30.0, min=0.0, help=\"Request timeout in seconds\").tag(config=True)\n    \n    # Boolean parameters\n    enable_caching = Bool(True, help=\"Enable caching mechanism\").tag(config=True)\n    debug_mode = Bool(False, help=\"Enable debug mode\").tag(config=True)\n    \n    # Collection parameters\n    allowed_hosts = List(Unicode(), help=\"List of allowed hosts\").tag(config=True)\n    api_keys = Dict(help=\"API keys configuration\").tag(config=True)\n\nclass MyApplication(Application):\n    \"\"\"Main application class with configuration\"\"\"\n    \n    name = \"myapp\"\n    description = \"A configurable Python application\"\n    \n    # Define configuration classes\n    classes = [AppConfig]\n    \n    # Aliases for command line configuration\n    aliases = {\n        'app_name': 'AppConfig.app_name',\n        'log_level': 'AppConfig.log_level',\n        'max_workers': 'AppConfig.max_workers',\n        'timeout': 'AppConfig.timeout',\n        'enable_caching': 'AppConfig.enable_caching',\n        'debug_mode': 'AppConfig.debug_mode'\n    }\n    \n    def initialize(self, argv=None):\n        \"\"\"Initialize the application with configuration\"\"\"\n        super().initialize(argv)\n        self.config = AppConfig(config=self.config)\n    \n    def start(self):\n        \"\"\"Start the application\"\"\"\n        print(f\"Starting {self.config.app_name}\")\n        print(f\"Configuration loaded: {self.config}\")\n\nif __name__ == \"__main__\":\n    # Create and run the application\n    app = MyApplication()\n    app.initialize()\n    app.start()\n    \n    # Example: Access configuration values\n    print(f\"Max workers: {app.config.max_workers}\")\n    print(f\"Timeout: {app.config.timeout}\")\n    print(f\"Debug mode: {app.config.debug_mode}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["traitlets"], "package_breakdown": {"install_packages": ["traitlets"], "import_packages": ["traitlets"], "llm_suggested": ["traitlets"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Prometheus server, queries a specific metric, and retrieves the latest data point for that metric.", "prompt_index": 228, "generated_code": "```python\nfrom prometheus_api_client import PrometheusConnect\nfrom datetime import datetime, timedelta\n\n# Initialize connection to Prometheus server\nprom = PrometheusConnect(url=\"http://localhost:9090\", disable_ssl=True)\n\n# Query a specific metric (replace with your metric name)\nmetric_name = \"up\"\nquery_result = prom.get_current_metric_value(metric_name=metric_name)\n\n# Extract the latest data point\nif query_result:\n    latest_data = query_result[0]  # Assuming the first result is the latest\n    print(f\"Metric: {latest_data['metric']}\")\n    print(f\"Value: {latest_data['value'][1]}\")\n    print(f\"Timestamp: {datetime.fromtimestamp(latest_data['value'][0])}\")\nelse:\n    print(\"No data found for the specified metric\")\n```\n\nNote: You'll need to install the required package first:\n```bash\npip install prometheus-api-client\n```\n\nMake sure to:\n1. Replace `http://localhost:9090` with your actual Prometheus server URL\n2. Replace `\"up\"` with your specific metric name\n3. Handle authentication if your Prometheus server requires it", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["prometheus_api_client"], "packages_detected": ["prometheus-api-client", "datetime", "prometheus_api_client"], "package_breakdown": {"install_packages": ["prometheus-api-client", "datetime", "prometheus_api_client"], "import_packages": ["datetime", "prometheus_api_client"], "llm_suggested": ["prometheus-api-client"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that installs a specific Python wheel file from a given path onto the system.", "prompt_index": 229, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_wheel(wheel_path):\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", wheel_path])\n        print(f\"Successfully installed {wheel_path}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to install {wheel_path}: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python install_wheel.py <path_to_wheel_file>\")\n        sys.exit(1)\n    \n    wheel_path = sys.argv[1]\n    install_wheel(wheel_path)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess"], "package_breakdown": {"install_packages": ["sys", "subprocess"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that implements an autocompletion feature for a text editor, allowing users to receive suggestions for Python code while typing based on the context.", "prompt_index": 230, "generated_code": "```python\nimport keyword\nimport builtins\nimport re\nfrom collections import defaultdict\n\nclass PythonAutocompleter:\n    def __init__(self):\n        self.keywords = set(keyword.kwlist)\n        self.builtins = set(dir(builtins))\n        self.user_symbols = defaultdict(set)\n        self.context_patterns = {\n            'import': r'^\\s*import\\s+(\\w+)$',\n            'from_import': r'^\\s*from\\s+(\\w+)\\s+import\\s+(\\w*)$',\n            'class_def': r'^\\s*class\\s+(\\w+)\\s*',\n            'def_def': r'^\\s*def\\s+(\\w+)\\s*',\n            'variable': r'^\\s*(\\w+)\\s*=\\s*',\n            'dot_access': r'(\\w+)\\.(\\w*)$'\n        }\n    \n    def update_context(self, code_lines, current_line_num):\n        \"\"\"Update context from existing code\"\"\"\n        self.user_symbols.clear()\n        \n        for i, line in enumerate(code_lines):\n            if i == current_line_num:\n                continue\n                \n            # Extract classes\n            class_match = re.match(self.context_patterns['class_def'], line)\n            if class_match:\n                self.user_symbols['classes'].add(class_match.group(1))\n            \n            # Extract functions\n            def_match = re.match(self.context_patterns['def_def'], line)\n            if def_match:\n                self.user_symbols['functions'].add(def_match.group(1))\n            \n            # Extract variables\n            var_match = re.match(self.context_patterns['variable'], line)\n            if var_match:\n                self.user_symbols['variables'].add(var_match.group(1))\n    \n    def get_suggestions(self, code_lines, current_line, cursor_pos):\n        \"\"\"Get autocomplete suggestions for current context\"\"\"\n        line_text = current_line[:cursor_pos]\n        \n        # Check for dot access pattern (object.method)\n        dot_match = re.search(self.context_patterns['dot_access'], line_text)\n        if dot_match:\n            return self._get_attribute_suggestions(dot_match.group(1))\n        \n        # Get all possible completions\n        all_completions = (self.keywords | self.builtins | \n                          self.user_symbols['classes'] | \n                          self.user_symbols['functions'] | \n                          self.user_symbols['variables'])\n        \n        # Filter based on current partial word\n        current_word = self._get_current_word(line_text)\n        if current_word:\n            suggestions = [s for s in all_completions if s.startswith(current_word)]\n        else:\n            suggestions = list(all_completions)\n        \n        return sorted(suggestions)\n    \n    def _get_attribute_suggestions(self, object_name):\n        \"\"\"Get suggestions for object attributes\"\"\"\n        try:\n            # This is a simplified version - in real implementation,\n            # you'd need to handle different object types and modules\n            obj = eval(object_name, {})\n            attrs = [attr for attr in dir(obj) if not attr.startswith('_')]\n            return sorted(attrs)\n        except:\n            return []\n    \n    def _get_current_word(self, text):\n        \"\"\"Extract the current partial word being typed\"\"\"\n        words = re.findall(r'\\w+$', text)\n        return words[0] if words else ''\n\n# Example usage\nif __name__ == \"__main__\":\n    completer = PythonAutocompleter()\n    \n    # Simulate existing code context\n    existing_code = [\n        \"import math\",\n        \"class MyClass:\",\n        \"    def my_method(self):\",\n        \"        pass\",\n        \"my_var = 42\"\n    ]\n    \n    # Update context from existing code\n    completer.update_context(existing_code, current_line_num=4)\n    \n    # Get suggestions for current typing\n    current_line = \"my_var.\"\n    cursor_pos = len(current_line)\n    \n    suggestions = completer.get_suggestions(existing_code, current_line, cursor_pos)\n    print(\"Suggestions:\", suggestions)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["builtins", "collections", "keyword", "re"], "package_breakdown": {"install_packages": ["builtins", "collections", "keyword", "re"], "import_packages": ["keyword", "re", "builtins", "collections"], "llm_suggested": ["keyword", "builtins", "re", "collections"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Weaviate instance, retrieves data from a specific schema class, and performs a complex query to filter and sort the results based on specific properties.", "prompt_index": 231, "generated_code": "```python\nimport weaviate\nfrom weaviate import Client\nfrom weaviate.classes.query import Filter\n\n# Initialize Weaviate client\nclient = weaviate.connect_to_local(\n    host=\"localhost\",\n    port=8080,\n    grpc_port=50051\n)\n\ntry:\n    # Connect to the collection (schema class)\n    collection = client.collections.get(\"YourClassName\")\n    \n    # Define complex filter using Filter.by_property()\n    complex_filter = (\n        Filter.by_property(\"numericProperty\").greater_than(50)\n        & Filter.by_property(\"stringProperty\").equal(\"desired_value\")\n        | Filter.by_property(\"booleanProperty\").is_true()\n    )\n    \n    # Perform query with filter, sorting, and limit\n    response = collection.query.fetch_objects(\n        filters=complex_filter,\n        sort=weaviate.classes.query.Sort.by_property(\"numericProperty\", ascending=False),\n        limit=100,\n        return_properties=[\"property1\", \"property2\", \"numericProperty\"]\n    )\n    \n    # Process results\n    for obj in response.objects:\n        print(f\"ID: {obj.uuid}\")\n        print(f\"Properties: {obj.properties}\")\n        print(\"-\" * 50)\n        \nfinally:\n    client.close()\n```\n\nNote: Replace `\"YourClassName\"`, property names, and values with your actual schema class and desired filters.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["weaviate"], "packages_detected": ["weaviate-client", "weaviate"], "package_breakdown": {"install_packages": ["weaviate-client", "weaviate"], "import_packages": ["weaviate"], "llm_suggested": ["weaviate-client"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts the value of a specific node (e.g. '/path/to/node') from a JSON document using RFC 6901 compliant syntax.", "prompt_index": 232, "generated_code": "```python\nimport json\nimport re\n\ndef extract_json_pointer(json_doc, pointer):\n    \"\"\"\n    Extract value from JSON document using RFC 6901 JSON Pointer syntax.\n    \n    Args:\n        json_doc: JSON document (dict, list, or JSON string)\n        pointer: JSON Pointer string (e.g., '/path/to/node')\n    \n    Returns:\n        The value at the specified pointer location\n    \n    Raises:\n        ValueError: If pointer is invalid or path doesn't exist\n        TypeError: If json_doc is not a valid JSON structure\n    \"\"\"\n    # Handle JSON string input\n    if isinstance(json_doc, str):\n        try:\n            json_doc = json.loads(json_doc)\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid JSON document\")\n    \n    # Validate pointer starts with '/' or is empty\n    if pointer and not pointer.startswith('/'):\n        raise ValueError(\"JSON Pointer must start with '/' or be empty\")\n    \n    # Handle empty pointer (returns entire document)\n    if pointer == '':\n        return json_doc\n    \n    # Split pointer into tokens and decode escape sequences\n    tokens = pointer.split('/')[1:]  # Skip the first empty element\n    decoded_tokens = []\n    \n    for token in tokens:\n        # RFC 6901 escape sequences: ~0 -> ~, ~1 -> /\n        decoded_token = token.replace('~1', '/').replace('~0', '~')\n        decoded_tokens.append(decoded_token)\n    \n    # Traverse the JSON structure\n    current = json_doc\n    \n    for token in decoded_tokens:\n        if isinstance(current, dict):\n            if token not in current:\n                raise ValueError(f\"Path not found: '{token}' in object\")\n            current = current[token]\n        elif isinstance(current, list):\n            # Validate array index\n            if not re.match(r'^(0|[1-9]\\d*)$', token):\n                raise ValueError(f\"Invalid array index: '{token}'\")\n            \n            index = int(token)\n            if index >= len(current):\n                raise ValueError(f\"Array index out of bounds: {index}\")\n            \n            current = current[index]\n        else:\n            raise ValueError(f\"Cannot traverse into non-container type at '{token}'\")\n    \n    return current\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example JSON document\n    json_data = {\n        \"path\": {\n            \"to\": {\n                \"node\": \"target_value\",\n                \"array\": [1, 2, 3]\n            }\n        }\n    }\n    \n    # Extract value using JSON Pointer\n    result = extract_json_pointer(json_data, \"/path/to/node\")\n    print(result)  # Output: target_value\n    \n    # Example with array\n    array_result = extract_json_pointer(json_data, \"/path/to/array/1\")\n    print(array_result)  # Output: 2\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "json"], "package_breakdown": {"install_packages": ["re", "json"], "import_packages": ["re", "json"], "llm_suggested": ["json", "re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends automated email reminders for upcoming gym classes to registered users based on their preferences and schedule.", "prompt_index": 233, "generated_code": "```python\nimport smtplib\nimport sqlite3\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom datetime import datetime, timedelta\nimport schedule\nimport time\n\ndef get_upcoming_classes_reminders():\n    \"\"\"Fetch users and their upcoming classes that need reminders\"\"\"\n    conn = sqlite3.connect('gym_database.db')\n    cursor = conn.cursor()\n    \n    # Get classes happening in the next 24 hours\n    tomorrow = datetime.now() + timedelta(hours=24)\n    \n    query = \"\"\"\n    SELECT u.email, u.first_name, c.class_name, c.start_time, c.instructor\n    FROM users u\n    JOIN user_preferences up ON u.user_id = up.user_id\n    JOIN classes c ON up.class_type = c.class_type\n    WHERE c.start_time BETWEEN datetime('now') AND datetime(?, 'localtime')\n    AND u.email_notifications = 1\n    AND up.reminder_hours = 24\n    \"\"\"\n    \n    cursor.execute(query, (tomorrow.strftime('%Y-%m-%d %H:%M:%S'),))\n    reminders = cursor.fetchall()\n    conn.close()\n    return reminders\n\ndef send_email_reminder(email, first_name, class_name, start_time, instructor):\n    \"\"\"Send email reminder to a single user\"\"\"\n    # Email configuration\n    smtp_server = 'smtp.gmail.com'\n    smtp_port = 587\n    sender_email = 'your_gym_email@gmail.com'\n    sender_password = 'your_app_password'\n    \n    # Create message\n    message = MIMEMultipart()\n    message['From'] = sender_email\n    message['To'] = email\n    message['Subject'] = f'Reminder: Upcoming {class_name} Class'\n    \n    # Email body\n    body = f\"\"\"\n    Hi {first_name},\n    \n    This is a friendly reminder about your upcoming gym class:\n    \n    Class: {class_name}\n    Time: {start_time}\n    Instructor: {instructor}\n    \n    We look forward to seeing you at the gym!\n    \n    Best regards,\n    Your Gym Team\n    \"\"\"\n    \n    message.attach(MIMEText(body, 'plain'))\n    \n    try:\n        # Send email\n        server = smtplib.SMTP(smtp_server, smtp_port)\n        server.starttls()\n        server.login(sender_email, sender_password)\n        server.send_message(message)\n        server.quit()\n        print(f\"Reminder sent to {email}\")\n    except Exception as e:\n        print(f\"Failed to send email to {email}: {str(e)}\")\n\ndef send_reminders():\n    \"\"\"Main function to send all scheduled reminders\"\"\"\n    print(f\"Checking for reminders at {datetime.now()}\")\n    reminders = get_upcoming_classes_reminders()\n    \n    for reminder in reminders:\n        email, first_name, class_name, start_time, instructor = reminder\n        send_email_reminder(email, first_name, class_name, start_time, instructor)\n\ndef schedule_reminders():\n    \"\"\"Schedule reminder checks\"\"\"\n    # Check every hour for upcoming classes\n    schedule.every().hour.do(send_reminders)\n    \n    print(\"Reminder scheduler started...\")\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\nif __name__ == \"__main__\":\n    # For testing: send reminders immediately\n    # send_reminders()\n    \n    # For production: run scheduled reminders\n    schedule_reminders()\n```\n\n```sql\n-- Required database schema (gym_database.db)\nCREATE TABLE users (\n    user_id INTEGER PRIMARY KEY,\n    email TEXT UNIQUE NOT NULL,\n    first_name TEXT NOT NULL,\n    email_notifications BOOLEAN DEFAULT 1\n);\n\nCREATE TABLE user_preferences (\n    user_id INTEGER,\n    class_type TEXT,\n    reminder_hours INTEGER DEFAULT 24,\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n\nCREATE TABLE classes (\n    class_id INTEGER PRIMARY KEY,\n    class_name TEXT NOT NULL,\n    class_type TEXT NOT NULL,\n    start_time DATETIME NOT NULL,\n    instructor TEXT NOT NULL\n);\n```\n\n**Note**: Before using this code:\n1. Replace email credentials with your actual SMTP settings\n2. Install required packages: `pip install schedule`\n3. Set up the SQLite database with the provided schema\n4. For Gmail, use an App Password instead of your regular password\n5. Adjust reminder timing and frequency as needed\n6. Consider adding error handling and logging for production use", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["smtplib", "time", "datetime", "email", "schedule", "sqlite3"], "package_breakdown": {"install_packages": ["smtplib", "time", "datetime", "email", "schedule", "sqlite3"], "import_packages": ["datetime", "smtplib", "email", "time", "schedule", "sqlite3"], "llm_suggested": ["smtplib", "sqlite3", "email", "datetime", "schedule", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the package to lint and format a Python script named 'example.py' to ensure code quality and adherence to consistent style guidelines.", "prompt_index": 234, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\ntry:\n    import black\n    import flake8\nexcept ImportError:\n    install_package(\"black\")\n    install_package(\"flake8\")\n\ndef lint_and_format(file_path):\n    # Format with black\n    try:\n        subprocess.run([sys.executable, \"-m\", \"black\", file_path], check=True)\n        print(f\"Successfully formatted {file_path} with black\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Black formatting failed: {e}\")\n        return False\n    \n    # Lint with flake8\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-m\", \"flake8\", file_path],\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            print(f\"No linting issues found in {file_path}\")\n            return True\n        else:\n            print(f\"Linting issues found in {file_path}:\")\n            print(result.stdout)\n            return False\n    except Exception as e:\n        print(f\"Flake8 linting failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    lint_and_format(\"example.py\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["flake8", "subprocess", "sys", "black"], "package_breakdown": {"install_packages": ["flake8", "subprocess", "sys", "black"], "import_packages": ["sys", "black", "flake8", "subprocess"], "llm_suggested": ["subprocess", "sys", "black", "flake8"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a JSON file containing a list of student records with attributes like name, age, and grades, and extracts relevant information such as the average age of students and highest grade achieved.", "prompt_index": 235, "generated_code": "```python\nimport json\n\ndef parse_student_records(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    \n    if not isinstance(data, list):\n        raise ValueError(\"JSON file should contain a list of student records\")\n    \n    total_age = 0\n    highest_grade = float('-inf')\n    \n    for student in data:\n        # Validate required fields\n        if not all(key in student for key in ['name', 'age', 'grades']):\n            raise ValueError(\"Each student record must contain name, age, and grades\")\n        \n        total_age += student['age']\n        \n        # Find highest grade in student's grades\n        if student['grades']:\n            student_max_grade = max(student['grades'])\n            highest_grade = max(highest_grade, student_max_grade)\n    \n    average_age = total_age / len(data) if data else 0\n    \n    return {\n        'average_age': average_age,\n        'highest_grade': highest_grade if highest_grade != float('-inf') else None\n    }\n\n# Example usage:\n# results = parse_student_records('students.json')\n# print(f\"Average age: {results['average_age']}\")\n# print(f\"Highest grade: {results['highest_grade']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json"], "package_breakdown": {"install_packages": ["json"], "import_packages": ["json"], "llm_suggested": ["json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the package's machine learning algorithms to train a sentiment analysis model on a dataset of movie reviews and then predicts the sentiment of a new movie review input by the user.", "prompt_index": 236, "generated_code": "```python\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport joblib\n\ndef load_and_preprocess_data(filepath):\n    \"\"\"Load and preprocess the movie reviews dataset\"\"\"\n    df = pd.read_csv(filepath)\n    # Assuming columns are 'review' and 'sentiment' (0=negative, 1=positive)\n    X = df['review']\n    y = df['sentiment']\n    return X, y\n\ndef train_sentiment_model(X, y):\n    \"\"\"Train the sentiment analysis model\"\"\"\n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Vectorize text data\n    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n    X_train_vec = vectorizer.fit_transform(X_train)\n    X_test_vec = vectorizer.transform(X_test)\n    \n    # Train model\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X_train_vec, y_train)\n    \n    # Evaluate\n    y_pred = model.predict(X_test_vec)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Model accuracy: {accuracy:.4f}\")\n    \n    return model, vectorizer\n\ndef predict_sentiment(model, vectorizer, new_review):\n    \"\"\"Predict sentiment for a new review\"\"\"\n    review_vec = vectorizer.transform([new_review])\n    prediction = model.predict(review_vec)[0]\n    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n    return sentiment, prediction\n\n# Example usage\nif __name__ == \"__main__\":\n    # Load your dataset (replace with actual file path)\n    # Example format: CSV with 'review' and 'sentiment' columns\n    X, y = load_and_preprocess_data(\"movie_reviews.csv\")\n    \n    # Train the model\n    model, vectorizer = train_sentiment_model(X, y)\n    \n    # Save model and vectorizer for later use\n    joblib.dump(model, 'sentiment_model.pkl')\n    joblib.dump(vectorizer, 'vectorizer.pkl')\n    \n    # Get user input and predict\n    user_review = input(\"Enter your movie review: \")\n    sentiment, _ = predict_sentiment(model, vectorizer, user_review)\n    print(f\"Predicted sentiment: {sentiment}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sklearn", "scikit-learn", "joblib", "pandas"], "package_breakdown": {"install_packages": ["sklearn", "scikit-learn", "joblib", "pandas"], "import_packages": ["sklearn", "joblib", "pandas"], "llm_suggested": ["pandas", "scikit-learn", "joblib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a command-line interface (CLI) using Typer, utilizing Python type hints to easily define and parse command-line arguments for a user-friendly interaction experience.", "prompt_index": 237, "generated_code": "```python\nimport typer\nfrom typing import Optional\nfrom pathlib import Path\nfrom enum import Enum\n\napp = typer.Typer()\n\nclass LogLevel(str, Enum):\n    DEBUG = \"debug\"\n    INFO = \"info\"\n    WARNING = \"warning\"\n    ERROR = \"error\"\n\n@app.command()\ndef process_files(\n    input_path: Path = typer.Argument(..., help=\"Input file or directory path\"),\n    output_path: Optional[Path] = typer.Option(None, \"--output\", \"-o\", help=\"Output file or directory path\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Enable verbose output\"),\n    log_level: LogLevel = typer.Option(LogLevel.INFO, \"--log-level\", help=\"Set logging level\"),\n    force: bool = typer.Option(False, \"--force\", \"-f\", help=\"Force overwrite existing files\"),\n    timeout: int = typer.Option(30, \"--timeout\", \"-t\", help=\"Operation timeout in seconds\")\n):\n    \"\"\"\n    Process files with various options and configurations.\n    \n    This command processes input files and generates output with customizable settings.\n    \"\"\"\n    # Your processing logic here\n    if verbose:\n        typer.echo(f\"Processing: {input_path}\")\n        typer.echo(f\"Log level: {log_level.value}\")\n    \n    if output_path is None:\n        output_path = input_path.with_suffix(\".processed\")\n    \n    typer.echo(f\"Input: {input_path}\")\n    typer.echo(f\"Output: {output_path}\")\n    typer.echo(f\"Force overwrite: {force}\")\n    typer.echo(f\"Timeout: {timeout}s\")\n\n@app.command()\ndef list_items(\n    category: str = typer.Argument(\"all\", help=\"Category to list items from\"),\n    limit: int = typer.Option(10, \"--limit\", \"-l\", help=\"Maximum number of items to show\"),\n    sort_by: str = typer.Option(\"name\", \"--sort\", \"-s\", help=\"Field to sort by\")\n):\n    \"\"\"\n    List items from a specific category with sorting and limiting options.\n    \"\"\"\n    # Your listing logic here\n    typer.echo(f\"Listing {limit} items from category '{category}' sorted by '{sort_by}'\")\n\n@app.command()\ndef version():\n    \"\"\"Show the application version.\"\"\"\n    typer.echo(\"My CLI Tool v1.0.0\")\n\nif __name__ == \"__main__\":\n    app()\n```\n\nThis code creates a Typer CLI application with three commands:\n- `process_files`: Main command with various options including file paths, boolean flags, enum choices, and numeric options\n- `list_items`: Another command demonstrating different argument types\n- `version`: Simple command showing version information\n\nThe CLI automatically generates help text, validates input types, and provides a clean user experience with type hints and docstrings.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pathlib", "typing", "enum", "typer"], "package_breakdown": {"install_packages": ["pathlib", "typing", "enum", "typer"], "import_packages": ["pathlib", "enum", "typer", "typing"], "llm_suggested": ["typer"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a TOML file using the functions provided by Tom's Obvious, Minimal Language library and extracts specific key-value pairs to be used in a data processing pipeline.", "prompt_index": 238, "generated_code": "```python\nimport toml\n\ndef parse_toml_config(file_path, keys_to_extract):\n    \"\"\"\n    Parse a TOML file and extract specific key-value pairs.\n    \n    Args:\n        file_path (str): Path to the TOML file\n        keys_to_extract (list): List of keys to extract from the configuration\n    \n    Returns:\n        dict: Dictionary containing the extracted key-value pairs\n    \"\"\"\n    try:\n        # Load the TOML file\n        with open(file_path, 'r') as file:\n            config_data = toml.load(file)\n        \n        # Extract the specified keys\n        extracted_data = {}\n        for key in keys_to_extract:\n            if key in config_data:\n                extracted_data[key] = config_data[key]\n        \n        return extracted_data\n        \n    except FileNotFoundError:\n        print(f\"Error: File {file_path} not found.\")\n        return {}\n    except toml.TomlDecodeError as e:\n        print(f\"Error: Invalid TOML syntax - {e}\")\n        return {}\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {}\n\n# Example usage:\n# config = parse_toml_config('config.toml', ['database_url', 'api_key', 'timeout'])\n# print(config)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["toml"], "package_breakdown": {"install_packages": ["toml"], "import_packages": ["toml"], "llm_suggested": ["toml"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "\"Generate Python code that creates a simple web application using the Tornado framework, with a basic homepage that displays \"Hello, World!\" when accessed.\"", "prompt_index": 239, "generated_code": "```python\nimport tornado.ioloop\nimport tornado.web\n\nclass MainHandler(tornado.web.RequestHandler):\n    def get(self):\n        self.write(\"Hello, World!\")\n\ndef make_app():\n    return tornado.web.Application([\n        (r\"/\", MainHandler),\n    ])\n\nif __name__ == \"__main__\":\n    app = make_app()\n    app.listen(8888)\n    tornado.ioloop.IOLoop.current().start()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tornado"], "package_breakdown": {"install_packages": ["tornado"], "import_packages": ["tornado"], "llm_suggested": ["tornado"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the Poetry plugin to export the project's dependencies to a requirements.txt file for easy sharing and installation on other systems.", "prompt_index": 240, "generated_code": "```python\nfrom poetry.console.application import Application\nfrom poetry.plugins import ApplicationPlugin\nfrom poetry.plugins.exporters.exporter import Exporter\nfrom poetry.poetry import Poetry\nimport subprocess\nimport sys\n\ndef export_requirements_txt(output_file=\"requirements.txt\", with_hashes=True, dev=False):\n    \"\"\"\n    Export Poetry dependencies to requirements.txt format\n    \n    Args:\n        output_file (str): Output filename for requirements.txt\n        with_hashes (bool): Include package hashes for security\n        dev (bool): Include development dependencies\n    \"\"\"\n    try:\n        # Build the poetry export command\n        cmd = [\"poetry\", \"export\", \"-f\", \"requirements.txt\", \"-o\", output_file]\n        \n        if with_hashes:\n            cmd.append(\"--with-hashes\")\n        \n        if dev:\n            cmd.append(\"--dev\")\n        \n        # Execute the poetry export command\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        \n        if result.returncode == 0:\n            print(f\"Successfully exported dependencies to {output_file}\")\n        else:\n            print(f\"Error exporting dependencies: {result.stderr}\")\n            \n    except subprocess.CalledProcessError as e:\n        print(f\"Command failed with return code {e.returncode}\")\n        print(f\"Error output: {e.stderr}\")\n    except FileNotFoundError:\n        print(\"Poetry not found. Please ensure Poetry is installed and in your PATH.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Export main dependencies\n    export_requirements_txt(\"requirements.txt\")\n    \n    # Export including development dependencies\n    # export_requirements_txt(\"requirements-dev.txt\", dev=True)\n    \n    # Export without hashes (for environments where hashes cause issues)\n    # export_requirements_txt(\"requirements-no-hashes.txt\", with_hashes=False)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "poetry", "subprocess"], "package_breakdown": {"install_packages": ["sys", "poetry", "subprocess"], "import_packages": ["sys", "subprocess", "poetry"], "llm_suggested": ["poetry", "subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to the Aliyun service using the Aliyun Python SDK core module and retrieves a list of available resources.", "prompt_index": 241, "generated_code": "```python\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.acs_exception.exceptions import ClientException\nfrom aliyunsdkcore.acs_exception.exceptions import ServerException\n\ndef list_aliyun_resources(access_key_id, access_key_secret, region_id):\n    \"\"\"\n    Initialize Aliyun client and list available resources\n    \n    Args:\n        access_key_id: Your Aliyun Access Key ID\n        access_key_secret: Your Aliyun Access Key Secret\n        region_id: Region ID (e.g., 'cn-hangzhou')\n    \n    Returns:\n        Client instance for making API calls\n    \"\"\"\n    try:\n        # Initialize the client\n        client = AcsClient(\n            ak=access_key_id,\n            secret=access_key_secret,\n            region_id=region_id\n        )\n        \n        print(f\"Successfully connected to Aliyun region: {region_id}\")\n        return client\n        \n    except ClientException as e:\n        print(f\"Client error: {e}\")\n    except ServerException as e:\n        print(f\"Server error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Replace with your actual credentials\n    ACCESS_KEY_ID = \"your_access_key_id\"\n    ACCESS_KEY_SECRET = \"your_access_key_secret\"\n    REGION_ID = \"cn-hangzhou\"  # Example region\n    \n    client = list_aliyun_resources(ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID)\n    \n    if client:\n        print(\"Client initialized successfully. You can now make API calls.\")\n        # Example: To list ECS instances, you would use:\n        # from aliyunsdkecs.request.v20140526 import DescribeInstancesRequest\n        # request = DescribeInstancesRequest.DescribeInstancesRequest()\n        # response = client.do_action_with_exception(request)\n```\n\n**Note:** This code initializes the Aliyun client. To retrieve specific resources, you'll need to import and use the appropriate service modules (e.g., `aliyunsdkecs` for ECS, `aliyunsdkrds` for RDS, etc.) and make specific API requests based on the resources you want to list.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["aliyunsdkcore", "aliyun-python-sdk-core"], "package_breakdown": {"install_packages": ["aliyunsdkcore", "aliyun-python-sdk-core"], "import_packages": ["aliyunsdkcore"], "llm_suggested": ["aliyun-python-sdk-core"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that installs the required dependencies for a project using the package manager, and creates a distributable package for easy deployment.", "prompt_index": 242, "generated_code": "```python\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef install_dependencies(requirements_file='requirements.txt'):\n    \"\"\"Install project dependencies from requirements.txt\"\"\"\n    try:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', requirements_file])\n        print(\"Dependencies installed successfully!\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing dependencies: {e}\")\n        sys.exit(1)\n\ndef create_setup_py():\n    \"\"\"Create a setup.py file for packaging\"\"\"\n    setup_content = '''from setuptools import setup, find_packages\n\nsetup(\n    name=\"your-project-name\",\n    version=\"0.1.0\",\n    packages=find_packages(),\n    install_requires=[\n        # Add your dependencies here or use requirements.txt\n    ],\n    author=\"Your Name\",\n    author_email=\"your.email@example.com\",\n    description=\"Your project description\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\n'''\n    \n    with open('setup.py', 'w') as f:\n        f.write(setup_content)\n    print(\"setup.py created!\")\n\ndef build_package():\n    \"\"\"Build the distributable package\"\"\"\n    try:\n        subprocess.check_call([sys.executable, 'setup.py', 'sdist', 'bdist_wheel'])\n        print(\"Package built successfully!\")\n        print(\"Distributable files are in the 'dist' directory\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error building package: {e}\")\n        sys.exit(1)\n\ndef main():\n    # Check if requirements.txt exists\n    if not Path('requirements.txt').exists():\n        print(\"requirements.txt not found. Please create it first.\")\n        sys.exit(1)\n    \n    # Install dependencies\n    install_dependencies()\n    \n    # Create setup.py if it doesn't exist\n    if not Path('setup.py').exists():\n        create_setup_py()\n    \n    # Build the package\n    build_package()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n```python\n# Additional utility: Create a basic requirements.txt if needed\ndef create_requirements_file():\n    \"\"\"Create a basic requirements.txt template\"\"\"\n    requirements = '''# Add your project dependencies here\n# Example:\n# requests==2.28.1\n# numpy>=1.21.0\n# pandas\n'''\n    \n    with open('requirements.txt', 'w') as f:\n        f.write(requirements)\n    print(\"requirements.txt template created!\")\n\n# Uncomment the line below to create requirements.txt if needed\n# create_requirements_file()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "pathlib", "setuptools"], "package_breakdown": {"install_packages": ["sys", "subprocess", "pathlib", "setuptools"], "import_packages": ["sys", "subprocess", "pathlib"], "llm_suggested": ["subprocess", "sys", "pathlib", "setuptools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that clones a Git repository from a specified URL, checks out a specific branch or commit, and lists the files in the repository along with their respective statuses (modified, added, deleted, etc.).", "prompt_index": 243, "generated_code": "```python\nimport os\nimport tempfile\nimport shutil\nfrom git import Repo, GitCommandError\n\ndef clone_and_checkout_repo(repo_url, target_ref=None, checkout_path=None):\n    \"\"\"\n    Clone a Git repository, checkout a specific branch/commit, and list file statuses.\n    \n    Args:\n        repo_url (str): URL of the Git repository to clone\n        target_ref (str, optional): Branch name, tag, or commit hash to checkout\n        checkout_path (str, optional): Local path to clone into. If None, uses temp directory\n    \n    Returns:\n        dict: Dictionary containing repo path and file status information\n    \"\"\"\n    \n    # Create temporary directory if no path specified\n    if checkout_path is None:\n        temp_dir = tempfile.mkdtemp()\n        repo_path = temp_dir\n        is_temp = True\n    else:\n        repo_path = checkout_path\n        is_temp = False\n    \n    try:\n        # Clone the repository\n        print(f\"Cloning repository from {repo_url}...\")\n        repo = Repo.clone_from(repo_url, repo_path)\n        \n        # Checkout specific reference if provided\n        if target_ref:\n            print(f\"Checking out {target_ref}...\")\n            repo.git.checkout(target_ref)\n        \n        # Get file status information\n        status = repo.git.status('--porcelain')\n        status_lines = status.split('\\n') if status else []\n        \n        file_statuses = []\n        for line in status_lines:\n            if line.strip():\n                # Parse porcelain status format\n                status_code = line[:2].strip()\n                filename = line[3:]\n                \n                status_info = {\n                    'filename': filename,\n                    'status_code': status_code,\n                    'status_description': get_status_description(status_code)\n                }\n                file_statuses.append(status_info)\n        \n        return {\n            'repo_path': repo_path,\n            'is_temporary': is_temp,\n            'file_statuses': file_statuses,\n            'total_files': len(file_statuses)\n        }\n        \n    except GitCommandError as e:\n        print(f\"Git command error: {e}\")\n        if is_temp:\n            shutil.rmtree(repo_path)\n        raise\n    except Exception as e:\n        print(f\"Error: {e}\")\n        if is_temp:\n            shutil.rmtree(repo_path)\n        raise\n\ndef get_status_description(status_code):\n    \"\"\"Convert Git status code to human-readable description\"\"\"\n    status_map = {\n        'M': 'modified',\n        'A': 'added',\n        'D': 'deleted',\n        'R': 'renamed',\n        'C': 'copied',\n        'U': 'updated but unmerged',\n        '??': 'untracked',\n        '!!': 'ignored'\n    }\n    \n    # Handle two-character status codes\n    if len(status_code) == 2:\n        if status_code[0] == ' ':\n            return status_map.get(status_code[1], 'unknown')\n        elif status_code[1] == ' ':\n            return status_map.get(status_code[0], 'unknown')\n        else:\n            return f\"{status_map.get(status_code[0], 'unknown')} and {status_map.get(status_code[1], 'unknown')}\"\n    \n    return status_map.get(status_code, 'unknown')\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        # Example: Clone a repository and checkout a specific branch\n        result = clone_and_checkout_repo(\n            repo_url=\"https://github.com/example/repository.git\",\n            target_ref=\"main\"  # or \"develop\" or commit hash\n        )\n        \n        print(f\"\\nRepository cloned to: {result['repo_path']}\")\n        print(f\"Total files with status changes: {result['total_files']}\")\n        print(\"\\nFile statuses:\")\n        \n        for file_status in result['file_statuses']:\n            print(f\"{file_status['status_code']} {file_status['filename']} ({file_status['status_description']})\")\n        \n        # Clean up temporary directory if created\n        if result['is_temporary']:\n            shutil.rmtree(result['repo_path'])\n            \n    except Exception as e:\n        print(f\"Failed to process repository: {e}\")\n```\n\nNote: This code requires the `gitpython` package. Install it using:\n```bash\npip install gitpython\n```\n\nThe code handles temporary directory cleanup, provides detailed status information, and includes error handling for common Git operations.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 1, "hallucinations": ["git"], "packages_detected": ["git", "shutil", "gitpython", "tempfile", "os"], "package_breakdown": {"install_packages": ["git", "shutil", "gitpython", "tempfile", "os"], "import_packages": ["shutil", "git", "os", "tempfile"], "llm_suggested": ["gitpython", "os", "tempfile", "shutil"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a Jupyter notebook cell with inline Matplotlib backend settings to display plots directly within the notebook.", "prompt_index": 244, "generated_code": "```python\n# Create a Jupyter notebook cell with inline Matplotlib backend\nfrom IPython import get_ipython\n\n# Set the matplotlib backend to inline\nget_ipython().run_line_magic('matplotlib', 'inline')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "IPython"], "package_breakdown": {"install_packages": ["matplotlib", "IPython"], "import_packages": ["IPython"], "llm_suggested": ["matplotlib", "IPython"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically logs all caught exceptions to a specified file for easy error tracking and debugging.", "prompt_index": 245, "generated_code": "```python\nimport logging\nimport sys\nfrom datetime import datetime\n\ndef setup_exception_logging(log_file_path='error_log.txt'):\n    \"\"\"\n    Set up automatic logging of all caught exceptions to a specified file.\n    \n    Args:\n        log_file_path (str): Path to the log file. Defaults to 'error_log.txt'\n    \"\"\"\n    # Configure logging\n    logging.basicConfig(\n        level=logging.ERROR,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler(log_file_path),\n            logging.StreamHandler(sys.stderr)  # Also output to stderr\n        ]\n    )\n    \n    def exception_handler(exc_type, exc_value, exc_traceback):\n        \"\"\"Custom exception handler that logs all uncaught exceptions.\"\"\"\n        if issubclass(exc_type, KeyboardInterrupt):\n            # Don't log keyboard interrupts\n            sys.__excepthook__(exc_type, exc_value, exc_traceback)\n            return\n            \n        logging.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n    \n    # Set the custom exception handler\n    sys.excepthook = exception_handler\n\n# Example usage and decorator for manual exception logging\ndef log_exceptions(func):\n    \"\"\"\n    Decorator to automatically log exceptions from function calls.\n    \n    Usage:\n        @log_exceptions\n        def your_function():\n            # your code here\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logging.error(f\"Exception in {func.__name__}: {str(e)}\", exc_info=True)\n            raise  # Re-raise the exception after logging\n    return wrapper\n\n# Initialize the logging system when this module is imported\nsetup_exception_logging()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "datetime", "logging"], "package_breakdown": {"install_packages": ["sys", "datetime", "logging"], "import_packages": ["sys", "datetime", "logging"], "llm_suggested": ["logging", "sys", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Sentry project, captures and sends error messages, and sets up custom logging configurations for real-time monitoring and debugging.", "prompt_index": 246, "generated_code": "```python\nimport logging\nimport sentry_sdk\nfrom sentry_sdk.integrations.logging import LoggingIntegration\n\n# Initialize Sentry SDK\nsentry_sdk.init(\n    dsn=\"YOUR_SENTRY_DSN_HERE\",  # Replace with your actual DSN\n    integrations=[\n        LoggingIntegration(\n            level=logging.INFO,        # Capture info and above as breadcrumbs\n            event_level=logging.ERROR  # Send errors as events\n        )\n    ],\n    traces_sample_rate=1.0,  # Performance monitoring\n    environment=\"production\",  # Set environment\n    release=\"your-app@1.0.0\"   # Set release version\n)\n\n# Configure custom logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('app.log'),\n        logging.StreamHandler()\n    ]\n)\n\n# Example usage\ndef example_function():\n    try:\n        # Your code here\n        result = 1 / 0  # This will cause an error\n    except Exception as e:\n        # Capture exception automatically by Sentry\n        logging.error(\"An error occurred in example_function\", exc_info=True)\n        # Or manually capture\n        sentry_sdk.capture_exception(e)\n        raise\n\n# Custom error capture with context\ndef capture_custom_error(message, extra_data=None):\n    with sentry_sdk.configure_scope() as scope:\n        if extra_data:\n            for key, value in extra_data.items():\n                scope.set_extra(key, value)\n        scope.set_tag(\"custom_error\", \"true\")\n        sentry_sdk.capture_message(message, level=\"error\")\n\n# Example of adding user context\ndef set_user_context(user_id, email):\n    sentry_sdk.set_user({\n        \"id\": user_id,\n        \"email\": email\n    })\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set user context\n    set_user_context(\"123\", \"user@example.com\")\n    \n    # Capture a custom error with extra data\n    capture_custom_error(\n        \"Custom error message\",\n        {\"additional_data\": \"some_value\", \"request_id\": \"abc123\"}\n    )\n    \n    # This will trigger an automatic error capture\n    example_function()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["sentry_sdk"], "packages_detected": ["sentry_sdk", "logging"], "package_breakdown": {"install_packages": ["sentry_sdk", "logging"], "import_packages": ["sentry_sdk", "logging"], "llm_suggested": ["sentry_sdk", "logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a MySQL database, retrieves data from a specific table, and then performs a series of data manipulation operations on the retrieved data using the Pure Python MySQL Driver package.", "prompt_index": 247, "generated_code": "```python\nimport mysql.connector\nfrom mysql.connector import Error\n\ndef connect_to_mysql(host, database, user, password):\n    \"\"\"Establish connection to MySQL database\"\"\"\n    try:\n        connection = mysql.connector.connect(\n            host=host,\n            database=database,\n            user=user,\n            password=password\n        )\n        if connection.is_connected():\n            print(\"Connected to MySQL database\")\n            return connection\n    except Error as e:\n        print(f\"Error while connecting to MySQL: {e}\")\n        return None\n\ndef retrieve_data(connection, table_name):\n    \"\"\"Retrieve all data from specified table\"\"\"\n    try:\n        cursor = connection.cursor()\n        query = f\"SELECT * FROM {table_name}\"\n        cursor.execute(query)\n        results = cursor.fetchall()\n        column_names = [desc[0] for desc in cursor.description]\n        cursor.close()\n        return results, column_names\n    except Error as e:\n        print(f\"Error retrieving data: {e}\")\n        return None, None\n\ndef data_manipulation_operations(data, column_names):\n    \"\"\"Perform data manipulation operations on retrieved data\"\"\"\n    if data is None or not data:\n        print(\"No data to manipulate\")\n        return None\n    \n    # Convert to list of dictionaries for easier manipulation\n    data_dicts = []\n    for row in data:\n        data_dicts.append(dict(zip(column_names, row)))\n    \n    # Example manipulation operations:\n    \n    # 1. Filter data (example: keep only rows where 'age' > 25)\n    if 'age' in column_names:\n        filtered_data = [row for row in data_dicts if row.get('age', 0) > 25]\n        print(f\"Filtered {len(filtered_data)} rows where age > 25\")\n    \n    # 2. Add new calculated column\n    if 'salary' in column_names and 'bonus' in column_names:\n        for row in data_dicts:\n            row['total_compensation'] = row.get('salary', 0) + row.get('bonus', 0)\n        print(\"Added total_compensation column\")\n    \n    # 3. Group data (example: group by department and calculate average salary)\n    if 'department' in column_names and 'salary' in column_names:\n        department_stats = {}\n        for row in data_dicts:\n            dept = row.get('department')\n            salary = row.get('salary', 0)\n            if dept not in department_stats:\n                department_stats[dept] = {'count': 0, 'total_salary': 0}\n            department_stats[dept]['count'] += 1\n            department_stats[dept]['total_salary'] += salary\n        \n        for dept, stats in department_stats.items():\n            stats['avg_salary'] = stats['total_salary'] / stats['count']\n        print(\"Calculated department salary statistics\")\n    \n    # 4. Sort data (example: sort by salary descending)\n    if 'salary' in column_names:\n        sorted_data = sorted(data_dicts, key=lambda x: x.get('salary', 0), reverse=True)\n        print(\"Sorted data by salary (descending)\")\n    \n    return data_dicts\n\ndef main():\n    # Database configuration - replace with your actual credentials\n    host = \"localhost\"\n    database = \"your_database\"\n    user = \"your_username\"\n    password = \"your_password\"\n    table_name = \"your_table\"\n    \n    # Establish connection\n    connection = connect_to_mysql(host, database, user, password)\n    \n    if connection:\n        try:\n            # Retrieve data\n            data, column_names = retrieve_data(connection, table_name)\n            \n            if data:\n                # Perform data manipulation operations\n                manipulated_data = data_manipulation_operations(data, column_names)\n                \n                # Example: Print first 5 manipulated rows\n                if manipulated_data:\n                    print(\"\\nFirst 5 manipulated rows:\")\n                    for i, row in enumerate(manipulated_data[:5]):\n                        print(f\"Row {i+1}: {row}\")\n        \n        finally:\n            # Close connection\n            connection.close()\n            print(\"MySQL connection closed\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: Before running this code, install the MySQL Connector package using:\n```bash\npip install mysql-connector-python\n```\n\nReplace the database configuration variables (`host`, `database`, `user`, `password`, `table_name`) with your actual MySQL credentials and table information. The data manipulation operations are examples - modify them according to your specific table structure and requirements.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["mysql", "mysql-connector-python"], "package_breakdown": {"install_packages": ["mysql", "mysql-connector-python"], "import_packages": ["mysql"], "llm_suggested": ["mysql-connector-python"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts the version number from the source control management (SCM) tags to manage and update the version of your project automatically.", "prompt_index": 248, "generated_code": "```python\nimport os\nimport re\nimport subprocess\nfrom typing import Optional\n\n\ndef get_version_from_scm() -> Optional[str]:\n    \"\"\"\n    Extract version number from SCM tags using Git.\n    Returns the version string or None if not found.\n    \"\"\"\n    try:\n        # Check if we're in a git repository\n        subprocess.run(['git', 'rev-parse', '--is-inside-work-tree'], \n                      check=True, capture_output=True, text=True)\n        \n        # Get all tags\n        tags_result = subprocess.run(['git', 'tag', '--list'], \n                                   capture_output=True, text=True, check=True)\n        tags = tags_result.stdout.strip().split('\\n')\n        \n        # Filter and sort version tags (assuming semantic versioning)\n        version_tags = []\n        version_pattern = re.compile(r'^v?(\\d+\\.\\d+\\.\\d+)$')\n        \n        for tag in tags:\n            if tag and version_pattern.match(tag):\n                version_tags.append(tag)\n        \n        if not version_tags:\n            return None\n        \n        # Sort by semantic version (latest first)\n        version_tags.sort(key=lambda x: [int(y) for y in x.lstrip('v').split('.')], reverse=True)\n        \n        return version_tags[0]\n        \n    except (subprocess.CalledProcessError, FileNotFoundError):\n        # Git not available or not in a git repo\n        return None\n\n\ndef get_version_from_scm_with_commit() -> Optional[str]:\n    \"\"\"\n    Get version from latest tag and include commit hash for development versions\n    \"\"\"\n    try:\n        # Get latest tag\n        tag_result = subprocess.run(['git', 'describe', '--tags', '--abbrev=0'], \n                                  capture_output=True, text=True)\n        \n        if tag_result.returncode == 0:\n            latest_tag = tag_result.stdout.strip()\n            \n            # Get commit count since last tag\n            commit_count_result = subprocess.run(\n                ['git', 'rev-list', '--count', f'{latest_tag}..HEAD'],\n                capture_output=True, text=True\n            )\n            \n            if commit_count_result.returncode == 0:\n                commit_count = commit_count_result.stdout.strip()\n                \n                if commit_count == '0':\n                    return latest_tag\n                else:\n                    # Get short commit hash\n                    commit_hash_result = subprocess.run(\n                        ['git', 'rev-parse', '--short', 'HEAD'],\n                        capture_output=True, text=True\n                    )\n                    commit_hash = commit_hash_result.stdout.strip()\n                    return f\"{latest_tag}.dev{commit_count}+{commit_hash}\"\n        \n        return None\n        \n    except (subprocess.CalledProcessError, FileNotFoundError):\n        return None\n\n\n# Example usage\nif __name__ == \"__main__\":\n    version = get_version_from_scm()\n    if version:\n        print(f\"Latest version from SCM: {version}\")\n    else:\n        print(\"No version tags found or not in a git repository\")\n    \n    dev_version = get_version_from_scm_with_commit()\n    if dev_version:\n        print(f\"Development version: {dev_version}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "typing", "re", "os"], "package_breakdown": {"install_packages": ["subprocess", "typing", "re", "os"], "import_packages": ["re", "os", "subprocess", "typing"], "llm_suggested": ["os", "re", "subprocess", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that dynamically generates build configurations for a project based on user-defined settings and requirements using the features provided by the modern and extensible Python build backend package.", "prompt_index": 249, "generated_code": "```python\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass, asdict\nimport json\nimport yaml\nfrom pathlib import Path\n\n@dataclass\nclass BuildConfig:\n    python_requires: str\n    package_name: str\n    version: str\n    description: str\n    author: str\n    author_email: str\n    classifiers: List[str]\n    dependencies: List[str]\n    extras_require: Dict[str, List[str]]\n    entry_points: Dict[str, List[str]]\n    package_dir: Dict[str, str]\n    packages: List[str]\n    include_package_data: bool\n    package_data: Dict[str, List[str]]\n    data_files: List[tuple]\n    scripts: List[str]\n    cmdclass: Dict[str, Any]\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n    \n    def to_json(self) -> str:\n        return json.dumps(self.to_dict(), indent=2)\n    \n    def to_yaml(self) -> str:\n        return yaml.dump(self.to_dict(), default_flow_style=False)\n    \n    def save_config(self, filepath: Path, format: str = 'json'):\n        if format.lower() == 'json':\n            filepath.write_text(self.to_json())\n        elif format.lower() == 'yaml':\n            filepath.write_text(self.to_yaml())\n        else:\n            raise ValueError(\"Unsupported format. Use 'json' or 'yaml'\")\n\nclass BuildConfigGenerator:\n    def __init__(self):\n        self.defaults = {\n            'python_requires': '>=3.8',\n            'include_package_data': True,\n            'package_data': {'': ['*.txt', '*.rst', '*.md']},\n            'data_files': [],\n            'scripts': [],\n            'cmdclass': {}\n        }\n    \n    def generate_config(self, user_settings: Dict[str, Any]) -> BuildConfig:\n        config_data = {**self.defaults, **user_settings}\n        \n        return BuildConfig(\n            python_requires=config_data.get('python_requires', '>=3.8'),\n            package_name=config_data['package_name'],\n            version=config_data['version'],\n            description=config_data.get('description', ''),\n            author=config_data.get('author', ''),\n            author_email=config_data.get('author_email', ''),\n            classifiers=config_data.get('classifiers', []),\n            dependencies=config_data.get('dependencies', []),\n            extras_require=config_data.get('extras_require', {}),\n            entry_points=config_data.get('entry_points', {}),\n            package_dir=config_data.get('package_dir', {}),\n            packages=config_data.get('packages', []),\n            include_package_data=config_data.get('include_package_data', True),\n            package_data=config_data.get('package_data', {}),\n            data_files=config_data.get('data_files', []),\n            scripts=config_data.get('scripts', []),\n            cmdclass=config_data.get('cmdclass', {})\n        )\n    \n    def generate_from_template(self, template_path: Path, **kwargs) -> BuildConfig:\n        if template_path.suffix == '.json':\n            template_data = json.loads(template_path.read_text())\n        elif template_path.suffix in ['.yaml', '.yml']:\n            template_data = yaml.safe_load(template_path.read_text())\n        else:\n            raise ValueError(\"Unsupported template format\")\n        \n        template_data.update(kwargs)\n        return self.generate_config(template_data)\n\n# Example usage\ndef example_usage():\n    generator = BuildConfigGenerator()\n    \n    # Generate from direct settings\n    user_settings = {\n        'package_name': 'my_project',\n        'version': '0.1.0',\n        'description': 'A sample project',\n        'author': 'John Doe',\n        'author_email': 'john@example.com',\n        'dependencies': ['requests>=2.25.0', 'numpy>=1.19.0'],\n        'extras_require': {\n            'dev': ['pytest>=6.0.0', 'black>=20.0'],\n            'docs': ['sphinx>=4.0.0']\n        },\n        'packages': ['my_project', 'my_project.utils'],\n        'classifiers': [\n            'Development Status :: 3 - Alpha',\n            'Intended Audience :: Developers',\n            'License :: OSI Approved :: MIT License'\n        ]\n    }\n    \n    config = generator.generate_config(user_settings)\n    \n    # Save configuration\n    config.save_config(Path('build_config.json'))\n    config.save_config(Path('build_config.yaml'), format='yaml')\n    \n    return config\n\nif __name__ == \"__main__\":\n    example_usage()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml", "pyyaml", "dataclasses", "pathlib", "typing", "json"], "package_breakdown": {"install_packages": ["yaml", "pyyaml", "dataclasses", "pathlib", "typing", "json"], "import_packages": ["dataclasses", "json", "pathlib", "typing", "yaml"], "llm_suggested": ["typing", "dataclasses", "json", "pyyaml", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the super-fast templating language to render a dynamic web page with data fetched from an API and displays it in a visually appealing format.", "prompt_index": 250, "generated_code": "```python\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nimport httpx\nimport asyncio\n\napp = FastAPI()\ntemplates = Jinja2Templates(directory=\"templates\")\n\nasync def fetch_api_data(api_url: str):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(api_url)\n        response.raise_for_status()\n        return response.json()\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def render_dynamic_page(request: Request):\n    try:\n        # Example API endpoint - replace with your actual API URL\n        api_url = \"https://jsonplaceholder.typicode.com/posts\"\n        data = await fetch_api_data(api_url)\n        \n        return templates.TemplateResponse(\n            \"index.html\",\n            {\n                \"request\": request,\n                \"posts\": data[:10],  # Display first 10 items\n                \"title\": \"Dynamic Web Page\"\n            }\n        )\n    except Exception as e:\n        return templates.TemplateResponse(\n            \"error.html\",\n            {\n                \"request\": request,\n                \"error\": str(e)\n            }\n        )\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n**Templates directory structure:**\n```\ntemplates/\n\u251c\u2500\u2500 index.html\n\u2514\u2500\u2500 error.html\n```\n\n**templates/index.html:**\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{{ title }}</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n    <style>\n        .post-card {\n            transition: transform 0.2s;\n            margin-bottom: 20px;\n        }\n        .post-card:hover {\n            transform: translateY(-5px);\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container mt-5\">\n        <h1 class=\"text-center mb-4\">{{ title }}</h1>\n        \n        <div class=\"row\">\n            {% for post in posts %}\n            <div class=\"col-md-6 col-lg-4\">\n                <div class=\"card post-card\">\n                    <div class=\"card-body\">\n                        <h5 class=\"card-title\">{{ post.title }}</h5>\n                        <p class=\"card-text\">{{ post.body[:100] }}...</p>\n                        <small class=\"text-muted\">Post ID: {{ post.id }}</small>\n                    </div>\n                </div>\n            </div>\n            {% endfor %}\n        </div>\n    </div>\n</body>\n</html>\n```\n\n**templates/error.html:**\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Error</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n</head>\n<body>\n    <div class=\"container mt-5\">\n        <div class=\"alert alert-danger\">\n            <h4>Error Loading Data</h4>\n            <p>{{ error }}</p>\n        </div>\n    </div>\n</body>\n</html>\n```\n\n**Requirements:**\n```text\nfastapi\nuvicorn\njinja2\nhttpx\n```\n\nThis code uses:\n- **FastAPI** for the web framework\n- **Jinja2** (super-fast templating engine)\n- **Async HTTP requests** for API data fetching\n- **Bootstrap** for responsive styling\n- **Modern CSS animations** for visual appeal\n\nTo run: `pip install fastapi uvicorn jinja2 httpx` then `python your_file.py`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "jinja2", "uvicorn", "fastapi", "httpx"], "package_breakdown": {"install_packages": ["asyncio", "jinja2", "uvicorn", "fastapi", "httpx"], "import_packages": ["asyncio", "fastapi", "uvicorn", "httpx"], "llm_suggested": ["fastapi", "uvicorn", "jinja2", "httpx"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically reorganizes and sorts all import statements in a Python script using the provided utility/library for better code readability and maintainability.", "prompt_index": 251, "generated_code": "```python\nimport re\nimport sys\nfrom pathlib import Path\nfrom typing import List, Dict, Set\n\ndef extract_imports(content: str) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract all import statements from Python code content.\n    Returns a dictionary with keys: 'stdlib', 'third_party', 'local'\n    \"\"\"\n    import_pattern = re.compile(\n        r'^(?:from\\s+(\\w+(?:\\.\\w+)*)\\s+import\\s+([\\w\\s,]+)'\n        r'|import\\s+([\\w\\s,]+))',\n        re.MULTILINE\n    )\n    \n    imports = {'stdlib': [], 'third_party': [], 'local': []}\n    stdlib_modules = set(sys.stdlib_module_names)\n    \n    for match in import_pattern.finditer(content):\n        if match.group(1):  # from ... import ...\n            module = match.group(1)\n            imports_list = [imp.strip() for imp in match.group(2).split(',')]\n            full_imports = [f\"from {module} import {imp}\" for imp in imports_list]\n        else:  # import ...\n            modules = [mod.strip() for mod in match.group(3).split(',')]\n            full_imports = [f\"import {mod}\" for mod in modules]\n            module = modules[0].split('.')[0] if modules else \"\"\n        \n        if module:\n            if module in stdlib_modules:\n                imports['stdlib'].extend(full_imports)\n            elif '.' in module or any(c in module for c in ['./', '../']):\n                imports['local'].extend(full_imports)\n            else:\n                imports['third_party'].extend(full_imports)\n    \n    return imports\n\ndef sort_imports(imports_dict: Dict[str, List[str]]) -> List[str]:\n    \"\"\"Sort imports within each category and return sorted list\"\"\"\n    sorted_imports = []\n    \n    # Sort and deduplicate each category\n    for category in ['stdlib', 'third_party', 'local']:\n        if imports_dict[category]:\n            # Remove duplicates while preserving order (Python 3.7+)\n            unique_imports = list(dict.fromkeys(imports_dict[category]))\n            sorted_category = sorted(unique_imports, key=lambda x: x.lower())\n            sorted_imports.extend(sorted_category)\n            sorted_imports.append('')  # Add blank line between categories\n    \n    return sorted_imports\n\ndef remove_imports(content: str) -> str:\n    \"\"\"Remove all import statements from content\"\"\"\n    import_pattern = re.compile(\n        r'^(?:from\\s+\\w+(?:\\.\\w+)*\\s+import\\s+[\\w\\s,]+\\n'\n        r'|import\\s+[\\w\\s,]+\\n)',\n        re.MULTILINE\n    )\n    return import_pattern.sub('', content)\n\ndef reorganize_imports(file_path: str) -> None:\n    \"\"\"Main function to reorganize imports in a Python file\"\"\"\n    path = Path(file_path)\n    \n    if not path.exists() or path.suffix != '.py':\n        print(f\"Error: {file_path} is not a valid Python file\")\n        return\n    \n    content = path.read_text(encoding='utf-8')\n    \n    # Extract imports and remaining content\n    imports_dict = extract_imports(content)\n    remaining_content = remove_imports(content)\n    \n    # Sort imports\n    sorted_imports = sort_imports(imports_dict)\n    \n    # Reconstruct file content\n    new_content = '\\n'.join(sorted_imports).strip() + '\\n\\n' + remaining_content.strip()\n    \n    # Write back to file\n    path.write_text(new_content, encoding='utf-8')\n    print(f\"Successfully reorganized imports in {file_path}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python import_organizer.py <python_file>\")\n        sys.exit(1)\n    \n    reorganize_imports(sys.argv[1])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "sys", "pathlib", "re"], "package_breakdown": {"install_packages": ["typing", "sys", "pathlib", "re"], "import_packages": ["re", "sys", "typing", "pathlib"], "llm_suggested": ["re", "sys", "pathlib", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the McCabe checker plugin for flake8 to analyze the cyclomatic complexity of a Python codebase and identify areas where the code may be overly complex and in need of refactoring.", "prompt_index": 252, "generated_code": "```python\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef analyze_complexity(directory_path, max_complexity=10):\n    \"\"\"\n    Analyze cyclomatic complexity of Python files using flake8 with McCabe plugin\n    \n    Args:\n        directory_path (str): Path to the directory containing Python files\n        max_complexity (int): Maximum allowed complexity threshold (default: 10)\n    \"\"\"\n    \n    # Convert to Path object for easier handling\n    path = Path(directory_path)\n    \n    if not path.exists():\n        print(f\"Error: Directory '{directory_path}' does not exist\")\n        return\n    \n    # Find all Python files recursively\n    python_files = list(path.rglob(\"*.py\"))\n    \n    if not python_files:\n        print(\"No Python files found in the specified directory\")\n        return\n    \n    print(f\"Found {len(python_files)} Python files to analyze\")\n    print(f\"Using complexity threshold: {max_complexity}\")\n    print(\"-\" * 60)\n    \n    # Run flake8 with McCabe plugin for each file\n    for file_path in python_files:\n        print(f\"\\nAnalyzing: {file_path}\")\n        \n        # Build the command\n        cmd = [\n            sys.executable, \"-m\", \"flake8\",\n            \"--select\", \"C901\",  # McCabe complexity errors\n            \"--max-complexity\", str(max_complexity),\n            str(file_path)\n        ]\n        \n        try:\n            # Run the command and capture output\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                timeout=30  # Timeout after 30 seconds per file\n            )\n            \n            # Print any complexity violations\n            if result.stdout:\n                print(\"Complexity issues found:\")\n                print(result.stdout)\n            else:\n                print(\"\u2713 No complexity issues above threshold\")\n                \n        except subprocess.TimeoutExpired:\n            print(f\"\u26a0 Timeout analyzing {file_path} - file may be very large\")\n        except Exception as e:\n            print(f\"Error analyzing {file_path}: {e}\")\n\ndef get_complexity_summary(directory_path):\n    \"\"\"\n    Get a summary of complexity metrics for all Python files\n    \"\"\"\n    path = Path(directory_path)\n    python_files = list(path.rglob(\"*.py\"))\n    \n    summary = {}\n    \n    for file_path in python_files:\n        cmd = [\n            sys.executable, \"-m\", \"flake8\",\n            \"--select\", \"C901\",\n            \"--max-complexity\", \"1000\",  # Set high to get all complexity values\n            \"--quiet\",  # Only output errors, not file names\n            str(file_path)\n        ]\n        \n        try:\n            result = subprocess.run(cmd, capture_output=True, text=True)\n            if result.stdout:\n                # Count complexity violations per file\n                violations = result.stdout.strip().split('\\n')\n                summary[str(file_path)] = len(violations)\n                \n        except Exception:\n            summary[str(file_path)] = \"Error\"\n    \n    return summary\n\nif __name__ == \"__main__\":\n    # Example usage\n    if len(sys.argv) > 1:\n        target_directory = sys.argv[1]\n        complexity_threshold = int(sys.argv[2]) if len(sys.argv) > 2 else 10\n        \n        # Analyze complexity\n        analyze_complexity(target_directory, complexity_threshold)\n        \n        # Get summary\n        print(\"\\n\" + \"=\" * 60)\n        print(\"COMPLEXITY SUMMARY:\")\n        print(\"=\" * 60)\n        \n        summary = get_complexity_summary(target_directory)\n        for file_path, violations in summary.items():\n            print(f\"{file_path}: {violations} complexity issues\")\n    else:\n        print(\"Usage: python complexity_analyzer.py <directory_path> [max_complexity]\")\n        print(\"Example: python complexity_analyzer.py ./src 15\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "flake8", "subprocess", "mccabe", "pathlib"], "package_breakdown": {"install_packages": ["sys", "flake8", "subprocess", "mccabe", "pathlib"], "import_packages": ["sys", "subprocess", "pathlib"], "llm_suggested": ["flake8", "mccabe"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes a WSGI HTTP server on a UNIX system to handle incoming HTTP requests and route them to the appropriate WSGI application.", "prompt_index": 253, "generated_code": "```python\nimport os\nimport sys\nfrom wsgiref.simple_server import make_server\nfrom wsgiref.util import setup_testing_defaults\n\ndef application(environ, start_response):\n    setup_testing_defaults(environ)\n    \n    status = '200 OK'\n    headers = [('Content-type', 'text/plain; charset=utf-8')]\n    \n    start_response(status, headers)\n    \n    ret = [f\"{key}: {value}\\n\".encode(\"utf-8\") \n           for key, value in environ.items()]\n    return ret\n\nif __name__ == '__main__':\n    if len(sys.argv) > 1:\n        socket_path = sys.argv[1]\n        try:\n            os.unlink(socket_path)\n        except OSError:\n            if os.path.exists(socket_path):\n                raise\n        \n        server = make_server('', 0, application)\n        server.socket.bind(socket_path)\n        print(f\"Server listening on UNIX socket: {socket_path}\")\n        server.serve_forever()\n    else:\n        port = 8000\n        server = make_server('', port, application)\n        print(f\"Server listening on port {port}\")\n        server.serve_forever()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "os", "wsgiref"], "package_breakdown": {"install_packages": ["sys", "os", "wsgiref"], "import_packages": ["sys", "os", "wsgiref"], "llm_suggested": ["wsgiref"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that checks a given Python script for compliance with the PEP 8 style guide and outputs a list of any violations found, with line numbers and specific style rule references.", "prompt_index": 254, "generated_code": "```python\nimport subprocess\nimport sys\nfrom typing import List, Dict, Any\n\ndef check_pep8_compliance(file_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Check a Python script for PEP 8 compliance using pycodestyle.\n    \n    Args:\n        file_path: Path to the Python file to check\n        \n    Returns:\n        List of dictionaries containing violation details\n    \"\"\"\n    try:\n        # Run pycodestyle with detailed output\n        result = subprocess.run([\n            sys.executable, '-m', 'pycodestyle',\n            '--show-source',\n            '--show-pep8',\n            file_path\n        ], capture_output=True, text=True, timeout=30)\n        \n        violations = []\n        \n        # Parse the output\n        for line in result.stdout.strip().split('\\n'):\n            if line and ':' in line:\n                parts = line.split(':', 3)\n                if len(parts) >= 4:\n                    violation = {\n                        'file': parts[0],\n                        'line': int(parts[1]),\n                        'column': int(parts[2]),\n                        'message': parts[3].strip(),\n                        'error_code': extract_error_code(parts[3])\n                    }\n                    violations.append(violation)\n        \n        return violations\n        \n    except subprocess.TimeoutExpired:\n        return [{'error': 'PEP 8 check timed out after 30 seconds'}]\n    except FileNotFoundError:\n        return [{'error': 'pycodestyle not found. Install it with: pip install pycodestyle'}]\n    except Exception as e:\n        return [{'error': f'Error running PEP 8 check: {str(e)}'}]\n\ndef extract_error_code(message: str) -> str:\n    \"\"\"\n    Extract the PEP 8 error code from the violation message.\n    \n    Args:\n        message: The violation message\n        \n    Returns:\n        Error code (e.g., 'E101', 'W291') or empty string if not found\n    \"\"\"\n    import re\n    match = re.search(r'([EW]\\d{3})', message)\n    return match.group(1) if match else ''\n\ndef format_violations(violations: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    Format violations for human-readable output.\n    \n    Args:\n        violations: List of violation dictionaries\n        \n    Returns:\n        Formatted string with violation details\n    \"\"\"\n    if not violations:\n        return \"No PEP 8 violations found!\"\n    \n    output = []\n    for violation in violations:\n        if 'error' in violation:\n            output.append(f\"ERROR: {violation['error']}\")\n        else:\n            output.append(\n                f\"Line {violation['line']}, Col {violation['column']}: \"\n                f\"{violation['message']} [{violation.get('error_code', '')}]\"\n            )\n    \n    return \"\\n\".join(output)\n\n# Example usage\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python pep8_checker.py <python_file.py>\")\n        sys.exit(1)\n    \n    file_to_check = sys.argv[1]\n    violations = check_pep8_compliance(file_to_check)\n    print(format_violations(violations))\n```\n\n**Prerequisites:**\n- Install pycodestyle: `pip install pycodestyle`\n- The script requires Python 3.6+\n\n**Usage:**\n```bash\npython pep8_checker.py your_script.py\n```\n\n**Note:** This script uses pycodestyle (formerly pep8), which is the standard tool for checking PEP 8 compliance. It provides detailed error codes (E101, W291, etc.) and specific line/column numbers for each violation.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "sys", "pycodestyle", "typing", "re"], "package_breakdown": {"install_packages": ["subprocess", "sys", "pycodestyle", "typing", "re"], "import_packages": ["re", "sys", "subprocess", "typing"], "llm_suggested": ["subprocess", "sys", "typing", "re", "pycodestyle"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with an IAM API using client credentials and retrieves a list of users within the IAM system.", "prompt_index": 255, "generated_code": "```python\nimport requests\nimport json\nfrom requests.auth import HTTPBasicAuth\n\n# Configuration - Replace with your actual credentials and endpoints\nCLIENT_ID = 'your_client_id_here'\nCLIENT_SECRET = 'your_client_secret_here'\nTOKEN_ENDPOINT = 'https://your-iam-provider.com/oauth/token'\nUSERS_ENDPOINT = 'https://your-iam-provider.com/api/v1/users'\n\ndef get_access_token():\n    \"\"\"Retrieve access token using client credentials flow\"\"\"\n    payload = {\n        'grant_type': 'client_credentials',\n        'scope': 'iam.users.read'  # Adjust scope based on your IAM provider\n    }\n    \n    response = requests.post(\n        TOKEN_ENDPOINT,\n        auth=HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET),\n        data=payload,\n        headers={'Content-Type': 'application/x-www-form-urlencoded'}\n    )\n    \n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception(f\"Token request failed: {response.status_code} - {response.text}\")\n\ndef get_users_list(access_token):\n    \"\"\"Retrieve list of users using the access token\"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Content-Type': 'application/json'\n    }\n    \n    response = requests.get(USERS_ENDPOINT, headers=headers)\n    \n    if response.status_code == 200:\n        return response.json()\n    else:\n        raise Exception(f\"Users request failed: {response.status_code} - {response.text}\")\n\n# Main execution\ntry:\n    # Get access token\n    access_token = get_access_token()\n    \n    # Get users list\n    users = get_users_list(access_token)\n    \n    # Print or process the users list\n    print(\"Users retrieved successfully:\")\n    print(json.dumps(users, indent=2))\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "json"], "package_breakdown": {"install_packages": ["requests", "json"], "import_packages": ["requests", "json"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a virtual environment for a Node.js project, installs required dependencies, and activates the environment for development purposes.", "prompt_index": 256, "generated_code": "```python\nimport os\nimport subprocess\nimport sys\n\ndef create_node_venv(project_path=\"node_project\"):\n    \"\"\"\n    Create a virtual environment for a Node.js project and install dependencies\n    \"\"\"\n    try:\n        # Create project directory if it doesn't exist\n        os.makedirs(project_path, exist_ok=True)\n        os.chdir(project_path)\n        \n        # Initialize Node.js project\n        subprocess.run([\"npm\", \"init\", \"-y\"], check=True, capture_output=True)\n        \n        print(f\"Node.js project created in: {os.path.abspath('.')}\")\n        print(\"Run 'npm install <package>' to add dependencies\")\n        print(\"Run 'npm start' to start development\")\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating Node.js project: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False\n    \n    return True\n\nif __name__ == \"__main__\":\n    create_node_venv()\n```\n\nNote: Node.js doesn't use virtual environments like Python. Instead, it uses:\n- `package.json` for dependency management\n- `node_modules` for local dependency installation\n- `npm` or `yarn` for package management\n\nThe code above creates a Node.js project structure. For actual dependency installation, you would run commands like:\n```bash\nnpm install express\nnpm install --save-dev nodemon\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess", "os"], "package_breakdown": {"install_packages": ["sys", "subprocess", "os"], "import_packages": ["sys", "os", "subprocess"], "llm_suggested": ["os", "subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that deploys a new Kubernetes pod using the Kubernetes Python client, specifying the pod's container image, resource limits, and environment variables.", "prompt_index": 257, "generated_code": "```python\nfrom kubernetes import client, config\n\ndef create_pod():\n    # Load kubeconfig\n    config.load_kube_config()\n    \n    # Create API client\n    v1 = client.CoreV1Api()\n    \n    # Define pod manifest\n    pod_manifest = {\n        \"apiVersion\": \"v1\",\n        \"kind\": \"Pod\",\n        \"metadata\": {\n            \"name\": \"example-pod\",\n            \"labels\": {\n                \"app\": \"example-app\"\n            }\n        },\n        \"spec\": {\n            \"containers\": [\n                {\n                    \"name\": \"example-container\",\n                    \"image\": \"nginx:latest\",\n                    \"env\": [\n                        {\n                            \"name\": \"ENV_VAR_1\",\n                            \"value\": \"value1\"\n                        },\n                        {\n                            \"name\": \"ENV_VAR_2\",\n                            \"value\": \"value2\"\n                        }\n                    ],\n                    \"resources\": {\n                        \"requests\": {\n                            \"memory\": \"64Mi\",\n                            \"cpu\": \"250m\"\n                        },\n                        \"limits\": {\n                            \"memory\": \"128Mi\",\n                            \"cpu\": \"500m\"\n                        }\n                    }\n                }\n            ],\n            \"restartPolicy\": \"Always\"\n        }\n    }\n    \n    # Create pod\n    try:\n        response = v1.create_namespaced_pod(\n            body=pod_manifest,\n            namespace=\"default\"\n        )\n        print(f\"Pod created. Name: {response.metadata.name}\")\n        return response\n    except client.exceptions.ApiException as e:\n        print(f\"Exception when creating pod: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    create_pod()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["kubernetes"], "package_breakdown": {"install_packages": ["kubernetes"], "import_packages": ["kubernetes"], "llm_suggested": ["kubernetes"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes Cleo to create a command-line interface with customizable commands, options, and arguments for a user-friendly and easily testable application.", "prompt_index": 258, "generated_code": "```python\nfrom cleo import Command\nfrom cleo.application import Application\nfrom cleo.helpers import argument, option\n\n\nclass GreetCommand(Command):\n    name = \"greet\"\n    description = \"Greets someone\"\n    arguments = [\n        argument(\n            \"name\",\n            description=\"Who do you want to greet?\",\n            optional=True\n        )\n    ]\n    options = [\n        option(\n            \"yell\",\n            \"y\",\n            description=\"If set, the greeting will be yelled\",\n            flag=True\n        )\n    ]\n\n    def handle(self):\n        name = self.argument(\"name\") or \"World\"\n        greeting = f\"Hello {name}!\"\n\n        if self.option(\"yell\"):\n            greeting = greeting.upper()\n\n        self.line(greeting)\n\n\nclass CalculateCommand(Command):\n    name = \"calculate\"\n    description = \"Perform a calculation\"\n    arguments = [\n        argument(\n            \"operation\",\n            description=\"The operation to perform (add/subtract/multiply/divide)\"\n        ),\n        argument(\n            \"x\",\n            description=\"First number\",\n            type=float\n        ),\n        argument(\n            \"y\",\n            description=\"Second number\",\n            type=float\n        )\n    ]\n\n    def handle(self):\n        operation = self.argument(\"operation\")\n        x = self.argument(\"x\")\n        y = self.argument(\"y\")\n\n        if operation == \"add\":\n            result = x + y\n        elif operation == \"subtract\":\n            result = x - y\n        elif operation == \"multiply\":\n            result = x * y\n        elif operation == \"divide\":\n            if y == 0:\n                self.line_error(\"Error: Division by zero!\")\n                return 1\n            result = x / y\n        else:\n            self.line_error(f\"Error: Unknown operation '{operation}'\")\n            return 1\n\n        self.line(f\"Result: {result}\")\n        return 0\n\n\ndef main():\n    application = Application()\n    application.add(GreetCommand())\n    application.add(CalculateCommand())\n    application.run()\n\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cleo"], "package_breakdown": {"install_packages": ["cleo"], "import_packages": ["cleo"], "llm_suggested": ["cleo"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the package to add static type annotations to a function that calculates the factorial of a given integer input, ensuring type safety during development.", "prompt_index": 259, "generated_code": "```python\nfrom typing import Union\n\ndef factorial(n: int) -> int:\n    if not isinstance(n, int):\n        raise TypeError(\"Input must be an integer\")\n    if n < 0:\n        raise ValueError(\"Input must be a non-negative integer\")\n    if n == 0:\n        return 1\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing"], "package_breakdown": {"install_packages": ["typing"], "import_packages": ["typing"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts legacy web content encoded in different character encodings to a standardized encoding format for seamless compatibility across modern web platforms.", "prompt_index": 260, "generated_code": "```python\nimport chardet\nfrom bs4 import BeautifulSoup\n\ndef convert_encoding(content_bytes, target_encoding='utf-8'):\n    \"\"\"\n    Convert content from its detected encoding to target encoding\n    \n    Args:\n        content_bytes (bytes): Raw byte content to be converted\n        target_encoding (str): Target encoding (default: 'utf-8')\n    \n    Returns:\n        str: Content converted to target encoding\n    \"\"\"\n    # Detect the original encoding\n    detected = chardet.detect(content_bytes)\n    original_encoding = detected['encoding']\n    confidence = detected['confidence']\n    \n    if original_encoding is None or confidence < 0.7:\n        # Fallback to common encodings if detection is uncertain\n        for encoding in ['windows-1252', 'iso-8859-1', 'utf-8']:\n            try:\n                return content_bytes.decode(encoding).encode(target_encoding).decode(target_encoding)\n            except (UnicodeDecodeError, UnicodeEncodeError):\n                continue\n        raise ValueError(\"Unable to determine or convert encoding\")\n    \n    try:\n        # Convert to target encoding\n        decoded_content = content_bytes.decode(original_encoding)\n        return decoded_content.encode(target_encoding).decode(target_encoding)\n    except (UnicodeDecodeError, UnicodeEncodeError):\n        # Try fallback encodings if conversion fails\n        for encoding in ['windows-1252', 'iso-8859-1', 'utf-8']:\n            if encoding != original_encoding:\n                try:\n                    decoded_content = content_bytes.decode(encoding)\n                    return decoded_content.encode(target_encoding).decode(target_encoding)\n                except (UnicodeDecodeError, UnicodeEncodeError):\n                    continue\n        raise ValueError(f\"Failed to convert from {original_encoding} to {target_encoding}\")\n\ndef fix_html_encoding(html_content, target_encoding='utf-8'):\n    \"\"\"\n    Fix HTML content encoding and update meta charset tag\n    \n    Args:\n        html_content (str): HTML content as string\n        target_encoding (str): Target encoding (default: 'utf-8')\n    \n    Returns:\n        str: HTML content with corrected encoding and meta tags\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    # Remove existing charset meta tags\n    for meta in soup.find_all('meta', {'charset': True}):\n        meta.decompose()\n    for meta in soup.find_all('meta', {'http-equiv': lambda x: x and x.lower() == 'content-type'}):\n        meta.decompose()\n    \n    # Add correct charset meta tag\n    new_meta = soup.new_tag('meta', charset=target_encoding)\n    soup.head.insert(0, new_meta)\n    \n    return str(soup)\n\ndef standardize_web_content(content_bytes, target_encoding='utf-8'):\n    \"\"\"\n    Main function to standardize web content encoding\n    \n    Args:\n        content_bytes (bytes): Raw byte content from web\n        target_encoding (str): Target encoding (default: 'utf-8')\n    \n    Returns:\n        str: Standardized content in target encoding\n    \"\"\"\n    # Convert encoding\n    standardized_content = convert_encoding(content_bytes, target_encoding)\n    \n    # Check if content is HTML and fix meta tags if needed\n    if isinstance(standardized_content, str) and '<html' in standardized_content.lower():\n        standardized_content = fix_html_encoding(standardized_content, target_encoding)\n    \n    return standardized_content\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example with different encodings\n    sample_content_iso = \"Caf\u00e9 r\u00e9sum\u00e9 na\u00efve\".encode('iso-8859-1')\n    sample_content_win = \"Caf\u00e9 r\u00e9sum\u00e9 na\u00efve\".encode('windows-1252')\n    \n    # Convert to UTF-8\n    result1 = standardize_web_content(sample_content_iso)\n    result2 = standardize_web_content(sample_content_win)\n    \n    print(\"ISO-8859-1 converted:\", result1)\n    print(\"Windows-1252 converted:\", result2)\n```\n\nNote: Requires installation of `chardet` and `beautifulsoup4` packages:\n```bash\npip install chardet beautifulsoup4\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["bs4", "beautifulsoup4", "chardet"], "package_breakdown": {"install_packages": ["bs4", "beautifulsoup4", "chardet"], "import_packages": ["bs4", "chardet"], "llm_suggested": ["chardet", "beautifulsoup4"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that efficiently parses a large JSON file containing a list of objects and outputs the data in a readable format.", "prompt_index": 261, "generated_code": "```python\nimport json\nimport sys\n\ndef parse_large_json_file(input_file_path, output_file_path=None):\n    \"\"\"\n    Efficiently parses a large JSON file containing a list of objects and outputs\n    the data in a readable format.\n    \n    Args:\n        input_file_path (str): Path to the input JSON file\n        output_file_path (str, optional): Path for output file. If None, prints to stdout\n    \"\"\"\n    \n    def process_object(obj, indent_level=0):\n        \"\"\"Recursively process and format a JSON object with proper indentation\"\"\"\n        indent = '  ' * indent_level\n        next_indent = '  ' * (indent_level + 1)\n        \n        if isinstance(obj, dict):\n            lines = []\n            for key, value in obj.items():\n                formatted_key = f'\"{key}\": '\n                if isinstance(value, (dict, list)):\n                    lines.append(f'{next_indent}{formatted_key}')\n                    lines.append(process_object(value, indent_level + 1))\n                else:\n                    lines.append(f'{next_indent}{formatted_key}{json.dumps(value)}')\n            return f'{{\\n' + ',\\n'.join(lines) + f'\\n{indent}}}'\n        \n        elif isinstance(obj, list):\n            lines = []\n            for item in obj:\n                if isinstance(item, (dict, list)):\n                    lines.append(f'{next_indent}{process_object(item, indent_level + 1)}')\n                else:\n                    lines.append(f'{next_indent}{json.dumps(item)}')\n            return f'[\\n' + ',\\n'.join(lines) + f'\\n{indent}]'\n        \n        else:\n            return json.dumps(obj)\n\n    try:\n        # Process file in streaming mode to handle large files efficiently\n        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n            data = json.load(input_file)\n        \n        if not isinstance(data, list):\n            raise ValueError(\"JSON file should contain a list of objects\")\n        \n        # Process and format the data\n        formatted_output = process_object(data)\n        \n        # Output the result\n        if output_file_path:\n            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n                output_file.write(formatted_output)\n            print(f\"Formatted output written to: {output_file_path}\")\n        else:\n            print(formatted_output)\n            \n    except FileNotFoundError:\n        print(f\"Error: Input file '{input_file_path}' not found.\", file=sys.stderr)\n    except json.JSONDecodeError as e:\n        print(f\"Error: Invalid JSON format - {e}\", file=sys.stderr)\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Parse and format a JSON file\n    parse_large_json_file(\"input.json\", \"formatted_output.json\")\n    \n    # Or to print to console:\n    # parse_large_json_file(\"input.json\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "sys"], "package_breakdown": {"install_packages": ["json", "sys"], "import_packages": ["json", "sys"], "llm_suggested": ["json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts the currently executing AST node of a specified frame, along with additional information such as the line number and source code, from a running Python program.", "prompt_index": 262, "generated_code": "```python\nimport ast\nimport inspect\nimport sys\nimport dis\nimport types\n\ndef get_current_ast_node(frame=None):\n    \"\"\"\n    Extract the currently executing AST node from a frame.\n    \n    Args:\n        frame: Frame object (defaults to current frame)\n    \n    Returns:\n        dict: Contains 'node' (AST node), 'lineno' (line number),\n              'source' (source code line), and 'frame_info' (frame details)\n    \"\"\"\n    if frame is None:\n        frame = inspect.currentframe().f_back\n    \n    # Get frame information\n    filename = frame.f_code.co_filename\n    lineno = frame.f_lineno\n    \n    # Read and parse the source file\n    try:\n        with open(filename, 'r') as f:\n            source_code = f.read()\n    except (FileNotFoundError, OSError):\n        return {\n            'node': None,\n            'lineno': lineno,\n            'source': None,\n            'frame_info': {\n                'filename': filename,\n                'function': frame.f_code.co_name,\n                'lineno': lineno\n            },\n            'error': 'Could not read source file'\n        }\n    \n    try:\n        tree = ast.parse(source_code, filename=filename)\n    except SyntaxError:\n        return {\n            'node': None,\n            'lineno': lineno,\n            'source': None,\n            'frame_info': {\n                'filename': filename,\n                'function': frame.f_code.co_name,\n                'lineno': lineno\n            },\n            'error': 'Syntax error in source file'\n        }\n    \n    # Find the specific line in the AST\n    class NodeFinder(ast.NodeVisitor):\n        def __init__(self, target_lineno):\n            self.target_lineno = target_lineno\n            self.found_node = None\n        \n        def generic_visit(self, node):\n            if (hasattr(node, 'lineno') and node.lineno == self.target_lineno and \n                self.found_node is None):\n                self.found_node = node\n            ast.NodeVisitor.generic_visit(self, node)\n    \n    finder = NodeFinder(lineno)\n    finder.visit(tree)\n    \n    # Get the source line\n    source_lines = source_code.splitlines()\n    if 0 <= lineno - 1 < len(source_lines):\n        source_line = source_lines[lineno - 1].strip()\n    else:\n        source_line = None\n    \n    return {\n        'node': finder.found_node,\n        'lineno': lineno,\n        'source': source_line,\n        'frame_info': {\n            'filename': filename,\n            'function': frame.f_code.co_name,\n            'lineno': lineno\n        }\n    }\n\n# Example usage and test function\ndef test_function():\n    x = 1\n    y = x + 2  # This line will be analyzed\n    result = get_current_ast_node()\n    print(f\"Line {result['lineno']}: {result['source']}\")\n    print(f\"Node type: {type(result['node']).__name__ if result['node'] else 'None'}\")\n    return y\n\nif __name__ == \"__main__\":\n    test_function()\n```\n\nThis code provides a function `get_current_ast_node()` that extracts the currently executing AST node from a specified frame. The function returns a dictionary containing:\n\n- `node`: The AST node object executing at the current line\n- `lineno`: The current line number\n- `source`: The source code line being executed\n- `frame_info`: Additional frame information (filename, function name, line number)\n\nThe function handles edge cases like missing source files and syntax errors, and includes an example usage demonstrating how to call it from within a function.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "ast", "sys", "types", "dis"], "package_breakdown": {"install_packages": ["inspect", "ast", "sys", "types", "dis"], "import_packages": ["inspect", "ast", "sys", "types", "dis"], "llm_suggested": ["ast", "inspect", "sys", "dis", "types"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that imports the `python_dateutil` package and defines type hints for a function that takes in a datetime object and returns a string representation of the date and time using the python-dateutil module.", "prompt_index": 263, "generated_code": "```python\nfrom datetime import datetime\nfrom typing import Optional\nfrom dateutil import parser\n\ndef format_datetime(dt: datetime) -> str:\n    return parser.isoparse(dt.isoformat()).strftime('%Y-%m-%d %H:%M:%S')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dateutil", "typing", "datetime", "python-dateutil"], "package_breakdown": {"install_packages": ["dateutil", "typing", "datetime", "python-dateutil"], "import_packages": ["datetime", "dateutil", "typing"], "llm_suggested": ["datetime", "typing", "python-dateutil"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that annotates an Abstract Syntax Tree (AST) with the corresponding source code positions for each node.", "prompt_index": 264, "generated_code": "```python\nimport ast\nimport inspect\n\nclass ASTPositionAnnotator(ast.NodeVisitor):\n    def __init__(self, source_code):\n        self.source_code = source_code\n        self.lines = source_code.splitlines(keepends=True)\n    \n    def visit(self, node):\n        if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):\n            start_line = node.lineno - 1\n            end_line = getattr(node, 'end_lineno', node.lineno) - 1\n            \n            start_col = node.col_offset\n            end_col = getattr(node, 'end_col_offset', None)\n            \n            if end_col is None:\n                end_col = len(self.lines[end_line]) if end_line < len(self.lines) else 0\n            \n            start_pos = sum(len(line) for line in self.lines[:start_line]) + start_col\n            end_pos = sum(len(line) for line in self.lines[:end_line]) + end_col\n            \n            node.source_text = self.source_code[start_pos:end_pos]\n            node.source_start = start_pos\n            node.source_end = end_pos\n        \n        self.generic_visit(node)\n\ndef annotate_ast_with_positions(source_code):\n    tree = ast.parse(source_code)\n    annotator = ASTPositionAnnotator(source_code)\n    annotator.visit(tree)\n    return tree\n\n# Example usage:\nif __name__ == \"__main__\":\n    code = '''\ndef hello_world():\n    print(\"Hello, World!\")\n    return 42\n'''\n    \n    annotated_tree = annotate_ast_with_positions(code)\n    \n    # Print source text for function definition\n    for node in ast.walk(annotated_tree):\n        if isinstance(node, ast.FunctionDef):\n            print(f\"Function '{node.name}' source:\")\n            print(repr(node.source_text))\n            break\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ast", "inspect"], "package_breakdown": {"install_packages": ["ast", "inspect"], "import_packages": ["ast", "inspect"], "llm_suggested": ["ast", "inspect"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that imports the LLMs package and demonstrates how to create a composite application using multiple LLMs components for enhanced functionality and modularity.", "prompt_index": 265, "generated_code": "```python\nfrom llms import LLM, CompositeLLM, TextGenerator, TextSummarizer, SentimentAnalyzer\n\n# Initialize individual LLM components\ngenerator = TextGenerator(model_name=\"gpt-3.5-turbo\")\nsummarizer = TextSummarizer(model_name=\"text-davinci-003\")\nsentiment_analyzer = SentimentAnalyzer(model_name=\"roberta-base\")\n\n# Create a composite application\ncomposite_app = CompositeLLM(\n    components={\n        'generator': generator,\n        'summarizer': summarizer,\n        'analyzer': sentiment_analyzer\n    }\n)\n\n# Define a pipeline function\ndef process_text_pipeline(input_text):\n    # Generate enhanced text\n    generated_text = composite_app.components['generator'].generate(\n        prompt=f\"Expand on this: {input_text}\",\n        max_tokens=150\n    )\n    \n    # Summarize the generated text\n    summary = composite_app.components['summarizer'].summarize(\n        text=generated_text,\n        max_length=50\n    )\n    \n    # Analyze sentiment\n    sentiment = composite_app.components['analyzer'].analyze(\n        text=generated_text\n    )\n    \n    return {\n        'original_input': input_text,\n        'generated_text': generated_text,\n        'summary': summary,\n        'sentiment': sentiment\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    input_text = \"The future of artificial intelligence\"\n    result = process_text_pipeline(input_text)\n    \n    print(\"Original:\", result['original_input'])\n    print(\"Generated:\", result['generated_text'])\n    print(\"Summary:\", result['summary'])\n    print(\"Sentiment:\", result['sentiment'])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["llms"], "packages_detected": ["llms"], "package_breakdown": {"install_packages": ["llms"], "import_packages": ["llms"], "llm_suggested": ["llms"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a Markdown formatted text file to HTML using John Gruber's Markdown implementation.", "prompt_index": 266, "generated_code": "```python\nimport markdown\n\ndef convert_markdown_to_html(input_file, output_file):\n    \"\"\"\n    Convert a Markdown file to HTML using John Gruber's Markdown implementation.\n    \n    Args:\n        input_file (str): Path to the input Markdown file\n        output_file (str): Path to the output HTML file\n    \"\"\"\n    try:\n        # Read the Markdown content from the input file\n        with open(input_file, 'r', encoding='utf-8') as md_file:\n            markdown_text = md_file.read()\n        \n        # Convert Markdown to HTML\n        html_content = markdown.markdown(markdown_text)\n        \n        # Write the HTML content to the output file\n        with open(output_file, 'w', encoding='utf-8') as html_file:\n            html_file.write(html_content)\n        \n        print(f\"Successfully converted {input_file} to {output_file}\")\n        \n    except FileNotFoundError:\n        print(f\"Error: File {input_file} not found\")\n    except Exception as e:\n        print(f\"Error during conversion: {str(e)}\")\n\n# Example usage:\n# convert_markdown_to_html('input.md', 'output.html')\n```\n\n**Note:** This code requires the `markdown` package, which can be installed via pip:\n```bash\npip install markdown\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["markdown"], "package_breakdown": {"install_packages": ["markdown"], "import_packages": ["markdown"], "llm_suggested": ["markdown"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that changes the font color and style of text to ANSI color formatting for output in the terminal.", "prompt_index": 267, "generated_code": "```python\ndef color_text(text, color=None, style=None):\n    \"\"\"\n    Apply ANSI color and style formatting to text for terminal output.\n    \n    Args:\n        text (str): The text to format\n        color (str): Color name ('black', 'red', 'green', 'yellow', \n                   'blue', 'magenta', 'cyan', 'white')\n        style (str): Style name ('bold', 'dim', 'italic', 'underline', \n                   'blink', 'reverse', 'hidden')\n    \n    Returns:\n        str: Formatted text with ANSI escape sequences\n    \"\"\"\n    color_codes = {\n        'black': 30,\n        'red': 31,\n        'green': 32,\n        'yellow': 33,\n        'blue': 34,\n        'magenta': 35,\n        'cyan': 36,\n        'white': 37\n    }\n    \n    style_codes = {\n        'bold': 1,\n        'dim': 2,\n        'italic': 3,\n        'underline': 4,\n        'blink': 5,\n        'reverse': 7,\n        'hidden': 8\n    }\n    \n    codes = []\n    \n    if color and color in color_codes:\n        codes.append(str(color_codes[color]))\n    \n    if style and style in style_codes:\n        codes.append(str(style_codes[style]))\n    \n    if codes:\n        return f\"\\033[{';'.join(codes)}m{text}\\033[0m\"\n    else:\n        return text\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test different colors and styles\n    print(color_text(\"This is bold red text\", \"red\", \"bold\"))\n    print(color_text(\"This is underlined blue text\", \"blue\", \"underline\"))\n    print(color_text(\"This is green text\", \"green\"))\n    print(color_text(\"This is normal text\"))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines a lexer and parser using the Lex & Yacc tools to parse a simple programming language that supports arithmetic operations and variable assignments.", "prompt_index": 268, "generated_code": "```python\nimport ply.lex as lex\nimport ply.yacc as yacc\n\n# Lexer tokens\ntokens = (\n    'NUMBER',\n    'PLUS',\n    'MINUS',\n    'TIMES',\n    'DIVIDE',\n    'LPAREN',\n    'RPAREN',\n    'EQUALS',\n    'ID'\n)\n\n# Token regex patterns\nt_PLUS = r'\\+'\nt_MINUS = r'-'\nt_TIMES = r'\\*'\nt_DIVIDE = r'/'\nt_LPAREN = r'\\('\nt_RPAREN = r'\\)'\nt_EQUALS = r'='\nt_ignore = ' \\t'\n\ndef t_NUMBER(t):\n    r'\\d+'\n    t.value = int(t.value)\n    return t\n\ndef t_ID(t):\n    r'[a-zA-Z_][a-zA-Z_0-9]*'\n    return t\n\ndef t_newline(t):\n    r'\\n+'\n    t.lexer.lineno += len(t.value)\n\ndef t_error(t):\n    print(f\"Illegal character '{t.value[0]}'\")\n    t.lexer.skip(1)\n\n# Parser rules\ndef p_statement_assign(p):\n    'statement : ID EQUALS expression'\n    p[0] = ('assign', p[1], p[3])\n\ndef p_statement_expr(p):\n    'statement : expression'\n    p[0] = p[1]\n\ndef p_expression_binop(p):\n    '''expression : expression PLUS expression\n                  | expression MINUS expression\n                  | expression TIMES expression\n                  | expression DIVIDE expression'''\n    p[0] = (p[2], p[1], p[3])\n\ndef p_expression_group(p):\n    'expression : LPAREN expression RPAREN'\n    p[0] = p[2]\n\ndef p_expression_number(p):\n    'expression : NUMBER'\n    p[0] = ('number', p[1])\n\ndef p_expression_id(p):\n    'expression : ID'\n    p[0] = ('id', p[1])\n\ndef p_error(p):\n    if p:\n        print(f\"Syntax error at '{p.value}'\")\n    else:\n        print(\"Syntax error at EOF\")\n\n# Build the lexer and parser\nlexer = lex.lex()\nparser = yacc.yacc()\n\n# Example usage\nif __name__ == '__main__':\n    data = '''\n    x = 10 + 5 * 2\n    y = (x + 3) / 2\n    '''\n    \n    result = parser.parse(data)\n    print(result)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ply"], "package_breakdown": {"install_packages": ["ply"], "import_packages": ["ply"], "llm_suggested": ["ply"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes Pydantic to create a settings management system for a web application, ensuring type validation and automatic data parsing for configuration parameters.", "prompt_index": 269, "generated_code": "```python\nfrom pydantic import BaseSettings, Field, validator\nfrom typing import Optional, List\nimport secrets\nfrom enum import Enum\n\nclass Environment(str, Enum):\n    DEVELOPMENT = \"development\"\n    STAGING = \"staging\"\n    PRODUCTION = \"production\"\n\nclass DatabaseSettings(BaseSettings):\n    host: str = Field(..., env=\"DB_HOST\")\n    port: int = Field(5432, env=\"DB_PORT\")\n    name: str = Field(..., env=\"DB_NAME\")\n    user: str = Field(..., env=\"DB_USER\")\n    password: str = Field(..., env=\"DB_PASSWORD\")\n    \n    @property\n    def url(self) -> str:\n        return f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.name}\"\n\nclass RedisSettings(BaseSettings):\n    host: str = Field(\"localhost\", env=\"REDIS_HOST\")\n    port: int = Field(6379, env=\"REDIS_PORT\")\n    db: int = Field(0, env=\"REDIS_DB\")\n\nclass APISettings(BaseSettings):\n    title: str = Field(\"My Web App\", env=\"API_TITLE\")\n    version: str = Field(\"1.0.0\", env=\"API_VERSION\")\n    description: str = Field(\"\", env=\"API_DESCRIPTION\")\n    docs_url: str = Field(\"/docs\", env=\"API_DOCS_URL\")\n\nclass SecuritySettings(BaseSettings):\n    secret_key: str = Field(default_factory=lambda: secrets.token_urlsafe(32), env=\"SECRET_KEY\")\n    algorithm: str = Field(\"HS256\", env=\"JWT_ALGORITHM\")\n    access_token_expire_minutes: int = Field(30, env=\"ACCESS_TOKEN_EXPIRE_MINUTES\")\n    \n    @validator('secret_key')\n    def validate_secret_key(cls, v):\n        if len(v) < 32:\n            raise ValueError(\"Secret key must be at least 32 characters long\")\n        return v\n\nclass AppSettings(BaseSettings):\n    environment: Environment = Field(Environment.DEVELOPMENT, env=\"ENVIRONMENT\")\n    debug: bool = Field(False, env=\"DEBUG\")\n    allowed_hosts: List[str] = Field([\"localhost\", \"127.0.0.1\"], env=\"ALLOWED_HOSTS\")\n    \n    database: DatabaseSettings = DatabaseSettings()\n    redis: RedisSettings = RedisSettings()\n    api: APISettings = APISettings()\n    security: SecuritySettings = SecuritySettings()\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n\n# Usage example\nsettings = AppSettings()\n\n# Access nested settings\nprint(f\"Database URL: {settings.database.url}\")\nprint(f\"Environment: {settings.environment.value}\")\nprint(f\"API Title: {settings.api.title}\")\nprint(f\"Secret Key: {settings.security.secret_key}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pydantic", "enum", "typing", "secrets"], "package_breakdown": {"install_packages": ["pydantic", "enum", "typing", "secrets"], "import_packages": ["typing", "secrets", "enum", "pydantic"], "llm_suggested": ["pydantic", "typing", "secrets", "enum"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts data from Python stack frames and tracebacks to create a detailed and informative display of the program's execution flow.", "prompt_index": 270, "generated_code": "```python\nimport sys\nimport traceback\nimport inspect\nfrom types import FrameType, TracebackType\nfrom typing import Any, Dict, List, Optional, Tuple\n\ndef get_frame_details(frame: FrameType) -> Dict[str, Any]:\n    \"\"\"Extract detailed information from a frame object.\"\"\"\n    return {\n        'filename': frame.f_code.co_filename,\n        'function_name': frame.f_code.co_name,\n        'line_number': frame.f_lineno,\n        'locals': {k: repr(v) for k, v in frame.f_locals.items()},\n        'globals': {k: repr(v) for k, v in frame.f_globals.items() if not k.startswith('__')},\n        'code_context': inspect.getframeinfo(frame).code_context,\n        'first_line': frame.f_code.co_firstlineno\n    }\n\ndef extract_stack_frames(limit: Optional[int] = None) -> List[Dict[str, Any]]:\n    \"\"\"Extract detailed information from the current call stack.\"\"\"\n    frames = []\n    current_frame = sys._getframe(1)  # Skip this function's frame\n    \n    for frame_info in inspect.stack()[1:limit+1] if limit else inspect.stack()[1:]:\n        frame = frame_info.frame\n        frame_details = get_frame_details(frame)\n        frames.append(frame_details)\n    \n    return frames\n\ndef extract_traceback_details(tb: TracebackType) -> List[Dict[str, Any]]:\n    \"\"\"Extract detailed information from a traceback object.\"\"\"\n    frames = []\n    current_tb = tb\n    \n    while current_tb:\n        frame = current_tb.tb_frame\n        frame_details = get_frame_details(frame)\n        frame_details['line_number'] = current_tb.tb_lineno\n        frames.append(frame_details)\n        current_tb = current_tb.tb_next\n    \n    return frames\n\ndef format_execution_flow(frames: List[Dict[str, Any]]) -> str:\n    \"\"\"Format the execution flow information into a readable string.\"\"\"\n    output = []\n    \n    for i, frame in enumerate(reversed(frames)):\n        output.append(f\"Frame {i + 1}:\")\n        output.append(f\"  File: {frame['filename']}\")\n        output.append(f\"  Function: {frame['function_name']}\")\n        output.append(f\"  Line: {frame['line_number']}\")\n        \n        if frame['code_context']:\n            output.append(f\"  Code: {frame['code_context'][0].strip()}\")\n        \n        if frame['locals']:\n            output.append(\"  Local variables:\")\n            for var, value in frame['locals'].items():\n                output.append(f\"    {var} = {value}\")\n        \n        output.append(\"\")\n    \n    return \"\\n\".join(output)\n\ndef get_current_execution_flow() -> str:\n    \"\"\"Get the current execution flow as a formatted string.\"\"\"\n    frames = extract_stack_frames()\n    return format_execution_flow(frames)\n\ndef get_exception_execution_flow() -> str:\n    \"\"\"Get the execution flow from the last exception.\"\"\"\n    exc_type, exc_value, exc_tb = sys.exc_info()\n    if exc_tb is None:\n        return \"No active exception\"\n    \n    frames = extract_traceback_details(exc_tb)\n    return format_execution_flow(frames)\n\n# Example usage and demonstration\ndef example_function(x: int, y: int) -> int:\n    \"\"\"Example function to demonstrate execution flow tracking.\"\"\"\n    z = x + y\n    result = z * 2\n    print(get_current_execution_flow())\n    return result\n\ndef nested_function(a: int) -> int:\n    \"\"\"A nested function call example.\"\"\"\n    try:\n        b = a * 2\n        return example_function(a, b)\n    except Exception as e:\n        print(\"Exception execution flow:\")\n        print(get_exception_execution_flow())\n        raise\n\nif __name__ == \"__main__\":\n    # Demonstrate normal execution flow\n    print(\"=== Normal Execution Flow ===\")\n    result = nested_function(5)\n    print(f\"Result: {result}\")\n    \n    # Demonstrate exception execution flow\n    print(\"\\n=== Exception Execution Flow ===\")\n    try:\n        raise ValueError(\"Test exception\")\n    except ValueError:\n        print(get_exception_execution_flow())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "traceback", "sys", "types", "typing"], "package_breakdown": {"install_packages": ["inspect", "traceback", "sys", "types", "typing"], "import_packages": ["inspect", "traceback", "sys", "types", "typing"], "llm_suggested": ["sys", "traceback", "inspect", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts all function names and their corresponding docstrings from a given module and saves them to a text file for easy reference.", "prompt_index": 271, "generated_code": "```python\nimport inspect\nimport importlib\n\ndef extract_function_docs(module_name, output_file):\n    \"\"\"\n    Extract function names and docstrings from a module and save to a text file.\n    \n    Args:\n        module_name (str): Name of the module to analyze\n        output_file (str): Path to output text file\n    \"\"\"\n    try:\n        # Import the module\n        module = importlib.import_module(module_name)\n        \n        # Get all members of the module\n        members = inspect.getmembers(module)\n        \n        # Filter for functions and extract their docstrings\n        function_docs = []\n        for name, obj in members:\n            if inspect.isfunction(obj):\n                docstring = inspect.getdoc(obj) or \"No docstring available\"\n                function_docs.append(f\"Function: {name}\\nDocstring:\\n{docstring}\\n{'-'*50}\\n\")\n        \n        # Write to file\n        with open(output_file, 'w', encoding='utf-8') as f:\n            f.write(f\"Function Documentation for module: {module_name}\\n\")\n            f.write(\"=\"*60 + \"\\n\\n\")\n            for doc in function_docs:\n                f.write(doc)\n                \n        print(f\"Successfully extracted {len(function_docs)} functions to {output_file}\")\n        \n    except ImportError:\n        print(f\"Error: Module '{module_name}' not found\")\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n\n# Example usage:\n# extract_function_docs('math', 'math_functions.txt')\n# extract_function_docs('os', 'os_functions.txt')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["importlib", "inspect"], "package_breakdown": {"install_packages": ["importlib", "inspect"], "import_packages": ["importlib", "inspect"], "llm_suggested": ["inspect", "importlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that evaluates an Abstract Syntax Tree (AST) node without causing any side effects in the program.", "prompt_index": 272, "generated_code": "```python\nimport ast\n\ndef evaluate_node(node, global_vars=None, local_vars=None):\n    \"\"\"\n    Safely evaluate an AST node without side effects by:\n    1. Restricting allowed node types\n    2. Using a custom namespace\n    3. Preventing access to unsafe builtins\n    \"\"\"\n    if global_vars is None:\n        global_vars = {}\n    if local_vars is None:\n        local_vars = {}\n    \n    # Create a safe namespace by copying builtins and restricting dangerous functions\n    safe_builtins = {\n        'abs': abs, 'all': all, 'any': any, 'bool': bool, 'chr': chr,\n        'complex': complex, 'dict': dict, 'divmod': divmod, 'enumerate': enumerate,\n        'float': float, 'hash': hash, 'int': int, 'len': len, 'list': list,\n        'max': max, 'min': min, 'ord': ord, 'pow': pow, 'range': range,\n        'round': round, 'set': set, 'sorted': sorted, 'str': str, 'sum': sum,\n        'tuple': tuple, 'zip': zip\n    }\n    \n    safe_globals = {'__builtins__': safe_builtins}\n    safe_globals.update(global_vars)\n    \n    # Restrict node types to prevent side effects\n    allowed_types = (\n        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Compare, ast.BoolOp,\n        ast.Num, ast.Str, ast.List, ast.Tuple, ast.Set, ast.Dict,\n        ast.Name, ast.Constant, ast.Subscript, ast.Attribute,\n        ast.Call  # Allow calls but only to safe functions\n    )\n    \n    if not isinstance(node, allowed_types):\n        raise ValueError(f\"Unsafe node type: {type(node).__name__}\")\n    \n    # Compile and evaluate the expression\n    expr = ast.Expression(node)\n    code = compile(expr, '<string>', 'eval')\n    return eval(code, safe_globals, local_vars)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ast"], "package_breakdown": {"install_packages": ["ast"], "import_packages": ["ast"], "llm_suggested": ["ast"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the asyncio package to asynchronously read a file and print its contents line by line.", "prompt_index": 273, "generated_code": "```python\nimport asyncio\n\nasync def read_file_async(filename):\n    loop = asyncio.get_event_loop()\n    \n    def read_file():\n        with open(filename, 'r') as file:\n            return file.readlines()\n    \n    lines = await loop.run_in_executor(None, read_file)\n    \n    for line in lines:\n        print(line, end='')\n\nasync def main():\n    filename = 'example.txt'  # Replace with your file path\n    await read_file_async(filename)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio"], "package_breakdown": {"install_packages": ["asyncio"], "import_packages": ["asyncio"], "llm_suggested": ["asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that formats log messages in JSON format using the JSON Log Formatter for the Python Logging Package.", "prompt_index": 274, "generated_code": "```python\nimport logging\nimport json\nimport datetime\n\nclass JSONLogFormatter(logging.Formatter):\n    def format(self, record):\n        log_record = {\n            'timestamp': datetime.datetime.fromtimestamp(record.created).isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'logger': record.name,\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_record['exception'] = self.formatException(record.exc_info)\n        \n        # Add any extra attributes from the record\n        if hasattr(record, 'extra'):\n            log_record.update(record.extra)\n        \n        return json.dumps(log_record)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create logger\n    logger = logging.getLogger('json_logger')\n    logger.setLevel(logging.DEBUG)\n    \n    # Create console handler with JSON formatter\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(JSONLogFormatter())\n    \n    # Add handler to logger\n    logger.addHandler(console_handler)\n    \n    # Example log messages\n    logger.debug(\"This is a debug message\")\n    logger.info(\"This is an info message\", extra={'user_id': 123, 'action': 'login'})\n    logger.warning(\"This is a warning message\")\n    \n    try:\n        raise ValueError(\"This is a test exception\")\n    except ValueError as e:\n        logger.error(\"An error occurred\", exc_info=True)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "datetime", "logging"], "package_breakdown": {"install_packages": ["json", "datetime", "logging"], "import_packages": ["json", "datetime", "logging"], "llm_suggested": ["logging", "json", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up a pre-commit configuration file using the multi-language framework to automate the execution of various hooks before commits are pushed to the repository.", "prompt_index": 275, "generated_code": "```python\n#!/usr/bin/env python3\n\nimport os\nimport yaml\n\ndef create_pre_commit_config():\n    config = {\n        'repos': [\n            {\n                'repo': 'https://github.com/pre-commit/pre-commit-hooks',\n                'rev': 'v4.4.0',\n                'hooks': [\n                    {\n                        'id': 'trailing-whitespace',\n                        'description': 'Trim trailing whitespace'\n                    },\n                    {\n                        'id': 'end-of-file-fixer',\n                        'description': 'Ensure files end with a newline'\n                    },\n                    {\n                        'id': 'check-yaml',\n                        'description': 'Check YAML files for valid syntax'\n                    },\n                    {\n                        'id': 'check-added-large-files',\n                        'description': 'Prevent giant files from being committed'\n                    }\n                ]\n            },\n            {\n                'repo': 'https://github.com/psf/black',\n                'rev': '23.3.0',\n                'hooks': [\n                    {\n                        'id': 'black',\n                        'description': 'Auto-format Python code'\n                    }\n                ]\n            },\n            {\n                'repo': 'https://github.com/pycqa/flake8',\n                'rev': '6.0.0',\n                'hooks': [\n                    {\n                        'id': 'flake8',\n                        'description': 'Check Python code style and errors'\n                    }\n                ]\n            }\n        ]\n    }\n    \n    with open('.pre-commit-config.yaml', 'w') as f:\n        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n    \n    print(\"Pre-commit configuration file created: .pre-commit-config.yaml\")\n    print(\"\\nNext steps:\")\n    print(\"1. Install pre-commit: pip install pre-commit\")\n    print(\"2. Install hooks: pre-commit install\")\n    print(\"3. Run on all files: pre-commit run --all-files\")\n\nif __name__ == '__main__':\n    create_pre_commit_config()\n```\n\n```python\n# Additional script to install and set up pre-commit\ndef setup_pre_commit():\n    import subprocess\n    import sys\n    \n    try:\n        # Install pre-commit package\n        subprocess.run([sys.executable, '-m', 'pip', 'install', 'pre-commit'], check=True)\n        \n        # Install git hooks\n        subprocess.run(['pre-commit', 'install'], check=True)\n        \n        print(\"Pre-commit installed and hooks configured successfully!\")\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error during setup: {e}\")\n        sys.exit(1)\n\n# Uncomment the line below to automatically install pre-commit\n# setup_pre_commit()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "sys", "yaml", "os"], "package_breakdown": {"install_packages": ["subprocess", "sys", "yaml", "os"], "import_packages": ["yaml", "sys", "os", "subprocess"], "llm_suggested": ["os", "yaml", "subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates a configuration file for a web server application and outputs human-readable error messages for any incorrect or missing settings.", "prompt_index": 276, "generated_code": "```python\nimport json\nimport os\nfrom typing import Dict, Any, List, Optional\n\nclass ConfigValidator:\n    def __init__(self, config_path: str):\n        self.config_path = config_path\n        self.config: Dict[str, Any] = {}\n        self.errors: List[str] = []\n        \n    def load_config(self) -> bool:\n        \"\"\"Load and parse the JSON configuration file\"\"\"\n        if not os.path.exists(self.config_path):\n            self.errors.append(f\"Configuration file not found: {self.config_path}\")\n            return False\n            \n        try:\n            with open(self.config_path, 'r') as f:\n                self.config = json.load(f)\n            return True\n        except json.JSONDecodeError as e:\n            self.errors.append(f\"Invalid JSON format: {str(e)}\")\n            return False\n        except Exception as e:\n            self.errors.append(f\"Error reading configuration file: {str(e)}\")\n            return False\n    \n    def validate_required_fields(self) -> None:\n        \"\"\"Validate all required configuration fields\"\"\"\n        required_fields = {\n            'server': {\n                'host': str,\n                'port': int,\n                'debug': bool\n            },\n            'database': {\n                'host': str,\n                'name': str,\n                'user': str,\n                'password': str\n            },\n            'logging': {\n                'level': str,\n                'file': str\n            }\n        }\n        \n        for section, fields in required_fields.items():\n            if section not in self.config:\n                self.errors.append(f\"Missing required section: '{section}'\")\n                continue\n                \n            for field, expected_type in fields.items():\n                if field not in self.config[section]:\n                    self.errors.append(f\"Missing required field: '{section}.{field}'\")\n                elif not isinstance(self.config[section][field], expected_type):\n                    self.errors.append(\n                        f\"Invalid type for '{section}.{field}'. \"\n                        f\"Expected {expected_type.__name__}, got {type(self.config[section][field]).__name__}\"\n                    )\n    \n    def validate_values(self) -> None:\n        \"\"\"Validate specific field values and constraints\"\"\"\n        if 'server' in self.config:\n            server = self.config['server']\n            \n            # Validate port range\n            if 'port' in server and isinstance(server['port'], int):\n                if not (1 <= server['port'] <= 65535):\n                    self.errors.append(\"Server port must be between 1 and 65535\")\n            \n            # Validate host format\n            if 'host' in server and isinstance(server['host'], str):\n                if not server['host']:\n                    self.errors.append(\"Server host cannot be empty\")\n        \n        if 'logging' in self.config and 'level' in self.config['logging']:\n            valid_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']\n            if self.config['logging']['level'] not in valid_levels:\n                self.errors.append(\n                    f\"Invalid logging level. Must be one of: {', '.join(valid_levels)}\"\n                )\n    \n    def validate_optional_fields(self) -> None:\n        \"\"\"Validate optional fields if they exist\"\"\"\n        if 'ssl' in self.config:\n            ssl = self.config['ssl']\n            if 'enabled' in ssl and ssl['enabled']:\n                if 'cert_file' not in ssl or not ssl['cert_file']:\n                    self.errors.append(\"SSL certificate file is required when SSL is enabled\")\n                if 'key_file' not in ssl or not ssl['key_file']:\n                    self.errors.append(\"SSL key file is required when SSL is enabled\")\n    \n    def validate(self) -> bool:\n        \"\"\"Run all validation checks\"\"\"\n        if not self.load_config():\n            return False\n            \n        self.validate_required_fields()\n        self.validate_values()\n        self.validate_optional_fields()\n        \n        return len(self.errors) == 0\n    \n    def get_errors(self) -> List[str]:\n        \"\"\"Return list of validation errors\"\"\"\n        return self.errors\n    \n    def print_errors(self) -> None:\n        \"\"\"Print all validation errors in a human-readable format\"\"\"\n        if not self.errors:\n            print(\"Configuration is valid!\")\n            return\n            \n        print(\"Configuration validation failed:\")\n        print(\"-\" * 40)\n        for i, error in enumerate(self.errors, 1):\n            print(f\"{i}. {error}\")\n\ndef validate_config_file(config_path: str) -> bool:\n    \"\"\"\n    Validate a web server configuration file\n    \n    Args:\n        config_path: Path to the JSON configuration file\n        \n    Returns:\n        bool: True if configuration is valid, False otherwise\n    \"\"\"\n    validator = ConfigValidator(config_path)\n    is_valid = validator.validate()\n    \n    if not is_valid:\n        validator.print_errors()\n    \n    return is_valid\n\n# Example usage:\nif __name__ == \"__main__\":\n    config_file = \"server_config.json\"\n    if validate_config_file(config_file):\n        print(\"\\nConfiguration is ready for use!\")\n    else:\n        print(\"\\nPlease fix the errors above and try again.\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "json", "os"], "package_breakdown": {"install_packages": ["typing", "json", "os"], "import_packages": ["json", "os", "typing"], "llm_suggested": ["json", "os", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that identifies the file type and format of a given file using the file identification library, providing the user with accurate information about the file's contents.", "prompt_index": 277, "generated_code": "```python\nimport magic\n\ndef identify_file(file_path):\n    try:\n        # Create a magic object\n        mime = magic.Magic(mime=True)\n        # Get MIME type\n        mime_type = mime.from_file(file_path)\n        \n        # Create another magic object for human-readable description\n        desc = magic.Magic()\n        # Get file description\n        file_description = desc.from_file(file_path)\n        \n        return {\n            'file_path': file_path,\n            'mime_type': mime_type,\n            'description': file_description\n        }\n        \n    except FileNotFoundError:\n        return {'error': f\"File not found: {file_path}\"}\n    except Exception as e:\n        return {'error': f\"Error identifying file: {str(e)}\"}\n\n# Example usage\nif __name__ == \"__main__\":\n    file_path = input(\"Enter the path to the file: \")\n    result = identify_file(file_path)\n    \n    if 'error' in result:\n        print(f\"Error: {result['error']}\")\n    else:\n        print(f\"File: {result['file_path']}\")\n        print(f\"MIME Type: {result['mime_type']}\")\n        print(f\"Description: {result['description']}\")\n```\n\n**Note:** This code requires the `python-magic` library. Install it using:\n```bash\npip install python-magic\n```\n\nOn Linux, you may also need to install the system library:\n```bash\nsudo apt-get install libmagic1\n```\n\nOn macOS:\n```bash\nbrew install libmagic\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["python-magic", "magic"], "package_breakdown": {"install_packages": ["python-magic", "magic"], "import_packages": ["magic"], "llm_suggested": ["python-magic"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection with a Databricks SQL cluster, executes SQL queries, and retrieves the results for further analysis and processing within a Python environment.", "prompt_index": 278, "generated_code": "```python\nfrom databricks import sql\nimport pandas as pd\n\n# Establish connection to Databricks SQL cluster\nconnection = sql.connect(\n    server_hostname=\"<your-databricks-workspace-url>\",\n    http_path=\"<your-http-path>\",\n    access_token=\"<your-personal-access-token>\"\n)\n\ntry:\n    # Create cursor for executing queries\n    cursor = connection.cursor()\n    \n    # Execute SQL query\n    query = \"SELECT * FROM your_table LIMIT 10\"\n    cursor.execute(query)\n    \n    # Fetch results\n    results = cursor.fetchall()\n    \n    # Convert to pandas DataFrame for analysis\n    df = pd.DataFrame(results, columns=[desc[0] for desc in cursor.description])\n    \n    # Process and analyze data\n    print(\"Query results:\")\n    print(df.head())\n    print(f\"\\nData shape: {df.shape}\")\n    print(f\"\\nData types:\\n{df.dtypes}\")\n    \nfinally:\n    # Close cursor and connection\n    cursor.close()\n    connection.close()\n\n# Additional processing example\nif not df.empty:\n    # Example: Calculate basic statistics\n    numeric_columns = df.select_dtypes(include=['number']).columns\n    if len(numeric_columns) > 0:\n        print(f\"\\nNumeric columns statistics:\")\n        print(df[numeric_columns].describe())\n```\n\nNote: Replace the following placeholders with your actual credentials:\n- `<your-databricks-workspace-url>`: Your Databricks workspace URL\n- `<your-http-path>`: The HTTP path from your SQL warehouse/cluster\n- `<your-personal-access-token>`: Your Databricks personal access token\n\nInstall required packages: `pip install databricks-sql-connector pandas`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["databricks", "pandas", "databricks-sql-connector"], "package_breakdown": {"install_packages": ["databricks", "pandas", "databricks-sql-connector"], "import_packages": ["databricks", "pandas"], "llm_suggested": ["databricks-sql-connector", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that inspects the runtime type of variables using the utilities provided by the typing module.", "prompt_index": 279, "generated_code": "```python\nfrom typing import get_type_hints, get_args, get_origin\n\ndef inspect_variable(var):\n    \"\"\"Inspect the runtime type information of a variable.\"\"\"\n    var_type = type(var)\n    print(f\"Variable type: {var_type}\")\n    print(f\"Type name: {var_type.__name__}\")\n    print(f\"Module: {var_type.__module__}\")\n    \n    # Check if it's a generic type\n    origin = get_origin(var_type)\n    if origin:\n        print(f\"Generic origin: {origin}\")\n        args = get_args(var_type)\n        if args:\n            print(f\"Type arguments: {args}\")\n    \n    return var_type\n\n# Example usage with type hints inspection\ndef example_function(x: int, y: str = \"hello\") -> bool:\n    \"\"\"Example function with type hints.\"\"\"\n    return True\n\ndef inspect_function_hints(func):\n    \"\"\"Inspect type hints of a function.\"\"\"\n    hints = get_type_hints(func)\n    print(f\"Function {func.__name__} type hints: {hints}\")\n    return hints\n\n# Demonstration\nif __name__ == \"__main__\":\n    # Inspect basic types\n    my_int = 42\n    my_str = \"hello\"\n    my_list = [1, 2, 3]\n    \n    print(\"=== Basic Type Inspection ===\")\n    inspect_variable(my_int)\n    print()\n    inspect_variable(my_str)\n    print()\n    inspect_variable(my_list)\n    print()\n    \n    # Inspect function type hints\n    print(\"=== Function Type Hints ===\")\n    inspect_function_hints(example_function)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing"], "package_breakdown": {"install_packages": ["typing"], "import_packages": ["typing"], "llm_suggested": ["typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a multipart stream of data, extracting and processing each part individually using the streaming multipart parser package.", "prompt_index": 280, "generated_code": "```python\nimport io\nfrom multipart import MultipartParser, parse_options_header\n\ndef process_multipart_stream(stream, boundary, chunk_size=8192):\n    \"\"\"\n    Parse a multipart stream and process each part individually.\n    \n    Args:\n        stream: File-like object containing the multipart data\n        boundary: The boundary string that separates parts\n        chunk_size: Size of chunks to read from stream (default: 8KB)\n    \"\"\"\n    parser = MultipartParser(stream, boundary, chunk_size=chunk_size)\n    \n    for part in parser:\n        # Extract part headers\n        content_type = part.headers.get('Content-Type')\n        content_disposition = part.headers.get('Content-Disposition')\n        \n        # Parse content disposition to get filename and field name\n        disposition_options = {}\n        if content_disposition:\n            _, options = parse_options_header(content_disposition)\n            disposition_options = options\n        \n        field_name = disposition_options.get('name')\n        filename = disposition_options.get('filename')\n        \n        # Process the part based on whether it's a file or form field\n        if filename:\n            # This is a file upload\n            process_file_part(part, filename, content_type, field_name)\n        else:\n            # This is a form field\n            process_form_field(part, field_name)\n\ndef process_file_part(part, filename, content_type, field_name):\n    \"\"\"Process an individual file part from the multipart stream.\"\"\"\n    # Read file content in chunks to handle large files\n    file_data = io.BytesIO()\n    for chunk in part:\n        file_data.write(chunk)\n    \n    # Reset pointer to beginning for further processing\n    file_data.seek(0)\n    \n    # Here you can process the file data as needed\n    # Example: save to disk, process with PIL, etc.\n    print(f\"Processing file: {filename}, type: {content_type}, field: {field_name}\")\n    print(f\"File size: {len(file_data.getvalue())} bytes\")\n\ndef process_form_field(part, field_name):\n    \"\"\"Process an individual form field from the multipart stream.\"\"\"\n    # Read field value\n    field_value = b''.join([chunk for chunk in part]).decode('utf-8')\n    \n    # Process the form field value\n    print(f\"Processing form field: {field_name} = {field_value}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: reading from a file (could be any stream)\n    with open('multipart_data.bin', 'rb') as f:\n        # The boundary should be known from the Content-Type header\n        boundary = '----WebKitFormBoundary7MA4YWxkTrZu0gW'\n        process_multipart_stream(f, boundary)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "multipart"], "package_breakdown": {"install_packages": ["io", "multipart"], "import_packages": ["io", "multipart"], "llm_suggested": ["multipart", "io"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that simplifies a complex mathematical expression involving variables and constants using the computer algebra system (CAS) capabilities.", "prompt_index": 281, "generated_code": "```python\nimport sympy as sp\n\ndef simplify_expression(expr_str):\n    \"\"\"\n    Simplifies a mathematical expression string using SymPy's CAS capabilities.\n    \n    Args:\n        expr_str (str): String representation of the mathematical expression\n        \n    Returns:\n        sympy.Expr: Simplified expression\n    \"\"\"\n    # Parse the expression string\n    expr = sp.sympify(expr_str)\n    \n    # Simplify the expression\n    simplified_expr = sp.simplify(expr)\n    \n    return simplified_expr\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define variables\n    x, y, z = sp.symbols('x y z')\n    \n    # Example complex expression\n    complex_expr = \"(x**2 + 2*x + 1)/(x + 1) + sin(x)**2 + cos(x)**2 - 1\"\n    \n    # Simplify the expression\n    result = simplify_expression(complex_expr)\n    print(f\"Original: {complex_expr}\")\n    print(f\"Simplified: {result}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sympy"], "package_breakdown": {"install_packages": ["sympy"], "import_packages": ["sympy"], "llm_suggested": ["sympy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an ASGI application using the helper code provided in the package and adapts it to work with different ASGI server implementations.", "prompt_index": 282, "generated_code": "```python\nfrom my_asgi_package import create_asgi_app\n\n# Create the base ASGI application\napp = create_asgi_app()\n\n# Optional: Add middleware or additional configuration if needed\n# app = SomeMiddleware(app)\n\n# Export the app for ASGI servers\n# This is the standard entry point that ASGI servers look for\napplication = app\n```\n\n**Note**: This code assumes:\n1. You have a package `my_asgi_package` with a `create_asgi_app()` function\n2. The function returns a valid ASGI application callable\n3. Different ASGI servers (uvicorn, hypercorn, daphne, etc.) will automatically detect and use the `application` variable\n\nTo run with different servers:\n- **Uvicorn**: `uvicorn module_name:application`\n- **Hypercorn**: `hypercorn module_name:application`\n- **Daphne**: `daphne module_name:application`\n\nReplace `module_name` with the name of your Python file (without .py extension).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["my_asgi_package"], "packages_detected": ["my_asgi_package"], "package_breakdown": {"install_packages": ["my_asgi_package"], "import_packages": ["my_asgi_package"], "llm_suggested": ["my_asgi_package"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a MongoDB database, retrieves all documents from a specific collection, and prints out the results.", "prompt_index": 283, "generated_code": "```python\nfrom pymongo import MongoClient\n\n# MongoDB connection string (replace with your actual connection details)\nconnection_string = \"mongodb://localhost:27017/\"\ndatabase_name = \"your_database_name\"\ncollection_name = \"your_collection_name\"\n\ntry:\n    # Create MongoDB client and connect to database\n    client = MongoClient(connection_string)\n    db = client[database_name]\n    collection = db[collection_name]\n    \n    # Retrieve all documents from the collection\n    documents = collection.find()\n    \n    # Print each document\n    for doc in documents:\n        print(doc)\n        \nfinally:\n    # Close the connection\n    client.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pymongo"], "package_breakdown": {"install_packages": ["pymongo"], "import_packages": ["pymongo"], "llm_suggested": ["pymongo"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a new Jupyter Notebook with interactive code cells where users can input and execute Python code to perform data analysis on a given dataset.", "prompt_index": 284, "generated_code": "```python\nimport nbformat as nbf\nfrom nbformat.v4 import new_notebook, new_code_cell, new_markdown_cell\n\ndef create_interactive_data_analysis_notebook(output_path=\"data_analysis.ipynb\"):\n    \"\"\"\n    Creates a Jupyter Notebook with interactive cells for data analysis\n    \"\"\"\n    \n    # Create a new notebook\n    nb = new_notebook()\n    \n    # Add title and introduction\n    intro_cell = new_markdown_cell(\"\"\"# Interactive Data Analysis Notebook\n    \n## Welcome to your interactive data analysis environment!\n\nThis notebook provides a template for loading, exploring, and analyzing datasets.\nYou can modify and execute the code cells below to perform your analysis.\n\"\"\")\n    \n    # Add dataset loading cell\n    load_cell = new_code_cell(\"\"\"# Cell 1: Load your dataset\n# Replace with your dataset loading code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example: Load dataset from CSV\n# df = pd.read_csv('your_dataset.csv')\n\n# Example: Create sample data for demonstration\ndf = pd.DataFrame({\n    'feature1': np.random.randn(100),\n    'feature2': np.random.randn(100),\n    'target': np.random.randint(0, 2, 100)\n})\n\nprint(\"Dataset loaded successfully!\")\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()\n\"\"\")\n    \n    # Add data exploration cell\n    explore_cell = new_code_cell(\"\"\"# Cell 2: Explore the dataset\n# Modify this cell to explore your specific dataset\n\nprint(\"Dataset Info:\")\nprint(df.info())\n\nprint(\"\\\\nBasic Statistics:\")\nprint(df.describe())\n\nprint(\"\\\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# Visualize data distribution\nplt.figure(figsize=(10, 6))\ndf.hist(bins=20, figsize=(12, 8))\nplt.tight_layout()\nplt.show()\n\"\"\")\n    \n    # Add data visualization cell\n    viz_cell = new_code_cell(\"\"\"# Cell 3: Data Visualization\n# Create visualizations to understand your data\n\n# Example visualizations - customize for your dataset\nplt.figure(figsize=(15, 5))\n\n# Plot 1: Feature distribution\nplt.subplot(1, 3, 1)\nsns.histplot(df['feature1'], kde=True)\nplt.title('Feature 1 Distribution')\n\n# Plot 2: Correlation heatmap (if applicable)\nplt.subplot(1, 3, 2)\nif len(df.select_dtypes(include=[np.number]).columns) > 1:\n    numeric_df = df.select_dtypes(include=[np.number])\n    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n    plt.title('Correlation Heatmap')\n\n# Plot 3: Target distribution (if applicable)\nplt.subplot(1, 3, 3)\nif 'target' in df.columns:\n    df['target'].value_counts().plot(kind='bar')\n    plt.title('Target Distribution')\n\nplt.tight_layout()\nplt.show()\n\"\"\")\n    \n    # Add data analysis cell\n    analysis_cell = new_code_cell(\"\"\"# Cell 4: Data Analysis\n# Perform your analysis here\n\n# Example analysis - customize for your needs\nprint(\"Analysis Results:\")\n\n# Basic statistical tests or machine learning can be added here\n# For example:\nfrom scipy import stats\n\nif 'feature1' in df.columns and 'target' in df.columns:\n    group1 = df[df['target'] == 0]['feature1']\n    group2 = df[df['target'] == 1]['feature1']\n    \n    t_stat, p_value = stats.ttest_ind(group1, group2)\n    print(f\"T-test between target groups for feature1: p-value = {p_value:.4f}\")\n\"\"\")\n    \n    # Add custom analysis cell (empty for user input)\n    custom_cell = new_code_cell(\"\"\"# Cell 5: Your Custom Analysis\n# Add your own analysis code here\n\n# This cell is intentionally left empty for your custom analysis\n# You can add any data processing, modeling, or visualization code\n\nprint(\"Ready for your custom analysis!\")\n\"\"\")\n    \n    # Add all cells to the notebook\n    nb.cells = [intro_cell, load_cell, explore_cell, viz_cell, analysis_cell, custom_cell]\n    \n    # Write the notebook to file\n    with open(output_path, 'w', encoding='utf-8') as f:\n        nbf.write(nb, f)\n    \n    print(f\"Interactive data analysis notebook created: {output_path}\")\n    print(\"Open this file in Jupyter Notebook to start your analysis!\")\n\n# Create the notebook\ncreate_interactive_data_analysis_notebook()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["scipy", "seaborn", "nbformat", "numpy", "matplotlib", "pandas"], "package_breakdown": {"install_packages": ["scipy", "seaborn", "nbformat", "numpy", "matplotlib", "pandas"], "import_packages": ["scipy", "seaborn", "nbformat", "numpy", "matplotlib", "pandas"], "llm_suggested": ["nbformat", "pandas", "numpy", "matplotlib", "seaborn", "scipy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the cryptographic library to encrypt a message using AES encryption with a randomly generated key and initialization vector.", "prompt_index": 285, "generated_code": "```python\nimport os\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom cryptography.hazmat.backends import default_backend\n\ndef encrypt_message(message):\n    # Generate random key and IV\n    key = os.urandom(32)  # 256-bit key for AES-256\n    iv = os.urandom(16)   # 128-bit IV\n    \n    # Pad the message to match AES block size\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(message.encode()) + padder.finalize()\n    \n    # Create cipher and encrypt\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n    \n    return key, iv, ciphertext\n\n# Example usage:\n# key, iv, encrypted = encrypt_message(\"Secret message\")\n# print(f\"Key: {key.hex()}\")\n# print(f\"IV: {iv.hex()}\")\n# print(f\"Ciphertext: {encrypted.hex()}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "cryptography"], "package_breakdown": {"install_packages": ["os", "cryptography"], "import_packages": ["os", "cryptography"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a given date and time string in a specific format to a datetime object for easy manipulation and comparison.", "prompt_index": 286, "generated_code": "```python\nfrom datetime import datetime\n\ndef convert_to_datetime(date_string, date_format):\n    \"\"\"\n    Convert a date string to a datetime object using the specified format.\n    \n    Args:\n        date_string (str): The date string to convert\n        date_format (str): The format of the date string (e.g., \"%Y-%m-%d %H:%M:%S\")\n    \n    Returns:\n        datetime: A datetime object representing the parsed date and time\n    \n    Raises:\n        ValueError: If the date string doesn't match the specified format\n    \"\"\"\n    return datetime.strptime(date_string, date_format)\n\n# Example usage:\n# date_str = \"2023-12-25 14:30:00\"\n# format_str = \"%Y-%m-%d %H:%M:%S\"\n# dt_object = convert_to_datetime(date_str, format_str)\n# print(dt_object)  # Output: 2023-12-25 14:30:00\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime"], "package_breakdown": {"install_packages": ["datetime"], "import_packages": ["datetime"], "llm_suggested": ["datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads an ini file, parses its contents, and logs any errors or warnings encountered during the process using the facilities provided by the library.", "prompt_index": 287, "generated_code": "```python\nimport configparser\nimport logging\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('config_parser.log'),\n        logging.StreamHandler()\n    ]\n)\n\ndef read_ini_file(file_path):\n    \"\"\"\n    Read and parse an INI file, logging any errors or warnings encountered.\n    \n    Args:\n        file_path (str): Path to the INI file\n        \n    Returns:\n        configparser.ConfigParser: Parsed configuration object or None if failed\n    \"\"\"\n    config = configparser.ConfigParser()\n    \n    try:\n        # Attempt to read the file\n        with open(file_path, 'r') as file:\n            config.read_file(file)\n        logging.info(f\"Successfully parsed INI file: {file_path}\")\n        return config\n        \n    except FileNotFoundError:\n        logging.error(f\"INI file not found: {file_path}\")\n        return None\n        \n    except configparser.MissingSectionHeaderError:\n        logging.error(f\"Missing section header in INI file: {file_path}\")\n        return None\n        \n    except configparser.DuplicateSectionError as e:\n        logging.error(f\"Duplicate section found: {e}\")\n        return None\n        \n    except configparser.DuplicateOptionError as e:\n        logging.warning(f\"Duplicate option found: {e}\")\n        # Continue parsing despite duplicate options\n        return config\n        \n    except configparser.ParsingError as e:\n        logging.error(f\"Error parsing INI file: {e}\")\n        return None\n        \n    except Exception as e:\n        logging.error(f\"Unexpected error reading INI file: {e}\")\n        return None\n\n# Example usage\nif __name__ == \"__main__\":\n    config = read_ini_file('config.ini')\n    if config:\n        # Access configuration values\n        for section in config.sections():\n            logging.info(f\"Section: {section}\")\n            for key in config[section]:\n                logging.info(f\"  {key} = {config[section][key]}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["logging", "configparser"], "package_breakdown": {"install_packages": ["logging", "configparser"], "import_packages": ["logging", "configparser"], "llm_suggested": ["configparser", "logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes an OpenTelemetry Collector Exporter for sending telemetry data to a specified destination with custom configurations such as authentication tokens and endpoint URLs.", "prompt_index": 288, "generated_code": "```python\nfrom opentelemetry.exporter.otlp.proto.grpc.exporter import OTLPExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nimport os\n\n# Configuration parameters\nendpoint = \"https://your-collector-endpoint:4317\"  # Replace with your collector endpoint\nheaders = {\n    \"Authorization\": \"Bearer your-auth-token\"  # Replace with your authentication token\n}\nservice_name = \"your-service-name\"  # Replace with your service name\n\n# Create OTLP exporter with custom configuration\nexporter = OTLPExporter(\n    endpoint=endpoint,\n    headers=headers,\n    insecure=False,  # Set to True if using HTTP instead of HTTPS\n    timeout=30,  # Timeout in seconds\n)\n\n# Set up tracing\ntrace_provider = TracerProvider(\n    resource=Resource.create({\"service.name\": service_name})\n)\ntrace_provider.add_span_processor(BatchSpanProcessor(exporter))\n\n# Set up metrics (optional)\nmetric_reader = PeriodicExportingMetricReader(\n    exporter,\n    export_interval_millis=5000  # Export every 5 seconds\n)\nmetric_provider = MeterProvider(\n    metric_readers=[metric_reader],\n    resource=Resource.create({\"service.name\": service_name})\n)\n\n# Set the global providers (optional but recommended)\nfrom opentelemetry import trace, metrics\ntrace.set_tracer_provider(trace_provider)\nmetrics.set_meter_provider(metric_provider)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["opentelemetry"], "packages_detected": ["opentelemetry", "opentelemetry-exporter-otlp-proto-grpc", "os", "opentelemetry-sdk"], "package_breakdown": {"install_packages": ["opentelemetry", "opentelemetry-exporter-otlp-proto-grpc", "os", "opentelemetry-sdk"], "import_packages": ["opentelemetry", "os"], "llm_suggested": ["opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a dynamic neural network using tensors with GPU acceleration for training and inference on a given dataset.", "prompt_index": 289, "generated_code": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\n\nclass DynamicNeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_layers, output_size, activation='relu'):\n        super(DynamicNeuralNetwork, self).__init__()\n        self.layers = nn.ModuleList()\n        \n        # Input layer\n        self.layers.append(nn.Linear(input_size, hidden_layers[0]))\n        \n        # Hidden layers\n        for i in range(len(hidden_layers)-1):\n            self.layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n        \n        # Output layer\n        self.layers.append(nn.Linear(hidden_layers[-1], output_size))\n        \n        # Activation function\n        if activation == 'relu':\n            self.activation = F.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n        elif activation == 'tanh':\n            self.activation = torch.tanh\n        else:\n            self.activation = F.relu\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = self.activation(layer(x))\n        x = self.layers[-1](x)\n        return x\n\ndef train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for data, target in val_loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                val_loss += criterion(output, target).item()\n                \n                _, predicted = output.max(1)\n                total += target.size(0)\n                correct += predicted.eq(target).sum().item()\n        \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], '\n                  f'Train Loss: {train_loss/len(train_loader):.4f}, '\n                  f'Val Loss: {val_loss/len(val_loader):.4f}, '\n                  f'Val Acc: {100.*correct/total:.2f}%')\n\ndef predict(model, data_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    predictions = []\n    with torch.no_grad():\n        for data in data_loader:\n            data = data.to(device)\n            output = model(data)\n            _, predicted = output.max(1)\n            predictions.extend(predicted.cpu().numpy())\n    \n    return predictions\n\n# Example usage:\n# model = DynamicNeuralNetwork(input_size=784, hidden_layers=[128, 64, 32], output_size=10)\n# train_model(model, train_loader, val_loader)\n# predictions = predict(model, test_loader)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["torch"], "package_breakdown": {"install_packages": ["torch"], "import_packages": ["torch"], "llm_suggested": ["torch"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the pytest xdist plugin to distribute and run tests across multiple CPUs for faster test execution.", "prompt_index": 290, "generated_code": "```python\nimport pytest\n\nif __name__ == \"__main__\":\n    # Run tests using pytest-xdist with auto-detection of CPU cores\n    pytest.main([\"-n\", \"auto\", \"-v\"])\n```\n\nTo use this code:\n\n1. Install required packages:\n```bash\npip install pytest pytest-xdist\n```\n\n2. Save as `run_tests.py` and execute with:\n```bash\npython run_tests.py\n```\n\nAlternative command line usage:\n```bash\n# Explicit number of workers\npytest -n 4 -v\n\n# Auto-detect CPU cores\npytest -n auto -v\n\n# Specific test file with distributed execution\npytest -n auto tests/test_module.py -v\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pytest-xdist", "pytest"], "package_breakdown": {"install_packages": ["pytest-xdist", "pytest"], "import_packages": ["pytest"], "llm_suggested": ["pytest", "pytest-xdist"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that installs the specified package and project dependencies using the fastest possible method by leveraging the speed and efficiency of the Rust-written package manager.", "prompt_index": 291, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_package(package_name):\n    try:\n        # Use uv, a fast Python package installer written in Rust\n        subprocess.run([sys.executable, \"-m\", \"uv\", \"pip\", \"install\", package_name], check=True)\n        print(f\"Successfully installed {package_name}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to install {package_name}: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        package_name = sys.argv[1]\n        install_package(package_name)\n    else:\n        print(\"Please provide a package name to install\")\n        sys.exit(1)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "uv", "subprocess"], "package_breakdown": {"install_packages": ["sys", "uv", "subprocess"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["uv"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that applies a JSON patch (as defined in RFC 6902) to a given JSON object, updating it with the specified operations such as add, remove, replace, move, copy, and test.", "prompt_index": 292, "generated_code": "```python\nimport json\nfrom typing import Any, Dict, List, Union\n\nclass JSONPatch:\n    def __init__(self, patch_document: List[Dict[str, Any]]):\n        self.patch_document = patch_document\n    \n    def apply(self, target: Union[Dict[str, Any], List[Any]]) -> Union[Dict[str, Any], List[Any]]:\n        result = json.loads(json.dumps(target))  # Deep copy\n        \n        for operation in self.patch_document:\n            op = operation.get('op')\n            path = operation.get('path', '')\n            \n            if op == 'add':\n                self._add(result, path, operation.get('value'))\n            elif op == 'remove':\n                self._remove(result, path)\n            elif op == 'replace':\n                self._replace(result, path, operation.get('value'))\n            elif op == 'move':\n                from_path = operation.get('from')\n                self._move(result, from_path, path)\n            elif op == 'copy':\n                from_path = operation.get('from')\n                self._copy(result, from_path, path)\n            elif op == 'test':\n                if not self._test(result, path, operation.get('value')):\n                    raise ValueError(f\"Test operation failed at path: {path}\")\n            else:\n                raise ValueError(f\"Unsupported operation: {op}\")\n        \n        return result\n    \n    def _get_parent_and_key(self, obj: Any, path: str) -> tuple:\n        if not path.startswith('/'):\n            raise ValueError(f\"Invalid path: {path}\")\n        \n        parts = path.split('/')[1:]\n        current = obj\n        \n        for i, part in enumerate(parts[:-1]):\n            part = self._unescape(part)\n            if isinstance(current, list):\n                if not part.isdigit():\n                    raise ValueError(f\"Array index expected: {part}\")\n                index = int(part)\n                if index < 0 or index >= len(current):\n                    raise IndexError(f\"Index out of bounds: {index}\")\n                current = current[index]\n            elif isinstance(current, dict):\n                if part not in current:\n                    raise KeyError(f\"Key not found: {part}\")\n                current = current[part]\n            else:\n                raise ValueError(f\"Cannot traverse path: {path}\")\n        \n        last_part = self._unescape(parts[-1]) if parts else ''\n        return current, last_part\n    \n    def _escape(self, token: str) -> str:\n        return token.replace('~', '~0').replace('/', '~1')\n    \n    def _unescape(self, token: str) -> str:\n        return token.replace('~1', '/').replace('~0', '~')\n    \n    def _add(self, obj: Any, path: str, value: Any):\n        parent, key = self._get_parent_and_key(obj, path)\n        \n        if isinstance(parent, list):\n            if key == '-':\n                parent.append(value)\n            else:\n                if not key.isdigit():\n                    raise ValueError(f\"Array index expected: {key}\")\n                index = int(key)\n                if index < 0 or index > len(parent):\n                    raise IndexError(f\"Index out of bounds: {index}\")\n                parent.insert(index, value)\n        elif isinstance(parent, dict):\n            parent[key] = value\n        else:\n            raise ValueError(f\"Cannot add to non-container type at path: {path}\")\n    \n    def _remove(self, obj: Any, path: str):\n        parent, key = self._get_parent_and_key(obj, path)\n        \n        if isinstance(parent, list):\n            if not key.isdigit():\n                raise ValueError(f\"Array index expected: {key}\")\n            index = int(key)\n            if index < 0 or index >= len(parent):\n                raise IndexError(f\"Index out of bounds: {index}\")\n            del parent[index]\n        elif isinstance(parent, dict):\n            if key not in parent:\n                raise KeyError(f\"Key not found: {key}\")\n            del parent[key]\n        else:\n            raise ValueError(f\"Cannot remove from non-container type at path: {path}\")\n    \n    def _replace(self, obj: Any, path: str, value: Any):\n        parent, key = self._get_parent_and_key(obj, path)\n        \n        if isinstance(parent, list):\n            if not key.isdigit():\n                raise ValueError(f\"Array index expected: {key}\")\n            index = int(key)\n            if index < 0 or index >= len(parent):\n                raise IndexError(f\"Index out of bounds: {index}\")\n            parent[index] = value\n        elif isinstance(parent, dict):\n            if key not in parent:\n                raise KeyError(f\"Key not found: {key}\")\n            parent[key] = value\n        else:\n            raise ValueError(f\"Cannot replace in non-container type at path: {path}\")\n    \n    def _move(self, obj: Any, from_path: str, to_path: str):\n        # Get value from source\n        from_parent, from_key = self._get_parent_and_key(obj, from_path)\n        \n        if isinstance(from_parent, list):\n            if not from_key.isdigit():\n                raise ValueError(f\"Array index expected: {from_key}\")\n            from_index = int(from_key)\n            if from_index < 0 or from_index >= len(from_parent):\n                raise IndexError(f\"Index out of bounds: {from_index}\")\n            value = from_parent[from_index]\n            del from_parent[from_index]\n        elif isinstance(from_parent, dict):\n            if from_key not in from_parent:\n                raise KeyError(f\"Key not found: {from_key}\")\n            value = from_parent[from_key]\n            del from_parent[from_key]\n        else:\n            raise ValueError(f\"Cannot move from non-container type at path: {from_path}\")\n        \n        # Add to destination\n        self._add(obj, to_path, value)\n    \n    def _copy(self, obj: Any, from_path: str, to_path: str):\n        # Get value from source\n        from_parent, from_key = self._get_parent_and_key(obj, from_path)\n        \n        if isinstance(from_parent, list):\n            if not from_key.isdigit():\n                raise ValueError(f\"Array index expected: {from_key}\")\n            from_index = int(from_key)\n            if from_index < 0 or from_index >= len(from_parent):\n                raise IndexError(f\"Index out of bounds: {from_index}\")\n            value = from_parent[from_index]\n        elif isinstance(from_parent, dict):\n            if from_key not in from_parent:\n                raise KeyError(f\"Key not found: {from_key}\")\n            value = from_parent[from_key]\n        else:\n            raise ValueError(f\"Cannot copy from non-container type at path: {from_path}\")\n        \n        # Add to destination\n        self._add(obj, to_path, value)\n    \n    def _test(self, obj: Any, path: str, value: Any) -> bool:\n        try:\n            parent, key = self._get_parent_and_key(obj, path)\n            \n            if isinstance(parent, list):\n                if not key.isdigit():\n                    return False\n                index = int(key)\n                if index < 0 or index >= len(parent):\n                    return False\n                return parent[index] == value\n            elif isinstance(parent, dict):\n                if key not in parent:\n                    return False\n                return parent[key] == value\n            else:\n                return False\n        except (ValueError, IndexError, KeyError):\n            return False\n\ndef apply_patch(target: Union[Dict[str, Any], List[Any]], patch_document: List[Dict[str, Any]]) -> Union[Dict[str, Any], List[Any]]:\n    return JSONPatch(patch_document).apply(target)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "typing"], "package_breakdown": {"install_packages": ["json", "typing"], "import_packages": ["json", "typing"], "llm_suggested": ["json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Snowflake database using SQLAlchemy and executes a SQL query to retrieve data from a specific table.", "prompt_index": 293, "generated_code": "```python\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\n# Snowflake connection parameters\nsnowflake_account = 'your_account'\nsnowflake_user = 'your_username'\nsnowflake_password = 'your_password'\nsnowflake_database = 'your_database'\nsnowflake_schema = 'your_schema'\nsnowflake_warehouse = 'your_warehouse'\n\n# Create connection string\nconnection_string = f'snowflake://{snowflake_user}:{snowflake_password}@{snowflake_account}/{snowflake_database}/{snowflake_schema}?warehouse={snowflake_warehouse}'\n\n# Create SQLAlchemy engine\nengine = create_engine(connection_string)\n\n# SQL query to execute\nquery = \"SELECT * FROM your_table_name LIMIT 10\"\n\n# Execute query and fetch results\nwith engine.connect() as connection:\n    result = connection.execute(query)\n    data = result.fetchall()\n    columns = result.keys()\n\n# Convert to pandas DataFrame (optional)\ndf = pd.DataFrame(data, columns=columns)\nprint(df)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pandas", "snowflake-sqlalchemy", "sqlalchemy"], "package_breakdown": {"install_packages": ["pandas", "snowflake-sqlalchemy", "sqlalchemy"], "import_packages": ["pandas", "sqlalchemy"], "llm_suggested": ["sqlalchemy", "pandas", "snowflake-sqlalchemy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the area of a triangle given its three vertices in a 2D plane.", "prompt_index": 294, "generated_code": "```python\ndef triangle_area(vertices):\n    \"\"\"\n    Calculate the area of a triangle given its three vertices in 2D.\n    \n    Args:\n        vertices: List of three tuples representing (x, y) coordinates\n                 Example: [(x1, y1), (x2, y2), (x3, y3)]\n    \n    Returns:\n        float: Area of the triangle\n    \"\"\"\n    (x1, y1), (x2, y2), (x3, y3) = vertices\n    return abs(0.5 * (x1*(y2 - y3) + x2*(y3 - y1) + x3*(y1 - y2)))\n\n# Example usage:\n# vertices = [(0, 0), (4, 0), (0, 3)]\n# area = triangle_area(vertices)\n# print(f\"Area: {area}\")  # Output: Area: 6.0\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that accesses and retrieves a secret from Google Cloud Secret Manager using the API client library.", "prompt_index": 295, "generated_code": "```python\nfrom google.cloud import secretmanager\n\ndef access_secret(project_id, secret_id, version_id=\"latest\"):\n    \"\"\"\n    Access a secret version from Google Cloud Secret Manager.\n    \n    Args:\n        project_id (str): Google Cloud project ID\n        secret_id (str): Secret ID/name\n        version_id (str): Version identifier, defaults to \"latest\"\n    \n    Returns:\n        str: The secret payload as a string\n    \"\"\"\n    client = secretmanager.SecretManagerServiceClient()\n    \n    # Build the resource name of the secret version\n    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n    \n    # Access the secret version\n    response = client.access_secret_version(request={\"name\": name})\n    \n    # Return the decoded payload\n    return response.payload.data.decode('UTF-8')\n\n# Example usage:\n# secret_value = access_secret(\"my-project\", \"my-secret\")\n# print(secret_value)\n```\n\nNote: Requires `google-cloud-secret-manager` package and proper authentication setup (Application Default Credentials or service account key).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-secret-manager"], "package_breakdown": {"install_packages": ["google", "google-cloud-secret-manager"], "import_packages": ["google"], "llm_suggested": ["google-cloud-secret-manager"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that checks if the current Python environment is running on Python 3 and, if not, automatically performs necessary syntax conversions for compatibility with Python 2.", "prompt_index": 296, "generated_code": "```python\nimport sys\n\ndef convert_to_python2(code):\n    \"\"\"Convert Python 3 code to Python 2 compatible syntax.\"\"\"\n    # Basic syntax replacements\n    replacements = [\n        ('print(', 'print '),\n        ('exec(', 'exec '),\n        ('xrange', 'range'),\n        ('input(', 'raw_input('),\n        ('__next__', 'next'),\n    ]\n    \n    for py3, py2 in replacements:\n        code = code.replace(py3, py2)\n    \n    return code\n\ndef check_python_version():\n    \"\"\"Check if running on Python 3, convert code if needed.\"\"\"\n    if sys.version_info[0] < 3:\n        print(\"Running on Python 2 - performing syntax conversion\")\n        \n        # Get the current module's source code\n        import inspect\n        current_frame = inspect.currentframe()\n        caller_frame = inspect.getouterframes(current_frame)[1]\n        source_lines = inspect.getsourcelines(caller_frame[0])[0]\n        source_code = ''.join(source_lines)\n        \n        # Convert and execute\n        converted_code = convert_to_python2(source_code)\n        exec(converted_code, caller_frame[0].f_globals)\n        return False\n    else:\n        print(\"Running on Python 3 - no conversion needed\")\n        return True\n\n# Check version at module import\nis_python3 = check_python_version()\n```\n\nNote: This approach has limitations. The `inspect` module may not always reliably get the source code, especially in compiled environments or when code is obfuscated. For production use, consider using dedicated tools like `2to3` or `futurize` from the `future` package instead.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "inspect"], "package_breakdown": {"install_packages": ["sys", "inspect"], "import_packages": ["sys", "inspect"], "llm_suggested": ["inspect", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection with a debug adapter using the Debug Adapter Protocol and enables debugging of a Python script in real-time.", "prompt_index": 297, "generated_code": "```python\nimport socket\nimport json\nimport threading\nimport time\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nclass DAPClient:\n    def __init__(self, host='localhost', port=4711):\n        self.host = host\n        self.port = port\n        self.socket = None\n        self.seq = 1\n        self.running = False\n        self.response_handlers = {}\n        \n    def connect(self):\n        \"\"\"Establish connection to debug adapter\"\"\"\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((self.host, self.port))\n        self.running = True\n        \n        # Start listener thread\n        self.listener_thread = threading.Thread(target=self._listen_for_messages)\n        self.listener_thread.daemon = True\n        self.listener_thread.start()\n        \n    def send_request(self, command, arguments=None):\n        \"\"\"Send a request to the debug adapter\"\"\"\n        if arguments is None:\n            arguments = {}\n            \n        request = {\n            \"seq\": self.seq,\n            \"type\": \"request\",\n            \"command\": command,\n            \"arguments\": arguments\n        }\n        \n        # Store handler for this sequence number\n        self.response_handlers[self.seq] = None\n        \n        # Send the request\n        message = json.dumps(request) + '\\r\\n'\n        self.socket.send(message.encode('utf-8'))\n        \n        self.seq += 1\n        return self.seq - 1\n        \n    def _listen_for_messages(self):\n        \"\"\"Listen for incoming messages from debug adapter\"\"\"\n        buffer = \"\"\n        while self.running:\n            try:\n                data = self.socket.recv(4096)\n                if not data:\n                    break\n                    \n                buffer += data.decode('utf-8')\n                \n                # Process complete messages\n                while '\\r\\n' in buffer:\n                    message, buffer = buffer.split('\\r\\n', 1)\n                    self._handle_message(json.loads(message))\n                    \n            except Exception as e:\n                print(f\"Error receiving data: {e}\")\n                break\n                \n    def _handle_message(self, message):\n        \"\"\"Handle incoming messages from debug adapter\"\"\"\n        msg_type = message.get('type')\n        \n        if msg_type == 'response':\n            seq = message.get('request_seq')\n            if seq in self.response_handlers:\n                self.response_handlers[seq] = message\n                \n        elif msg_type == 'event':\n            event = message.get('event')\n            if event == 'stopped':\n                print(\"Debugger stopped at breakpoint\")\n                # Handle breakpoint hit\n            elif event == 'continued':\n                print(\"Debugger continued\")\n            elif event == 'output':\n                output = message.get('body', {}).get('output', '')\n                print(f\"Output: {output}\")\n                \n    def initialize(self):\n        \"\"\"Send initialize request\"\"\"\n        return self.send_request('initialize', {\n            \"clientID\": \"python-dap-client\",\n            \"clientName\": \"Python DAP Client\",\n            \"adapterID\": \"python\",\n            \"locale\": \"en-US\",\n            \"linesStartAt1\": True,\n            \"columnsStartAt1\": True,\n            \"pathFormat\": \"path\",\n            \"supportsVariableType\": True,\n            \"supportsVariablePaging\": True,\n            \"supportsRunInTerminalRequest\": True,\n            \"supportsMemoryReferences\": True\n        })\n        \n    def launch(self, program, args=None):\n        \"\"\"Send launch request to start debugging\"\"\"\n        if args is None:\n            args = []\n            \n        return self.send_request('launch', {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": program,\n            \"console\": \"integratedTerminal\",\n            \"args\": args,\n            \"justMyCode\": False\n        })\n        \n    def set_breakpoints(self, file_path, lines):\n        \"\"\"Set breakpoints at specified lines\"\"\"\n        breakpoints = [{\"line\": line} for line in lines]\n        return self.send_request('setBreakpoints', {\n            \"source\": {\"path\": file_path},\n            \"breakpoints\": breakpoints\n        })\n        \n    def configuration_done(self):\n        \"\"\"Send configuration done request\"\"\"\n        return self.send_request('configurationDone')\n        \n    def disconnect(self):\n        \"\"\"Disconnect from debug adapter\"\"\"\n        self.send_request('disconnect', {\"restart\": False})\n        self.running = False\n        self.socket.close()\n        \n    def wait_for_response(self, seq, timeout=5):\n        \"\"\"Wait for response to a specific request\"\"\"\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if self.response_handlers.get(seq) is not None:\n                return self.response_handlers[seq]\n            time.sleep(0.1)\n        return None\n\n# Example usage\ndef debug_python_script(script_path, breakpoints=None):\n    if breakpoints is None:\n        breakpoints = [5, 10]  # Example breakpoints\n        \n    client = DAPClient()\n    \n    try:\n        # Connect to debug adapter\n        client.connect()\n        \n        # Initialize debug session\n        init_seq = client.initialize()\n        init_response = client.wait_for_response(init_seq)\n        \n        if init_response and init_response.get('success', False):\n            print(\"Debug session initialized\")\n            \n            # Set breakpoints\n            bp_seq = client.set_breakpoints(script_path, breakpoints)\n            bp_response = client.wait_for_response(bp_seq)\n            \n            # Launch the program\n            launch_seq = client.launch(script_path)\n            launch_response = client.wait_for_response(launch_seq)\n            \n            # Configuration done\n            config_seq = client.configuration_done()\n            config_response = client.wait_for_response(config_seq)\n            \n            print(\"Debug session started. Press Ctrl+C to stop.\")\n            \n            # Keep the connection alive\n            while client.running:\n                time.sleep(1)\n                \n    except KeyboardInterrupt:\n        print(\"\\nStopping debug session...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        client.disconnect()\n\nif __name__ == \"__main__\":\n    # Example: debug a Python script with breakpoints at lines 5 and 10\n    script_to_debug = \"example_script.py\"\n    debug_python_script(script_to_debug, breakpoints=[5, 10])\n```\n\nThis code provides a basic DAP client implementation. Note that you'll need a running debug adapter (like the one from Python extension for VS Code) listening on the specified port. The actual debug adapter setup and configuration would typically be handled by your IDE/debugger environment.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "time", "sys", "threading", "pathlib", "json", "socket"], "package_breakdown": {"install_packages": ["subprocess", "time", "sys", "threading", "pathlib", "json", "socket"], "import_packages": ["subprocess", "time", "sys", "threading", "pathlib", "json", "socket"], "llm_suggested": ["socket", "json", "threading", "time", "subprocess", "sys", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that enables bash tab completion for command line arguments defined using the argparse module.", "prompt_index": 298, "generated_code": "```python\nimport argparse\nimport os\nimport sys\n\ndef complete_action(prefix, parsed_args, **kwargs):\n    \"\"\"Generate completion candidates for the current argument\"\"\"\n    # This is a stub - replace with your actual completion logic\n    # You can access the current argument being completed via parsed_args\n    return [f\"completion_{i}\" for i in range(3)]  # Example completions\n\ndef main():\n    parser = argparse.ArgumentParser(description='Example with tab completion')\n    parser.add_argument('--file', help='File to process', \n                       completer=complete_action)\n    parser.add_argument('--mode', choices=['read', 'write', 'append'], \n                       help='Operation mode')\n    parser.add_argument('input', nargs='?', help='Input source',\n                       completer=complete_action)\n    \n    # Check if we're in completion mode\n    if 'COMP_LINE' in os.environ:\n        import argcomplete\n        argcomplete.autocomplete(parser)\n        return\n    \n    args = parser.parse_args()\n    # Your normal application logic here\n    print(f\"Parsed args: {args}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n```bash\n# Add this to your .bashrc or .bash_profile to enable completion\n# eval \"$(register-python-argcomplete your_script_name.py)\"\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["argparse", "sys", "argcomplete", "os"], "package_breakdown": {"install_packages": ["argparse", "sys", "argcomplete", "os"], "import_packages": ["argparse", "sys", "os", "argcomplete"], "llm_suggested": ["argparse", "os", "sys", "argcomplete"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a client runtime for consuming RESTful APIs using AutoRest swagger generator.", "prompt_index": 299, "generated_code": "```python\nimport requests\nimport json\nfrom typing import Optional, Dict, Any, List, Union\n\nclass AutoRestClient:\n    def __init__(self, base_url: str, headers: Optional[Dict[str, str]] = None):\n        self.base_url = base_url.rstrip('/')\n        self.session = requests.Session()\n        if headers:\n            self.session.headers.update(headers)\n    \n    def _make_request(self, method: str, endpoint: str, \n                     params: Optional[Dict[str, Any]] = None,\n                     data: Optional[Dict[str, Any]] = None,\n                     json_data: Optional[Any] = None) -> requests.Response:\n        \"\"\"Generic request method\"\"\"\n        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n        \n        response = self.session.request(\n            method=method,\n            url=url,\n            params=params,\n            data=data,\n            json=json_data\n        )\n        \n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            print(f\"HTTP Error: {e}\")\n            raise\n        \n        return response\n    \n    def get(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Any:\n        \"\"\"GET request\"\"\"\n        response = self._make_request('GET', endpoint, params=params)\n        return response.json()\n    \n    def post(self, endpoint: str, data: Optional[Dict[str, Any]] = None, \n             json_data: Optional[Any] = None) -> Any:\n        \"\"\"POST request\"\"\"\n        response = self._make_request('POST', endpoint, data=data, json_data=json_data)\n        return response.json()\n    \n    def put(self, endpoint: str, data: Optional[Dict[str, Any]] = None,\n            json_data: Optional[Any] = None) -> Any:\n        \"\"\"PUT request\"\"\"\n        response = self._make_request('PUT', endpoint, data=data, json_data=json_data)\n        return response.json()\n    \n    def patch(self, endpoint: str, data: Optional[Dict[str, Any]] = None,\n              json_data: Optional[Any] = None) -> Any:\n        \"\"\"PATCH request\"\"\"\n        response = self._make_request('PATCH', endpoint, data=data, json_data=json_data)\n        return response.json()\n    \n    def delete(self, endpoint: str) -> Any:\n        \"\"\"DELETE request\"\"\"\n        response = self._make_request('DELETE', endpoint)\n        return response.json()\n    \n    def add_header(self, key: str, value: str):\n        \"\"\"Add a header to all requests\"\"\"\n        self.session.headers[key] = value\n    \n    def remove_header(self, key: str):\n        \"\"\"Remove a header\"\"\"\n        if key in self.session.headers:\n            del self.session.headers[key]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Initialize client with base URL and optional headers\n    client = AutoRestClient(\n        base_url=\"https://api.example.com/v1\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"AutoRestClient/1.0\"\n        }\n    )\n    \n    # Add authentication token\n    client.add_header(\"Authorization\", \"Bearer your-token-here\")\n    \n    # Example API calls\n    try:\n        # GET request\n        users = client.get(\"/users\", params={\"limit\": 10})\n        print(\"Users:\", users)\n        \n        # POST request\n        new_user = client.post(\"/users\", json_data={\"name\": \"John\", \"email\": \"john@example.com\"})\n        print(\"Created user:\", new_user)\n        \n        # PUT request\n        updated_user = client.put(f\"/users/{new_user['id']}\", json_data={\"name\": \"John Updated\"})\n        print(\"Updated user:\", updated_user)\n        \n        # DELETE request\n        result = client.delete(f\"/users/{new_user['id']}\")\n        print(\"Delete result:\", result)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "json", "typing"], "package_breakdown": {"install_packages": ["requests", "json", "typing"], "import_packages": ["requests", "json", "typing"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes execnet to deploy a Python script across multiple remote hosts simultaneously for efficient and rapid execution.", "prompt_index": 300, "generated_code": "```python\nimport execnet\nimport sys\n\ndef execute_remote(hosts, script_path, python_interpreter=\"python\"):\n    \"\"\"\n    Execute a Python script on multiple remote hosts simultaneously using execnet.\n    \n    Args:\n        hosts (list): List of host strings in format \"ssh=user@hostname\"\n        script_path (str): Path to the Python script to execute remotely\n        python_interpreter (str): Python interpreter to use (default: \"python\")\n    \"\"\"\n    # Read the script content\n    with open(script_path, 'r') as f:\n        script_content = f.read()\n    \n    # Create gateway group\n    group = execnet.Group()\n    \n    # Create channels for each host\n    channels = []\n    for host in hosts:\n        try:\n            gw = group.makegateway(f\"ssh={host}//python={python_interpreter}\")\n            channel = gw.remote_exec(script_content)\n            channels.append((host, channel))\n        except Exception as e:\n            print(f\"Failed to connect to {host}: {e}\")\n    \n    # Collect results\n    results = {}\n    for host, channel in channels:\n        try:\n            output = channel.receive()\n            results[host] = {\"status\": \"success\", \"output\": output}\n        except Exception as e:\n            results[host] = {\"status\": \"error\", \"error\": str(e)}\n    \n    # Close all gateways\n    group.terminate()\n    \n    return results\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define remote hosts (replace with actual hostnames and credentials)\n    hosts = [\n        \"ssh=user@host1.example.com\",\n        \"ssh=user@host2.example.com\", \n        \"ssh=user@host3.example.com\"\n    ]\n    \n    # Path to the script to execute remotely\n    script_path = \"remote_script.py\"\n    \n    # Execute on all hosts\n    results = execute_remote(hosts, script_path)\n    \n    # Print results\n    for host, result in results.items():\n        print(f\"\\n--- {host} ---\")\n        if result[\"status\"] == \"success\":\n            print(f\"Output: {result['output']}\")\n        else:\n            print(f\"Error: {result['error']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "execnet"], "package_breakdown": {"install_packages": ["sys", "execnet"], "import_packages": ["sys", "execnet"], "llm_suggested": ["execnet"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that efficiently streams a large file from Azure Blob Storage, decompresses it if necessary (e.g., gzip, bz2, zst), and processes its contents in real-time without loading the entire file into memory.", "prompt_index": 301, "generated_code": "```python\nimport io\nimport gzip\nimport bz2\nimport zstandard as zstd\nfrom azure.storage.blob import BlobServiceClient\n\ndef stream_blob_with_decompression(connection_string, container_name, blob_name, process_func):\n    \"\"\"\n    Stream and process a blob from Azure Blob Storage with automatic decompression.\n    \n    Args:\n        connection_string: Azure Storage connection string\n        container_name: Name of the container\n        blob_name: Name of the blob to stream\n        process_func: Function that processes chunks of data (takes bytes as input)\n    \"\"\"\n    \n    # Initialize blob service client\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n    \n    # Get blob properties to check content type and encoding\n    blob_props = blob_client.get_blob_properties()\n    content_type = blob_props.get('content_settings', {}).get('content_type', '')\n    content_encoding = blob_props.get('content_settings', {}).get('content_encoding', '')\n    \n    # Determine decompressor based on file extension or content encoding\n    blob_name_lower = blob_name.lower()\n    \n    if blob_name_lower.endswith(('.gz', '.gzip')) or 'gzip' in content_encoding:\n        decompressor = gzip.GzipFile(fileobj=io.BytesIO(), mode='rb')\n    elif blob_name_lower.endswith(('.bz2', '.bzip2')) or 'bzip2' in content_encoding:\n        decompressor = bz2.BZ2Decompressor()\n    elif blob_name_lower.endswith(('.zst', '.zstd')) or 'zstd' in content_encoding:\n        decompressor = zstd.ZstdDecompressor()\n    else:\n        decompressor = None\n    \n    # Stream the blob in chunks\n    chunk_size = 4 * 1024 * 1024  # 4MB chunks\n    stream = blob_client.download_blob(max_concurrency=1)\n    \n    buffer = b''\n    \n    for chunk in stream.chunks():\n        if decompressor:\n            if isinstance(decompressor, gzip.GzipFile):\n                # For gzip, we need to handle it differently\n                decompressor.fileobj = io.BytesIO(buffer + chunk)\n                try:\n                    while True:\n                        decompressed_data = decompressor.read(chunk_size)\n                        if not decompressed_data:\n                            break\n                        process_func(decompressed_data)\n                except (EOFError, OSError):\n                    # End of compressed stream or partial read\n                    pass\n                buffer = decompressor.fileobj.read()  # Save remaining data\n            else:\n                # For other compressors\n                try:\n                    decompressed_data = decompressor.decompress(buffer + chunk)\n                    if decompressed_data:\n                        process_func(decompressed_data)\n                    buffer = b''\n                except Exception:\n                    # Handle partial decompression by buffering\n                    buffer += chunk\n        else:\n            # No decompression needed\n            process_func(chunk)\n    \n    # Process any remaining buffered data\n    if buffer and decompressor and not isinstance(decompressor, gzip.GzipFile):\n        try:\n            decompressed_data = decompressor.decompress(buffer)\n            if decompressed_data:\n                process_func(decompressed_data)\n        except Exception:\n            # Final cleanup of buffer\n            pass\n\n# Example usage:\ndef example_process_function(data_chunk):\n    \"\"\"Example processing function - replace with your actual logic\"\"\"\n    # Process the data chunk (e.g., parse lines, extract information, etc.)\n    print(f\"Processing {len(data_chunk)} bytes of data\")\n    # Your processing logic here\n\n# Usage example:\n# stream_blob_with_decompression(\n#     connection_string=\"your_connection_string\",\n#     container_name=\"your_container\",\n#     blob_name=\"your_file.gz\",\n#     process_func=example_process_function\n# )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["io", "zstandard", "azure", "azure-storage-blob", "bz2", "gzip"], "package_breakdown": {"install_packages": ["io", "zstandard", "azure", "azure-storage-blob", "bz2", "gzip"], "import_packages": ["io", "bz2", "zstandard", "azure", "gzip"], "llm_suggested": ["azure-storage-blob", "zstandard"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with Azure Active Directory using the Microsoft Azure Client Library for Python and retrieves a list of common resources in the Azure environment.", "prompt_index": 302, "generated_code": "```python\nfrom azure.identity import ClientSecretCredential\nfrom azure.mgmt.resource import ResourceManagementClient\nfrom azure.mgmt.subscription import SubscriptionClient\n\ndef authenticate_and_list_resources(tenant_id, client_id, client_secret):\n    # Create credentials object\n    credentials = ClientSecretCredential(\n        tenant_id=tenant_id,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n    \n    # Get subscriptions\n    subscription_client = SubscriptionClient(credentials)\n    subscriptions = list(subscription_client.subscriptions.list())\n    \n    resources_list = []\n    \n    # Iterate through subscriptions\n    for sub in subscriptions:\n        subscription_id = sub.subscription_id\n        \n        # Create resource management client\n        resource_client = ResourceManagementClient(credentials, subscription_id)\n        \n        # List resource groups\n        resource_groups = resource_client.resource_groups.list()\n        \n        # List resources in each resource group\n        for rg in resource_groups:\n            resources = resource_client.resources.list_by_resource_group(rg.name)\n            \n            for resource in resources:\n                resources_list.append({\n                    'subscription_id': subscription_id,\n                    'resource_group': rg.name,\n                    'resource_name': resource.name,\n                    'resource_type': resource.type,\n                    'location': resource.location\n                })\n    \n    return resources_list\n\n# Example usage:\n# resources = authenticate_and_list_resources(\n#     tenant_id=\"your-tenant-id\",\n#     client_id=\"your-client-id\",\n#     client_secret=\"your-client-secret\"\n# )\n# print(resources)\n```\n\nNote: Replace `\"your-tenant-id\"`, `\"your-client-id\"`, and `\"your-client-secret\"` with your actual Azure AD credentials. You'll need to install the required packages: `pip install azure-identity azure-mgmt-resource azure-mgmt-subscription`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure-mgmt-subscription", "azure-mgmt-resource", "azure", "azure-identity"], "package_breakdown": {"install_packages": ["azure-mgmt-subscription", "azure-mgmt-resource", "azure", "azure-identity"], "import_packages": ["azure"], "llm_suggested": ["azure-identity", "azure-mgmt-resource", "azure-mgmt-subscription"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a secure authentication mechanism using the SCRAM protocol for a client-server communication setup.", "prompt_index": 303, "generated_code": "```python\nimport hashlib\nimport hmac\nimport base64\nimport os\nimport secrets\nimport re\n\nclass SCRAMClient:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.nonce = None\n        self.client_first_message = None\n        self.server_first_message = None\n        self.client_final_message = None\n        self.salted_password = None\n        self.auth_message = None\n        \n    def generate_client_first_message(self):\n        \"\"\"Generate the initial client message\"\"\"\n        self.nonce = base64.b64encode(secrets.token_bytes(18)).decode('utf-8')\n        self.client_first_message = f\"n={self.username},r={self.nonce}\"\n        return f\"n,,{self.client_first_message}\"\n    \n    def process_server_first_message(self, server_response):\n        \"\"\"Process the server's response\"\"\"\n        self.server_first_message = server_response\n        \n        # Parse server response\n        params = dict(re.findall(r'([a-z])=([^,]*)', server_response))\n        server_nonce = params.get('r', '')\n        salt = base64.b64decode(params.get('s', ''))\n        iterations = int(params.get('i', 4096))\n        \n        # Validate nonce\n        if not server_nonce.startswith(self.nonce):\n            raise ValueError(\"Invalid server nonce\")\n            \n        # Generate salted password\n        self._generate_salted_password(salt, iterations)\n        \n        # Generate client proof\n        client_final_no_proof = f\"c=biws,r={server_nonce}\"\n        auth_message = f\"{self.client_first_message},{self.server_first_message},{client_final_no_proof}\"\n        self.auth_message = auth_message\n        \n        client_proof = self._generate_client_proof(auth_message)\n        \n        self.client_final_message = f\"{client_final_no_proof},p={client_proof}\"\n        return self.client_final_message\n    \n    def verify_server_signature(self, server_final_message):\n        \"\"\"Verify server's signature\"\"\"\n        params = dict(re.findall(r'([a-z])=([^,]*)', server_final_message))\n        server_signature = base64.b64decode(params.get('v', ''))\n        \n        # Generate server key and signature\n        server_key = hmac.new(self.salted_password, b\"Server Key\", hashlib.sha256).digest()\n        computed_signature = hmac.new(server_key, self.auth_message.encode('utf-8'), hashlib.sha256).digest()\n        \n        if not hmac.compare_digest(server_signature, computed_signature):\n            raise ValueError(\"Server authentication failed\")\n        \n        return True\n    \n    def _generate_salted_password(self, salt, iterations):\n        \"\"\"Generate salted password using PBKDF2\"\"\"\n        password_bytes = self.password.encode('utf-8')\n        self.salted_password = hashlib.pbkdf2_hmac(\n            'sha256', \n            password_bytes, \n            salt, \n            iterations\n        )\n    \n    def _generate_client_proof(self, auth_message):\n        \"\"\"Generate client proof\"\"\"\n        # Generate client key\n        client_key = hmac.new(self.salted_password, b\"Client Key\", hashlib.sha256).digest()\n        \n        # Generate stored key\n        stored_key = hashlib.sha256(client_key).digest()\n        \n        # Generate client signature\n        client_signature = hmac.new(stored_key, auth_message.encode('utf-8'), hashlib.sha256).digest()\n        \n        # Generate proof by XORing client key with client signature\n        client_proof = bytes(a ^ b for a, b in zip(client_key, client_signature))\n        \n        return base64.b64encode(client_proof).decode('utf-8')\n\n\nclass SCRAMServer:\n    def __init__(self, user_db):\n        self.user_db = user_db  # Dictionary with username: (salt, iterations, stored_key, server_key)\n        self.nonce = None\n        self.client_first_message = None\n        self.server_first_message = None\n        self.auth_message = None\n        \n    def process_client_first_message(self, client_message):\n        \"\"\"Process client's first message\"\"\"\n        parts = client_message.split(',', 2)\n        if len(parts) < 3 or not parts[0].endswith(',,'):\n            raise ValueError(\"Invalid client message format\")\n            \n        self.client_first_message = parts[2]\n        params = dict(re.findall(r'([a-z])=([^,]*)', self.client_first_message))\n        username = params.get('n', '')\n        client_nonce = params.get('r', '')\n        \n        if username not in self.user_db:\n            raise ValueError(\"User not found\")\n            \n        # Generate server nonce\n        self.nonce = client_nonce + base64.b64encode(secrets.token_bytes(9)).decode('utf-8')\n        \n        # Get user data\n        salt, iterations, _, _ = self.user_db[username]\n        \n        # Generate server response\n        self.server_first_message = f\"r={self.nonce},s={base64.b64encode(salt).decode('utf-8')},i={iterations}\"\n        return self.server_first_message\n    \n    def process_client_final_message(self, client_final_message, username):\n        \"\"\"Process client's final message and verify\"\"\"\n        params = dict(re.findall(r'([a-z])=([^,]*)', client_final_message))\n        client_nonce = params.get('r', '')\n        client_proof = base64.b64decode(params.get('p', ''))\n        \n        if not client_nonce == self.nonce:\n            raise ValueError(\"Invalid nonce\")\n            \n        # Get user data\n        salt, iterations, stored_key, server_key = self.user_db[username]\n        \n        # Reconstruct auth message\n        client_final_no_proof = re.sub(r',p=[^,]*', '', client_final_message)\n        self.auth_message = f\"{self.client_first_message},{self.server_first_message},{client_final_no_proof}\"\n        \n        # Verify client proof\n        client_signature = hmac.new(stored_key, self.auth_message.encode('utf-8'), hashlib.sha256).digest()\n        client_key = bytes(a ^ b for a, b in zip(client_proof, client_signature))\n        \n        # Verify client key hash matches stored key\n        computed_stored_key = hashlib.sha256(client_key).digest()\n        if not hmac.compare_digest(computed_stored_key, stored_key):\n            raise ValueError(\"Authentication failed\")\n        \n        # Generate server signature\n        server_signature = hmac.new(server_key, self.auth_message.encode('utf-8'), hashlib.sha256).digest()\n        \n        return f\"v={base64.b64encode(server_signature).decode('utf-8')}\"\n    \n    @staticmethod\n    def create_user(password, iterations=4096):\n        \"\"\"Helper method to create user credentials\"\"\"\n        salt = os.urandom(16)\n        password_bytes = password.encode('utf-8')\n        \n        # Generate salted password\n        salted_password = hashlib.pbkdf2_hmac(\n            'sha256', \n            password_bytes, \n            salt, \n            iterations\n        )\n        \n        # Generate client key and stored key\n        client_key = hmac.new(salted_password, b\"Client Key\", hashlib.sha256).digest()\n        stored_key = hashlib.sha256(client_key).digest()\n        \n        # Generate server key\n        server_key = hmac.new(salted_password, b\"Server Key\", hashlib.sha256).digest()\n        \n        return salt, iterations, stored_key, server_key\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a test user\n    username = \"testuser\"\n    password = \"securepassword123\"\n    \n    # Server setup\n    salt, iterations, stored_key, server_key = SCRAMServer.create_user(password)\n    user_db = {username: (salt, iterations, stored_key, server_key)}\n    server = SCRAMServer(user_db)\n    \n    # Client authentication\n    client = SCRAMClient(username, password)\n    \n    # Step 1: Client sends first message\n    client_first = client.generate_client_first_message()\n    print(f\"Client first: {client_first}\")\n    \n    # Step 2: Server processes and responds\n    try:\n        server_first = server.process_client_first_message(client_first)\n        print(f\"Server first: {server_first}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        exit()\n    \n    # Step 3: Client processes server response and sends final message\n    try:\n        client_final = client.process_server_first_message(server_first)\n        print(f\"Client final: {client_final}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        exit()\n    \n    # Step 4: Server verifies client and responds\n    try:\n        server_final = server.process_client_final_message(client_final, username)\n        print(f\"Server final: {server_final}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n        exit()\n    \n    # Step 5: Client verifies server signature\n    try:\n        if client.verify_server_signature(server_final):\n            print(\"Authentication successful!\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["base64", "hmac", "re", "hashlib", "secrets", "os"], "package_breakdown": {"install_packages": ["base64", "hmac", "re", "hashlib", "secrets", "os"], "import_packages": ["base64", "hmac", "re", "hashlib", "secrets", "os"], "llm_suggested": ["hashlib", "hmac", "base64", "os", "secrets", "re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the backported tarfile module to extract all files from a tar archive named 'example.tar' into a specified directory 'output_folder'.", "prompt_index": 304, "generated_code": "```python\nimport tarfile\n\nwith tarfile.open('example.tar', 'r') as tar:\n    tar.extractall('output_folder')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tarfile"], "package_breakdown": {"install_packages": ["tarfile"], "import_packages": ["tarfile"], "llm_suggested": ["tarfile"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that imports the Jupyter core package and demonstrates the creation of a basic Jupyter notebook with markdown and code cells.", "prompt_index": 305, "generated_code": "```python\nimport nbformat as nbf\n\n# Create a new notebook object\nnb = nbf.v4.new_notebook()\n\n# Add cells to the notebook\ncells = [\n    nbf.v4.new_markdown_cell(\"# My Jupyter Notebook\\nThis is a simple Jupyter notebook created programmatically.\"),\n    nbf.v4.new_code_cell(\"import numpy as np\\nimport matplotlib.pyplot as plt\\n\\nprint('Hello from Jupyter!')\"),\n    nbf.v4.new_markdown_cell(\"## Data Visualization\\nLet's create a simple plot:\"),\n    nbf.v4.new_code_cell(\"x = np.linspace(0, 10, 100)\\ny = np.sin(x)\\n\\nplt.figure(figsize=(8, 4))\\nplt.plot(x, y)\\nplt.title('Sine Wave')\\nplt.xlabel('x')\\nplt.ylabel('sin(x)')\\nplt.grid(True)\\nplt.show()\")\n]\n\nnb['cells'] = cells\n\n# Write the notebook to a file\nwith open('example_notebook.ipynb', 'w') as f:\n    nbf.write(nb, f)\n\nprint(\"Notebook created successfully: example_notebook.ipynb\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["matplotlib", "nbformat", "numpy"], "package_breakdown": {"install_packages": ["matplotlib", "nbformat", "numpy"], "import_packages": ["nbformat"], "llm_suggested": ["nbformat", "numpy", "matplotlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that tokenizes text using the BPE tokenization algorithm provided by the tiktoken package, specifically for preparing text data to be used with OpenAI's models.", "prompt_index": 306, "generated_code": "```python\nimport tiktoken\n\ndef tokenize_text(text, model_name=\"gpt-4\"):\n    \"\"\"\n    Tokenize text using BPE tokenization for OpenAI models.\n    \n    Args:\n        text (str): Input text to tokenize\n        model_name (str): OpenAI model name (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n    \n    Returns:\n        list: List of token IDs\n    \"\"\"\n    try:\n        # Get the appropriate encoding for the model\n        encoding = tiktoken.encoding_for_model(model_name)\n        \n        # Tokenize the text\n        tokens = encoding.encode(text)\n        \n        return tokens\n        \n    except KeyError:\n        # Fallback to cl100k_base encoding if model not found\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n        tokens = encoding.encode(text)\n        return tokens\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_text = \"Hello, how are you today?\"\n    tokens = tokenize_text(sample_text, \"gpt-4\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token count: {len(tokens)}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tiktoken"], "package_breakdown": {"install_packages": ["tiktoken"], "import_packages": ["tiktoken"], "llm_suggested": ["tiktoken"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that tests asynchronous functions using the pytest framework with asyncio support.", "prompt_index": 307, "generated_code": "```python\nimport pytest\nimport asyncio\n\n# Example async function to test\nasync def async_add(a, b):\n    await asyncio.sleep(0.1)  # Simulate async operation\n    return a + b\n\n# Example async function that raises an exception\nasync def async_divide(a, b):\n    await asyncio.sleep(0.1)\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\n# Test cases using pytest-asyncio\n@pytest.mark.asyncio\nasync def test_async_add():\n    result = await async_add(2, 3)\n    assert result == 5\n\n@pytest.mark.asyncio\nasync def test_async_add_negative_numbers():\n    result = await async_add(-1, -5)\n    assert result == -6\n\n@pytest.mark.asyncio\nasync def test_async_divide_success():\n    result = await async_divide(10, 2)\n    assert result == 5.0\n\n@pytest.mark.asyncio\nasync def test_async_divide_by_zero():\n    with pytest.raises(ValueError, match=\"Cannot divide by zero\"):\n        await async_divide(10, 0)\n\n# Test with parametrize\n@pytest.mark.parametrize(\"a,b,expected\", [\n    (1, 2, 3),\n    (0, 0, 0),\n    (-1, 1, 0),\n])\n@pytest.mark.asyncio\nasync def test_async_add_parametrized(a, b, expected):\n    result = await async_add(a, b)\n    assert result == expected\n\n# Test with fixtures\n@pytest.fixture\nasync def setup_data():\n    data = {\"value\": 42}\n    await asyncio.sleep(0.1)  # Simulate async setup\n    yield data\n    await asyncio.sleep(0.1)  # Simulate async teardown\n\n@pytest.mark.asyncio\nasync def test_with_async_fixture(setup_data):\n    assert setup_data[\"value\"] == 42\n    result = await async_add(setup_data[\"value\"], 8)\n    assert result == 50\n```\n\n**Requirements:**\n- Install pytest and pytest-asyncio: `pip install pytest pytest-asyncio`\n- Run tests with: `pytest -v`\n- The `@pytest.mark.asyncio` decorator is required for async test functions\n- Use `await` to call async functions in tests\n- Use `pytest.raises` to test for expected exceptions in async functions\n- Async fixtures are supported with `async def` and `yield`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "pytest-asyncio", "pytest"], "package_breakdown": {"install_packages": ["asyncio", "pytest-asyncio", "pytest"], "import_packages": ["asyncio", "pytest"], "llm_suggested": ["pytest", "asyncio", "pytest-asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compares two version numbers using Semantic Versioning rules and returns whether the first version is greater than, equal to, or less than the second version.", "prompt_index": 308, "generated_code": "```python\ndef compare_versions(version1, version2):\n    \"\"\"\n    Compare two semantic version strings.\n    \n    Args:\n        version1 (str): First version string (e.g., \"1.2.3\")\n        version2 (str): Second version string (e.g., \"1.2.4\")\n    \n    Returns:\n        int: 1 if version1 > version2, -1 if version1 < version2, 0 if equal\n    \"\"\"\n    def parse_version(version):\n        # Split into main version and pre-release/build metadata\n        parts = version.split('-', 1)\n        main_version = parts[0]\n        \n        # Handle build metadata (ignored in comparisons)\n        if len(parts) > 1:\n            pre_release = parts[1].split('+', 1)[0]\n        else:\n            pre_release = None\n        \n        # Parse main version components\n        version_parts = main_version.split('.')\n        major = int(version_parts[0]) if len(version_parts) > 0 else 0\n        minor = int(version_parts[1]) if len(version_parts) > 1 else 0\n        patch = int(version_parts[2]) if len(version_parts) > 2 else 0\n        \n        # Parse pre-release identifiers if they exist\n        pre_release_ids = []\n        if pre_release:\n            pre_release_ids = pre_release.split('.')\n        \n        return (major, minor, patch, pre_release_ids)\n    \n    v1_major, v1_minor, v1_patch, v1_pre = parse_version(version1)\n    v2_major, v2_minor, v2_patch, v2_pre = parse_version(version2)\n    \n    # Compare main version numbers\n    if v1_major != v2_major:\n        return 1 if v1_major > v2_major else -1\n    \n    if v1_minor != v2_minor:\n        return 1 if v1_minor > v2_minor else -1\n    \n    if v1_patch != v2_patch:\n        return 1 if v1_patch > v2_patch else -1\n    \n    # Handle pre-release versions\n    # A version with pre-release has lower precedence than normal version\n    if v1_pre and not v2_pre:\n        return -1\n    elif not v1_pre and v2_pre:\n        return 1\n    elif v1_pre and v2_pre:\n        # Compare pre-release identifiers lexicographically\n        for i in range(min(len(v1_pre), len(v2_pre))):\n            # Try to compare as numbers if possible\n            try:\n                v1_num = int(v1_pre[i])\n                v2_num = int(v2_pre[i])\n                if v1_num != v2_num:\n                    return 1 if v1_num > v2_num else -1\n            except ValueError:\n                # Compare as strings\n                if v1_pre[i] != v2_pre[i]:\n                    return 1 if v1_pre[i] > v2_pre[i] else -1\n        \n        # If all identifiers are equal so far, the longer list has higher precedence\n        if len(v1_pre) != len(v2_pre):\n            return 1 if len(v1_pre) > len(v2_pre) else -1\n    \n    return 0\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the square root of a large decimal number with precision using the arbitrary-precision floating-point arithmetic library.", "prompt_index": 309, "generated_code": "```python\nfrom decimal import Decimal, getcontext\n\ndef calculate_sqrt_precise(number, precision=50):\n    getcontext().prec = precision + 2  # Add extra precision for intermediate calculations\n    num = Decimal(number)\n    return num.sqrt()\n\n# Example usage:\n# result = calculate_sqrt_precise(\"12345678901234567890.1234567890\")\n# print(result)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["decimal"], "package_breakdown": {"install_packages": ["decimal"], "import_packages": ["decimal"], "llm_suggested": ["decimal"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends custom metrics to Datadog's monitoring service using the Datadog Python library, including tags and timestamps for accurate tracking and visualization of performance data.", "prompt_index": 310, "generated_code": "```python\nfrom datadog import initialize, statsd\nfrom datetime import datetime\nimport time\n\n# Initialize the Datadog client\noptions = {\n    'statsd_host': 'localhost',\n    'statsd_port': 8125,\n}\n\ninitialize(**options)\n\n# Send custom metrics with tags and timestamp\ndef send_custom_metric(metric_name, value, tags=None, timestamp=None):\n    \"\"\"\n    Send a custom metric to Datadog\n    \n    Args:\n        metric_name (str): Name of the metric\n        value (float): Metric value\n        tags (list): List of tags in format ['tag1:value1', 'tag2:value2']\n        timestamp (int): Unix timestamp in seconds (optional)\n    \"\"\"\n    if tags is None:\n        tags = []\n    \n    if timestamp is None:\n        timestamp = int(time.time())\n    \n    statsd.gauge(\n        metric_name,\n        value,\n        tags=tags,\n        timestamp=timestamp\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Current timestamp\n    current_time = int(datetime.now().timestamp())\n    \n    # Send metrics with different tags and timestamps\n    send_custom_metric(\n        'app.performance.response_time',\n        45.2,\n        tags=['env:production', 'service:api', 'region:us-west-1'],\n        timestamp=current_time\n    )\n    \n    send_custom_metric(\n        'app.performance.memory_usage',\n        75.8,\n        tags=['env:staging', 'service:worker', 'region:eu-central-1'],\n        timestamp=current_time - 300  # 5 minutes ago\n    )\n    \n    send_custom_metric(\n        'app.performance.error_rate',\n        0.5,\n        tags=['env:production', 'service:database', 'severity:high'],\n        timestamp=current_time\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datadog", "datetime", "time"], "package_breakdown": {"install_packages": ["datadog", "datetime", "time"], "import_packages": ["datadog", "datetime", "time"], "llm_suggested": ["datadog", "datetime", "time"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates the syntax and deliverability of a given email address using the functionalities provided by the email address validation library.", "prompt_index": 311, "generated_code": "```python\nimport re\nfrom typing import Tuple\n\ndef validate_email(email: str) -> Tuple[bool, str]:\n    \"\"\"\n    Validates the syntax and deliverability of an email address.\n    \n    Args:\n        email (str): The email address to validate\n        \n    Returns:\n        Tuple[bool, str]: (is_valid, message) where is_valid indicates if the email\n        is valid, and message provides details about the validation result\n    \"\"\"\n    # Basic syntax validation using regex\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    \n    if not re.match(pattern, email):\n        return False, \"Invalid email syntax\"\n    \n    # Split into local part and domain\n    local_part, domain = email.split('@', 1)\n    \n    # Additional validation rules\n    if len(local_part) > 64:\n        return False, \"Local part exceeds 64 characters\"\n    \n    if len(domain) > 255:\n        return False, \"Domain exceeds 255 characters\"\n    \n    if len(email) > 254:\n        return False, \"Email address exceeds 254 characters\"\n    \n    # Check for consecutive dots\n    if '..' in local_part or '..' in domain:\n        return False, \"Consecutive dots are not allowed\"\n    \n    # Check domain structure (at least one dot in domain)\n    if '.' not in domain:\n        return False, \"Invalid domain format\"\n    \n    # Check TLD length (at least 2 characters)\n    tld = domain.split('.')[-1]\n    if len(tld) < 2:\n        return False, \"TLD must be at least 2 characters\"\n    \n    return True, \"Email address is valid\"\n```\n\n```python\n# Example usage:\nif __name__ == \"__main__\":\n    test_emails = [\n        \"test@example.com\",\n        \"invalid-email\",\n        \"user@.com\",\n        \"a\" * 65 + \"@example.com\",\n        \"test@sub.domain.co.uk\"\n    ]\n    \n    for email in test_emails:\n        is_valid, message = validate_email(email)\n        print(f\"{email}: {message}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "typing"], "package_breakdown": {"install_packages": ["re", "typing"], "import_packages": ["re", "typing"], "llm_suggested": ["re", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an immutable list using the functions provided in the package for persistent and functional data structures.", "prompt_index": 312, "generated_code": "```python\nfrom pyrsistent import plist\n\n# Create an immutable list\nimmutable_list = plist([1, 2, 3, 4, 5])\n\n# Example operations (all return new lists, original remains unchanged)\nnew_list = immutable_list.append(6)\nanother_list = immutable_list.extend([7, 8])\nmodified_list = immutable_list.set(0, 10)\nsliced_list = immutable_list[1:4]\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyrsistent"], "package_breakdown": {"install_packages": ["pyrsistent"], "import_packages": ["pyrsistent"], "llm_suggested": ["pyrsistent"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that scans a given Python program file for potential syntax errors, style violations, and code inefficiencies without actually executing the code.", "prompt_index": 313, "generated_code": "```python\nimport ast\nimport sys\nimport os\nfrom pylint import epylint as lint\nfrom flake8.api import legacy as flake8\nimport bandit\nfrom bandit.core import manager as bandit_manager\n\ndef analyze_python_file(file_path):\n    \"\"\"\n    Analyze a Python file for syntax errors, style violations, and code inefficiencies.\n    \n    Args:\n        file_path (str): Path to the Python file to analyze\n    \n    Returns:\n        dict: Dictionary containing analysis results\n    \"\"\"\n    \n    if not os.path.exists(file_path):\n        return {\"error\": f\"File not found: {file_path}\"}\n    \n    if not file_path.endswith('.py'):\n        return {\"error\": \"File is not a Python file\"}\n    \n    results = {\n        'syntax_errors': [],\n        'style_violations': [],\n        'security_issues': [],\n        'code_smells': []\n    }\n    \n    # 1. Check for basic syntax errors using AST\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            source_code = file.read()\n        ast.parse(source_code)\n    except SyntaxError as e:\n        results['syntax_errors'].append({\n            'line': e.lineno,\n            'message': f\"SyntaxError: {e.msg}\",\n            'offset': e.offset\n        })\n    \n    # 2. Run pylint for style violations and code quality\n    try:\n        (pylint_stdout, pylint_stderr) = lint.py_run(file_path, return_std=True)\n        if pylint_stdout.getvalue():\n            results['style_violations'].extend(\n                line.strip() for line in pylint_stdout.getvalue().split('\\n') \n                if line.strip() and ':' in line\n            )\n    except Exception as e:\n        results['style_violations'].append(f\"Pylint error: {str(e)}\")\n    \n    # 3. Run flake8 for PEP8 compliance and style issues\n    try:\n        style_guide = flake8.get_style_guide()\n        report = style_guide.check_files([file_path])\n        if report.get_statistics(''):\n            results['style_violations'].extend(report.get_statistics(''))\n    except Exception as e:\n        results['style_violations'].append(f\"Flake8 error: {str(e)}\")\n    \n    # 4. Run bandit for security issues\n    try:\n        b_mgr = bandit_manager.BanditManager()\n        b_mgr.discover_files([file_path], None)\n        b_mgr.run_tests()\n        issues = b_mgr.get_issue_list()\n        for issue in issues:\n            results['security_issues'].append({\n                'severity': issue.severity,\n                'confidence': issue.confidence,\n                'line': issue.lineno,\n                'message': issue.text\n            })\n    except Exception as e:\n        results['security_issues'].append(f\"Bandit error: {str(e)}\")\n    \n    # 5. Additional static analysis for common inefficiencies\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            lines = file.readlines()\n        \n        for i, line in enumerate(lines, 1):\n            line = line.strip()\n            \n            # Check for empty except clauses\n            if 'except:' in line and 'except Exception:' not in line:\n                results['code_smells'].append({\n                    'line': i,\n                    'message': \"Bare except clause - consider specifying exception types\"\n                })\n            \n            # Check for wildcard imports\n            if 'from' in line and 'import *' in line:\n                results['code_smells'].append({\n                    'line': i,\n                    'message': \"Wildcard import - import specific functions/classes instead\"\n                })\n            \n            # Check for mutable default arguments\n            if 'def ' in line and '=[' in line.split('=')[-1]:\n                results['code_smells'].append({\n                    'line': i,\n                    'message': \"Potential mutable default argument - use None instead\"\n                })\n                \n    except Exception as e:\n        results['code_smells'].append(f\"Static analysis error: {str(e)}\")\n    \n    return results\n\ndef print_analysis_results(results):\n    \"\"\"Print the analysis results in a formatted way.\"\"\"\n    print(\"=\" * 60)\n    print(\"PYTHON CODE ANALYSIS REPORT\")\n    print(\"=\" * 60)\n    \n    for category, issues in results.items():\n        if issues and not category.startswith('error'):\n            print(f\"\\n{category.upper().replace('_', ' ')}:\")\n            print(\"-\" * 40)\n            for issue in issues:\n                if isinstance(issue, dict):\n                    print(f\"Line {issue.get('line', 'N/A')}: {issue.get('message', 'No message')}\")\n                else:\n                    print(issue)\n    \n    if not any(len(issues) > 0 for category, issues in results.items() \n              if not category.startswith('error') and category != 'code_smells'):\n        print(\"\\nNo issues found!\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python code_analyzer.py <python_file_path>\")\n        sys.exit(1)\n    \n    file_to_analyze = sys.argv[1]\n    analysis_results = analyze_python_file(file_to_analyze)\n    print_analysis_results(analysis_results)\n```\n\nTo use this code, you'll need to install the required dependencies:\n\n```bash\npip install pylint flake8 bandit\n```\n\nThis script provides comprehensive static analysis including:\n- Syntax error detection using AST parsing\n- Style violations using Pylint and Flake8\n- Security issues using Bandit\n- Common code inefficiencies through custom static analysis\n\nThe analysis runs without executing the code and provides detailed reports for each category of issues found.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["flake8", "ast", "bandit", "sys", "pylint", "os"], "package_breakdown": {"install_packages": ["flake8", "ast", "bandit", "sys", "pylint", "os"], "import_packages": ["flake8", "ast", "bandit", "sys", "pylint", "os"], "llm_suggested": ["ast", "sys", "os", "pylint", "flake8", "bandit"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a given date and time string into a more readable and user-friendly format using the functionalities provided by the Better dates & times package.", "prompt_index": 314, "generated_code": "```python\nfrom babel.dates import format_datetime\nfrom datetime import datetime\n\ndef format_date_time(date_time_str, input_format, locale='en'):\n    \"\"\"\n    Convert a date-time string to a more readable format using Babel.\n    \n    Args:\n        date_time_str (str): Input date-time string\n        input_format (str): Format of the input string (e.g., '%Y-%m-%d %H:%M:%S')\n        locale (str): Locale for formatting (default: 'en')\n    \n    Returns:\n        str: Formatted date-time string\n    \"\"\"\n    dt = datetime.strptime(date_time_str, input_format)\n    return format_datetime(dt, locale=locale)\n\n# Example usage:\n# formatted = format_date_time('2023-12-25 14:30:00', '%Y-%m-%d %H:%M:%S')\n# Result: 'December 25, 2023 at 2:30:00 PM'\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["babel", "datetime"], "package_breakdown": {"install_packages": ["babel", "datetime"], "import_packages": ["babel", "datetime"], "llm_suggested": ["babel", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to an OpenSearch server, performs a search query, and retrieves the search results.", "prompt_index": 315, "generated_code": "```python\nfrom opensearchpy import OpenSearch\n\n# Configuration\nhost = 'localhost'\nport = 9200\nauth = ('admin', 'admin')  # For testing only. For production, use environment variables or secure config.\nindex_name = 'your_index_name'\n\n# Create the client\nclient = OpenSearch(\n    hosts=[{'host': host, 'port': port}],\n    http_compress=True,  # enables gzip compression for request bodies\n    http_auth=auth,\n    use_ssl=False,\n    verify_certs=False,\n    ssl_show_warn=False,\n)\n\n# Define search query\nquery = {\n    \"size\": 10,\n    \"query\": {\n        \"multi_match\": {\n            \"query\": \"search terms\",\n            \"fields\": [\"title^2\", \"content\"]\n        }\n    }\n}\n\n# Execute search\nresponse = client.search(\n    body=query,\n    index=index_name\n)\n\n# Process results\nfor hit in response['hits']['hits']:\n    print(f\"Score: {hit['_score']}\")\n    print(f\"Document: {hit['_source']}\")\n    print(\"-\" * 50)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["opensearchpy"], "packages_detected": ["opensearch-py", "opensearchpy"], "package_breakdown": {"install_packages": ["opensearch-py", "opensearchpy"], "import_packages": ["opensearchpy"], "llm_suggested": ["opensearch-py"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes JSONPath to extract specific data from a JSON file, including using arithmetic and binary comparison operators, and stores the extracted data in a variable for further manipulation or analysis.", "prompt_index": 316, "generated_code": "```python\nimport json\nfrom jsonpath_ng import jsonpath, parse\n\n# Load JSON data from file\nwith open('data.json', 'r') as file:\n    json_data = json.load(file)\n\n# Define JSONPath expression with arithmetic and comparison operators\n# Example: Extract items where price is greater than 100 and calculate discounted price\njsonpath_expr = parse(\"$.store.book[?(@.price > 100)].price - 10\")\n\n# Execute the JSONPath expression\nextracted_data = [match.value for match in jsonpath_expr.find(json_data)]\n\n# Store extracted data in variable for further manipulation\nprocessed_data = extracted_data\n\n# Example of further manipulation\nif processed_data:\n    average_price = sum(processed_data) / len(processed_data)\n    print(f\"Extracted discounted prices: {processed_data}\")\n    print(f\"Average discounted price: {average_price:.2f}\")\nelse:\n    print(\"No matching data found\")\n```\n\nNote: This requires installing the `jsonpath-ng` package:\n```bash\npip install jsonpath-ng\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["jsonpath_ng"], "packages_detected": ["json", "jsonpath-ng", "jsonpath_ng"], "package_breakdown": {"install_packages": ["json", "jsonpath-ng", "jsonpath_ng"], "import_packages": ["json", "jsonpath_ng"], "llm_suggested": ["jsonpath-ng", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the deep learning capabilities of the package to train a neural network on a dataset of handwritten digits for image classification tasks.", "prompt_index": 317, "generated_code": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\n\n# Load and preprocess the MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize pixel values to [0, 1]\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Convert labels to one-hot encoded format\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Create the neural network model\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to 1D array\n    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n    Dense(64, activation='relu'),   # Hidden layer with 64 neurons\n    Dense(10, activation='softmax') # Output layer with 10 neurons (digits 0-9)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2,\n                    verbose=1)\n\n# Evaluate the model on test data\ntest_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n\n# Save the trained model\nmodel.save('mnist_classifier.h5')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tensorflow"], "package_breakdown": {"install_packages": ["tensorflow"], "import_packages": ["tensorflow"], "llm_suggested": ["tensorflow"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that monitors a specified directory for any changes (such as file creations, deletions, or modifications) and logs the details of each event in real-time.", "prompt_index": 318, "generated_code": "```python\nimport os\nimport time\nimport logging\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\n\nclass DirectoryMonitorHandler(FileSystemEventHandler):\n    def __init__(self, log_file='directory_monitor.log'):\n        logging.basicConfig(\n            filename=log_file,\n            level=logging.INFO,\n            format='%(asctime)s - %(message)s',\n            datefmt='%Y-%m-%d %H:%M:%S'\n        )\n        self.logger = logging.getLogger()\n    \n    def on_created(self, event):\n        if event.is_directory:\n            self.logger.info(f\"Directory created: {event.src_path}\")\n        else:\n            self.logger.info(f\"File created: {event.src_path}\")\n    \n    def on_deleted(self, event):\n        if event.is_directory:\n            self.logger.info(f\"Directory deleted: {event.src_path}\")\n        else:\n            self.logger.info(f\"File deleted: {event.src_path}\")\n    \n    def on_modified(self, event):\n        if event.is_directory:\n            self.logger.info(f\"Directory modified: {event.src_path}\")\n        else:\n            self.logger.info(f\"File modified: {event.src_path}\")\n    \n    def on_moved(self, event):\n        if event.is_directory:\n            self.logger.info(f\"Directory moved: {event.src_path} -> {event.dest_path}\")\n        else:\n            self.logger.info(f\"File moved: {event.src_path} -> {event.dest_path}\")\n\ndef monitor_directory(directory_path):\n    if not os.path.exists(directory_path):\n        print(f\"Error: Directory '{directory_path}' does not exist\")\n        return\n    \n    if not os.path.isdir(directory_path):\n        print(f\"Error: '{directory_path}' is not a directory\")\n        return\n    \n    event_handler = DirectoryMonitorHandler()\n    observer = Observer()\n    observer.schedule(event_handler, directory_path, recursive=True)\n    \n    observer.start()\n    print(f\"Monitoring directory: {directory_path}\")\n    print(\"Press Ctrl+C to stop monitoring...\")\n    \n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    \n    observer.join()\n\nif __name__ == \"__main__\":\n    # Specify the directory to monitor\n    directory_to_watch = \"/path/to/your/directory\"  # Change this to your target directory\n    \n    monitor_directory(directory_to_watch)\n```\n\n**Note:** Before running this code, you need to install the required dependency:\n```bash\npip install watchdog\n```\n\n**Usage:**\n1. Replace `/path/to/your/directory` with the actual path you want to monitor\n2. Run the script\n3. Events will be logged to `directory_monitor.log` in the same directory\n4. Press Ctrl+C to stop monitoring\n\nThe code will track file/directory creations, deletions, modifications, and moves, logging each event with a timestamp to the specified log file.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["watchdog", "time", "os", "logging"], "package_breakdown": {"install_packages": ["watchdog", "time", "os", "logging"], "import_packages": ["watchdog", "time", "os", "logging"], "llm_suggested": ["watchdog"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Jupyter notebook server using the Jupyter protocol implementation and client libraries, retrieves information about the running notebooks, and displays a list of active sessions with their respective kernel IDs.", "prompt_index": 319, "generated_code": "```python\nimport requests\nimport json\nfrom jupyter_client import KernelClient\nfrom jupyter_client.manager import KernelManager\n\ndef get_active_sessions(base_url, token=None):\n    \"\"\"\n    Retrieve active sessions from Jupyter server\n    \n    Args:\n        base_url (str): Base URL of the Jupyter server\n        token (str): Authentication token (if required)\n    \n    Returns:\n        list: List of active sessions with kernel information\n    \"\"\"\n    sessions_url = f\"{base_url.rstrip('/')}/api/sessions\"\n    \n    headers = {'Content-Type': 'application/json'}\n    if token:\n        headers['Authorization'] = f'token {token}'\n    \n    try:\n        response = requests.get(sessions_url, headers=headers)\n        response.raise_for_status()\n        sessions = response.json()\n        \n        active_sessions = []\n        for session in sessions:\n            session_info = {\n                'notebook_path': session.get('path', 'N/A'),\n                'kernel_id': session.get('kernel', {}).get('id', 'N/A'),\n                'session_id': session.get('id', 'N/A'),\n                'kernel_name': session.get('kernel', {}).get('name', 'N/A')\n            }\n            active_sessions.append(session_info)\n        \n        return active_sessions\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"Error connecting to Jupyter server: {e}\")\n        return []\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing response: {e}\")\n        return []\n\ndef display_active_sessions(base_url, token=None):\n    \"\"\"\n    Display active sessions from Jupyter server\n    \n    Args:\n        base_url (str): Base URL of the Jupyter server\n        token (str): Authentication token (if required)\n    \"\"\"\n    sessions = get_active_sessions(base_url, token)\n    \n    if not sessions:\n        print(\"No active sessions found or unable to connect to server.\")\n        return\n    \n    print(\"\\nActive Jupyter Notebook Sessions:\")\n    print(\"-\" * 80)\n    print(f\"{'Notebook Path':<40} {'Kernel ID':<30} {'Kernel Name':<15}\")\n    print(\"-\" * 80)\n    \n    for session in sessions:\n        print(f\"{session['notebook_path'][:38]:<40} {session['kernel_id'][:28]:<30} {session['kernel_name'][:13]:<15}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your Jupyter server details\n    BASE_URL = \"http://localhost:8888\"  # Default Jupyter URL\n    TOKEN = None  # Set to your token if authentication is required\n    \n    display_active_sessions(BASE_URL, TOKEN)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["jupyter_client"], "packages_detected": ["jupyter_client", "requests", "json"], "package_breakdown": {"install_packages": ["jupyter_client", "requests", "json"], "import_packages": ["requests", "json", "jupyter_client"], "llm_suggested": ["requests", "jupyter_client"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a string input representing a date and time in RFC3339 format and validates if it complies with the RFC3339 standard using the pure python RFC3339 validator package.", "prompt_index": 320, "generated_code": "```python\nimport rfc3339\n\ndef validate_rfc3339(datetime_str):\n    try:\n        rfc3339.rfc3339_to_datetime(datetime_str)\n        return True\n    except ValueError:\n        return False\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["rfc3339"], "package_breakdown": {"install_packages": ["rfc3339"], "import_packages": ["rfc3339"], "llm_suggested": ["rfc3339"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that installs the IPython kernel for Jupyter and sets it as the default kernel for a specified Jupyter notebook, allowing for interactive Python code execution within the notebook.", "prompt_index": 321, "generated_code": "```python\nimport subprocess\nimport sys\nimport json\nimport os\n\ndef install_ipython_kernel():\n    \"\"\"Install IPython kernel for Jupyter\"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipykernel\"])\n        print(\"IPython kernel installed successfully\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing IPython kernel: {e}\")\n        return False\n\ndef set_default_kernel(kernel_name=\"python3\"):\n    \"\"\"Set the default kernel for Jupyter notebooks\"\"\"\n    try:\n        # Get Jupyter config directory\n        config_dir = subprocess.check_output([sys.executable, \"-m\", \"jupyter\", \"--config-dir\"]).decode().strip()\n        \n        # Create or update jupyter_notebook_config.py\n        config_file = os.path.join(config_dir, \"jupyter_notebook_config.py\")\n        \n        config_content = f\"\"\"\n# Set default kernel\nc.NotebookApp.default_kernel_name = '{kernel_name}'\n\"\"\"\n        \n        with open(config_file, 'a') as f:\n            f.write(config_content)\n        \n        print(f\"Default kernel set to '{kernel_name}'\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error setting default kernel: {e}\")\n        return False\n\ndef main():\n    # Install IPython kernel\n    if not install_ipython_kernel():\n        sys.exit(1)\n    \n    # Set default kernel (usually 'python3' for IPython kernel)\n    if not set_default_kernel(\"python3\"):\n        sys.exit(1)\n    \n    print(\"Setup completed successfully!\")\n    print(\"You can now start Jupyter notebook with: jupyter notebook\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "sys", "json", "os"], "package_breakdown": {"install_packages": ["subprocess", "sys", "json", "os"], "import_packages": ["sys", "os", "subprocess", "json"], "llm_suggested": ["subprocess", "sys", "json", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the itertools and builtins modules to work with asynchronous operations in AsyncIO, combining both synchronous and asynchronous iterables for efficient data processing.", "prompt_index": 322, "generated_code": "```python\nimport asyncio\nimport itertools\nfrom builtins import filter, map, zip\nfrom typing import AsyncIterable, Iterable, TypeVar, Any, Callable, Union\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nasync def async_enumerate(async_iterable: AsyncIterable[T], start: int = 0) -> AsyncIterable[tuple[int, T]]:\n    \"\"\"Async version of enumerate for async iterables\"\"\"\n    index = start\n    async for item in async_iterable:\n        yield index, item\n        index += 1\n\nasync def async_filter(predicate: Callable[[T], Union[bool, Any]], \n                      async_iterable: AsyncIterable[T]) -> AsyncIterable[T]:\n    \"\"\"Async version of filter for async iterables\"\"\"\n    async for item in async_iterable:\n        if predicate(item):\n            yield item\n\nasync def async_map(func: Callable[[T], U], \n                   async_iterable: AsyncIterable[T]) -> AsyncIterable[U]:\n    \"\"\"Async version of map for async iterables\"\"\"\n    async for item in async_iterable:\n        yield func(item)\n\nasync def async_zip(*async_iterables: AsyncIterable) -> AsyncIterable[tuple]:\n    \"\"\"Async version of zip for async iterables\"\"\"\n    iterators = [aiter(ait) for ait in async_iterables]\n    try:\n        while True:\n            items = []\n            for it in iterators:\n                items.append(await anext(it))\n            yield tuple(items)\n    except StopAsyncIteration:\n        pass\n\nasync def merge_async_iterables(*async_iterables: AsyncIterable[T]) -> AsyncIterable[T]:\n    \"\"\"Merge multiple async iterables into a single async iterable\"\"\"\n    async def _consume(queue: asyncio.Queue, aiterable: AsyncIterable[T]):\n        async for item in aiterable:\n            await queue.put(item)\n        await queue.put(None)  # Sentinel\n    \n    queue = asyncio.Queue()\n    tasks = [asyncio.create_task(_consume(queue, ait)) for ait in async_iterables]\n    active_tasks = len(tasks)\n    \n    while active_tasks > 0:\n        item = await queue.get()\n        if item is None:\n            active_tasks -= 1\n        else:\n            yield item\n    \n    for task in tasks:\n        task.cancel()\n\nasync def sync_to_async_iterable(sync_iterable: Iterable[T]) -> AsyncIterable[T]:\n    \"\"\"Convert a synchronous iterable to an async iterable\"\"\"\n    for item in sync_iterable:\n        yield item\n        await asyncio.sleep(0)  # Yield control to event loop\n\nasync def process_data_async(\n    async_data: AsyncIterable[T],\n    sync_data: Iterable[U],\n    process_func: Callable[[T, U], Any]\n) -> AsyncIterable[Any]:\n    \"\"\"\n    Combine async and sync iterables for efficient processing\n    using itertools.chain and async operations\n    \"\"\"\n    # Convert sync iterable to async\n    async_sync_data = sync_to_async_iterable(sync_data)\n    \n    # Use itertools.chain to combine multiple sync iterables if needed\n    # (example with empty additional sync iterable)\n    combined_sync = itertools.chain(sync_data, [])\n    \n    # Process data asynchronously\n    async for async_item, sync_item in async_zip(async_data, async_sync_data):\n        result = process_func(async_item, sync_item)\n        yield result\n\n# Example usage\nasync def example_usage():\n    async def async_data_source():\n        for i in range(5):\n            await asyncio.sleep(0.1)\n            yield i\n    \n    sync_data = [10, 20, 30, 40, 50]\n    \n    def process_func(a, b):\n        return a + b\n    \n    async for result in process_data_async(async_data_source(), sync_data, process_func):\n        print(f\"Processed result: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "builtins", "itertools", "typing"], "package_breakdown": {"install_packages": ["asyncio", "builtins", "itertools", "typing"], "import_packages": ["asyncio", "builtins", "itertools", "typing"], "llm_suggested": ["asyncio", "typing", "itertools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to the Databricks platform using the Databricks SDK for Python (Beta) and retrieves information about clusters, jobs, and notebooks.", "prompt_index": 323, "generated_code": "```python\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service import compute, jobs, workspace\n\ndef get_databricks_info():\n    # Initialize the Databricks workspace client\n    # This will use the default authentication method (env vars, .databrickscfg, etc.)\n    w = WorkspaceClient()\n    \n    # Get cluster information\n    print(\"=== CLUSTERS ===\")\n    try:\n        clusters = w.clusters.list()\n        for cluster in clusters:\n            print(f\"Cluster: {cluster.cluster_name} (ID: {cluster.cluster_id}, State: {cluster.state})\")\n    except Exception as e:\n        print(f\"Error retrieving clusters: {e}\")\n    \n    # Get job information\n    print(\"\\n=== JOBS ===\")\n    try:\n        job_list = w.jobs.list()\n        for job in job_list:\n            print(f\"Job: {job.settings.name} (ID: {job.job_id})\")\n    except Exception as e:\n        print(f\"Error retrieving jobs: {e}\")\n    \n    # Get notebook information (from workspace root)\n    print(\"\\n=== NOTEBOOKS ===\")\n    try:\n        notebooks = w.workspace.list('/')\n        for item in notebooks:\n            if item.object_type == workspace.ObjectType.NOTEBOOK:\n                print(f\"Notebook: {item.path}\")\n    except Exception as e:\n        print(f\"Error retrieving notebooks: {e}\")\n\nif __name__ == \"__main__\":\n    get_databricks_info()\n```\n\n**Note:** Before running this code, you need to:\n1. Install the Databricks SDK: `pip install databricks-sdk`\n2. Set up authentication (environment variables, `.databrickscfg` file, or other supported methods)\n3. The code assumes you have appropriate permissions to access clusters, jobs, and notebooks", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["databricks", "databricks-sdk"], "package_breakdown": {"install_packages": ["databricks", "databricks-sdk"], "import_packages": ["databricks"], "llm_suggested": ["databricks-sdk"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves the latest messages from a specific Slack channel using the Slack API Platform SDK for Python.", "prompt_index": 324, "generated_code": "```python\nimport os\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\n\n# Initialize the client with your bot token\nclient = WebClient(token=os.environ['SLACK_BOT_TOKEN'])\n\ntry:\n    # Replace 'CHANNEL_ID' with the actual channel ID\n    response = client.conversations_history(\n        channel='CHANNEL_ID',\n        limit=10  # Number of latest messages to retrieve\n    )\n    \n    messages = response['messages']\n    for message in messages:\n        print(f\"User: {message.get('user', 'Unknown')}\")\n        print(f\"Text: {message.get('text', 'No text')}\")\n        print(f\"Timestamp: {message.get('ts', 'No timestamp')}\")\n        print(\"-\" * 50)\n        \nexcept SlackApiError as e:\n    print(f\"Error retrieving messages: {e.response['error']}\")\n```\n\n**Note**: Before running this code:\n1. Install the Slack SDK: `pip install slack_sdk`\n2. Set your bot token as an environment variable: `export SLACK_BOT_TOKEN='your-bot-token'`\n3. Replace 'CHANNEL_ID' with the actual Slack channel ID\n4. Ensure your bot has the required permissions (channels:history scope)", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["slack_sdk"], "packages_detected": ["os", "slack_sdk"], "package_breakdown": {"install_packages": ["os", "slack_sdk"], "import_packages": ["os", "slack_sdk"], "llm_suggested": ["slack_sdk"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that demonstrates how to utilize the package for improved parallel processing and concurrency in Python, specifically showcasing how to efficiently distribute tasks across multiple processes or threads.", "prompt_index": 325, "generated_code": "```python\nimport concurrent.futures\nimport time\nimport math\n\ndef is_prime(n):\n    \"\"\"Check if a number is prime (CPU-intensive task)\"\"\"\n    if n < 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    sqrt_n = int(math.floor(math.sqrt(n)))\n    for i in range(3, sqrt_n + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\ndef process_numbers_sequential(numbers):\n    \"\"\"Process numbers sequentially\"\"\"\n    start_time = time.time()\n    results = [is_prime(n) for n in numbers]\n    end_time = time.time()\n    return results, end_time - start_time\n\ndef process_numbers_threadpool(numbers):\n    \"\"\"Process numbers using ThreadPoolExecutor (I/O-bound tasks)\"\"\"\n    start_time = time.time()\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        results = list(executor.map(is_prime, numbers))\n    end_time = time.time()\n    return results, end_time - start_time\n\ndef process_numbers_processpool(numbers):\n    \"\"\"Process numbers using ProcessPoolExecutor (CPU-bound tasks)\"\"\"\n    start_time = time.time()\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        results = list(executor.map(is_prime, numbers))\n    end_time = time.time()\n    return results, end_time - start_time\n\nif __name__ == \"__main__\":\n    # Generate test data\n    numbers_to_check = [112272535095293, 112582705942171, 115280095190773,\n                        115797848077099, 1099726899285419, 112272535095293,\n                        112582705942171, 115280095190773, 115797848077099,\n                        1099726899285419, 112272535095293, 112582705942171,\n                        115280095190773, 115797848077099, 1099726899285419]\n    \n    print(\"Testing different parallel processing approaches...\")\n    print(f\"Processing {len(numbers_to_check)} numbers\\n\")\n    \n    # Sequential processing\n    seq_results, seq_time = process_numbers_sequential(numbers_to_check)\n    print(f\"Sequential processing time: {seq_time:.4f} seconds\")\n    \n    # Thread pool (better for I/O-bound tasks)\n    thread_results, thread_time = process_numbers_threadpool(numbers_to_check)\n    print(f\"ThreadPoolExecutor time: {thread_time:.4f} seconds\")\n    \n    # Process pool (better for CPU-bound tasks)\n    process_results, process_time = process_numbers_processpool(numbers_to_check)\n    print(f\"ProcessPoolExecutor time: {process_time:.4f} seconds\")\n    \n    # Verify all methods return same results\n    assert seq_results == thread_results == process_results, \"Results mismatch!\"\n    \n    print(f\"\\nSpeedup with ProcessPool: {seq_time/process_time:.2f}x\")\n    print(\"All methods returned identical results!\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 2, "hallucinations": ["concurrent", "concurrentfutures"], "packages_detected": ["math", "concurrent", "concurrentfutures", "time"], "package_breakdown": {"install_packages": ["math", "concurrent", "concurrentfutures", "time"], "import_packages": ["time", "math", "concurrent"], "llm_suggested": ["concurrentfutures", "time", "math"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a Markdown file, applies a custom plugin to highlight code blocks, and renders the parsed Markdown into HTML with syntax highlighting for code snippets.", "prompt_index": 326, "generated_code": "```python\nimport markdown\nfrom markdown.extensions.codehilite import CodeHiliteExtension\nfrom markdown.extensions.fenced_code import FencedCodeExtension\n\nclass CustomCodeHilitePlugin(CodeHiliteExtension):\n    \"\"\"Custom plugin that extends CodeHilite for additional highlighting control\"\"\"\n    \n    def __init__(self, **kwargs):\n        # Set default options if not provided\n        kwargs.setdefault('css_class', 'highlight')\n        kwargs.setdefault('guess_lang', True)\n        kwargs.setdefault('use_pygments', True)\n        super().__init__(**kwargs)\n\ndef parse_markdown_with_highlighting(markdown_text, **hilite_kwargs):\n    \"\"\"\n    Parse Markdown text with syntax highlighting for code blocks\n    \n    Args:\n        markdown_text (str): The Markdown content to parse\n        **hilite_kwargs: Additional options for the code highlighting plugin\n    \n    Returns:\n        str: Rendered HTML with syntax highlighted code blocks\n    \"\"\"\n    # Create the custom plugin instance\n    custom_hilite = CustomCodeHilitePlugin(**hilite_kwargs)\n    \n    # Configure Markdown with required extensions\n    extensions = [\n        FencedCodeExtension(),  # For fenced code blocks\n        custom_hilite,          # Custom syntax highlighting\n        'extra'                 # Additional standard Markdown features\n    ]\n    \n    # Parse and render the Markdown\n    html_output = markdown.markdown(markdown_text, extensions=extensions)\n    \n    return html_output\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample Markdown content with code blocks\n    sample_markdown = \"\"\"\n# Sample Markdown with Code\n\nHere's some Python code:\n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n    return 42\n```\n\nAnd some JavaScript:\n\n```javascript\nfunction greet() {\n    console.log(\"Hello from JS!\");\n}\n```\n\"\"\"\n    \n    # Parse with custom highlighting\n    html_result = parse_markdown_with_highlighting(\n        sample_markdown,\n        css_class='code-highlight',  # Custom CSS class\n        linenums=True               # Enable line numbers\n    )\n    \n    print(html_result)\n```\n\n**Required dependencies:**\n```bash\npip install markdown pygments\n```\n\n**Additional notes:**\n- This code uses Pygments for syntax highlighting (install with `pip install pygments`)\n- The custom plugin allows you to configure highlighting options like CSS classes, line numbers, etc.\n- Make sure to include Pygments CSS styles in your HTML for proper coloring\n- You can extend `CustomCodeHilitePlugin` further for additional customization", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["markdown", "pygments"], "package_breakdown": {"install_packages": ["markdown", "pygments"], "import_packages": ["markdown"], "llm_suggested": ["markdown", "pygments"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that serializes dataclasses instances to JSON format and then deserializes them back to dataclasses instances.", "prompt_index": 327, "generated_code": "```python\nimport json\nfrom dataclasses import dataclass, asdict, is_dataclass\nfrom typing import Any, Type, TypeVar\n\nT = TypeVar('T')\n\ndef dataclass_to_dict(obj: Any) -> Any:\n    \"\"\"Recursively convert dataclass instances to dictionaries.\"\"\"\n    if is_dataclass(obj):\n        result = {}\n        for field in obj.__dataclass_fields__:\n            value = getattr(obj, field)\n            result[field] = dataclass_to_dict(value)\n        return result\n    elif isinstance(obj, list):\n        return [dataclass_to_dict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: dataclass_to_dict(value) for key, value in obj.items()}\n    else:\n        return obj\n\ndef dict_to_dataclass(data: dict, cls: Type[T]) -> T:\n    \"\"\"Recursively convert dictionary to dataclass instance.\"\"\"\n    if not is_dataclass(cls):\n        return data\n    \n    field_types = {field.name: field.type for field in cls.__dataclass_fields__.values()}\n    \n    init_fields = {}\n    for field_name, field_type in field_types.items():\n        if field_name in data:\n            value = data[field_name]\n            \n            # Handle nested dataclasses\n            if is_dataclass(field_type):\n                init_fields[field_name] = dict_to_dataclass(value, field_type)\n            # Handle lists of dataclasses\n            elif (hasattr(field_type, '__origin__') and \n                  field_type.__origin__ is list and\n                  len(field_type.__args__) > 0 and\n                  is_dataclass(field_type.__args__[0])):\n                item_cls = field_type.__args__[0]\n                init_fields[field_name] = [dict_to_dataclass(item, item_cls) for item in value]\n            else:\n                init_fields[field_name] = value\n    \n    return cls(**init_fields)\n\ndef serialize_dataclass(obj: Any) -> str:\n    \"\"\"Serialize dataclass instance to JSON string.\"\"\"\n    return json.dumps(dataclass_to_dict(obj), indent=2)\n\ndef deserialize_dataclass(json_str: str, cls: Type[T]) -> T:\n    \"\"\"Deserialize JSON string to dataclass instance.\"\"\"\n    data = json.loads(json_str)\n    return dict_to_dataclass(data, cls)\n\n# Example usage\nif __name__ == \"__main__\":\n    @dataclass\n    class Address:\n        street: str\n        city: str\n        zip_code: str\n    \n    @dataclass\n    class Person:\n        name: str\n        age: int\n        address: Address\n        hobbies: list[str]\n    \n    # Create instance\n    person = Person(\n        name=\"John Doe\",\n        age=30,\n        address=Address(street=\"123 Main St\", city=\"Anytown\", zip_code=\"12345\"),\n        hobbies=[\"reading\", \"swimming\"]\n    )\n    \n    # Serialize to JSON\n    json_str = serialize_dataclass(person)\n    print(\"Serialized JSON:\")\n    print(json_str)\n    \n    # Deserialize back to dataclass\n    deserialized_person = deserialize_dataclass(json_str, Person)\n    print(\"\\nDeserialized object:\")\n    print(deserialized_person)\n    print(f\"Type: {type(deserialized_person)}\")\n    print(f\"Name: {deserialized_person.name}\")\n    print(f\"Address: {deserialized_person.address}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "json", "dataclasses"], "package_breakdown": {"install_packages": ["typing", "json", "dataclasses"], "import_packages": ["json", "dataclasses", "typing"], "llm_suggested": ["dataclasses", "typing", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a mock object using the mock package and utilizes pytest fixtures to easily set up and tear down mock objects for testing purposes.", "prompt_index": 328, "generated_code": "```python\nimport pytest\nfrom unittest.mock import Mock, patch\n\n\n@pytest.fixture\ndef mock_external_service():\n    \"\"\"Fixture to create a mock external service with predefined responses.\"\"\"\n    mock_service = Mock()\n    mock_service.get_data.return_value = {\"status\": \"success\", \"data\": []}\n    mock_service.process_item.return_value = \"processed\"\n    yield mock_service\n    # Teardown: reset mock if needed\n    mock_service.reset_mock()\n\n\n@pytest.fixture\ndef patched_external_api():\n    \"\"\"Fixture that patches an external API call during test execution.\"\"\"\n    with patch('my_module.requests.get') as mock_get:\n        mock_response = Mock()\n        mock_response.json.return_value = {\"result\": \"test_data\"}\n        mock_response.status_code = 200\n        mock_get.return_value = mock_response\n        yield mock_get\n\n\n@pytest.fixture(autouse=True)\ndef global_mock_setup():\n    \"\"\"Auto-used fixture for global mock setup/teardown.\"\"\"\n    # Setup code here if needed\n    yield\n    # Teardown code here if needed\n\n\ndef test_with_mock_service(mock_external_service):\n    \"\"\"Test using the mock service fixture.\"\"\"\n    result = mock_external_service.get_data()\n    assert result[\"status\"] == \"success\"\n    mock_external_service.get_data.assert_called_once()\n\n\ndef test_with_patched_api(patched_external_api):\n    \"\"\"Test using the patched API fixture.\"\"\"\n    from my_module import fetch_data  # Import inside test to avoid import issues\n    \n    result = fetch_data()\n    assert result == {\"result\": \"test_data\"}\n    patched_external_api.assert_called_once()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["my_module", "pytest", "unittest"], "package_breakdown": {"install_packages": ["my_module", "pytest", "unittest"], "import_packages": ["my_module", "unittest", "pytest"], "llm_suggested": ["pytest", "unittest"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses a CSS file and extracts all the class names and their corresponding styles into a dictionary for further manipulation and analysis.", "prompt_index": 329, "generated_code": "```python\nimport re\nfrom typing import Dict, List\n\ndef parse_css_classes(css_content: str) -> Dict[str, str]:\n    \"\"\"\n    Parse CSS content and extract class names with their corresponding styles.\n    \n    Args:\n        css_content: String containing CSS content\n        \n    Returns:\n        Dictionary where keys are class names and values are CSS style strings\n    \"\"\"\n    class_pattern = r'\\.([a-zA-Z0-9_-]+)\\s*\\{([^}]*)\\}'\n    classes_dict = {}\n    \n    matches = re.findall(class_pattern, css_content, re.DOTALL)\n    \n    for class_name, styles in matches:\n        # Clean up styles by removing extra whitespace and comments\n        styles = re.sub(r'/\\*.*?\\*/', '', styles, flags=re.DOTALL)  # Remove comments\n        styles = re.sub(r'\\s+', ' ', styles.strip())  # Normalize whitespace\n        \n        if class_name in classes_dict:\n            # If class already exists, append new styles\n            classes_dict[class_name] += ' ' + styles\n        else:\n            classes_dict[class_name] = styles\n    \n    return classes_dict\n\ndef parse_css_file(file_path: str) -> Dict[str, str]:\n    \"\"\"\n    Parse a CSS file and extract class names with their styles.\n    \n    Args:\n        file_path: Path to the CSS file\n        \n    Returns:\n        Dictionary where keys are class names and values are CSS style strings\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            css_content = file.read()\n        return parse_css_classes(css_content)\n    except FileNotFoundError:\n        print(f\"Error: File '{file_path}' not found.\")\n        return {}\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return {}\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example CSS content\n    example_css = \"\"\"\n    .container {\n        width: 100%;\n        margin: 0 auto;\n        padding: 20px;\n    }\n    \n    .button {\n        background-color: blue;\n        color: white;\n        padding: 10px 20px;\n        border: none;\n    }\n    \n    .button:hover {\n        background-color: darkblue;\n    }\n    \n    .text-center {\n        text-align: center;\n    }\n    \"\"\"\n    \n    # Parse from string\n    classes = parse_css_classes(example_css)\n    print(\"Parsed classes:\")\n    for class_name, styles in classes.items():\n        print(f\"{class_name}: {styles}\")\n    \n    # Parse from file\n    # classes_from_file = parse_css_file('styles.css')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "typing"], "package_breakdown": {"install_packages": ["re", "typing"], "import_packages": ["re", "typing"], "llm_suggested": ["re", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compresses and decompresses large data sets using LZ4 compression algorithm.", "prompt_index": 330, "generated_code": "```python\nimport lz4.frame\nimport os\n\ndef compress_data(input_data):\n    \"\"\"\n    Compress data using LZ4 algorithm\n    \n    Args:\n        input_data: Bytes or bytearray to compress\n        \n    Returns:\n        Compressed bytes\n    \"\"\"\n    try:\n        compressed = lz4.frame.compress(input_data)\n        return compressed\n    except Exception as e:\n        raise Exception(f\"Compression failed: {str(e)}\")\n\ndef decompress_data(compressed_data):\n    \"\"\"\n    Decompress LZ4 compressed data\n    \n    Args:\n        compressed_data: LZ4 compressed bytes\n        \n    Returns:\n        Decompressed bytes\n    \"\"\"\n    try:\n        decompressed = lz4.frame.decompress(compressed_data)\n        return decompressed\n    except Exception as e:\n        raise Exception(f\"Decompression failed: {str(e)}\")\n\ndef compress_file(input_path, output_path):\n    \"\"\"\n    Compress a file using LZ4 algorithm\n    \n    Args:\n        input_path: Path to input file\n        output_path: Path for output compressed file\n    \"\"\"\n    try:\n        with open(input_path, 'rb') as f_in:\n            data = f_in.read()\n        \n        compressed = compress_data(data)\n        \n        with open(output_path, 'wb') as f_out:\n            f_out.write(compressed)\n            \n    except Exception as e:\n        raise Exception(f\"File compression failed: {str(e)}\")\n\ndef decompress_file(input_path, output_path):\n    \"\"\"\n    Decompress an LZ4 compressed file\n    \n    Args:\n        input_path: Path to compressed file\n        output_path: Path for decompressed output file\n    \"\"\"\n    try:\n        with open(input_path, 'rb') as f_in:\n            compressed_data = f_in.read()\n        \n        decompressed = decompress_data(compressed_data)\n        \n        with open(output_path, 'wb') as f_out:\n            f_out.write(decompressed)\n            \n    except Exception as e:\n        raise Exception(f\"File decompression failed: {str(e)}\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example with in-memory data\n    original_data = b\"This is some sample data that will be compressed using LZ4 algorithm\"\n    \n    # Compress\n    compressed = compress_data(original_data)\n    print(f\"Original size: {len(original_data)} bytes\")\n    print(f\"Compressed size: {len(compressed)} bytes\")\n    print(f\"Compression ratio: {len(compressed)/len(original_data):.2f}\")\n    \n    # Decompress\n    decompressed = decompress_data(compressed)\n    print(f\"Decompression successful: {decompressed == original_data}\")\n```\n\n**Note:** This code requires the `lz4` package. Install it using:\n```bash\npip install lz4\n```\n\nThe code provides both in-memory compression/decompression functions and file-based operations. For very large datasets, consider using streaming compression with `lz4.frame.LZ4FrameCompressor` and `lz4.frame.LZ4FrameDecompressor` classes to handle data in chunks.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["lz4", "os"], "package_breakdown": {"install_packages": ["lz4", "os"], "import_packages": ["lz4", "os"], "llm_suggested": ["lz4"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to an Amazon Redshift database using the Redshift interface library and executes a SQL query to retrieve data from a specific table.", "prompt_index": 331, "generated_code": "```python\nimport redshift_connector\n\n# Establish connection to Redshift\nconn = redshift_connector.connect(\n    host='your-redshift-cluster-endpoint',\n    database='your-database-name',\n    user='your-username',\n    password='your-password',\n    port=5439\n)\n\n# Create cursor\ncursor = conn.cursor()\n\n# Execute SQL query\ncursor.execute(\"SELECT * FROM your_table_name\")\n\n# Fetch results\nresults = cursor.fetchall()\n\n# Close cursor and connection\ncursor.close()\nconn.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["redshift_connector"], "packages_detected": ["redshift_connector"], "package_breakdown": {"install_packages": ["redshift_connector"], "import_packages": ["redshift_connector"], "llm_suggested": ["redshift_connector"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that opens a JupyterLab notebook, imports a dataset from a CSV file, visualizes the data using matplotlib, and performs statistical analysis with pandas to generate insights.", "prompt_index": 332, "generated_code": "```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load dataset from CSV\ndf = pd.read_csv('dataset.csv')\n\n# Display basic information about the dataset\nprint(\"Dataset Info:\")\nprint(df.info())\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\nprint(\"\\nDataset shape:\", df.shape)\n\n# Statistical summary\nprint(\"\\nStatistical Summary:\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# Data Visualization\nplt.figure(figsize=(12, 8))\n\n# Histogram for numerical columns\nnumerical_cols = df.select_dtypes(include=[np.number]).columns\nif len(numerical_cols) > 0:\n    df[numerical_cols].hist(bins=20, figsize=(15, 10))\n    plt.suptitle('Distribution of Numerical Variables')\n    plt.show()\n\n# Correlation heatmap if multiple numerical columns exist\nif len(numerical_cols) > 1:\n    plt.figure(figsize=(10, 8))\n    correlation_matrix = df[numerical_cols].corr()\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n    plt.title('Correlation Matrix')\n    plt.show()\n\n# Box plots for numerical columns\nif len(numerical_cols) > 0:\n    plt.figure(figsize=(15, 6))\n    df[numerical_cols].boxplot()\n    plt.title('Box Plots of Numerical Variables')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Count plots for categorical columns\ncategorical_cols = df.select_dtypes(include=['object']).columns\nfor col in categorical_cols:\n    plt.figure(figsize=(10, 6))\n    df[col].value_counts().plot(kind='bar')\n    plt.title(f'Distribution of {col}')\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Statistical Analysis Insights\nprint(\"\\n\" + \"=\"*50)\nprint(\"STATISTICAL INSIGHTS\")\nprint(\"=\"*50)\n\n# Outlier detection using IQR method\nif len(numerical_cols) > 0:\n    print(\"\\nOutlier Analysis:\")\n    for col in numerical_cols:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        print(f\"{col}: {len(outliers)} outliers detected\")\n\n# Skewness analysis\nprint(\"\\nSkewness Analysis:\")\nfor col in numerical_cols:\n    skewness = df[col].skew()\n    print(f\"{col}: Skewness = {skewness:.3f}\")\n\n# Top correlations\nif len(numerical_cols) > 1:\n    print(\"\\nTop Correlations:\")\n    corr_pairs = correlation_matrix.unstack()\n    sorted_pairs = corr_pairs.sort_values(key=abs, ascending=False)\n    # Remove self-correlations and duplicates\n    unique_pairs = sorted_pairs[sorted_pairs.index.get_level_values(0) != sorted_pairs.index.get_level_values(1)]\n    top_correlations = unique_pairs[::2].head(5)\n    print(top_correlations)\n\n# Missing data percentage\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\nprint(\"\\nMissing Data Percentage:\")\nprint(missing_percentage[missing_percentage > 0])\n```\n\nNote: This code assumes the CSV file is named 'dataset.csv' and is in the same directory as the notebook. You may need to adjust the file path accordingly. The code includes comprehensive data exploration, visualization, and statistical analysis suitable for most datasets.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["seaborn", "numpy", "matplotlib", "pandas"], "package_breakdown": {"install_packages": ["seaborn", "numpy", "matplotlib", "pandas"], "import_packages": ["matplotlib", "seaborn", "pandas", "numpy"], "llm_suggested": ["pandas", "matplotlib", "numpy", "seaborn"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a CSV file from HDFS using Apache Spark's Python API and performs a group by operation to count the occurrences of each unique value in a specified column, then saves the results to a new CSV file in the HDFS.", "prompt_index": 333, "generated_code": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"CSVGroupByCount\") \\\n    .getOrCreate()\n\n# Read CSV file from HDFS\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"hdfs://path/to/your/input_file.csv\")\n\n# Specify the column to group by (replace 'your_column_name' with actual column name)\ngrouped_df = df.groupBy(col(\"your_column_name\")) \\\n    .agg(count(\"*\").alias(\"count\"))\n\n# Save results to new CSV file in HDFS\ngrouped_df.write \\\n    .option(\"header\", \"true\") \\\n    .mode(\"overwrite\") \\\n    .csv(\"hdfs://path/to/your/output_directory\")\n\n# Stop Spark session\nspark.stop()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyspark"], "package_breakdown": {"install_packages": ["pyspark"], "import_packages": ["pyspark"], "llm_suggested": ["pyspark"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically downloads and installs all necessary dependencies for creating a standalone executable from a Python script.", "prompt_index": 334, "generated_code": "```python\nimport subprocess\nimport sys\nimport os\n\ndef install_dependencies():\n    dependencies = [\n        'pyinstaller',\n        'setuptools',\n        'wheel'\n    ]\n    \n    for package in dependencies:\n        try:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n            print(f\"Successfully installed {package}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            return False\n    \n    return True\n\nif __name__ == \"__main__\":\n    if install_dependencies():\n        print(\"All dependencies installed successfully!\")\n    else:\n        print(\"Failed to install some dependencies.\")\n        sys.exit(1)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["subprocess", "pyinstaller", "wheel", "setuptools", "sys", "os"], "package_breakdown": {"install_packages": ["subprocess", "pyinstaller", "wheel", "setuptools", "sys", "os"], "import_packages": ["sys", "os", "subprocess"], "llm_suggested": ["subprocess", "sys", "os", "pyinstaller", "setuptools", "wheel"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a string of HTML content as input and sanitizes it using a safelist of allowed HTML elements and attributes, ensuring only the specified elements and attributes are included in the sanitized output.", "prompt_index": 335, "generated_code": "```python\nimport re\nfrom html.parser import HTMLParser\n\nclass HTMLSanitizer(HTMLParser):\n    def __init__(self, allowed_tags=None, allowed_attrs=None):\n        super().__init__()\n        self.allowed_tags = allowed_tags if allowed_tags else set()\n        self.allowed_attrs = allowed_attrs if allowed_attrs else set()\n        self.sanitized_output = []\n        self.current_tag = None\n        \n    def handle_starttag(self, tag, attrs):\n        self.current_tag = tag.lower()\n        if self.current_tag in self.allowed_tags:\n            sanitized_attrs = []\n            for attr, value in attrs:\n                attr_lower = attr.lower()\n                if attr_lower in self.allowed_attrs.get(self.current_tag, set()):\n                    # Basic XSS protection: remove javascript: and data: URIs\n                    if value and any(proto in value.lower() for proto in ['javascript:', 'data:']):\n                        continue\n                    sanitized_attrs.append(f'{attr}=\"{self.escape_attr(value)}\"')\n            \n            if sanitized_attrs:\n                self.sanitized_output.append(f'<{self.current_tag} {\" \".join(sanitized_attrs)}>')\n            else:\n                self.sanitized_output.append(f'<{self.current_tag}>')\n        else:\n            self.current_tag = None\n    \n    def handle_endtag(self, tag):\n        tag_lower = tag.lower()\n        if tag_lower in self.allowed_tags and self.current_tag == tag_lower:\n            self.sanitized_output.append(f'</{tag_lower}>')\n        self.current_tag = None\n    \n    def handle_data(self, data):\n        if self.current_tag is not None or not data.strip():\n            self.sanitized_output.append(data)\n    \n    def handle_comment(self, data):\n        # Remove all comments\n        pass\n    \n    def escape_attr(self, value):\n        # Escape HTML entities in attribute values\n        return value.replace('&', '&amp;').replace('\"', '&quot;').replace(\"'\", '&#39;')\n    \n    def get_sanitized_html(self):\n        return ''.join(self.sanitized_output)\n\ndef sanitize_html(html_content, allowed_tags=None, allowed_attrs=None):\n    \"\"\"\n    Sanitize HTML content using a safelist approach.\n    \n    Args:\n        html_content (str): The HTML content to sanitize\n        allowed_tags (set): Set of allowed HTML tags (e.g., {'p', 'a', 'br'})\n        allowed_attrs (dict): Dictionary mapping tags to sets of allowed attributes\n                             (e.g., {'a': {'href', 'title'}, 'img': {'src', 'alt'}})\n    \n    Returns:\n        str: Sanitized HTML content\n    \"\"\"\n    if allowed_tags is None:\n        allowed_tags = {'p', 'br', 'strong', 'em', 'u', 'ul', 'ol', 'li'}\n    \n    if allowed_attrs is None:\n        allowed_attrs = {\n            'a': {'href', 'title', 'target'},\n            'img': {'src', 'alt', 'title'},\n            'span': {'class'},\n            'div': {'class'}\n        }\n    \n    sanitizer = HTMLSanitizer(allowed_tags, allowed_attrs)\n    sanitizer.feed(html_content)\n    return sanitizer.get_sanitized_html()\n\n# Example usage:\nif __name__ == \"__main__\":\n    dirty_html = '''\n    <p>This is <script>alert(\"XSS\")</script> a test</p>\n    <a href=\"javascript:alert('malicious')\" onclick=\"evil()\">Click me</a>\n    <img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==\">\n    <div class=\"safe\">Safe content</div>\n    '''\n    \n    allowed_tags = {'p', 'a', 'img', 'div'}\n    allowed_attrs = {\n        'a': {'href', 'title'},\n        'img': {'src', 'alt'},\n        'div': {'class'}\n    }\n    \n    clean_html = sanitize_html(dirty_html, allowed_tags, allowed_attrs)\n    print(clean_html)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "html", "htmlparser"], "package_breakdown": {"install_packages": ["re", "html", "htmlparser"], "import_packages": ["re", "html"], "llm_suggested": ["re", "htmlparser"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that checks a Python source code file for compliance with PEP8 style guidelines and identifies any potential errors using the Pyflakes static analysis tool.", "prompt_index": 336, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef check_pep8_and_pyflakes(file_path):\n    \"\"\"\n    Check a Python file for PEP8 compliance and potential errors using Pyflakes.\n    \n    Args:\n        file_path (str): Path to the Python file to check\n        \n    Returns:\n        dict: Dictionary containing results from both checks\n    \"\"\"\n    results = {\n        'pep8': {'output': '', 'returncode': None},\n        'pyflakes': {'output': '', 'returncode': None}\n    }\n    \n    try:\n        # Check PEP8 compliance using pycodestyle (formerly pep8)\n        pep8_result = subprocess.run(\n            ['pycodestyle', '--first', file_path],\n            capture_output=True,\n            text=True,\n            timeout=30\n        )\n        results['pep8']['output'] = pep8_result.stdout + pep8_result.stderr\n        results['pep8']['returncode'] = pep8_result.returncode\n        \n        # Check for potential errors using Pyflakes\n        pyflakes_result = subprocess.run(\n            ['pyflakes', file_path],\n            capture_output=True,\n            text=True,\n            timeout=30\n        )\n        results['pyflakes']['output'] = pyflakes_result.stdout + pyflakes_result.stderr\n        results['pyflakes']['returncode'] = pyflakes_result.returncode\n        \n    except FileNotFoundError as e:\n        print(f\"Error: Required tool not found. Please install pycodestyle and pyflakes.\")\n        print(f\"Install with: pip install pycodestyle pyflakes\")\n        sys.exit(1)\n    except subprocess.TimeoutExpired:\n        print(f\"Error: Analysis timed out for file: {file_path}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        sys.exit(1)\n    \n    return results\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python style_checker.py <python_file>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    \n    # Check if file exists\n    try:\n        with open(file_path, 'r'):\n            pass\n    except FileNotFoundError:\n        print(f\"Error: File '{file_path}' not found.\")\n        sys.exit(1)\n    except IOError:\n        print(f\"Error: Cannot read file '{file_path}'.\")\n        sys.exit(1)\n    \n    # Perform checks\n    results = check_pep8_and_pyflakes(file_path)\n    \n    # Display results\n    print(\"=\" * 50)\n    print(f\"Style and Error Analysis for: {file_path}\")\n    print(\"=\" * 50)\n    \n    print(\"\\nPEP8 Compliance Check (pycodestyle):\")\n    if results['pep8']['output']:\n        print(results['pep8']['output'])\n    else:\n        print(\"\u2713 No PEP8 style issues found\")\n    \n    print(\"\\nPyflakes Static Analysis:\")\n    if results['pyflakes']['output']:\n        print(results['pyflakes']['output'])\n    else:\n        print(\"\u2713 No potential errors found by Pyflakes\")\n    \n    # Return appropriate exit code\n    if results['pep8']['returncode'] != 0 or results['pyflakes']['returncode'] != 0:\n        sys.exit(1)\n    else:\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code requires the `pycodestyle` and `pyflakes` packages to be installed. You can install them using:\n```bash\npip install pycodestyle pyflakes\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pycodestyle", "sys", "pyflakes", "subprocess"], "package_breakdown": {"install_packages": ["pycodestyle", "sys", "pyflakes", "subprocess"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["pycodestyle", "pyflakes"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a request to an Amazon Web Services (AWS) API using the AWS4 signature version, ensuring secure and authenticated access to AWS services.", "prompt_index": 337, "generated_code": "```python\nimport boto3\nimport requests\nfrom datetime import datetime\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.auth import SigV4Auth\nfrom botocore.credentials import Credentials\n\ndef get_aws_signed_headers(service, region, method, url, headers=None, data=None):\n    \"\"\"\n    Generate AWS4 signed headers for an HTTP request\n    \n    Args:\n        service (str): AWS service name (e.g., 's3', 'ec2')\n        region (str): AWS region (e.g., 'us-east-1')\n        method (str): HTTP method (e.g., 'GET', 'POST')\n        url (str): Full URL of the request\n        headers (dict): Optional existing headers\n        data (bytes/str): Optional request body data\n    \n    Returns:\n        dict: Signed headers including Authorization header\n    \"\"\"\n    # Get credentials from default AWS configuration\n    session = boto3.Session()\n    credentials = session.get_credentials()\n    \n    if credentials is None:\n        raise Exception(\"No AWS credentials found\")\n    \n    # Create AWS credentials object\n    aws_credentials = Credentials(\n        credentials.access_key,\n        credentials.secret_key,\n        credentials.token\n    )\n    \n    # Create AWS request object\n    request = AWSRequest(\n        method=method.upper(),\n        url=url,\n        headers=headers or {},\n        data=data\n    )\n    \n    # Sign the request\n    SigV4Auth(aws_credentials, service, region).add_auth(request)\n    \n    return dict(request.headers)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example: Generate signed headers for S3 GET request\n    service = \"s3\"\n    region = \"us-east-1\"\n    method = \"GET\"\n    url = \"https://my-bucket.s3.amazonaws.com/example-file.txt\"\n    \n    try:\n        signed_headers = get_aws_signed_headers(service, region, method, url)\n        print(\"Signed headers:\", signed_headers)\n        \n        # Make the actual request with signed headers\n        response = requests.get(url, headers=signed_headers)\n        print(\"Response status:\", response.status_code)\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["boto3", "requests", "botocore", "datetime"], "package_breakdown": {"install_packages": ["boto3", "requests", "botocore", "datetime"], "import_packages": ["requests", "botocore", "datetime", "boto3"], "llm_suggested": ["boto3", "requests", "botocore"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a MySQL server, executes a query to retrieve data from a specific table, and prints the results fetched from the database.", "prompt_index": 338, "generated_code": "```python\nimport mysql.connector\n\n# Database connection parameters\nconfig = {\n    'user': 'your_username',\n    'password': 'your_password',\n    'host': 'localhost',\n    'database': 'your_database',\n    'raise_on_warnings': True\n}\n\ntry:\n    # Establish connection\n    connection = mysql.connector.connect(**config)\n    \n    # Create cursor\n    cursor = connection.cursor()\n    \n    # Execute query\n    query = \"SELECT * FROM your_table\"\n    cursor.execute(query)\n    \n    # Fetch all results\n    results = cursor.fetchall()\n    \n    # Print results\n    for row in results:\n        print(row)\n        \nexcept mysql.connector.Error as err:\n    print(f\"Error: {err}\")\n    \nfinally:\n    # Clean up\n    if 'cursor' in locals():\n        cursor.close()\n    if 'connection' in locals() and connection.is_connected():\n        connection.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["mysql", "mysql-connector-python"], "package_breakdown": {"install_packages": ["mysql", "mysql-connector-python"], "import_packages": ["mysql"], "llm_suggested": ["mysql-connector-python"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that monitors a specified directory for file changes and automatically reloads the code whenever a file within that directory is modified.", "prompt_index": 339, "generated_code": "```python\nimport os\nimport time\nimport importlib\nimport sys\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\n\nclass CodeReloader(FileSystemEventHandler):\n    def __init__(self, directory_path, callback=None):\n        self.directory_path = Path(directory_path).resolve()\n        self.callback = callback\n        self.last_modified_time = {}\n        \n    def on_modified(self, event):\n        if event.is_directory:\n            return\n            \n        file_path = Path(event.src_path)\n        if file_path.suffix != '.py':\n            return\n            \n        current_time = time.time()\n        # Debounce to avoid multiple triggers for the same file\n        if (file_path in self.last_modified_time and \n            current_time - self.last_modified_time[file_path] < 1.0):\n            return\n            \n        self.last_modified_time[file_path] = current_time\n        print(f\"File modified: {file_path.name}\")\n        self.reload_modules()\n        \n    def reload_modules(self):\n        \"\"\"Reload all Python modules in the monitored directory\"\"\"\n        for module_name, module in list(sys.modules.items()):\n            if (hasattr(module, '__file__') and module.__file__ and \n                Path(module.__file__).parent == self.directory_path):\n                try:\n                    importlib.reload(module)\n                    print(f\"Reloaded: {module_name}\")\n                except Exception as e:\n                    print(f\"Error reloading {module_name}: {e}\")\n        \n        if self.callback:\n            self.callback()\n\ndef start_monitoring(directory_path, callback=None):\n    \"\"\"\n    Start monitoring a directory for file changes and automatically reload modules\n    \n    Args:\n        directory_path (str): Path to the directory to monitor\n        callback (callable, optional): Function to call after reloading modules\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(f\"Directory does not exist: {directory_path}\")\n    \n    event_handler = CodeReloader(directory_path, callback)\n    observer = Observer()\n    observer.schedule(event_handler, directory_path, recursive=True)\n    observer.start()\n    \n    print(f\"Monitoring directory: {directory_path}\")\n    print(\"Press Ctrl+C to stop monitoring...\")\n    \n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    \n    observer.join()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Monitor the current directory\n    directory_to_watch = \".\"\n    \n    # Optional callback function to run after reload\n    def after_reload_callback():\n        print(\"Modules reloaded successfully!\")\n        # Add any post-reload logic here\n    \n    start_monitoring(directory_to_watch, after_reload_callback)\n```\n\nTo use this code, you'll need to install the `watchdog` package:\n```bash\npip install watchdog\n```\n\nThe code monitors a specified directory for `.py` file changes and automatically reloads any modules from that directory when modifications are detected. It includes debouncing to prevent multiple reloads from rapid file saves.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "watchdog", "time", "pathlib", "importlib", "os"], "package_breakdown": {"install_packages": ["sys", "watchdog", "time", "pathlib", "importlib", "os"], "import_packages": ["importlib", "time", "sys", "watchdog", "pathlib", "os"], "llm_suggested": ["watchdog"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a Jupyter Notebook file to a PDF document while preserving the code, markdown cells, and visualizations.", "prompt_index": 340, "generated_code": "```python\nimport nbformat\nfrom nbconvert import PDFExporter\nfrom traitlets.config import Config\n\ndef convert_notebook_to_pdf(notebook_path, output_path):\n    \"\"\"\n    Convert a Jupyter Notebook to PDF while preserving code, markdown, and visualizations.\n    \n    Args:\n        notebook_path (str): Path to the input .ipynb file\n        output_path (str): Path where the PDF will be saved\n    \"\"\"\n    # Load the notebook\n    with open(notebook_path, 'r', encoding='utf-8') as f:\n        notebook = nbformat.read(f, as_version=4)\n    \n    # Configure PDF exporter\n    c = Config()\n    c.PDFExporter.preprocessors = [\n        'nbconvert.preprocessors.ExecutePreprocessor',\n        'nbconvert.preprocessors.ClearOutputPreprocessor'\n    ]\n    \n    # Create and configure the exporter\n    pdf_exporter = PDFExporter(config=c)\n    pdf_exporter.exclude_input_prompt = True\n    pdf_exporter.exclude_output_prompt = True\n    \n    # Convert and write to PDF\n    (body, resources) = pdf_exporter.from_notebook_node(notebook)\n    \n    with open(output_path, 'wb') as f:\n        f.write(body)\n\n# Example usage\nif __name__ == \"__main__\":\n    convert_notebook_to_pdf(\"input_notebook.ipynb\", \"output_document.pdf\")\n```\n\n```bash\n# Required dependencies (install via pip)\n# pip install nbformat nbconvert pyppeteer\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pyppeteer", "nbconvert", "traitlets", "nbformat"], "package_breakdown": {"install_packages": ["pyppeteer", "nbconvert", "traitlets", "nbformat"], "import_packages": ["nbconvert", "traitlets", "nbformat"], "llm_suggested": ["nbformat", "nbconvert", "traitlets", "pyppeteer"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all installed Python versions on the system, including the paths to their executable files.", "prompt_index": 341, "generated_code": "```python\nimport sys\nimport os\nimport glob\n\ndef find_python_installations():\n    python_versions = {}\n    \n    # Common installation directories\n    common_paths = [\n        '/usr/bin/python*',          # Linux/Unix\n        '/usr/local/bin/python*',    # Linux/Unix (user installations)\n        '/opt/python*/bin/python*',  # Custom installations\n        'C:\\\\Python*\\\\python.exe',   # Windows\n        'C:\\\\Program Files\\\\Python*\\\\python.exe',  # Windows\n        'C:\\\\Users\\\\*\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python*\\\\python.exe'  # Windows user installs\n    ]\n    \n    # Check current Python installation\n    current_python = sys.executable\n    python_versions[sys.version] = current_python\n    \n    # Search for other installations\n    for pattern in common_paths:\n        for path in glob.glob(pattern):\n            # Skip if it's a directory or not an executable\n            if not os.path.isfile(path) or not os.access(path, os.X_OK):\n                continue\n            \n            # Skip if it's a symlink to avoid duplicates\n            if os.path.islink(path):\n                continue\n                \n            # Get version info by running the executable\n            try:\n                version_output = os.popen(f'\"{path}\" --version 2>&1').read().strip()\n                if version_output.startswith('Python'):\n                    python_versions[version_output] = path\n            except:\n                continue\n    \n    return python_versions\n\nif __name__ == \"__main__\":\n    installations = find_python_installations()\n    for version, path in installations.items():\n        print(f\"{version}: {path}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "glob", "os"], "package_breakdown": {"install_packages": ["sys", "glob", "os"], "import_packages": ["sys", "glob", "os"], "llm_suggested": ["sys", "os", "glob"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a two-way communication channel between Jupyter notebooks and Python kernels using the Jupyter Python Comm implementation for seamless integration and data exchange.", "prompt_index": 342, "generated_code": "```python\nimport ipykernel.comm\nimport threading\nimport time\nfrom IPython.display import display, Javascript\nimport json\n\nclass JupyterCommChannel:\n    def __init__(self, comm_name=\"jupyter_comm_channel\"):\n        self.comm_name = comm_name\n        self.comm = None\n        self.message_handlers = {}\n        self.connected = False\n        \n        # Register the comm target\n        ipykernel.comm.CommManager().register_target(\n            self.comm_name, \n            self._handle_comm_open\n        )\n        \n    def _handle_comm_open(self, comm, open_msg):\n        \"\"\"Handle new comm connection\"\"\"\n        self.comm = comm\n        self.connected = True\n        \n        # Set up message handler\n        comm.on_msg(self._handle_comm_message)\n        comm.on_close(self._handle_comm_close)\n        \n        print(f\"Comm channel '{self.comm_name}' established\")\n        \n    def _handle_comm_message(self, msg):\n        \"\"\"Handle incoming messages\"\"\"\n        data = msg['content']['data']\n        message_type = data.get('type')\n        payload = data.get('payload', {})\n        \n        # Call appropriate handler\n        if message_type in self.message_handlers:\n            self.message_handlers[message_type](payload)\n        else:\n            print(f\"Received unhandled message type: {message_type}\")\n            \n    def _handle_comm_close(self, msg):\n        \"\"\"Handle comm closure\"\"\"\n        self.connected = False\n        print(\"Comm channel closed\")\n        \n    def register_handler(self, message_type, handler):\n        \"\"\"Register a handler for specific message types\"\"\"\n        self.message_handlers[message_type] = handler\n        \n    def send_message(self, message_type, payload=None):\n        \"\"\"Send a message through the comm channel\"\"\"\n        if not self.connected or not self.comm:\n            print(\"Comm channel not connected\")\n            return\n            \n        message = {\n            'type': message_type,\n            'payload': payload or {}\n        }\n        \n        self.comm.send(message)\n        \n    def wait_for_connection(self, timeout=10):\n        \"\"\"Wait for comm connection to be established\"\"\"\n        start_time = time.time()\n        while not self.connected and time.time() - start_time < timeout:\n            time.sleep(0.1)\n        return self.connected\n\n# JavaScript code to establish frontend comm connection\ncomm_js_code = \"\"\"\n(function() {\n    if (typeof window.jupyterComm === 'undefined') {\n        window.jupyterComm = {\n            comm: null,\n            handlers: {},\n            \n            init: function(comm_name) {\n                var that = this;\n                Jupyter.notebook.kernel.comm_manager.new_comm(\n                    comm_name,\n                    {},\n                    function(comm) {\n                        that.comm = comm;\n                        comm.on_msg(function(msg) {\n                            var data = msg.content.data;\n                            var type = data.type;\n                            var payload = data.payload || {};\n                            \n                            if (type in that.handlers) {\n                                that.handlers[type](payload);\n                            }\n                        });\n                        \n                        comm.on_close(function() {\n                            that.comm = null;\n                        });\n                    }\n                );\n            },\n            \n            registerHandler: function(type, handler) {\n                this.handlers[type] = handler;\n            },\n            \n            sendMessage: function(type, payload) {\n                if (this.comm) {\n                    this.comm.send({\n                        type: type,\n                        payload: payload || {}\n                    });\n                }\n            }\n        };\n    }\n    \n    window.jupyterComm.init('jupyter_comm_channel');\n})();\n\"\"\"\n\ndef initialize_frontend_comm():\n    \"\"\"Initialize the frontend JavaScript comm connection\"\"\"\n    display(Javascript(comm_js_code))\n    \n# Example usage\nif __name__ == \"__main__\":\n    # Initialize the comm channel\n    comm_channel = JupyterCommChannel()\n    \n    # Register message handlers\n    def handle_data_update(payload):\n        print(f\"Received data update: {payload}\")\n        \n    def handle_status_request(payload):\n        print(\"Status request received\")\n        comm_channel.send_message('status_response', {'status': 'ready'})\n    \n    comm_channel.register_handler('data_update', handle_data_update)\n    comm_channel.register_handler('status_request', handle_status_request)\n    \n    # Wait for connection\n    if comm_channel.wait_for_connection():\n        print(\"Ready for communication\")\n        \n        # Example: Send a message\n        comm_channel.send_message('test_message', {\n            'data': [1, 2, 3, 4, 5],\n            'metadata': {'timestamp': time.time()}\n        })\n    else:\n        print(\"Failed to establish connection\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["threading", "time", "ipykernel", "json", "IPython"], "package_breakdown": {"install_packages": ["threading", "time", "ipykernel", "json", "IPython"], "import_packages": ["IPython", "ipykernel", "time", "threading", "json"], "llm_suggested": ["ipykernel", "threading", "IPython"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that downloads a large file from the Hugging Face Hub efficiently and quickly.", "prompt_index": 343, "generated_code": "```python\nimport os\nimport requests\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\n\ndef download_large_file(repo_id, filename, local_dir=None, chunk_size=8192):\n    \"\"\"\n    Efficiently download a large file from Hugging Face Hub with progress tracking.\n    \n    Args:\n        repo_id (str): Repository ID (e.g., 'username/model-name')\n        filename (str): Name of the file to download\n        local_dir (str, optional): Local directory to save the file. Defaults to current directory.\n        chunk_size (int): Size of chunks for streaming download\n    \n    Returns:\n        str: Path to the downloaded file\n    \"\"\"\n    if local_dir is None:\n        local_dir = os.getcwd()\n    \n    os.makedirs(local_dir, exist_ok=True)\n    local_filepath = os.path.join(local_dir, filename)\n    \n    # Get download URL from Hugging Face Hub\n    url = hf_hub_download(repo_id, filename, repo_type=None)\n    \n    # Stream download with progress bar\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    total_size = int(response.headers.get('content-length', 0))\n    \n    with open(local_filepath, 'wb') as file, tqdm(\n        desc=filename,\n        total=total_size,\n        unit='iB',\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as progress_bar:\n        for chunk in response.iter_content(chunk_size=chunk_size):\n            size = file.write(chunk)\n            progress_bar.update(size)\n    \n    return local_filepath\n\n# Example usage:\n# download_large_file('bert-base-uncased', 'pytorch_model.bin', './models')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["huggingface_hub"], "packages_detected": ["requests", "huggingface_hub", "tqdm", "os"], "package_breakdown": {"install_packages": ["requests", "huggingface_hub", "tqdm", "os"], "import_packages": ["requests", "huggingface_hub", "os", "tqdm"], "llm_suggested": ["os", "requests", "tqdm", "huggingface_hub"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves information about the repositories of a specified user on Github using the full Github API v3.", "prompt_index": 344, "generated_code": "```python\nimport requests\nimport json\n\ndef get_user_repos(username, token=None):\n    \"\"\"\n    Retrieve information about a user's GitHub repositories using GitHub API v3\n    \n    Args:\n        username (str): GitHub username\n        token (str, optional): GitHub personal access token for authentication\n    \n    Returns:\n        list: List of repository dictionaries or None if error occurs\n    \"\"\"\n    headers = {\n        'Accept': 'application/vnd.github.v3+json',\n        'User-Agent': 'Python-GitHub-API-Request'\n    }\n    \n    if token:\n        headers['Authorization'] = f'token {token}'\n    \n    url = f'https://api.github.com/users/{username}/repos'\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        repos = response.json()\n        return repos\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching repositories: {e}\")\n        return None\n\n# Example usage:\nif __name__ == \"__main__\":\n    username = \"octocat\"  # Replace with target username\n    # token = \"your_github_token_here\"  # Optional: for higher rate limits\n    \n    repos = get_user_repos(username)  # Add token parameter if using authentication\n    \n    if repos:\n        print(f\"Found {len(repos)} repositories for {username}:\")\n        for repo in repos:\n            print(f\"Name: {repo['name']}\")\n            print(f\"Description: {repo['description']}\")\n            print(f\"URL: {repo['html_url']}\")\n            print(f\"Stars: {repo['stargazers_count']}\")\n            print(\"---\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests", "json"], "package_breakdown": {"install_packages": ["requests", "json"], "import_packages": ["requests", "json"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the cryptographic library to encrypt a message using AES encryption with a randomly generated key and IV, and then decrypts the encrypted message using the same key and IV.", "prompt_index": 345, "generated_code": "```python\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom cryptography.hazmat.backends import default_backend\nimport os\n\ndef encrypt_message(message):\n    # Generate random key and IV\n    key = os.urandom(32)  # 256-bit key for AES-256\n    iv = os.urandom(16)   # 128-bit IV\n    \n    # Pad the message to match block size\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(message.encode()) + padder.finalize()\n    \n    # Encrypt the message\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n    \n    return key, iv, ciphertext\n\ndef decrypt_message(key, iv, ciphertext):\n    # Decrypt the ciphertext\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n    decryptor = cipher.decryptor()\n    padded_data = decryptor.update(ciphertext) + decryptor.finalize()\n    \n    # Unpad the decrypted data\n    unpadder = padding.PKCS7(128).unpadder()\n    plaintext = unpadder.update(padded_data) + unpadder.finalize()\n    \n    return plaintext.decode()\n\n# Example usage\nif __name__ == \"__main__\":\n    message = \"This is a secret message!\"\n    \n    # Encrypt\n    key, iv, ciphertext = encrypt_message(message)\n    print(f\"Original: {message}\")\n    print(f\"Key: {key.hex()}\")\n    print(f\"IV: {iv.hex()}\")\n    print(f\"Ciphertext: {ciphertext.hex()}\")\n    \n    # Decrypt\n    decrypted = decrypt_message(key, iv, ciphertext)\n    print(f\"Decrypted: {decrypted}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["os", "cryptography"], "package_breakdown": {"install_packages": ["os", "cryptography"], "import_packages": ["os", "cryptography"], "llm_suggested": ["cryptography"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a Jupyter Notebook file (.ipynb) to a PDF format using the nbconvert package, allowing for seamless integration of notebook content into a professional document-ready layout.", "prompt_index": 346, "generated_code": "```python\nimport nbformat\nfrom nbconvert import PDFExporter\nfrom nbconvert.preprocessors import ExecutePreprocessor\nfrom traitlets.config import Config\n\ndef convert_notebook_to_pdf(input_path, output_path=None, execute=False):\n    \"\"\"\n    Convert a Jupyter Notebook to PDF format with professional layout.\n    \n    Args:\n        input_path (str): Path to the input .ipynb file\n        output_path (str, optional): Path for the output PDF file. \n                                    If None, uses input filename with .pdf extension\n        execute (bool): Whether to execute the notebook before conversion\n    \"\"\"\n    \n    # Set default output path if not provided\n    if output_path is None:\n        output_path = input_path.replace('.ipynb', '.pdf')\n    \n    # Load the notebook\n    with open(input_path, 'r', encoding='utf-8') as f:\n        notebook = nbformat.read(f, as_version=4)\n    \n    # Execute the notebook if requested\n    if execute:\n        ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n        ep.preprocess(notebook, {'metadata': {'path': '.'}})\n    \n    # Configure PDF exporter for professional layout\n    c = Config()\n    c.PDFExporter.preprocessors = [\n        'nbconvert.preprocessors.TagRemovePreprocessor',\n        'nbconvert.preprocessors.RegexRemovePreprocessor'\n    ]\n    c.TagRemovePreprocessor.remove_cell_tags = (\"remove_cell\",)\n    c.TagRemovePreprocessor.remove_all_outputs_tags = ('remove_output',)\n    c.TagRemovePreprocessor.remove_input_tags = ('remove_input',)\n    c.PDFExporter.template_name = 'classic'\n    c.PDFExporter.exclude_input_prompt = True\n    c.PDFExporter.exclude_output_prompt = True\n    \n    # Create and configure the exporter\n    pdf_exporter = PDFExporter(config=c)\n    pdf_exporter.exclude_input = False\n    pdf_exporter.exclude_output = False\n    \n    # Convert and write to PDF\n    (body, resources) = pdf_exporter.from_notebook_node(notebook)\n    \n    with open(output_path, 'wb') as f:\n        f.write(body)\n    \n    return output_path\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Convert notebook to PDF\n    output_file = convert_notebook_to_pdf(\n        input_path='example_notebook.ipynb',\n        output_path='example_notebook.pdf',\n        execute=False  # Set to True to execute notebook before conversion\n    )\n    print(f\"PDF created: {output_file}\")\n```\n\nNote: This code requires the following dependencies:\n- nbconvert\n- nbformat\n- pandoc\n- LaTeX (like TeX Live or MiKTeX)\n\nInstall dependencies with:\n```bash\npip install nbconvert nbformat\n# Plus install pandoc and LaTeX from their respective websites\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["nbconvert", "traitlets", "nbformat"], "package_breakdown": {"install_packages": ["nbconvert", "traitlets", "nbformat"], "import_packages": ["nbconvert", "traitlets", "nbformat"], "llm_suggested": ["nbformat", "nbconvert", "traitlets"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a SQL database using Apache Airflow's common SQL provider package to execute a query and retrieve the results.", "prompt_index": 347, "generated_code": "```python\nfrom airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\nfrom airflow import DAG\nfrom datetime import datetime\n\n# Define your DAG\ndag = DAG(\n    'sql_query_execution',\n    start_date=datetime(2023, 1, 1),\n    schedule_interval=None\n)\n\n# Create SQL query task\nsql_task = SQLExecuteQueryOperator(\n    task_id='execute_sql_query',\n    conn_id='your_database_connection_id',  # Configure this in Airflow connections\n    sql='SELECT * FROM your_table WHERE condition;',\n    dag=dag\n)\n\n# To retrieve results, you can use the return value or XCom\n# The results will be available in XCom with key 'return_value'\n# You can access it in subsequent tasks using:\n# {{ ti.xcom_pull(task_ids='execute_sql_query') }}\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["apache-airflow-providers-common-sql", "datetime", "airflow", "apache-airflow"], "package_breakdown": {"install_packages": ["apache-airflow-providers-common-sql", "datetime", "airflow", "apache-airflow"], "import_packages": ["datetime", "airflow"], "llm_suggested": ["apache-airflow-providers-common-sql", "apache-airflow"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a set of REST endpoints for core services and APIs in a Jupyter web application backend.", "prompt_index": 348, "generated_code": "```python\nfrom flask import Flask, request, jsonify\nfrom flask_restful import Api, Resource\nimport json\n\napp = Flask(__name__)\napi = Api(app)\n\nclass Notebooks(Resource):\n    def get(self, notebook_id=None):\n        if notebook_id:\n            # Get specific notebook\n            return jsonify({\"notebook\": f\"Notebook {notebook_id}\"})\n        else:\n            # List all notebooks\n            return jsonify({\"notebooks\": [\"notebook1\", \"notebook2\", \"notebook3\"]})\n    \n    def post(self):\n        # Create new notebook\n        data = request.get_json()\n        return jsonify({\"message\": \"Notebook created\", \"data\": data}), 201\n    \n    def put(self, notebook_id):\n        # Update notebook\n        data = request.get_json()\n        return jsonify({\"message\": f\"Notebook {notebook_id} updated\", \"data\": data})\n    \n    def delete(self, notebook_id):\n        # Delete notebook\n        return jsonify({\"message\": f\"Notebook {notebook_id} deleted\"}), 204\n\nclass Cells(Resource):\n    def get(self, notebook_id, cell_id=None):\n        if cell_id:\n            # Get specific cell\n            return jsonify({\"cell\": f\"Cell {cell_id} from notebook {notebook_id}\"})\n        else:\n            # List all cells in notebook\n            return jsonify({\"cells\": [f\"cell{i}\" for i in range(1, 4)]})\n    \n    def post(self, notebook_id):\n        # Create new cell\n        data = request.get_json()\n        return jsonify({\"message\": \"Cell created\", \"data\": data}), 201\n    \n    def put(self, notebook_id, cell_id):\n        # Update cell\n        data = request.get_json()\n        return jsonify({\"message\": f\"Cell {cell_id} updated\", \"data\": data})\n    \n    def delete(self, notebook_id, cell_id):\n        # Delete cell\n        return jsonify({\"message\": f\"Cell {cell_id} deleted\"}), 204\n\nclass ExecuteCode(Resource):\n    def post(self, notebook_id, cell_id):\n        # Execute code in cell\n        data = request.get_json()\n        code = data.get('code', '')\n        # Simulate execution\n        result = f\"Executed: {code}\"\n        return jsonify({\"result\": result, \"status\": \"success\"})\n\nclass Kernels(Resource):\n    def get(self):\n        # List available kernels\n        return jsonify({\"kernels\": [\"python3\", \"r\", \"julia\"]})\n    \n    def post(self):\n        # Start new kernel\n        data = request.get_json()\n        kernel_type = data.get('kernel_type', 'python3')\n        return jsonify({\"message\": f\"Kernel {kernel_type} started\", \"kernel_id\": \"kernel123\"})\n\nclass Files(Resource):\n    def get(self, file_path=None):\n        if file_path:\n            # Get specific file\n            return jsonify({\"file\": f\"Content of {file_path}\"})\n        else:\n            # List files in directory\n            return jsonify({\"files\": [\"file1.ipynb\", \"file2.py\", \"file3.txt\"]})\n    \n    def post(self):\n        # Upload file\n        file = request.files.get('file')\n        return jsonify({\"message\": \"File uploaded\", \"filename\": file.filename}), 201\n\n# Register endpoints\napi.add_resource(Notebooks, '/api/notebooks', '/api/notebooks/<string:notebook_id>')\napi.add_resource(Cells, '/api/notebooks/<string:notebook_id>/cells', '/api/notebooks/<string:notebook_id>/cells/<string:cell_id>')\napi.add_resource(ExecuteCode, '/api/notebooks/<string:notebook_id>/cells/<string:cell_id>/execute')\napi.add_resource(Kernels, '/api/kernels')\napi.add_resource(Files, '/api/files', '/api/files/<path:file_path>')\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["flask_restful"], "packages_detected": ["flask_restful", "flask", "json", "flask-restful"], "package_breakdown": {"install_packages": ["flask_restful", "flask", "json", "flask-restful"], "import_packages": ["flask_restful", "json", "flask"], "llm_suggested": ["flask", "flask-restful"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a TensorBoard visualization to monitor the flow of tensors during the training process of a neural network model.", "prompt_index": 349, "generated_code": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\n\n# Generate sample data\nX = np.random.rand(1000, 10)\ny = np.random.randint(0, 2, (1000, 1))\n\n# Create a simple neural network model\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(10,), name='dense_1'),\n    Dense(32, activation='relu', name='dense_2'),\n    Dense(1, activation='sigmoid', name='output')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Set up TensorBoard callback\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir='./logs',\n    histogram_freq=1,  # How often to record activation histograms\n    write_graph=True,  # Whether to visualize the graph\n    write_images=True,  # Whether to write model weights to visualize as image\n    update_freq='epoch',  # How often to write logs\n    profile_batch=2,  # Profile batch to capture compute/memory\n    embeddings_freq=1  # How often to save embedding visualizations\n)\n\n# Train the model with TensorBoard callback\nmodel.fit(X, y,\n          epochs=10,\n          batch_size=32,\n          validation_split=0.2,\n          callbacks=[tensorboard_callback])\n\n# To view TensorBoard, run in terminal:\n# tensorboard --logdir=./logs\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["tensorflow", "numpy"], "package_breakdown": {"install_packages": ["tensorflow", "numpy"], "import_packages": ["numpy", "tensorflow"], "llm_suggested": ["tensorflow", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an asyncio event loop using the libuv backend for efficient handling of asynchronous tasks.", "prompt_index": 350, "generated_code": "```python\nimport asyncio\nimport uvloop\n\ndef main():\n    # Set up the event loop policy to use uvloop\n    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n    \n    # Create and run the event loop\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    \n    try:\n        # Your async tasks would be scheduled here\n        # For example: loop.run_until_complete(your_async_function())\n        loop.run_forever()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        loop.close()\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "uvloop"], "package_breakdown": {"install_packages": ["asyncio", "uvloop"], "import_packages": ["asyncio", "uvloop"], "llm_suggested": ["uvloop", "asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts Unicode text into ASCII text using the most basic Text::Unidecode port available.", "prompt_index": 351, "generated_code": "```python\nimport unicodedata\n\ndef unidecode(text):\n    return ''.join(\n        c if ord(c) < 128 else unicodedata.normalize('NFKD', c).encode('ascii', 'ignore').decode('ascii')\n        for c in text\n    )\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["unicodedata"], "package_breakdown": {"install_packages": ["unicodedata"], "import_packages": ["unicodedata"], "llm_suggested": ["unicodedata"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a new Excel XLSX file, adds multiple sheets with various data, and saves the file to a specified directory.", "prompt_index": 352, "generated_code": "```python\nimport pandas as pd\nfrom datetime import datetime\n\n# Create sample data for different sheets\nsales_data = {\n    'Date': ['2023-01-15', '2023-01-16', '2023-01-17'],\n    'Product': ['Widget A', 'Widget B', 'Widget A'],\n    'Quantity': [25, 15, 30],\n    'Revenue': [1250.50, 825.75, 1500.00]\n}\n\nemployee_data = {\n    'Employee_ID': [101, 102, 103, 104],\n    'Name': ['John Doe', 'Jane Smith', 'Bob Johnson', 'Alice Brown'],\n    'Department': ['Sales', 'Marketing', 'IT', 'HR'],\n    'Salary': [75000, 68000, 85000, 72000]\n}\n\ninventory_data = {\n    'Product_ID': ['P001', 'P002', 'P003', 'P004'],\n    'Product_Name': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n    'Stock_Quantity': [150, 75, 200, 50],\n    'Price': [50.00, 55.50, 45.75, 60.25]\n}\n\n# Create DataFrames\nsales_df = pd.DataFrame(sales_data)\nemployees_df = pd.DataFrame(employee_data)\ninventory_df = pd.DataFrame(inventory_data)\n\n# Specify the output directory and filename\noutput_path = '/path/to/your/directory/multi_sheet_data.xlsx'\n\n# Create Excel writer object\nwith pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n    # Write each DataFrame to a different sheet\n    sales_df.to_excel(writer, sheet_name='Sales', index=False)\n    employees_df.to_excel(writer, sheet_name='Employees', index=False)\n    inventory_df.to_excel(writer, sheet_name='Inventory', index=False)\n    \n    # Add a summary sheet with metadata\n    summary_data = {\n        'Report_Generated': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n        'Total_Sheets': [3],\n        'Sheets': ['Sales, Employees, Inventory']\n    }\n    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n\nprint(f\"Excel file created successfully at: {output_path}\")\n```\n\nNote: Replace `/path/to/your/directory/` with your actual desired directory path. Make sure you have the required packages installed:\n```bash\npip install pandas openpyxl\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["openpyxl", "datetime", "pandas"], "package_breakdown": {"install_packages": ["openpyxl", "datetime", "pandas"], "import_packages": ["datetime", "pandas"], "llm_suggested": ["pandas", "openpyxl"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts docstrings from a Python file in reST, Google, or Numpydoc format and saves them to a separate text file for documentation purposes.", "prompt_index": 353, "generated_code": "```python\nimport ast\nimport sys\nfrom pathlib import Path\n\ndef extract_docstrings(file_path):\n    \"\"\"Extract docstrings from a Python file using AST parsing.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            source_code = file.read()\n    except FileNotFoundError:\n        print(f\"Error: File {file_path} not found.\")\n        return []\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return []\n\n    try:\n        tree = ast.parse(source_code)\n    except SyntaxError as e:\n        print(f\"Syntax error in file: {e}\")\n        return []\n\n    docstrings = []\n    \n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module, ast.AsyncFunctionDef)):\n            if (docstring := ast.get_docstring(node)) is not None:\n                # Get the type and name of the node\n                if isinstance(node, ast.Module):\n                    element_type = \"Module\"\n                    element_name = Path(file_path).stem\n                elif isinstance(node, ast.ClassDef):\n                    element_type = \"Class\"\n                    element_name = node.name\n                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                    element_type = \"Function\" if isinstance(node, ast.FunctionDef) else \"AsyncFunction\"\n                    element_name = node.name\n                \n                docstrings.append({\n                    'type': element_type,\n                    'name': element_name,\n                    'docstring': docstring\n                })\n    \n    return docstrings\n\ndef save_docstrings(docstrings, output_file):\n    \"\"\"Save extracted docstrings to a text file.\"\"\"\n    try:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            for doc in docstrings:\n                f.write(f\"{'='*50}\\n\")\n                f.write(f\"{doc['type']}: {doc['name']}\\n\")\n                f.write(f\"{'='*50}\\n\")\n                f.write(doc['docstring'])\n                f.write(\"\\n\\n\")\n        print(f\"Docstrings saved to {output_file}\")\n    except Exception as e:\n        print(f\"Error saving docstrings: {e}\")\n\ndef main():\n    if len(sys.argv) != 3:\n        print(\"Usage: python extract_docstrings.py <input_file.py> <output_file.txt>\")\n        sys.exit(1)\n    \n    input_file = sys.argv[1]\n    output_file = sys.argv[2]\n    \n    if not input_file.endswith('.py'):\n        print(\"Error: Input file must be a Python file (.py)\")\n        sys.exit(1)\n    \n    docstrings = extract_docstrings(input_file)\n    \n    if docstrings:\n        save_docstrings(docstrings, output_file)\n        print(f\"Extracted {len(docstrings)} docstring(s)\")\n    else:\n        print(\"No docstrings found or error occurred.\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "pathlib", "ast"], "package_breakdown": {"install_packages": ["sys", "pathlib", "ast"], "import_packages": ["sys", "ast", "pathlib"], "llm_suggested": ["ast", "sys", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a sentence containing Unicode characters into a URL-friendly slug using the slugify function from the package, ensuring proper handling of Unicode characters.", "prompt_index": 354, "generated_code": "```python\nfrom slugify import slugify\n\ndef unicode_to_slug(sentence):\n    return slugify(sentence)\n```\n\nNote: This requires the `python-slugify` package to be installed (`pip install python-slugify`). The slugify function automatically handles Unicode characters by default.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["slugify", "python-slugify"], "package_breakdown": {"install_packages": ["slugify", "python-slugify"], "import_packages": ["slugify"], "llm_suggested": ["python-slugify"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the client library to execute a Jupyter notebook and save the output results for further analysis.", "prompt_index": 355, "generated_code": "```python\nfrom google.cloud import notebooks_v1\nfrom google.cloud.notebooks_v1.services.notebook_service import NotebookServiceClient\nfrom google.cloud.notebooks_v1.types import ExecuteNotebookRequest\nimport json\n\ndef execute_notebook(project_id, location, notebook_name, output_path):\n    \"\"\"Execute a Jupyter notebook and save the output results.\n    \n    Args:\n        project_id: GCP project ID\n        location: Notebook instance location (e.g., 'us-central1-a')\n        notebook_name: Name of the notebook instance\n        output_path: Path to save the output results\n    \"\"\"\n    \n    # Initialize the client\n    client = NotebookServiceClient()\n    \n    # Format the notebook instance name\n    notebook_instance = f\"projects/{project_id}/locations/{location}/instances/{notebook_name}\"\n    \n    # Create execute request\n    request = ExecuteNotebookRequest(\n        name=notebook_instance,\n        output_notebook_folder=output_path\n    )\n    \n    # Execute the notebook\n    operation = client.execute_notebook(request=request)\n    \n    # Wait for the operation to complete\n    response = operation.result()\n    \n    # Save the execution results\n    with open(f\"{output_path}/execution_results.json\", \"w\") as f:\n        json.dump({\n            \"operation_name\": operation.operation.name,\n            \"execution_status\": response.status.name,\n            \"output_path\": output_path\n        }, f, indent=2)\n    \n    print(f\"Notebook execution completed. Results saved to {output_path}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    execute_notebook(\n        project_id=\"your-project-id\",\n        location=\"us-central1-a\", \n        notebook_name=\"your-notebook-instance\",\n        output_path=\"gs://your-bucket/output-folder/\"\n    )\n```\n\nNote: This code requires the Google Cloud Notebooks API client library (`google-cloud-notebooks`). Install it with:\n```bash\npip install google-cloud-notebooks\n```\n\nYou'll also need to set up authentication using:\n```bash\ngcloud auth application-default login\n```\nor configure service account credentials.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "google", "google-cloud-notebooks"], "package_breakdown": {"install_packages": ["json", "google", "google-cloud-notebooks"], "import_packages": ["json", "google"], "llm_suggested": ["google-cloud-notebooks", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that installs a specified package in editable mode, allowing for development and modification of the package's source code directly within the project environment.", "prompt_index": 356, "generated_code": "```python\nimport subprocess\nimport sys\n\ndef install_editable_package(package_path):\n    \"\"\"\n    Install a package in editable mode from the specified path.\n    \n    Args:\n        package_path (str): Path to the package directory containing setup.py/pyproject.toml\n    \"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", package_path])\n        print(f\"Successfully installed {package_path} in editable mode\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing package: {e}\")\n        sys.exit(1)\n\n# Example usage:\n# install_editable_package(\"/path/to/your/package\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "subprocess"], "package_breakdown": {"install_packages": ["sys", "subprocess"], "import_packages": ["sys", "subprocess"], "llm_suggested": ["subprocess", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to the Google Cloud Pub/Sub API, creates a new topic, subscribes to messages from that topic, and publishes a sample message to the topic.", "prompt_index": 357, "generated_code": "```python\nfrom google.cloud import pubsub_v1\nimport os\n\n# Set environment variable for authentication (replace with your service account key path)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/service-account-key.json\"\n\n# Initialize clients\npublisher = pubsub_v1.PublisherClient()\nsubscriber = pubsub_v1.SubscriberClient()\n\n# Project and topic details\nproject_id = \"your-project-id\"\ntopic_id = \"sample-topic\"\nsubscription_id = \"sample-subscription\"\n\n# Create topic\ntopic_path = publisher.topic_path(project_id, topic_id)\ntry:\n    topic = publisher.create_topic(request={\"name\": topic_path})\n    print(f\"Topic created: {topic.name}\")\nexcept Exception as e:\n    print(f\"Topic may already exist: {e}\")\n\n# Create subscription\nsubscription_path = subscriber.subscription_path(project_id, subscription_id)\ntry:\n    subscription = subscriber.create_subscription(\n        request={\"name\": subscription_path, \"topic\": topic_path}\n    )\n    print(f\"Subscription created: {subscription.name}\")\nexcept Exception as e:\n    print(f\"Subscription may already exist: {e}\")\n\n# Publish a sample message\ndef publish_message():\n    data = \"Hello, Pub/Sub!\".encode(\"utf-8\")\n    future = publisher.publish(topic_path, data)\n    message_id = future.result()\n    print(f\"Published message with ID: {message_id}\")\n\n# Callback function for received messages\ndef callback(message):\n    print(f\"Received message: {message.data.decode('utf-8')}\")\n    message.ack()\n\n# Subscribe and listen for messages\ndef subscribe_messages():\n    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n    print(f\"Listening for messages on {subscription_path}...\")\n    \n    try:\n        streaming_pull_future.result()\n    except KeyboardInterrupt:\n        streaming_pull_future.cancel()\n\n# Execute functions\npublish_message()\nsubscribe_messages()\n```\n\nNote: Replace `\"your-project-id\"` with your actual GCP project ID and `\"path/to/your/service-account-key.json\"` with the path to your service account key file. The subscription will run indefinitely until interrupted (Ctrl+C).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-cloud-pubsub", "google", "os"], "package_breakdown": {"install_packages": ["google-cloud-pubsub", "google", "os"], "import_packages": ["google", "os"], "llm_suggested": ["google-cloud-pubsub"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the decorator to detect and raise an error when a subclass method attempts to override a superclass method with a different signature.", "prompt_index": 358, "generated_code": "```python\nimport inspect\nfrom functools import wraps\n\ndef enforce_signature(method):\n    @wraps(method)\n    def wrapper(*args, **kwargs):\n        return method(*args, **kwargs)\n    \n    wrapper._enforce_signature = True\n    return wrapper\n\nclass SignatureEnforcementMeta(type):\n    def __new__(cls, name, bases, namespace):\n        for attr_name, attr_value in namespace.items():\n            if callable(attr_value) and hasattr(attr_value, '_enforce_signature'):\n                # Check all base classes for methods with the same name\n                for base in bases:\n                    if hasattr(base, attr_name):\n                        base_method = getattr(base, attr_name)\n                        if callable(base_method):\n                            # Compare signatures\n                            base_sig = inspect.signature(base_method)\n                            derived_sig = inspect.signature(attr_value)\n                            \n                            if base_sig != derived_sig:\n                                raise TypeError(\n                                    f\"Method '{attr_name}' in class '{name}' \"\n                                    f\"attempts to override method in '{base.__name__}' \"\n                                    f\"with different signature.\\n\"\n                                    f\"Base signature: {base_sig}\\n\"\n                                    f\"Derived signature: {derived_sig}\"\n                                )\n        return super().__new__(cls, name, bases, namespace)\n\n# Example usage:\nclass BaseClass(metaclass=SignatureEnforcementMeta):\n    @enforce_signature\n    def example_method(self, param1: int, param2: str = \"default\") -> None:\n        pass\n\nclass ValidSubClass(BaseClass):\n    @enforce_signature\n    def example_method(self, param1: int, param2: str = \"default\") -> None:\n        pass\n\n# This would raise a TypeError:\n# class InvalidSubClass(BaseClass):\n#     @enforce_signature\n#     def example_method(self, param1: int) -> None:  # Missing param2\n#         pass\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["functools", "inspect"], "package_breakdown": {"install_packages": ["functools", "inspect"], "import_packages": ["functools", "inspect"], "llm_suggested": ["inspect", "functools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates a user with the Vertex AI API client library, retrieves a list of available datasets, and then creates a new dataset for training machine learning models.", "prompt_index": 359, "generated_code": "```python\nfrom google.cloud import aiplatform\nfrom google.oauth2 import service_account\n\n# Authenticate using service account credentials\ncredentials = service_account.Credentials.from_service_account_file(\n    'path/to/your/service-account-key.json'\n)\n\n# Initialize the Vertex AI client\naiplatform.init(\n    project='your-project-id',\n    location='your-region',  # e.g., 'us-central1'\n    credentials=credentials\n)\n\n# Retrieve list of available datasets\ndatasets = aiplatform.datasets._Dataset.list()\nprint(\"Available datasets:\")\nfor dataset in datasets:\n    print(f\"- {dataset.display_name} (ID: {dataset.name})\")\n\n# Create a new dataset\nnew_dataset = aiplatform.TabularDataset.create(\n    display_name='your-dataset-name',\n    gcs_source='gs://your-bucket/path/to/data.csv',  # Path to your training data\n    project='your-project-id',\n    location='your-region'\n)\n\nprint(f\"Created new dataset: {new_dataset.display_name} (ID: {new_dataset.name})\")\n```\n\nNote: Replace placeholders with your actual values:\n- `path/to/your/service-account-key.json`\n- `your-project-id`\n- `your-region` (e.g., 'us-central1')\n- `your-dataset-name`\n- `gs://your-bucket/path/to/data.csv`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-auth", "google", "google-cloud-aiplatform"], "package_breakdown": {"install_packages": ["google-auth", "google", "google-cloud-aiplatform"], "import_packages": ["google"], "llm_suggested": ["google-cloud-aiplatform", "google-auth"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the file transport adapter from the Requests package to upload a local file to a specified URL endpoint using a POST request.", "prompt_index": 360, "generated_code": "```python\nimport requests\n\ndef upload_file(url, file_path):\n    with open(file_path, 'rb') as file:\n        files = {'file': file}\n        response = requests.post(url, files=files)\n        return response\n\n# Example usage:\n# response = upload_file('https://example.com/upload', 'local_file.txt')\n# print(response.status_code)\n# print(response.text)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["requests"], "package_breakdown": {"install_packages": ["requests"], "import_packages": ["requests"], "llm_suggested": ["requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that extracts HTTP headers from a given URL using framework-independent HTTP protocol utils.", "prompt_index": 361, "generated_code": "```python\nimport socket\nfrom urllib.parse import urlparse\n\ndef extract_http_headers(url):\n    try:\n        # Parse URL\n        parsed_url = urlparse(url)\n        host = parsed_url.hostname\n        port = parsed_url.port or 80\n        path = parsed_url.path or '/'\n        if parsed_url.query:\n            path += '?' + parsed_url.query\n        \n        # Create socket connection\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(10)\n        sock.connect((host, port))\n        \n        # Send HTTP GET request\n        request = f\"GET {path} HTTP/1.1\\r\\nHost: {host}\\r\\nConnection: close\\r\\n\\r\\n\"\n        sock.send(request.encode())\n        \n        # Receive response\n        response = b''\n        while True:\n            data = sock.recv(4096)\n            if not data:\n                break\n            response += data\n        \n        sock.close()\n        \n        # Parse headers from response\n        response_str = response.decode('utf-8', errors='ignore')\n        header_end = response_str.find('\\r\\n\\r\\n')\n        \n        if header_end == -1:\n            return {}\n        \n        headers_part = response_str[:header_end]\n        headers = {}\n        \n        lines = headers_part.split('\\r\\n')\n        # Skip the HTTP status line\n        for line in lines[1:]:\n            if ': ' in line:\n                key, value = line.split(': ', 1)\n                headers[key] = value\n        \n        return headers\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {}\n\n# Example usage:\n# headers = extract_http_headers('http://example.com')\n# print(headers)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socket", "urllib"], "package_breakdown": {"install_packages": ["socket", "urllib"], "import_packages": ["socket", "urllib"], "llm_suggested": ["socket", "urllib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to an Elasticsearch instance, searches for documents containing a specific keyword, and returns the results.", "prompt_index": 362, "generated_code": "```python\nfrom elasticsearch import Elasticsearch\n\ndef search_elasticsearch(keyword, index_name, host='localhost', port=9200):\n    \"\"\"\n    Connect to Elasticsearch and search for documents containing the specified keyword.\n    \n    Args:\n        keyword (str): The search keyword\n        index_name (str): The Elasticsearch index to search in\n        host (str): Elasticsearch host (default: localhost)\n        port (int): Elasticsearch port (default: 9200)\n    \n    Returns:\n        dict: Search results\n    \"\"\"\n    try:\n        # Create Elasticsearch client\n        es = Elasticsearch([{'host': host, 'port': port}])\n        \n        # Verify connection\n        if not es.ping():\n            raise ValueError(\"Could not connect to Elasticsearch\")\n        \n        # Search query\n        query = {\n            \"query\": {\n                \"match\": {\n                    \"content\": keyword  # Assuming 'content' field contains the text\n                }\n            }\n        }\n        \n        # Execute search\n        response = es.search(index=index_name, body=query)\n        \n        return response\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n# Example usage:\n# results = search_elasticsearch(\"python\", \"my_index\", \"localhost\", 9200)\n# if results:\n#     print(f\"Found {results['hits']['total']['value']} results\")\n#     for hit in results['hits']['hits']:\n#         print(f\"Score: {hit['_score']}\")\n#         print(f\"Document: {hit['_source']}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["elasticsearch"], "package_breakdown": {"install_packages": ["elasticsearch"], "import_packages": ["elasticsearch"], "llm_suggested": ["elasticsearch"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that defines a class with methods for creating interfaces to enforce type-checking and provide a clear contract for classes that implement the interface.", "prompt_index": 363, "generated_code": "```python\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Type, get_type_hints\n\nclass Interface(ABC):\n    \"\"\"Base class for creating interfaces with type checking.\"\"\"\n    \n    @classmethod\n    def enforce_implementation(cls, implementation_class: Type) -> None:\n        \"\"\"\n        Enforce that a class properly implements all interface methods with correct signatures.\n        \n        Args:\n            implementation_class: The class to check against the interface\n            \n        Raises:\n            TypeError: If the class doesn't properly implement the interface\n        \"\"\"\n        if not issubclass(implementation_class, cls):\n            raise TypeError(f\"{implementation_class.__name__} must inherit from {cls.__name__}\")\n        \n        # Get method signatures from both interface and implementation\n        interface_methods = cls._get_abstract_methods()\n        implementation_methods = implementation_class._get_public_methods()\n        \n        # Check all abstract methods are implemented\n        for method_name in interface_methods:\n            if method_name not in implementation_methods:\n                raise TypeError(f\"Missing implementation for method: {method_name}\")\n            \n            # Check method signatures match\n            interface_sig = get_type_hints(getattr(cls, method_name))\n            implementation_sig = get_type_hints(getattr(implementation_class, method_name))\n            \n            if interface_sig != implementation_sig:\n                raise TypeError(f\"Method signature mismatch for {method_name}: \"\n                               f\"expected {interface_sig}, got {implementation_sig}\")\n    \n    @classmethod\n    def _get_abstract_methods(cls) -> list:\n        \"\"\"Get all abstract methods from the interface.\"\"\"\n        return [name for name in dir(cls) \n                if not name.startswith('_') and \n                callable(getattr(cls, name)) and \n                getattr(getattr(cls, name), '__isabstractmethod__', False)]\n    \n    def _get_public_methods(self) -> list:\n        \"\"\"Get all public methods from the instance.\"\"\"\n        return [name for name in dir(self) \n                if not name.startswith('_') and callable(getattr(self, name))]\n\n# Example usage\nclass DatabaseInterface(Interface):\n    \"\"\"Interface for database operations.\"\"\"\n    \n    @abstractmethod\n    def connect(self, connection_string: str) -> bool:\n        \"\"\"Connect to the database.\"\"\"\n        pass\n    \n    @abstractmethod\n    def query(self, sql: str, params: dict = None) -> list:\n        \"\"\"Execute a SQL query.\"\"\"\n        pass\n    \n    @abstractmethod\n    def close(self) -> None:\n        \"\"\"Close the database connection.\"\"\"\n        pass\n\n# Example implementation\nclass MySQLDatabase:\n    \"\"\"Implementation of DatabaseInterface for MySQL.\"\"\"\n    \n    def connect(self, connection_string: str) -> bool:\n        return True\n    \n    def query(self, sql: str, params: dict = None) -> list:\n        return [{\"id\": 1, \"name\": \"test\"}]\n    \n    def close(self) -> None:\n        pass\n\n# Usage\ntry:\n    DatabaseInterface.enforce_implementation(MySQLDatabase)\n    print(\"MySQLDatabase properly implements DatabaseInterface\")\nexcept TypeError as e:\n    print(f\"Implementation error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["abc", "typing"], "package_breakdown": {"install_packages": ["abc", "typing"], "import_packages": ["abc", "typing"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that securely hashes a user's password using the Argon2 algorithm to protect sensitive data.", "prompt_index": 364, "generated_code": "```python\nimport argon2\n\ndef hash_password(password: str) -> str:\n    \"\"\"\n    Securely hash a password using Argon2 algorithm.\n    \n    Args:\n        password: The plain text password to hash\n        \n    Returns:\n        str: The hashed password string\n    \"\"\"\n    # Create Argon2 password hasher with recommended parameters\n    ph = argon2.PasswordHasher(\n        time_cost=2,          # Number of iterations\n        memory_cost=102400,   # Memory usage in KiB (100MB)\n        parallelism=8,        # Number of parallel threads\n        hash_len=32,          # Hash length in bytes\n        salt_len=16           # Salt length in bytes\n    )\n    \n    return ph.hash(password)\n\ndef verify_password(hashed_password: str, password: str) -> bool:\n    \"\"\"\n    Verify a password against a stored hash.\n    \n    Args:\n        hashed_password: The stored hashed password\n        password: The plain text password to verify\n        \n    Returns:\n        bool: True if password matches, False otherwise\n    \"\"\"\n    ph = argon2.PasswordHasher()\n    \n    try:\n        return ph.verify(hashed_password, password)\n    except (argon2.exceptions.VerifyMismatchError, \n            argon2.exceptions.VerificationError,\n            argon2.exceptions.InvalidHashError):\n        return False\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Hash a password\n    password = \"my_secure_password_123\"\n    hashed = hash_password(password)\n    print(f\"Hashed password: {hashed}\")\n    \n    # Verify the password\n    is_valid = verify_password(hashed, password)\n    print(f\"Password verification: {is_valid}\")\n    \n    # Test with wrong password\n    is_valid_wrong = verify_password(hashed, \"wrong_password\")\n    print(f\"Wrong password verification: {is_valid_wrong}\")\n```\n\n**Note:** To use this code, you'll need to install the `argon2-cffi` package:\n```bash\npip install argon2-cffi\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["argon2", "argon2-cffi"], "package_breakdown": {"install_packages": ["argon2", "argon2-cffi"], "import_packages": ["argon2"], "llm_suggested": ["argon2-cffi"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that decodes a JSON string into a Python object using the simple, fast, and extensible JSON encoder/decoder package.", "prompt_index": 365, "generated_code": "```python\nimport json\n\n# Example JSON string\njson_string = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n\n# Decode JSON string to Python object\npython_object = json.loads(json_string)\n\n# Print the resulting Python object\nprint(python_object)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json"], "package_breakdown": {"install_packages": ["json"], "import_packages": ["json"], "llm_suggested": ["json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a complex class utilizing both attrs and dataclasses to allow for easy composition of attributes and methods within the class.", "prompt_index": 366, "generated_code": "```python\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional, Dict, Any\nimport attr\n\n@attr.s(auto_attribs=True)\nclass BaseComponent:\n    \"\"\"Base component using attrs for validation and defaults\"\"\"\n    id: str = attr.ib(validator=attr.validators.instance_of(str))\n    created_at: str = attr.ib(factory=lambda: \"2024-01-01\")\n    metadata: Dict[str, Any] = attr.ib(factory=dict)\n\n@dataclass\nclass DataComponent:\n    \"\"\"Data component using dataclass for simple data storage\"\"\"\n    name: str\n    value: float\n    tags: List[str] = field(default_factory=list)\n    description: Optional[str] = None\n\nclass ComplexComposite:\n    \"\"\"Complex class combining attrs and dataclass components\"\"\"\n    \n    def __init__(self, base_component: BaseComponent, data_components: List[DataComponent]):\n        self.base = base_component\n        self.data_components = data_components\n        self._cache = {}\n    \n    @property\n    def total_value(self) -> float:\n        \"\"\"Calculate total value from all data components\"\"\"\n        return sum(comp.value for comp in self.data_components)\n    \n    def get_component_by_name(self, name: str) -> Optional[DataComponent]:\n        \"\"\"Retrieve data component by name\"\"\"\n        for comp in self.data_components:\n            if comp.name == name:\n                return comp\n        return None\n    \n    def add_data_component(self, component: DataComponent) -> None:\n        \"\"\"Add a new data component\"\"\"\n        self.data_components.append(component)\n        self._cache.clear()  # Clear cache on modification\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary representation\"\"\"\n        return {\n            'base': attr.asdict(self.base),\n            'data_components': [{\n                'name': comp.name,\n                'value': comp.value,\n                'tags': comp.tags,\n                'description': comp.description\n            } for comp in self.data_components],\n            'total_value': self.total_value\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ComplexComposite':\n        \"\"\"Create instance from dictionary\"\"\"\n        base = BaseComponent(**data['base'])\n        data_components = [\n            DataComponent(\n                name=comp_data['name'],\n                value=comp_data['value'],\n                tags=comp_data.get('tags', []),\n                description=comp_data.get('description')\n            ) for comp_data in data['data_components']\n        ]\n        return cls(base, data_components)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "attrs", "attr", "dataclasses"], "package_breakdown": {"install_packages": ["typing", "attrs", "attr", "dataclasses"], "import_packages": ["attr", "dataclasses", "typing"], "llm_suggested": ["attrs", "typing", "dataclasses"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts complex data structures into a user-friendly text interface display, including formatting and organizing the data for easy readability.", "prompt_index": 367, "generated_code": "```python\nimport json\nfrom typing import Any, Dict, List, Union\n\ndef format_data_structure(data: Any, indent: int = 2, max_width: int = 80) -> str:\n    \"\"\"\n    Convert complex data structures into a user-friendly text display.\n    \n    Args:\n        data: The data structure to format (dict, list, tuple, etc.)\n        indent: Number of spaces for indentation\n        max_width: Maximum line width before wrapping\n    \n    Returns:\n        Formatted string representation of the data\n    \"\"\"\n    def _format_value(value: Any, current_indent: int, is_last: bool = True) -> str:\n        \"\"\"Recursively format individual values\"\"\"\n        if isinstance(value, (dict, Dict)):\n            return _format_dict(value, current_indent, is_last)\n        elif isinstance(value, (list, tuple, List)):\n            return _format_list(value, current_indent, is_last)\n        elif isinstance(value, str):\n            # Handle long strings by wrapping them\n            if len(value) > max_width - current_indent:\n                return f'\"{value}\"'\n            return f'\"{value}\"'\n        else:\n            return str(value)\n    \n    def _format_dict(d: Dict, current_indent: int, is_last: bool) -> str:\n        \"\"\"Format dictionary with proper indentation\"\"\"\n        if not d:\n            return \"{}\"\n        \n        items = []\n        keys = list(d.keys())\n        for i, key in enumerate(keys):\n            key_is_last = i == len(keys) - 1\n            formatted_key = f'\"{key}\": ' if isinstance(key, str) else f\"{key}: \"\n            formatted_value = _format_value(d[key], current_indent + indent, key_is_last)\n            \n            # Handle line wrapping for long key-value pairs\n            line = f\"{' ' * current_indent}{formatted_key}{formatted_value}\"\n            if len(line) > max_width and isinstance(d[key], (dict, list)):\n                # For complex values, put value on new line\n                formatted_value = _format_value(d[key], current_indent + indent, key_is_last)\n                items.append(f\"{' ' * current_indent}{formatted_key}\")\n                items.append(formatted_value)\n            else:\n                items.append(line)\n        \n        return \"{\\n\" + \"\\n\".join(items) + f\"\\n{' ' * (current_indent - indent)}}}\"\n    \n    def _format_list(lst: List, current_indent: int, is_last: bool) -> str:\n        \"\"\"Format list with proper indentation\"\"\"\n        if not lst:\n            return \"[]\"\n        \n        items = []\n        for i, item in enumerate(lst):\n            item_is_last = i == len(lst) - 1\n            formatted_item = _format_value(item, current_indent + indent, item_is_last)\n            \n            # Handle line wrapping\n            line = f\"{' ' * current_indent}{formatted_item}\"\n            if len(line) > max_width and isinstance(item, (dict, list)):\n                items.append(f\"{' ' * current_indent}\")\n                items.append(formatted_item)\n            else:\n                items.append(line)\n        \n        return \"[\\n\" + \"\\n\".join(items) + f\"\\n{' ' * (current_indent - indent)}]\"\n    \n    # Handle top-level data types\n    if isinstance(data, (dict, Dict)):\n        return _format_dict(data, indent, True)\n    elif isinstance(data, (list, tuple, List)):\n        return _format_list(data, indent, True)\n    else:\n        return str(data)\n\ndef display_data(data: Any, title: str = None) -> None:\n    \"\"\"\n    Display formatted data with optional title\n    \n    Args:\n        data: Data structure to display\n        title: Optional title for the data section\n    \"\"\"\n    if title:\n        print(f\"\\n{'=' * 50}\")\n        print(f\"{title.upper()}\")\n        print(f\"{'=' * 50}\")\n    \n    formatted = format_data_structure(data)\n    print(formatted)\n    \n    if title:\n        print(f\"{'=' * 50}\\n\")\n\n# Example usage and demonstration\nif __name__ == \"__main__\":\n    # Sample complex data structure\n    sample_data = {\n        \"users\": [\n            {\n                \"id\": 1,\n                \"name\": \"John Doe\",\n                \"email\": \"john.doe@example.com\",\n                \"roles\": [\"admin\", \"user\"],\n                \"preferences\": {\n                    \"theme\": \"dark\",\n                    \"notifications\": True,\n                    \"language\": \"en\"\n                }\n            },\n            {\n                \"id\": 2,\n                \"name\": \"Jane Smith\",\n                \"email\": \"jane.smith@example.com\",\n                \"roles\": [\"user\"],\n                \"preferences\": {\n                    \"theme\": \"light\",\n                    \"notifications\": False,\n                    \"language\": \"es\"\n                }\n            }\n        ],\n        \"metadata\": {\n            \"total_users\": 2,\n            \"last_updated\": \"2023-12-07T10:30:00Z\",\n            \"version\": \"1.2.3\"\n        }\n    }\n    \n    # Display the formatted data\n    display_data(sample_data, \"User Data Export\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "typing"], "package_breakdown": {"install_packages": ["json", "typing"], "import_packages": ["json", "typing"], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a given Go Duration string into a datetime.timedelta object.", "prompt_index": 368, "generated_code": "```python\nimport re\nfrom datetime import timedelta\n\ndef parse_go_duration(duration_str):\n    \"\"\"\n    Convert a Go duration string to a datetime.timedelta object.\n    \n    Go duration format examples: \"1h30m\", \"2h45m30s\", \"300ms\", \"1.5h\"\n    \n    Args:\n        duration_str (str): Go duration string\n        \n    Returns:\n        datetime.timedelta: Equivalent timedelta object\n        \n    Raises:\n        ValueError: If the input string format is invalid\n    \"\"\"\n    if not duration_str:\n        return timedelta()\n    \n    # Regular expression to match Go duration components\n    pattern = r'([-+]?\\d*\\.?\\d+)([a-zA-Z]+)'\n    matches = re.findall(pattern, duration_str)\n    \n    if not matches:\n        raise ValueError(f\"Invalid Go duration format: {duration_str}\")\n    \n    total_seconds = 0.0\n    \n    for value, unit in matches:\n        try:\n            numeric_value = float(value)\n        except ValueError:\n            raise ValueError(f\"Invalid numeric value '{value}' in duration string\")\n        \n        # Convert each unit to seconds\n        if unit == 'ns':\n            total_seconds += numeric_value / 1e9\n        elif unit == 'us' or unit == '\u00b5s':\n            total_seconds += numeric_value / 1e6\n        elif unit == 'ms':\n            total_seconds += numeric_value / 1000\n        elif unit == 's':\n            total_seconds += numeric_value\n        elif unit == 'm':\n            total_seconds += numeric_value * 60\n        elif unit == 'h':\n            total_seconds += numeric_value * 3600\n        else:\n            raise ValueError(f\"Unknown time unit '{unit}' in duration string\")\n    \n    return timedelta(seconds=total_seconds)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "datetime"], "package_breakdown": {"install_packages": ["re", "datetime"], "import_packages": ["re", "datetime"], "llm_suggested": ["re", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses CFFI bindings to implement the Argon2 password hashing algorithm for securely hashing passwords.", "prompt_index": 369, "generated_code": "```python\nfrom cffi import FFI\nimport os\n\nffi = FFI()\n\n# Define the C interface for Argon2\nffi.cdef(\"\"\"\n    int argon2id_hash_encoded(uint32_t t_cost, uint32_t m_cost,\n                             uint32_t parallelism, const void *pwd,\n                             size_t pwdlen, const void *salt,\n                             size_t saltlen, size_t hashlen, char *encoded,\n                             size_t encodedlen);\n    \n    int argon2id_verify(const char *encoded, const void *pwd, size_t pwdlen);\n    \n    const char *argon2_error_message(int error_code);\n\"\"\")\n\n# Load the Argon2 library\ntry:\n    # Try to load libargon2 (common on Linux/macOS)\n    argon2 = ffi.dlopen(\"argon2\")\nexcept OSError:\n    try:\n        # Try alternative name (some distributions)\n        argon2 = ffi.dlopen(\"libargon2.so.1\")\n    except OSError:\n        try:\n            # Try Windows naming convention\n            argon2 = ffi.dlopen(\"libargon2.dll\")\n        except OSError:\n            raise RuntimeError(\"Argon2 library not found. Please install libargon2\")\n\ndef hash_password(password, salt=None, time_cost=2, memory_cost=512, parallelism=2, hash_length=32):\n    \"\"\"\n    Hash a password using Argon2id algorithm.\n    \n    Args:\n        password (str): The password to hash\n        salt (bytes): Optional salt (random 16 bytes if not provided)\n        time_cost (int): Number of iterations\n        memory_cost (int): Memory usage in KiB\n        parallelism (int): Number of threads\n        hash_length (int): Length of the hash output\n    \n    Returns:\n        str: Encoded hash string\n    \"\"\"\n    if salt is None:\n        salt = os.urandom(16)\n    \n    if len(salt) < 8:\n        raise ValueError(\"Salt must be at least 8 bytes\")\n    \n    # Convert Python types to C types\n    password_bytes = password.encode('utf-8')\n    pwd_ptr = ffi.from_buffer(password_bytes)\n    salt_ptr = ffi.from_buffer(salt)\n    \n    # Allocate buffer for encoded hash\n    encoded_len = 256  # Sufficient for Argon2 encoded hashes\n    encoded_buffer = ffi.new(\"char[]\", encoded_len)\n    \n    # Call Argon2\n    result = argon2.argon2id_hash_encoded(\n        time_cost, memory_cost, parallelism,\n        pwd_ptr, len(password_bytes),\n        salt_ptr, len(salt),\n        hash_length, encoded_buffer, encoded_len\n    )\n    \n    if result != 0:\n        error_msg = ffi.string(argon2.argon2_error_message(result))\n        raise RuntimeError(f\"Argon2 error {result}: {error_msg}\")\n    \n    return ffi.string(encoded_buffer).decode('utf-8')\n\ndef verify_password(encoded_hash, password):\n    \"\"\"\n    Verify a password against an Argon2 encoded hash.\n    \n    Args:\n        encoded_hash (str): The encoded hash to verify against\n        password (str): The password to verify\n    \n    Returns:\n        bool: True if password matches, False otherwise\n    \"\"\"\n    password_bytes = password.encode('utf-8')\n    pwd_ptr = ffi.from_buffer(password_bytes)\n    encoded_hash_bytes = encoded_hash.encode('utf-8')\n    \n    result = argon2.argon2id_verify(\n        encoded_hash_bytes, pwd_ptr, len(password_bytes)\n    )\n    \n    return result == 0\n\n# Example usage\nif __name__ == \"__main__\":\n    # Hash a password\n    password = \"my_secure_password\"\n    hashed = hash_password(password)\n    print(f\"Hashed password: {hashed}\")\n    \n    # Verify the password\n    is_valid = verify_password(hashed, password)\n    print(f\"Password verification: {is_valid}\")\n    \n    # Test with wrong password\n    is_valid_wrong = verify_password(hashed, \"wrong_password\")\n    print(f\"Wrong password verification: {is_valid_wrong}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cffi", "os"], "package_breakdown": {"install_packages": ["cffi", "os"], "import_packages": ["cffi", "os"], "llm_suggested": ["cffi"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a GraphQL schema and resolves queries using the GraphQL.js implementation for Python, ensuring compatibility with the JavaScript reference implementation for GraphQL.", "prompt_index": 370, "generated_code": "```python\nimport graphene\n\nclass Query(graphene.ObjectType):\n    hello = graphene.String(name=graphene.String(default_value=\"World\"))\n\n    def resolve_hello(self, info, name):\n        return f\"Hello {name}!\"\n\nschema = graphene.Schema(query=Query)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["graphene"], "package_breakdown": {"install_packages": ["graphene"], "import_packages": ["graphene"], "llm_suggested": ["graphene"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves a list of all resources (projects, folders, and organizations) managed by the Google Cloud Resource Manager API client library.", "prompt_index": 371, "generated_code": "```python\nfrom google.cloud import resourcemanager_v3\nfrom google.api_core.exceptions import PermissionDenied\n\ndef list_all_resources():\n    \"\"\"\n    Retrieves a list of all resources (projects, folders, and organizations)\n    managed by the Google Cloud Resource Manager API.\n    \n    Returns:\n        List of resource dictionaries with type, name, and display name\n    \"\"\"\n    client = resourcemanager_v3.ProjectsClient()\n    folders_client = resourcemanager_v3.FoldersClient()\n    orgs_client = resourcemanager_v3.OrganizationsClient()\n    \n    resources = []\n    \n    try:\n        # List all projects\n        projects_request = resourcemanager_v3.ListProjectsRequest()\n        projects = client.list_projects(request=projects_request)\n        for project in projects:\n            resources.append({\n                'type': 'project',\n                'name': project.name,\n                'display_name': project.display_name\n            })\n    except PermissionDenied:\n        print(\"Permission denied for projects\")\n    \n    try:\n        # List all folders\n        folders_request = resourcemanager_v3.ListFoldersRequest()\n        folders = folders_client.list_folders(request=folders_request)\n        for folder in folders:\n            resources.append({\n                'type': 'folder',\n                'name': folder.name,\n                'display_name': folder.display_name\n            })\n    except PermissionDenied:\n        print(\"Permission denied for folders\")\n    \n    try:\n        # List all organizations\n        orgs_request = resourcemanager_v3.SearchOrganizationsRequest()\n        orgs = orgs_client.search_organizations(request=orgs_request)\n        for org in orgs:\n            resources.append({\n                'type': 'organization',\n                'name': org.name,\n                'display_name': org.display_name\n            })\n    except PermissionDenied:\n        print(\"Permission denied for organizations\")\n    \n    return resources\n\n# Example usage:\nif __name__ == \"__main__\":\n    all_resources = list_all_resources()\n    for resource in all_resources:\n        print(f\"{resource['type']}: {resource['display_name']} ({resource['name']})\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-api-core", "google", "google-cloud-resource-manager"], "package_breakdown": {"install_packages": ["google-api-core", "google", "google-cloud-resource-manager"], "import_packages": ["google"], "llm_suggested": ["google-cloud-resource-manager", "google-api-core"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads a Markdown file, applies a custom Pandoc filter written in Python to modify the document's elements, and then writes the modified Markdown output back to a file.", "prompt_index": 372, "generated_code": "```python\nimport panflute as pf\n\ndef custom_filter(elem, doc):\n    # Add your custom transformation logic here\n    # Example: Convert all level 2 headers to level 3\n    if isinstance(elem, pf.Header) and elem.level == 2:\n        elem.level = 3\n        return elem\n    \n    # Example: Remove all horizontal rules\n    if isinstance(elem, pf.HorizontalRule):\n        return []\n    \n    # Return None to leave the element unchanged\n    return None\n\ndef process_markdown(input_file, output_file):\n    with open(input_file, 'r', encoding='utf-8') as f:\n        md_content = f.read()\n    \n    # Convert markdown to panflute document\n    doc = pf.convert_text(md_content, input_format='markdown')\n    \n    # Apply the filter\n    doc = doc.walk(custom_filter)\n    \n    # Convert back to markdown\n    output_content = pf.convert_text(\n        doc, \n        input_format='panflute', \n        output_format='markdown',\n        extra_args=['--wrap=none']  # Prevent line wrapping\n    )\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(output_content)\n\n# Example usage\nif __name__ == '__main__':\n    input_file = 'input.md'\n    output_file = 'output.md'\n    process_markdown(input_file, output_file)\n```\n\nNote: This requires the `panflute` package (`pip install panflute`), which provides a Python interface to Pandoc. The `custom_filter` function can be modified to implement your specific document transformations.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["panflute"], "package_breakdown": {"install_packages": ["panflute"], "import_packages": ["panflute"], "llm_suggested": ["panflute"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to a remote server using SOCKS protocol through PySocks module, and sends a GET request to retrieve a webpage content.", "prompt_index": 373, "generated_code": "```python\nimport socks\nimport socket\nimport requests\nfrom urllib.parse import urlparse\n\n# Set up SOCKS proxy\nsocks.set_default_proxy(socks.SOCKS5, \"proxy_host\", 1080)  # Replace with actual proxy host and port\nsocket.socket = socks.socksocket\n\n# Send GET request through SOCKS proxy\nurl = \"http://example.com\"  # Replace with target URL\nresponse = requests.get(url)\n\n# Print the response content\nprint(response.text)\n```\n\nNote: Replace `\"proxy_host\"` with your SOCKS proxy server address and `1080` with the appropriate port number. Also replace `\"http://example.com\"` with your target URL.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["socks", "urllib", "requests", "socket"], "package_breakdown": {"install_packages": ["socks", "urllib", "requests", "socket"], "import_packages": ["socks", "requests", "socket", "urllib"], "llm_suggested": ["socks", "requests"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a reinforcement learning environment using the Gym package, specifically setting up a CartPole environment with default parameters for training an agent to balance a pole on a moving cart.", "prompt_index": 374, "generated_code": "```python\nimport gym\n\n# Create the CartPole environment with default parameters\nenv = gym.make('CartPole-v1')\n\n# Reset the environment to initial state\nobservation = env.reset()\n\n# Example of environment interaction loop\ndone = False\nwhile not done:\n    # Take a random action (replace with agent's policy in actual training)\n    action = env.action_space.sample()\n    \n    # Step through the environment\n    observation, reward, done, info = env.step(action)\n    \n    # Render the environment (optional - can be commented out for faster training)\n    env.render()\n\n# Close the environment\nenv.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["gym"], "package_breakdown": {"install_packages": ["gym"], "import_packages": ["gym"], "llm_suggested": ["gym"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a custom Pygments theme by utilizing JupyterLab CSS variables for syntax highlighting in code snippets within a Jupyter notebook.", "prompt_index": 375, "generated_code": "```python\nfrom pygments.style import Style\nfrom pygments.token import (\n    Keyword, Name, Comment, String, Error, Number, Operator, Generic, \n    Whitespace, Punctuation, Text, Literal\n)\n\nclass JupyterLabTheme(Style):\n    \"\"\"\n    A Pygments theme that uses JupyterLab CSS variables for syntax highlighting.\n    This allows the theme to adapt to JupyterLab's light/dark mode and custom themes.\n    \"\"\"\n    \n    default_style = \"\"\n    \n    styles = {\n        Text:                  \"var(--jp-content-font-color1)\",\n        Whitespace:           \"\",\n        Error:                \"var(--jp-error-color1)\",\n        Other:                \"\",\n        \n        Comment:              \"var(--jp-mirror-editor-comment-color)\",\n        Comment.Preproc:      \"var(--jp-mirror-editor-comment-color)\",\n        Comment.Special:      \"var(--jp-mirror-editor-comment-color) bold\",\n        \n        Keyword:              \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Constant:     \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Declaration:  \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Namespace:    \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Pseudo:       \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Reserved:     \"var(--jp-mirror-editor-keyword-color)\",\n        Keyword.Type:         \"var(--jp-mirror-editor-keyword-color)\",\n        \n        Operator:             \"var(--jp-mirror-editor-operator-color)\",\n        Operator.Word:        \"var(--jp-mirror-editor-operator-color)\",\n        \n        Punctuation:          \"var(--jp-mirror-editor-punctuation-color)\",\n        \n        Name:                 \"var(--jp-mirror-editor-variable-color)\",\n        Name.Attribute:       \"var(--jp-mirror-editor-property-color)\",\n        Name.Builtin:         \"var(--jp-mirror-editor-builtin-color)\",\n        Name.Builtin.Pseudo:  \"var(--jp-mirror-editor-builtin-color)\",\n        Name.Class:           \"var(--jp-mirror-editor-type-color)\",\n        Name.Constant:        \"var(--jp-mirror-editor-constant-color)\",\n        Name.Decorator:       \"var(--jp-mirror-editor-decorator-color)\",\n        Name.Entity:          \"var(--jp-mirror-editor-variable-color)\",\n        Name.Exception:       \"var(--jp-mirror-editor-exception-color)\",\n        Name.Function:        \"var(--jp-mirror-editor-function-color)\",\n        Name.Label:           \"var(--jp-mirror-editor-label-color)\",\n        Name.Namespace:       \"var(--jp-mirror-editor-namespace-color)\",\n        Name.Other:           \"var(--jp-mirror-editor-variable-color)\",\n        Name.Property:        \"var(--jp-mirror-editor-property-color)\",\n        Name.Tag:             \"var(--jp-mirror-editor-tag-color)\",\n        Name.Variable:        \"var(--jp-mirror-editor-variable-color)\",\n        Name.Variable.Global: \"var(--jp-mirror-editor-variable-color)\",\n        Name.Variable.Instance: \"var(--jp-mirror-editor-variable-color)\",\n        \n        Number:               \"var(--jp-mirror-editor-number-color)\",\n        Number.Float:         \"var(--jp-mirror-editor-number-color)\",\n        Number.Hex:           \"var(--jp-mirror-editor-number-color)\",\n        Number.Integer:       \"var(--jp-mirror-editor-number-color)\",\n        Number.Integer.Long:  \"var(--jp-mirror-editor-number-color)\",\n        Number.Oct:           \"var(--jp-mirror-editor-number-color)\",\n        \n        Literal:              \"var(--jp-mirror-editor-string-color)\",\n        Literal.Date:         \"var(--jp-mirror-editor-string-color)\",\n        \n        String:               \"var(--jp-mirror-editor-string-color)\",\n        String.Backtick:      \"var(--jp-mirror-editor-string-color)\",\n        String.Char:          \"var(--jp-mirror-editor-string-color)\",\n        String.Doc:           \"var(--jp-mirror-editor-string-color)\",\n        String.Double:        \"var(--jp-mirror-editor-string-color)\",\n        String.Escape:        \"var(--jp-mirror-editor-string-color)\",\n        String.Heredoc:       \"var(--jp-mirror-editor-string-color)\",\n        String.Interpol:      \"var(--jp-mirror-editor-string-color)\",\n        String.Other:         \"var(--jp-mirror-editor-string-color)\",\n        String.Regex:         \"var(--jp-mirror-editor-string-color)\",\n        String.Single:        \"var(--jp-mirror-editor-string-color)\",\n        String.Symbol:        \"var(--jp-mirror-editor-string-color)\",\n        \n        Generic:              \"\",\n        Generic.Deleted:      \"var(--jp-error-color1)\",\n        Generic.Emph:         \"italic\",\n        Generic.Error:        \"var(--jp-error-color1)\",\n        Generic.Heading:      \"var(--jp-content-font-color1) bold\",\n        Generic.Inserted:     \"var(--jp-success-color1)\",\n        Generic.Output:       \"var(--jp-content-font-color2)\",\n        Generic.Prompt:       \"var(--jp-content-font-color3)\",\n        Generic.Strong:       \"bold\",\n        Generic.Subheading:   \"var(--jp-content-font-color1) bold\",\n        Generic.Traceback:    \"var(--jp-error-color1)\",\n    }\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pygments"], "package_breakdown": {"install_packages": ["pygments"], "import_packages": ["pygments"], "llm_suggested": ["pygments"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the dbt package to transform raw data from a database into structured analytics tables, allowing for seamless collaboration and efficient data modeling within a data team.", "prompt_index": 376, "generated_code": "```python\nimport dbt\nfrom dbt.cli.main import dbtRunner\n\n# Initialize dbt runner\ndbt_runner = dbtRunner()\n\n# Run dbt commands programmatically\ndef run_dbt_commands():\n    # Parse and validate dbt project\n    parse_result = dbt_runner.invoke([\"parse\"])\n    if not parse_result.success:\n        print(\"dbt parse failed\")\n        return False\n    \n    # Run models to transform raw data\n    run_result = dbt_runner.invoke([\"run\"])\n    if not run_result.success:\n        print(\"dbt run failed\")\n        return False\n    \n    # Test data quality and transformations\n    test_result = dbt_runner.invoke([\"test\"])\n    if not test_result.success:\n        print(\"dbt test failed\")\n        return False\n    \n    # Generate documentation\n    docs_result = dbt_runner.invoke([\"docs\", \"generate\"])\n    if not docs_result.success:\n        print(\"dbt docs generate failed\")\n        return False\n    \n    return True\n\n# Example usage\nif __name__ == \"__main__\":\n    success = run_dbt_commands()\n    if success:\n        print(\"dbt transformation completed successfully\")\n    else:\n        print(\"dbt transformation failed\")\n```\n\nNote: This code requires:\n1. A properly configured dbt project with `dbt_project.yml`\n2. Database connection profiles configured in `~/.dbt/profiles.yml`\n3. SQL models defined in the `models/` directory\n4. Tests and documentation configured in the project\n\nThe actual transformation logic would be implemented in SQL files within the dbt project structure, not in Python code.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["dbt-core", "dbt"], "package_breakdown": {"install_packages": ["dbt-core", "dbt"], "import_packages": ["dbt"], "llm_suggested": ["dbt-core"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a custom enumeration class using the Advanced Enumerations package, defining distinct values with specific names and associated constant values.", "prompt_index": 377, "generated_code": "```python\nfrom enum import Enum, auto\n\nclass CustomEnum(Enum):\n    VALUE_ONE = auto()\n    VALUE_TWO = auto()\n    VALUE_THREE = auto()\n    \n    @classmethod\n    def get_value(cls, name):\n        return cls[name].value\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["enum"], "package_breakdown": {"install_packages": ["enum"], "import_packages": ["enum"], "llm_suggested": ["enum"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that tokenizes a given text using the Natural Language Toolkit (NLTK) library, and prints out a list of all the words in the text along with their corresponding part-of-speech tags.", "prompt_index": 378, "generated_code": "```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\n# Download required NLTK data (run once)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\ndef tokenize_and_tag(text):\n    tokens = word_tokenize(text)\n    tagged_tokens = pos_tag(tokens)\n    return tagged_tokens\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_text = \"The quick brown fox jumps over the lazy dog.\"\n    tagged_words = tokenize_and_tag(sample_text)\n    \n    for word, tag in tagged_words:\n        print(f\"{word}: {tag}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["nltk"], "package_breakdown": {"install_packages": ["nltk"], "import_packages": ["nltk"], "llm_suggested": ["nltk"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that lists all files in a specific directory on Google Cloud Storage using the Convenient Filesystem interface provided by this package.", "prompt_index": 379, "generated_code": "```python\nfrom gcsfs import GCSFileSystem\n\n# Initialize the GCS filesystem\nfs = GCSFileSystem()\n\n# Specify your bucket name\nbucket_name = \"your-bucket-name\"\n\n# List all files in the bucket\nfiles = fs.ls(bucket_name)\n\n# Print the list of files\nfor file in files:\n    print(file)\n```\n\nNote: You'll need to install the `gcsfs` package (`pip install gcsfs`) and ensure you have appropriate Google Cloud credentials configured (either through environment variables, service account key file, or other GCP authentication methods).", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["gcsfs"], "package_breakdown": {"install_packages": ["gcsfs"], "import_packages": ["gcsfs"], "llm_suggested": ["gcsfs"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads data from a specific column in a Microsoft Excel .xls spreadsheet file and stores it in a list for further processing.", "prompt_index": 380, "generated_code": "```python\nimport xlrd\n\ndef read_excel_column(file_path, sheet_index, column_index):\n    workbook = xlrd.open_workbook(file_path)\n    sheet = workbook.sheet_by_index(sheet_index)\n    \n    column_data = []\n    for row_index in range(sheet.nrows):\n        cell_value = sheet.cell_value(row_index, column_index)\n        column_data.append(cell_value)\n    \n    return column_data\n\n# Example usage:\n# data = read_excel_column('your_file.xls', 0, 2)  # Reads column index 2 from first sheet\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["xlrd"], "package_breakdown": {"install_packages": ["xlrd"], "import_packages": ["xlrd"], "llm_suggested": ["xlrd"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a PostgreSQL database, retrieves data from a specific table, and performs a data analysis using pandas library.", "prompt_index": 381, "generated_code": "```python\nimport psycopg2\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# Database connection parameters\ndb_params = {\n    'host': 'localhost',\n    'database': 'your_database',\n    'user': 'your_username',\n    'password': 'your_password',\n    'port': '5432'\n}\n\n# Create SQLAlchemy engine for pandas\nengine = create_engine(f'postgresql+psycopg2://{db_params[\"user\"]}:{db_params[\"password\"]}@{db_params[\"host\"]}:{db_params[\"port\"]}/{db_params[\"database\"]}')\n\n# SQL query to retrieve data\ntable_name = 'your_table_name'\nquery = f'SELECT * FROM {table_name}'\n\n# Load data into pandas DataFrame\ndf = pd.read_sql(query, engine)\n\n# Basic data analysis examples\nprint(\"DataFrame Info:\")\nprint(df.info())\n\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\n\nprint(\"\\nDescriptive statistics:\")\nprint(df.describe())\n\nprint(\"\\nColumn data types:\")\nprint(df.dtypes)\n\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Example: Group by analysis\nif 'category_column' in df.columns:\n    grouped_data = df.groupby('category_column').agg({\n        'numeric_column': ['mean', 'sum', 'count']\n    })\n    print(\"\\nGrouped analysis:\")\n    print(grouped_data)\n\n# Example: Filtering and analysis\nif 'date_column' in df.columns:\n    df['date_column'] = pd.to_datetime(df['date_column'])\n    recent_data = df[df['date_column'] > '2023-01-01']\n    print(f\"\\nRecords after 2023-01-01: {len(recent_data)}\")\n\n# Close the connection\nengine.dispose()\n```\n\nNote: Replace the database parameters, table name, and column names with your actual values. Install required packages using: `pip install psycopg2-binary pandas sqlalchemy`", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["psycopg2", "psycopg2-binary", "pandas", "sqlalchemy"], "package_breakdown": {"install_packages": ["psycopg2", "psycopg2-binary", "pandas", "sqlalchemy"], "import_packages": ["psycopg2", "pandas", "sqlalchemy"], "llm_suggested": ["psycopg2-binary", "pandas", "sqlalchemy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an OrderedSet object and adds elements in a specific order, ensuring that duplicates are not allowed and the order of insertion is preserved when iterating over the set.", "prompt_index": 382, "generated_code": "```python\nfrom collections.abc import MutableSet\n\nclass OrderedSet(MutableSet):\n    def __init__(self, iterable=None):\n        self._dict = {}\n        if iterable is not None:\n            self.update(iterable)\n    \n    def __contains__(self, value):\n        return value in self._dict\n    \n    def __iter__(self):\n        return iter(self._dict)\n    \n    def __len__(self):\n        return len(self._dict)\n    \n    def add(self, value):\n        self._dict[value] = None\n    \n    def discard(self, value):\n        self._dict.pop(value, None)\n    \n    def update(self, iterable):\n        for item in iterable:\n            self.add(item)\n    \n    def __repr__(self):\n        if not self:\n            return f'{self.__class__.__name__}()'\n        return f'{self.__class__.__name__}({list(self)!r})'\n    \n    def __eq__(self, other):\n        if isinstance(other, OrderedSet):\n            return list(self) == list(other)\n        return set(self) == set(other)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["collections"], "package_breakdown": {"install_packages": ["collections"], "import_packages": ["collections"], "llm_suggested": ["collections"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that filters a list of numbers to only include even numbers using the tools and utilities provided in the package.", "prompt_index": 383, "generated_code": "```python\ndef filter_even_numbers(numbers):\n    return list(filter(lambda x: x % 2 == 0, numbers))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the typing stubs from PyYAML to annotate a function that loads a YAML file and returns its content as a dictionary.", "prompt_index": 384, "generated_code": "```python\nfrom typing import Dict, Any\nimport yaml\nfrom yaml import Loader\n\ndef load_yaml_file(file_path: str) -> Dict[Any, Any]:\n    \"\"\"\n    Load a YAML file and return its content as a dictionary.\n    \n    Args:\n        file_path: Path to the YAML file to load\n        \n    Returns:\n        Dictionary containing the parsed YAML content\n    \"\"\"\n    with open(file_path, 'r') as file:\n        return yaml.load(file, Loader=Loader)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["yaml", "PyYAML", "typing"], "package_breakdown": {"install_packages": ["yaml", "PyYAML", "typing"], "import_packages": ["yaml", "typing"], "llm_suggested": ["PyYAML"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a JSON5 file containing data in JavaScript-like syntax into a standard JSON file for compatibility with other systems.", "prompt_index": 385, "generated_code": "```python\nimport json\nimport json5\n\ndef convert_json5_to_json(json5_file_path, json_file_path):\n    \"\"\"\n    Converts a JSON5 file to a standard JSON file.\n    \n    Args:\n        json5_file_path (str): Path to the input JSON5 file\n        json_file_path (str): Path to the output JSON file\n    \"\"\"\n    try:\n        # Read and parse JSON5 file\n        with open(json5_file_path, 'r', encoding='utf-8') as f:\n            data = json5.load(f)\n        \n        # Write as standard JSON\n        with open(json_file_path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n        \n        print(f\"Successfully converted {json5_file_path} to {json_file_path}\")\n        \n    except FileNotFoundError:\n        print(f\"Error: File not found - {json5_file_path}\")\n    except json5.JSON5DecodeError as e:\n        print(f\"Error: Invalid JSON5 syntax - {e}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    convert_json5_to_json(\"input.json5\", \"output.json\")\n```\n\n**Note:** This code requires the `json5` package. Install it using:\n```bash\npip install json5\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["json", "json5"], "package_breakdown": {"install_packages": ["json", "json5"], "import_packages": ["json", "json5"], "llm_suggested": ["json5"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a Python code snippet as input and outputs the abstract syntax tree of the code, with support for type inference.", "prompt_index": 386, "generated_code": "```python\nimport ast\nimport astor\nfrom typing import Dict, Any, Optional\n\nclass TypeInferenceVisitor(ast.NodeVisitor):\n    def __init__(self):\n        self.type_env: Dict[str, str] = {}\n        self.annotated_ast = None\n    \n    def visit_Assign(self, node):\n        # Visit the value first to infer its type\n        self.generic_visit(node)\n        \n        # Infer type from the assigned value\n        if hasattr(node, 'value_type'):\n            inferred_type = node.value_type\n        else:\n            # Default to 'Any' if type cannot be inferred\n            inferred_type = 'Any'\n        \n        # Add type annotation to target variables\n        for target in node.targets:\n            if isinstance(target, ast.Name):\n                self.type_env[target.id] = inferred_type\n                # Add type annotation to the AST node\n                target.annotation = ast.Name(id=inferred_type, ctx=ast.Load())\n        \n        return node\n    \n    def visit_BinOp(self, node):\n        self.generic_visit(node)\n        \n        # Simple type inference for binary operations\n        left_type = getattr(node.left, 'inferred_type', 'Any')\n        right_type = getattr(node.right, 'inferred_type', 'Any')\n        \n        if left_type == right_type and left_type != 'Any':\n            node.inferred_type = left_type\n        else:\n            node.inferred_type = 'Any'\n        \n        return node\n    \n    def visit_Constant(self, node):\n        # Infer type from constant values\n        if isinstance(node.value, int):\n            node.inferred_type = 'int'\n        elif isinstance(node.value, float):\n            node.inferred_type = 'float'\n        elif isinstance(node.value, str):\n            node.inferred_type = 'str'\n        elif isinstance(node.value, bool):\n            node.inferred_type = 'bool'\n        else:\n            node.inferred_type = 'Any'\n        \n        return node\n    \n    def visit_Name(self, node):\n        # Look up type from environment\n        if node.id in self.type_env:\n            node.inferred_type = self.type_env[node.id]\n        else:\n            node.inferred_type = 'Any'\n        \n        return node\n\ndef get_ast_with_type_inference(code_snippet: str) -> ast.AST:\n    \"\"\"\n    Parse Python code and perform basic type inference on the AST.\n    \n    Args:\n        code_snippet: Python code as a string\n        \n    Returns:\n        AST with type annotations added based on inference\n    \"\"\"\n    try:\n        # Parse the code into AST\n        tree = ast.parse(code_snippet)\n        \n        # Create type inference visitor and traverse the AST\n        visitor = TypeInferenceVisitor()\n        visitor.visit(tree)\n        \n        return tree\n        \n    except SyntaxError as e:\n        raise ValueError(f\"Invalid Python code: {e}\")\n\ndef ast_to_dict(node: ast.AST) -> Dict[str, Any]:\n    \"\"\"\n    Convert AST node to a dictionary representation.\n    \n    Args:\n        node: AST node to convert\n        \n    Returns:\n        Dictionary representation of the AST\n    \"\"\"\n    if isinstance(node, ast.AST):\n        result = {'_type': type(node).__name__}\n        for field, value in ast.iter_fields(node):\n            if isinstance(value, list):\n                result[field] = [ast_to_dict(item) for item in value]\n            elif isinstance(value, ast.AST):\n                result[field] = ast_to_dict(value)\n            else:\n                result[field] = value\n        \n        # Add inferred type if available\n        if hasattr(node, 'inferred_type'):\n            result['inferred_type'] = getattr(node, 'inferred_type')\n        \n        return result\n    elif isinstance(node, list):\n        return [ast_to_dict(item) for item in node]\n    else:\n        return node\n\ndef code_to_ast_with_types(code_snippet: str) -> Dict[str, Any]:\n    \"\"\"\n    Main function that takes Python code and returns AST with type inference.\n    \n    Args:\n        code_snippet: Python code as a string\n        \n    Returns:\n        Dictionary representation of AST with type annotations\n    \"\"\"\n    ast_tree = get_ast_with_type_inference(code_snippet)\n    return ast_to_dict(ast_tree)\n\n# Example usage:\nif __name__ == \"__main__\":\n    example_code = \"\"\"\nx = 42\ny = \"hello\"\nz = x + 10\n\"\"\"\n    \n    result = code_to_ast_with_types(example_code)\n    import json\n    print(json.dumps(result, indent=2))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["astor", "ast", "typing", "json"], "package_breakdown": {"install_packages": ["astor", "ast", "typing", "json"], "import_packages": ["json", "astor", "ast", "typing"], "llm_suggested": ["ast", "astor", "typing", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that encodes HTTP headers using the HPACK algorithm in a purely Pythonic way, without relying on external libraries.", "prompt_index": 387, "generated_code": "```python\nimport sys\n\nclass HuffmanEncoder:\n    \"\"\"Huffman encoding for HPACK\"\"\"\n    \n    # HPACK Huffman table (from RFC 7541)\n    HUFFMAN_TABLE = [\n        (0x1ff8, 13), (0x7fffd8, 23), (0xfffffe2, 28), (0xfffffe3, 28),\n        (0xfffffe4, 28), (0xfffffe5, 28), (0xfffffe6, 28), (0xfffffe7, 28),\n        (0xfffffe8, 28), (0xffffea, 24), (0x3ffffffc, 30), (0xfffffe9, 28),\n        (0xfffffea, 28), (0x3ffffffd, 30), (0xfffffeb, 28), (0xfffffec, 28),\n        (0xfffffed, 28), (0xfffffee, 28), (0xfffffef, 28), (0xffffff0, 28),\n        (0xffffff1, 28), (0xffffff2, 28), (0x3ffffffe, 30), (0xffffff3, 28),\n        (0xffffff4, 28), (0xffffff5, 28), (0xffffff6, 28), (0xffffff7, 28),\n        (0xffffff8, 28), (0xffffff9, 28), (0xffffffa, 28), (0xffffffb, 28),\n        (0x14, 6), (0x3f8, 10), (0x3f9, 10), (0xffa, 12), (0x1ff9, 13),\n        (0x15, 6), (0xf8, 8), (0x7fa, 11), (0x3fa, 10), (0x3fb, 10),\n        (0xf9, 8), (0x7fb, 11), (0xfa, 8), (0x16, 6), (0x17, 6), (0x18, 6),\n        (0x0, 5), (0x1, 5), (0x2, 5), (0x19, 6), (0x1a, 6), (0x1b, 6),\n        (0x1c, 6), (0x1d, 6), (0x1e, 6), (0x1f, 6), (0x5c, 7), (0xfb, 8),\n        (0x7ffc, 15), (0x20, 6), (0xffb, 12), (0x3fc, 10), (0x1ffa, 13),\n        (0x21, 6), (0x5d, 7), (0x5e, 7), (0x5f, 7), (0x60, 7), (0x61, 7),\n        (0x62, 7), (0x63, 7), (0x64, 7), (0x65, 7), (0x66, 7), (0x67, 7),\n        (0x68, 7), (0x69, 7), (0x6a, 7), (0x6b, 7), (0x6c, 7), (0x6d, 7),\n        (0x6e, 7), (0x6f, 7), (0x70, 7), (0x71, 7), (0x72, 7), (0xfc, 8),\n        (0x73, 7), (0xfd, 8), (0x1ffb, 13), (0x7fff0, 19), (0x1ffc, 13),\n        (0x3ffc, 14), (0x22, 6), (0x7ffd, 15), (0x3, 5), (0x23, 6), (0x4, 5),\n        (0x24, 6), (0x5, 5), (0x25, 6), (0x26, 6), (0x27, 6), (0x6, 5),\n        (0x74, 7), (0x75, 7), (0x28, 6), (0x29, 6), (0x2a, 6), (0x7, 5),\n        (0x2b, 6), (0x76, 7), (0x2c, 6), (0x8, 5), (0x9, 5), (0x2d, 6),\n        (0x77, 7), (0x78, 7), (0x79, 7), (0x7a, 7), (0x7b, 7), (0x7ffe, 15),\n        (0x7fc, 11), (0x3ffd, 14), (0x1ffd, 13), (0xffffffc, 28), (0xfffe6, 20),\n        (0x3fffd2, 22), (0xfffe7, 20), (0xfffe8, 20), (0x3fffd3, 22),\n        (0x3fffd4, 22), (0x3fffd5, 22), (0x7fffd9, 23), (0x3fffd6, 22),\n        (0x7fffda, 23), (0x7fffdb, 23), (0x7fffdc, 23), (0x7fffdd, 23),\n        (0x7fffde, 23), (0xffffeb, 24), (0x7fffdf, 23), (0xffffec, 24),\n        (0xffffed, 24), (0x3fffd7, 22), (0x7fffe0, 23), (0xffffee, 24),\n        (0x7fffe1, 23), (0x7fffe2, 23), (0x7fffe3, 23), (0x7fffe4, 23),\n        (0x1fffdc, 21), (0x3fffd8, 22), (0x7fffe5, 23), (0x3fffd9, 22),\n        (0x7fffe6, 23), (0x7fffe7, 23), (0xffffef, 24), (0x3fffda, 22),\n        (0x1fffdd, 21), (0xfffe9, 20), (0x3fffdb, 22), (0x3fffdc, 22),\n        (0x7fffe8, 23), (0x7fffe9, 23), (0x1fffde, 21), (0x7fffea, 23),\n        (0x3fffdd, 22), (0x3fffde, 22), (0xfffff0, 24), (0x1fffdf, 21),\n        (0x3fffdf, 22), (0x7fffeb, 23), (0x7fffec, 23), (0x1fffe0, 21),\n        (0x1fffe1, 21), (0x3fffe0, 22), (0x1fffe2, 21), (0x7fffed, 23),\n        (0x3fffe1, 22), (0x7fffee, 23), (0x7fffef, 23), (0xfffea, 20),\n        (0x3fffe2, 22), (0x3fffe3, 22), (0x3fffe4, 22), (0x7ffff0, 23),\n        (0x3fffe5, 22), (0x3fffe6, 22), (0x7ffff1, 23), (0x3ffffe0, 26),\n        (0x3ffffe1, 26), (0x3ffffe2, 26), (0x7ffff2, 23), (0x3ffffe3, 26),\n        (0x3ffffe4, 26), (0x7ffff3, 23), (0x7ffff4, 23), (0x7ffff5, 23),\n        (0xfffeb, 20), (0x7fff6, 19), (0x3fffe7, 22), (0x7ffff6, 23),\n        (0x3fffe8, 22), (0x1ffffec, 25), (0x3ffffe5, 26), (0x3ffffe", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys"], "package_breakdown": {"install_packages": ["sys"], "import_packages": ["sys"], "llm_suggested": ["sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the CUBLAS native runtime libraries to perform matrix multiplication on two large matrices stored on the GPU.", "prompt_index": 388, "generated_code": "```python\nimport numpy as np\nimport cupy as cp\n\n# Generate two large matrices\nn = 4096  # Matrix size (n x n)\na_cpu = np.random.rand(n, n).astype(np.float32)\nb_cpu = np.random.rand(n, n).astype(np.float32)\n\n# Transfer matrices to GPU\na_gpu = cp.asarray(a_cpu)\nb_gpu = cp.asarray(b_cpu)\n\n# Perform matrix multiplication using CUBLAS (via CuPy)\nc_gpu = cp.matmul(a_gpu, b_gpu)\n\n# Optional: Transfer result back to CPU\nc_cpu = cp.asnumpy(c_gpu)\n\nprint(f\"Matrix multiplication completed: {n}x{n} matrices\")\nprint(f\"Result shape: {c_cpu.shape}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cupy", "numpy"], "package_breakdown": {"install_packages": ["cupy", "numpy"], "import_packages": ["cupy", "numpy"], "llm_suggested": ["numpy", "cupy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that parses HTTP/2 frames using only native Python code, without relying on any external libraries.", "prompt_index": 389, "generated_code": "```python\nimport struct\nimport enum\nfrom typing import Dict, Tuple, Optional, Union\n\nclass FrameType(enum.IntEnum):\n    DATA = 0x0\n    HEADERS = 0x1\n    PRIORITY = 0x2\n    RST_STREAM = 0x3\n    SETTINGS = 0x4\n    PUSH_PROMISE = 0x5\n    PING = 0x6\n    GOAWAY = 0x7\n    WINDOW_UPDATE = 0x8\n    CONTINUATION = 0x9\n\nclass Flags(enum.IntFlag):\n    ACK = 0x1\n    END_STREAM = 0x1\n    END_HEADERS = 0x4\n    PADDED = 0x8\n    PRIORITY = 0x20\n\nclass HTTP2Frame:\n    def __init__(self, length: int, type_: FrameType, flags: int, stream_id: int, payload: bytes):\n        self.length = length\n        self.type = type_\n        self.flags = flags\n        self.stream_id = stream_id\n        self.payload = payload\n\n    def __repr__(self):\n        return (f\"HTTP2Frame(length={self.length}, type={self.type.name}, \"\n                f\"flags=0x{self.flags:02x}, stream_id={self.stream_id})\")\n\ndef parse_frame_header(data: bytes) -> Tuple[int, int, int, int, int]:\n    \"\"\"Parse HTTP/2 frame header (9 bytes)\"\"\"\n    if len(data) < 9:\n        raise ValueError(\"Incomplete frame header\")\n    \n    length = (data[0] << 16) | (data[1] << 8) | data[2]\n    type_byte = data[3]\n    flags = data[4]\n    stream_id = struct.unpack('!I', data[5:9])[0] & 0x7FFFFFFF\n    \n    return length, type_byte, flags, stream_id, 9\n\ndef parse_frame(data: bytes) -> HTTP2Frame:\n    \"\"\"Parse complete HTTP/2 frame\"\"\"\n    if len(data) < 9:\n        raise ValueError(\"Data too short for frame header\")\n    \n    length, type_byte, flags, stream_id, header_len = parse_frame_header(data)\n    \n    if len(data) < header_len + length:\n        raise ValueError(\"Incomplete frame payload\")\n    \n    payload = data[header_len:header_len + length]\n    \n    return HTTP2Frame(length, FrameType(type_byte), flags, stream_id, payload)\n\ndef parse_data_frame(frame: HTTP2Frame) -> Dict:\n    \"\"\"Parse DATA frame\"\"\"\n    result = {'type': 'DATA', 'flags': frame.flags}\n    \n    if frame.flags & Flags.PADDED:\n        if len(frame.payload) < 1:\n            raise ValueError(\"PADDED flag set but no padding length\")\n        pad_length = frame.payload[0]\n        result['data'] = frame.payload[1:-pad_length] if pad_length > 0 else frame.payload[1:]\n        result['padding'] = pad_length\n    else:\n        result['data'] = frame.payload\n    \n    return result\n\ndef parse_headers_frame(frame: HTTP2Frame) -> Dict:\n    \"\"\"Parse HEADERS frame\"\"\"\n    result = {'type': 'HEADERS', 'flags': frame.flags}\n    offset = 0\n    \n    if frame.flags & Flags.PADDED:\n        pad_length = frame.payload[offset]\n        offset += 1\n        result['padding'] = pad_length\n    \n    if frame.flags & Flags.PRIORITY:\n        if len(frame.payload) - offset < 5:\n            raise ValueError(\"PRIORITY flag set but incomplete priority data\")\n        stream_dep = struct.unpack('!I', frame.payload[offset:offset+4])[0]\n        weight = frame.payload[offset+4] + 1\n        offset += 5\n        result['priority'] = {'stream_dependency': stream_dep, 'weight': weight}\n    \n    result['header_block_fragment'] = frame.payload[offset:]\n    return result\n\ndef parse_settings_frame(frame: HTTP2Frame) -> Dict:\n    \"\"\"Parse SETTINGS frame\"\"\"\n    result = {'type': 'SETTINGS', 'flags': frame.flags, 'settings': {}}\n    \n    if frame.flags & Flags.ACK:\n        if frame.length != 0:\n            raise ValueError(\"SETTINGS ACK frame must have zero length\")\n        return result\n    \n    if frame.length % 6 != 0:\n        raise ValueError(\"SETTINGS frame length must be multiple of 6\")\n    \n    for i in range(0, frame.length, 6):\n        identifier = struct.unpack('!H', frame.payload[i:i+2])[0]\n        value = struct.unpack('!I', frame.payload[i+2:i+6])[0]\n        result['settings'][identifier] = value\n    \n    return result\n\ndef parse_frame_payload(frame: HTTP2Frame) -> Dict:\n    \"\"\"Parse frame payload based on frame type\"\"\"\n    if frame.type == FrameType.DATA:\n        return parse_data_frame(frame)\n    elif frame.type == FrameType.HEADERS:\n        return parse_headers_frame(frame)\n    elif frame.type == FrameType.SETTINGS:\n        return parse_settings_frame(frame)\n    elif frame.type == FrameType.PING:\n        return {'type': 'PING', 'opaque_data': frame.payload}\n    elif frame.type == FrameType.RST_STREAM:\n        error_code = struct.unpack('!I', frame.payload)[0] if len(frame.payload) >= 4 else 0\n        return {'type': 'RST_STREAM', 'error_code': error_code}\n    elif frame.type == FrameType.GOAWAY:\n        last_stream_id = struct.unpack('!I', frame.payload[0:4])[0] if len(frame.payload) >= 8 else 0\n        error_code = struct.unpack('!I', frame.payload[4:8])[0] if len(frame.payload) >= 8 else 0\n        debug_data = frame.payload[8:] if len(frame.payload) > 8 else b''\n        return {'type': 'GOAWAY', 'last_stream_id': last_stream_id, \n                'error_code': error_code, 'debug_data': debug_data}\n    else:\n        return {'type': frame.type.name, 'raw_payload': frame.payload}\n\n# Example usage:\ndef parse_http2_frames(data: bytes) -> list:\n    \"\"\"Parse multiple HTTP/2 frames from byte data\"\"\"\n    frames = []\n    offset = 0\n    \n    while offset < len(data):\n        try:\n            frame = parse_frame(data[offset:])\n            parsed = parse_frame_payload(frame)\n            frames.append(parsed)\n            offset += 9 + frame.length  # Header (9 bytes) + payload length\n        except (ValueError, IndexError) as e:\n            print(f\"Error parsing frame at offset {offset}: {e}\")\n            break\n    \n    return frames\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "struct", "enum"], "package_breakdown": {"install_packages": ["typing", "struct", "enum"], "import_packages": ["struct", "enum", "typing"], "llm_suggested": ["enum", "struct", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes CUSPARSE native runtime libraries to perform sparse matrix operations for efficient computation on NVIDIA GPUs.", "prompt_index": 390, "generated_code": "```python\nimport cupy as cp\nfrom cupyx.scipy.sparse import csr_matrix\nfrom cupy.cuda import cusparse\n\ndef sparse_matrix_operations():\n    # Initialize cuSPARSE handle\n    handle = cusparse.create()\n    \n    try:\n        # Create sample sparse matrix in CSR format\n        data = cp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype=cp.float32)\n        indices = cp.array([0, 1, 1, 2, 2, 3], dtype=cp.int32)\n        indptr = cp.array([0, 2, 4, 6, 6], dtype=cp.int32)\n        \n        # Create sparse matrix\n        sparse_mat = csr_matrix((data, indices, indptr), shape=(4, 4))\n        \n        # Create dense vector\n        dense_vec = cp.array([1.0, 2.0, 3.0, 4.0], dtype=cp.float32)\n        \n        # Perform sparse matrix-vector multiplication (SpMV)\n        result = cp.zeros(4, dtype=cp.float32)\n        \n        # cuSPARSE SpMV operation\n        cusparse.csrmvEx(\n            handle=handle,\n            alg=0,  # CUSPARSE_ALG_MERGE_PATH\n            transA=cusparse.CUSPARSE_OPERATION_NON_TRANSPOSE,\n            m=4, n=4, nnz=6,\n            alpha=cp.array([1.0], dtype=cp.float32),\n            descrA=cusparse.createMatDescr(),\n            csrValA=sparse_mat.data.astype(cp.float32),\n            csrRowPtrA=sparse_mat.indptr.astype(cp.int32),\n            csrColIndA=sparse_mat.indices.astype(cp.int32),\n            x=dense_vec.astype(cp.float32),\n            beta=cp.array([0.0], dtype=cp.float32),\n            y=result\n        )\n        \n        return result\n        \n    finally:\n        # Cleanup\n        cusparse.destroy(handle)\n\n# Example usage\nif __name__ == \"__main__\":\n    result = sparse_matrix_operations()\n    print(\"Sparse matrix-vector multiplication result:\")\n    print(result)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["cupyx"], "packages_detected": ["cupy", "cupyx"], "package_breakdown": {"install_packages": ["cupy", "cupyx"], "import_packages": ["cupy", "cupyx"], "llm_suggested": ["cupy", "cupyx"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves and displays logs from a Google Cloud App Engine project using the Appengine Logging API client library.", "prompt_index": 391, "generated_code": "```python\nfrom google.cloud import logging\nfrom google.cloud.logging_v2 import DESCENDING\n\ndef get_appengine_logs(project_id, filter_str=None, max_results=10):\n    \"\"\"\n    Retrieve and display logs from Google Cloud App Engine project.\n    \n    Args:\n        project_id (str): Google Cloud project ID\n        filter_str (str): Optional filter string for log entries\n        max_results (int): Maximum number of log entries to retrieve\n    \"\"\"\n    # Initialize the client\n    client = logging.Client(project=project_id)\n    \n    # Default filter for App Engine logs if none provided\n    if filter_str is None:\n        filter_str = 'resource.type=\"gae_app\"'\n    \n    # List log entries\n    entries = client.list_entries(\n        filter_=filter_str,\n        order_by=DESCENDING,\n        page_size=max_results\n    )\n    \n    # Display log entries\n    print(f\"App Engine Logs for project '{project_id}':\")\n    print(\"-\" * 50)\n    \n    for entry in entries:\n        print(f\"Timestamp: {entry.timestamp}\")\n        print(f\"Severity: {entry.severity}\")\n        print(f\"Log Name: {entry.log_name}\")\n        print(f\"Payload: {entry.payload}\")\n        if entry.resource:\n            print(f\"Resource: {entry.resource}\")\n        if entry.labels:\n            print(f\"Labels: {entry.labels}\")\n        print(\"-\" * 30)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Replace with your actual project ID\n    PROJECT_ID = \"your-project-id\"\n    \n    # Optional: Add specific filters\n    # filter_string = 'resource.type=\"gae_app\" AND severity>=ERROR'\n    \n    get_appengine_logs(PROJECT_ID, max_results=5)\n```\n\n**Prerequisites:**\n1. Install the client library: `pip install google-cloud-logging`\n2. Set up authentication: `gcloud auth application-default login` or use service account credentials\n3. Replace `\"your-project-id\"` with your actual Google Cloud project ID\n\n**Optional filters examples:**\n- `'resource.type=\"gae_app\" AND severity>=ERROR'` - Only error logs\n- `'resource.type=\"gae_app\" AND resource.labels.module_id=\"default\"'` - Specific service\n- `'resource.type=\"gae_app\" AND timestamp>=\"2023-01-01T00:00:00Z\"'` - Time-based filtering", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-cloud-logging", "google"], "package_breakdown": {"install_packages": ["google-cloud-logging", "google"], "import_packages": ["google"], "llm_suggested": ["google-cloud-logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes an HTTP/2 connection and sends a GET request to a specific URL, then prints out the response received from the server.", "prompt_index": 392, "generated_code": "```python\nimport httpx\n\nurl = 'https://http2.pro/api/v1'  # Example URL that supports HTTP/2\n\nwith httpx.Client(http2=True) as client:\n    response = client.get(url)\n    \n    print(f\"Status Code: {response.status_code}\")\n    print(f\"HTTP Version: {response.http_version}\")\n    print(f\"Headers: {dict(response.headers)}\")\n    print(f\"Response Body:\\n{response.text}\")\n```\n\nNote: This code requires the `httpx` library (`pip install httpx`). The example URL `https://http2.pro/api/v1` is a known HTTP/2 testing endpoint, but you can replace it with any URL that supports HTTP/2.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["httpx"], "package_breakdown": {"install_packages": ["httpx"], "import_packages": ["httpx"], "llm_suggested": ["httpx"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that configures the logging module to display log messages in different colors based on their severity levels, such as red for errors, yellow for warnings, and green for info messages.", "prompt_index": 393, "generated_code": "```python\nimport logging\n\nclass ColoredFormatter(logging.Formatter):\n    COLORS = {\n        'DEBUG': '\\033[94m',    # Blue\n        'INFO': '\\033[92m',     # Green\n        'WARNING': '\\033[93m',  # Yellow\n        'ERROR': '\\033[91m',    # Red\n        'CRITICAL': '\\033[95m', # Magenta\n    }\n    RESET = '\\033[0m'\n    \n    def format(self, record):\n        log_message = super().format(record)\n        color = self.COLORS.get(record.levelname, self.RESET)\n        return f\"{color}{log_message}{self.RESET}\"\n\ndef setup_colored_logging():\n    handler = logging.StreamHandler()\n    formatter = ColoredFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler.setFormatter(formatter)\n    \n    logger = logging.getLogger()\n    logger.addHandler(handler)\n    logger.setLevel(logging.INFO)\n    \n    return logger\n\n# Example usage\nif __name__ == \"__main__\":\n    logger = setup_colored_logging()\n    logger.debug(\"This is a debug message\")\n    logger.info(\"This is an info message\")\n    logger.warning(\"This is a warning message\")\n    logger.error(\"This is an error message\")\n    logger.critical(\"This is a critical message\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["logging"], "package_breakdown": {"install_packages": ["logging"], "import_packages": ["logging"], "llm_suggested": ["logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that initializes and configures a JupyterLab server with custom server components for enhanced functionality and user experience in JupyterLab-like applications.", "prompt_index": 394, "generated_code": "```python\nfrom jupyter_server.serverapp import ServerApp\nfrom jupyter_server.utils import url_path_join\nfrom jupyter_server.base.handlers import JupyterHandler\nimport tornado.web\nimport os\n\nclass CustomHandler(JupyterHandler):\n    \"\"\"Custom handler for enhanced functionality\"\"\"\n    \n    @tornado.web.authenticated\n    def get(self):\n        self.write({\n            'status': 'success',\n            'message': 'Custom server component is working!',\n            'custom_data': {'feature_enabled': True, 'version': '1.0.0'}\n        })\n\ndef load_jupyter_server_extension(serverapp: ServerApp):\n    \"\"\"Main server extension entry point\"\"\"\n    \n    # Add custom API endpoint\n    web_app = serverapp.web_app\n    base_url = web_app.settings['base_url']\n    custom_route = url_path_join(base_url, 'custom-api', 'status')\n    web_app.add_handlers('.*$', [(custom_route, CustomHandler)])\n    \n    # Configure custom settings\n    serverapp.log.info(\"Custom JupyterLab server extension loaded\")\n    \n    # Add custom template paths or static files if needed\n    # template_path = os.path.join(os.path.dirname(__file__), 'templates')\n    # static_path = os.path.join(os.path.dirname(__file__), 'static')\n    \n    # Additional configuration can be added here\n    configure_custom_settings(serverapp)\n\ndef configure_custom_settings(serverapp):\n    \"\"\"Configure custom server settings\"\"\"\n    \n    # Enable additional features\n    serverapp.allow_origin = '*'  # Adjust for production use\n    serverapp.disable_check_xsrf = False\n    \n    # Set custom workspace and user settings\n    serverapp.workspaces_dir = os.path.expanduser('~/.jupyter/custom_workspaces')\n    serverapp.user_settings_dir = os.path.expanduser('~/.jupyter/custom_settings')\n    \n    # Configure additional kernel options\n    serverapp.kernel_manager.default_kernel_name = 'python3'\n    \n    # Add custom environment variables\n    os.environ['CUSTOM_JUPYTER_FEATURE'] = 'enabled'\n\n# For JupyterLab 3.0+ compatibility\ndef _jupyter_server_extension_points():\n    return [{\n        'module': __name__,\n        'app': ServerApp\n    }]\n\n# Example usage configuration\nif __name__ == '__main__':\n    # Initialize and configure the server\n    server = ServerApp.instance()\n    \n    # Set custom configuration\n    server.port = 8888\n    server.ip = 'localhost'\n    server.allow_root = False\n    server.open_browser = False\n    \n    # Load extensions\n    server.load_jupyter_server_extension(__name__)\n    \n    # Start the server\n    server.initialize([])\n    server.start()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["jupyter_server"], "packages_detected": ["tornado", "os", "jupyter_server"], "package_breakdown": {"install_packages": ["tornado", "os", "jupyter_server"], "import_packages": ["tornado", "os", "jupyter_server"], "llm_suggested": ["jupyter_server", "tornado"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compares two Python objects and identifies the differences between them, then recreates one object by applying the identified deltas from the other object.", "prompt_index": 395, "generated_code": "```python\nimport copy\nfrom typing import Any, Dict, List, Tuple, Union\n\ndef deep_compare(obj1: Any, obj2: Any, path: str = \"\") -> List[Dict[str, Any]]:\n    \"\"\"\n    Compare two objects recursively and return a list of differences.\n    \n    Args:\n        obj1: First object to compare\n        obj2: Second object to compare\n        path: Current path in the object hierarchy (for nested differences)\n    \n    Returns:\n        List of dictionaries containing difference information\n    \"\"\"\n    differences = []\n    \n    # Type mismatch\n    if type(obj1) != type(obj2):\n        differences.append({\n            'path': path,\n            'type': 'type_mismatch',\n            'value1': obj1,\n            'value2': obj2\n        })\n        return differences\n    \n    # Handle different data types\n    if isinstance(obj1, dict):\n        # Compare dictionaries\n        all_keys = set(obj1.keys()) | set(obj2.keys())\n        for key in all_keys:\n            new_path = f\"{path}.{key}\" if path else key\n            if key in obj1 and key not in obj2:\n                differences.append({\n                    'path': new_path,\n                    'type': 'key_removed',\n                    'value': obj1[key]\n                })\n            elif key not in obj1 and key in obj2:\n                differences.append({\n                    'path': new_path,\n                    'type': 'key_added',\n                    'value': obj2[key]\n                })\n            else:\n                differences.extend(deep_compare(obj1[key], obj2[key], new_path))\n                \n    elif isinstance(obj1, (list, tuple)):\n        # Compare sequences\n        len1, len2 = len(obj1), len(obj2)\n        min_len = min(len1, len2)\n        \n        # Compare common elements\n        for i in range(min_len):\n            new_path = f\"{path}[{i}]\"\n            differences.extend(deep_compare(obj1[i], obj2[i], new_path))\n        \n        # Handle extra elements\n        if len1 > len2:\n            for i in range(len2, len1):\n                new_path = f\"{path}[{i}]\"\n                differences.append({\n                    'path': new_path,\n                    'type': 'element_removed',\n                    'value': obj1[i]\n                })\n        elif len2 > len1:\n            for i in range(len1, len2):\n                new_path = f\"{path}[{i}]\"\n                differences.append({\n                    'path': new_path,\n                    'type': 'element_added',\n                    'value': obj2[i]\n                })\n                \n    else:\n        # Compare primitive values\n        if obj1 != obj2:\n            differences.append({\n                'path': path,\n                'type': 'value_changed',\n                'value1': obj1,\n                'value2': obj2\n            })\n    \n    return differences\n\ndef apply_deltas(base_obj: Any, differences: List[Dict[str, Any]]) -> Any:\n    \"\"\"\n    Apply differences to base object to recreate target object.\n    \n    Args:\n        base_obj: The base object to apply changes to\n        differences: List of differences from deep_compare()\n    \n    Returns:\n        Modified object with differences applied\n    \"\"\"\n    result = copy.deepcopy(base_obj)\n    \n    for diff in differences:\n        path = diff['path']\n        diff_type = diff['type']\n        \n        # Navigate to the target location\n        current = result\n        path_parts = path.split('.')\n        \n        # Handle array indices in path\n        for i, part in enumerate(path_parts):\n            if '[' in part and ']' in part:\n                # Handle array access like \"items[0]\"\n                key_part = part.split('[')[0]\n                if key_part:\n                    current = current[key_part]\n                \n                # Get all indices in this part\n                import re\n                indices = re.findall(r'\\[(\\d+)\\]', part)\n                for index in indices:\n                    current = current[int(index)]\n            else:\n                if i < len(path_parts) - 1:\n                    current = current[part]\n        \n        # Apply the difference\n        if diff_type == 'value_changed':\n            if isinstance(path_parts[-1], str) and '[' not in path_parts[-1]:\n                current[path_parts[-1]] = diff['value2']\n            else:\n                # For arrays, we're already at the element\n                if isinstance(current, list):\n                    # Find which index we're at\n                    last_part = path_parts[-1]\n                    if '[' in last_part:\n                        index = int(re.search(r'\\[(\\d+)\\]', last_part).group(1))\n                        current[index] = diff['value2']\n        \n        elif diff_type == 'key_added':\n            current[diff['path'].split('.')[-1]] = diff['value']\n        \n        elif diff_type == 'key_removed':\n            del current[diff['path'].split('.')[-1]]\n        \n        elif diff_type == 'element_added':\n            if isinstance(current, list):\n                current.append(diff['value'])\n        \n        elif diff_type == 'element_removed':\n            if isinstance(current, list):\n                # For simplicity, we'll just clear and rebuild if needed\n                # In a real implementation, you'd track specific indices\n                pass\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example objects to compare\n    obj1 = {\n        \"name\": \"John\",\n        \"age\": 30,\n        \"hobbies\": [\"reading\", \"swimming\"],\n        \"address\": {\n            \"street\": \"123 Main St\",\n            \"city\": \"New York\"\n        }\n    }\n    \n    obj2 = {\n        \"name\": \"John\",\n        \"age\": 31,\n        \"hobbies\": [\"reading\", \"running\", \"hiking\"],\n        \"address\": {\n            \"street\": \"123 Main St\",\n            \"city\": \"Boston\"\n        },\n        \"married\": True\n    }\n    \n    # Find differences\n    differences = deep_compare(obj1, obj2)\n    print(\"Differences found:\")\n    for diff in differences:\n        print(f\"{diff['path']}: {diff['type']}\")\n    \n    # Recreate obj2 from obj1 by applying differences\n    recreated_obj2 = apply_deltas(obj1, differences)\n    print(\"\\nRecreated object matches original:\", recreated_obj2 == obj2)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "copy", "re"], "package_breakdown": {"install_packages": ["typing", "copy", "re"], "import_packages": ["typing", "re", "copy"], "llm_suggested": ["copy", "typing", "re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that opens a Chrome browser using Selenium WebDriver, navigates to a specified URL, fills out a form with given data, submits the form, and captures the resulting page source.", "prompt_index": 396, "generated_code": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ndef automate_form_submission(url, form_data):\n    # Set up Chrome options\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")  # Remove this line if you want to see the browser\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    \n    # Initialize the WebDriver\n    driver = webdriver.Chrome(options=chrome_options)\n    \n    try:\n        # Navigate to the specified URL\n        driver.get(url)\n        \n        # Wait for the form to be present\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.TAG_NAME, \"form\"))\n        )\n        \n        # Fill out the form with the provided data\n        for field_name, field_value in form_data.items():\n            try:\n                # Try to find input field by name\n                element = driver.find_element(By.NAME, field_name)\n                element.clear()\n                element.send_keys(field_value)\n            except:\n                try:\n                    # Try to find input field by id if name doesn't work\n                    element = driver.find_element(By.ID, field_name)\n                    element.clear()\n                    element.send_keys(field_value)\n                except:\n                    print(f\"Could not find form field: {field_name}\")\n        \n        # Submit the form\n        try:\n            # Try to find and click submit button\n            submit_button = driver.find_element(By.XPATH, \"//input[@type='submit']\")\n            submit_button.click()\n        except:\n            try:\n                # Try alternative submit button selectors\n                submit_button = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n                submit_button.click()\n            except:\n                # If no submit button found, submit the form directly\n                form = driver.find_element(By.TAG_NAME, \"form\")\n                form.submit()\n        \n        # Wait for the page to load after submission\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n        )\n        \n        # Capture the resulting page source\n        page_source = driver.page_source\n        \n        return page_source\n        \n    finally:\n        # Close the browser\n        driver.quit()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Specify the URL and form data\n    url = \"https://example.com/form\"\n    form_data = {\n        \"username\": \"testuser\",\n        \"email\": \"test@example.com\",\n        \"password\": \"testpassword123\"\n    }\n    \n    # Run the automation\n    result = automate_form_submission(url, form_data)\n    print(result)\n```\n\nNote: You'll need to install the required packages:\n```bash\npip install selenium webdriver-manager\n```\n\nAlso make sure you have Chrome browser installed and the appropriate ChromeDriver for your Chrome version. The code includes headless mode by default - remove the `--headless` argument if you want to see the browser window.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["webdriver-manager", "selenium"], "package_breakdown": {"install_packages": ["webdriver-manager", "selenium"], "import_packages": ["selenium"], "llm_suggested": ["selenium", "webdriver-manager"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a Pandas DataFrame to a format compatible with SQL systems like BigQuery or Spanner, including handling data types and column mappings for seamless integration.", "prompt_index": 397, "generated_code": "```python\nimport pandas as pd\nfrom google.cloud import bigquery\nfrom google.cloud.bigquery import SchemaField\nimport pyarrow as pa\n\ndef pandas_to_bigquery_schema(df):\n    \"\"\"\n    Convert pandas DataFrame dtypes to BigQuery schema fields\n    \n    Args:\n        df: pandas DataFrame\n    \n    Returns:\n        List of BigQuery SchemaField objects\n    \"\"\"\n    type_mapping = {\n        'int64': 'INTEGER',\n        'int32': 'INTEGER',\n        'float64': 'FLOAT',\n        'float32': 'FLOAT',\n        'bool': 'BOOLEAN',\n        'datetime64[ns]': 'TIMESTAMP',\n        'datetime64[ns, UTC]': 'TIMESTAMP',\n        'object': 'STRING',\n        'string': 'STRING'\n    }\n    \n    schema = []\n    for column, dtype in df.dtypes.items():\n        bq_type = type_mapping.get(str(dtype), 'STRING')\n        schema.append(SchemaField(column, bq_type))\n    \n    return schema\n\ndef pandas_to_bigquery_format(df, project_id, dataset_id, table_id):\n    \"\"\"\n    Convert pandas DataFrame to BigQuery-compatible format and load to BigQuery\n    \n    Args:\n        df: pandas DataFrame\n        project_id: BigQuery project ID\n        dataset_id: BigQuery dataset ID\n        table_id: BigQuery table ID\n    \"\"\"\n    # Handle NaN values and data type conversions\n    df_clean = df.copy()\n    \n    # Convert datetime columns to proper format\n    for col in df_clean.select_dtypes(include=['datetime64']).columns:\n        df_clean[col] = df_clean[col].astype('datetime64[ns, UTC]')\n    \n    # Convert boolean columns\n    for col in df_clean.select_dtypes(include=['bool']).columns:\n        df_clean[col] = df_clean[col].astype(bool)\n    \n    # Convert to pyarrow table for efficient BigQuery loading\n    table = pa.Table.from_pandas(df_clean)\n    \n    # Initialize BigQuery client\n    client = bigquery.Client(project=project_id)\n    \n    # Get or create dataset reference\n    dataset_ref = client.dataset(dataset_id)\n    \n    # Create table reference\n    table_ref = dataset_ref.table(table_id)\n    \n    # Load data to BigQuery\n    job_config = bigquery.LoadJobConfig(\n        schema=pandas_to_bigquery_schema(df_clean),\n        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n    )\n    \n    job = client.load_table_from_dataframe(\n        df_clean, table_ref, job_config=job_config\n    )\n    job.result()  # Wait for job completion\n    \n    return f\"Loaded {df_clean.shape[0]} rows to {table_id}\"\n\ndef pandas_to_spanner_format(df, instance_id, database_id, table_id):\n    \"\"\"\n    Convert pandas DataFrame to Cloud Spanner-compatible format\n    \n    Args:\n        df: pandas DataFrame\n        instance_id: Spanner instance ID\n        database_id: Spanner database ID\n        table_id: Spanner table ID\n    \n    Returns:\n        List of dictionaries ready for Spanner insertion\n    \"\"\"\n    from google.cloud import spanner\n    \n    # Handle data type conversions for Spanner\n    df_spanner = df.copy()\n    \n    # Convert datetime columns\n    for col in df_spanner.select_dtypes(include=['datetime64']).columns:\n        df_spanner[col] = df_spanner[col].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n    \n    # Convert boolean columns to strings (Spanner expects 'true'/'false')\n    for col in df_spanner.select_dtypes(include=['bool']).columns:\n        df_spanner[col] = df_spanner[col].astype(str).str.lower()\n    \n    # Convert NaN values to None\n    df_spanner = df_spanner.where(pd.notnull(df_spanner), None)\n    \n    # Convert to list of dictionaries for Spanner\n    spanner_data = df_spanner.to_dict('records')\n    \n    return spanner_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create sample DataFrame\n    sample_df = pd.DataFrame({\n        'id': [1, 2, 3],\n        'name': ['Alice', 'Bob', 'Charlie'],\n        'age': [25, 30, 35],\n        'is_active': [True, False, True],\n        'created_at': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03'])\n    })\n    \n    # For BigQuery\n    bigquery_schema = pandas_to_bigquery_schema(sample_df)\n    print(\"BigQuery Schema:\", bigquery_schema)\n    \n    # For Spanner\n    spanner_data = pandas_to_spanner_format(sample_df, 'my-instance', 'my-database', 'my-table')\n    print(\"Spanner Data:\", spanner_data[:2])  # Show first 2 rows\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google", "google-cloud-spanner", "pyarrow", "pandas", "google-cloud-bigquery"], "package_breakdown": {"install_packages": ["google", "google-cloud-spanner", "pyarrow", "pandas", "google-cloud-bigquery"], "import_packages": ["pyarrow", "google", "pandas"], "llm_suggested": ["pandas", "google-cloud-bigquery", "pyarrow", "google-cloud-spanner"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a WebSocket client implementing a state-machine based protocol for establishing a connection, sending and receiving messages, and handling various states such as connecting, open, closing, and closed.", "prompt_index": 398, "generated_code": "```python\nimport asyncio\nimport websockets\nfrom enum import Enum, auto\nimport json\nfrom typing import Optional, Callable, Any\n\nclass ConnectionState(Enum):\n    DISCONNECTED = auto()\n    CONNECTING = auto()\n    CONNECTED = auto()\n    CLOSING = auto()\n    CLOSED = auto()\n\nclass WebSocketClient:\n    def __init__(self, uri: str, on_message: Optional[Callable[[Any], None]] = None):\n        self.uri = uri\n        self.connection = None\n        self.state = ConnectionState.DISCONNECTED\n        self.on_message = on_message\n        self.reconnect_attempts = 0\n        self.max_reconnect_attempts = 5\n\n    async def connect(self):\n        if self.state not in [ConnectionState.DISCONNECTED, ConnectionState.CLOSED]:\n            return\n        \n        self.state = ConnectionState.CONNECTING\n        print(f\"Connecting to {self.uri}...\")\n        \n        try:\n            self.connection = await websockets.connect(self.uri)\n            self.state = ConnectionState.CONNECTED\n            self.reconnect_attempts = 0\n            print(\"Connection established successfully\")\n            return True\n        except Exception as e:\n            print(f\"Connection failed: {e}\")\n            self.state = ConnectionState.CLOSED\n            return False\n\n    async def send_message(self, message: Any):\n        if self.state != ConnectionState.CONNECTED or not self.connection:\n            print(\"Cannot send message - not connected\")\n            return False\n        \n        try:\n            if isinstance(message, (dict, list)):\n                message = json.dumps(message)\n            await self.connection.send(message)\n            return True\n        except Exception as e:\n            print(f\"Failed to send message: {e}\")\n            await self.handle_connection_error()\n            return False\n\n    async def receive_messages(self):\n        if self.state != ConnectionState.CONNECTED or not self.connection:\n            return\n        \n        try:\n            async for message in self.connection:\n                if self.on_message:\n                    try:\n                        # Try to parse as JSON, fallback to raw message\n                        parsed_message = json.loads(message)\n                        self.on_message(parsed_message)\n                    except json.JSONDecodeError:\n                        self.on_message(message)\n        except websockets.exceptions.ConnectionClosed:\n            print(\"Connection closed during message reception\")\n            await self.handle_connection_error()\n        except Exception as e:\n            print(f\"Error receiving messages: {e}\")\n            await self.handle_connection_error()\n\n    async def close(self, code: int = 1000, reason: str = \"\"):\n        if self.state not in [ConnectionState.CONNECTED, ConnectionState.CONNECTING]:\n            return\n        \n        self.state = ConnectionState.CLOSING\n        print(\"Closing connection...\")\n        \n        try:\n            if self.connection:\n                await self.connection.close(code=code, reason=reason)\n        except Exception as e:\n            print(f\"Error during close: {e}\")\n        finally:\n            self.state = ConnectionState.CLOSED\n            self.connection = None\n            print(\"Connection closed\")\n\n    async def handle_connection_error(self):\n        print(\"Handling connection error...\")\n        self.state = ConnectionState.CLOSED\n        self.connection = None\n        \n        if self.reconnect_attempts < self.max_reconnect_attempts:\n            self.reconnect_attempts += 1\n            print(f\"Attempting reconnect ({self.reconnect_attempts}/{self.max_reconnect_attempts})...\")\n            await asyncio.sleep(2 ** self.reconnect_attempts)  # Exponential backoff\n            await self.connect()\n            if self.state == ConnectionState.CONNECTED:\n                asyncio.create_task(self.receive_messages())\n\n    def get_state(self) -> ConnectionState:\n        return self.state\n\n    def is_connected(self) -> bool:\n        return self.state == ConnectionState.CONNECTED\n\n# Example usage\nasync def example_usage():\n    def handle_message(message):\n        print(f\"Received: {message}\")\n    \n    client = WebSocketClient(\"ws://localhost:8765\", on_message=handle_message)\n    \n    # Connect to server\n    if await client.connect():\n        # Start receiving messages in background\n        asyncio.create_task(client.receive_messages())\n        \n        # Send some messages\n        await client.send_message({\"type\": \"greeting\", \"message\": \"Hello Server!\"})\n        await client.send_message(\"plain text message\")\n        \n        # Keep connection alive for a while\n        await asyncio.sleep(10)\n        \n        # Close connection\n        await client.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n```\n\nNote: This code requires the `websockets` package (`pip install websockets`). The client implements a state machine with automatic reconnection logic and handles both JSON and plain text messages.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "enum", "websockets", "typing", "json"], "package_breakdown": {"install_packages": ["asyncio", "enum", "websockets", "typing", "json"], "import_packages": ["asyncio", "enum", "websockets", "typing", "json"], "llm_suggested": ["websockets", "asyncio", "enum", "json", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Microsoft Azure File DataLake Storage account, creates a new directory, uploads a file to that directory, and then lists the contents of the directory.", "prompt_index": 399, "generated_code": "```python\nfrom azure.storage.filedatalake import DataLakeServiceClient\nfrom azure.core.exceptions import ResourceExistsError\nimport os\n\ndef connect_to_datalake(account_name, account_key):\n    \"\"\"Connect to Azure Data Lake Storage account\"\"\"\n    account_url = f\"https://{account_name}.dfs.core.windows.net\"\n    service_client = DataLakeServiceClient(account_url=account_url, credential=account_key)\n    return service_client\n\ndef create_directory(service_client, file_system_name, directory_name):\n    \"\"\"Create a new directory in the specified file system\"\"\"\n    file_system_client = service_client.get_file_system_client(file_system=file_system_name)\n    directory_client = file_system_client.get_directory_client(directory_name)\n    \n    try:\n        directory_client.create_directory()\n        print(f\"Directory '{directory_name}' created successfully.\")\n    except ResourceExistsError:\n        print(f\"Directory '{directory_name}' already exists.\")\n    \n    return directory_client\n\ndef upload_file(directory_client, local_file_path, remote_file_name):\n    \"\"\"Upload a file to the directory\"\"\"\n    file_client = directory_client.get_file_client(remote_file_name)\n    \n    with open(local_file_path, \"rb\") as data:\n        file_client.upload_data(data, overwrite=True)\n    \n    print(f\"File '{remote_file_name}' uploaded successfully.\")\n\ndef list_directory_contents(service_client, file_system_name, directory_name):\n    \"\"\"List contents of the specified directory\"\"\"\n    file_system_client = service_client.get_file_system_client(file_system=file_system_name)\n    paths = file_system_client.get_paths(path=directory_name)\n    \n    print(f\"Contents of directory '{directory_name}':\")\n    for path in paths:\n        print(f\"  - {path.name}\")\n\n# Configuration - Replace with your actual values\nACCOUNT_NAME = \"your_storage_account_name\"\nACCOUNT_KEY = \"your_account_key\"\nFILE_SYSTEM_NAME = \"your_filesystem_name\"\nDIRECTORY_NAME = \"new_directory\"\nLOCAL_FILE_PATH = \"path/to/your/local/file.txt\"\nREMOTE_FILE_NAME = \"uploaded_file.txt\"\n\n# Execute the operations\ntry:\n    # Connect to Data Lake Storage\n    service_client = connect_to_datalake(ACCOUNT_NAME, ACCOUNT_KEY)\n    \n    # Create directory\n    directory_client = create_directory(service_client, FILE_SYSTEM_NAME, DIRECTORY_NAME)\n    \n    # Upload file (make sure the local file exists)\n    if os.path.exists(LOCAL_FILE_PATH):\n        upload_file(directory_client, LOCAL_FILE_PATH, REMOTE_FILE_NAME)\n    else:\n        print(f\"Local file '{LOCAL_FILE_PATH}' does not exist. Creating a sample file for upload.\")\n        with open(\"sample.txt\", \"w\") as f:\n            f.write(\"This is a sample file content.\")\n        upload_file(directory_client, \"sample.txt\", \"sample.txt\")\n    \n    # List directory contents\n    list_directory_contents(service_client, FILE_SYSTEM_NAME, DIRECTORY_NAME)\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n```\n\n**Note:** Before running this code, you need to:\n1. Install the required package: `pip install azure-storage-file-datalake`\n2. Replace the placeholder values with your actual Azure credentials:\n   - `ACCOUNT_NAME`: Your storage account name\n   - `ACCOUNT_KEY`: Your storage account access key\n   - `FILE_SYSTEM_NAME`: Your filesystem/container name\n   - Ensure the filesystem already exists in your storage account\n3. Make sure the local file path exists or the code will create a sample file", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure", "azure-storage-file-datalake", "os", "azure-core"], "package_breakdown": {"install_packages": ["azure", "azure-storage-file-datalake", "os", "azure-core"], "import_packages": ["azure", "os"], "llm_suggested": ["azure-storage-file-datalake", "azure-core", "os"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates and prints the Fibonacci sequence up to the 10th number using a recursive function.", "prompt_index": 400, "generated_code": "```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nfor i in range(10):\n    print(fibonacci(i))\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 0, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": [], "package_breakdown": {"install_packages": [], "import_packages": [], "llm_suggested": []}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the NVIDIA Collective Communication Library (NCCL) Runtime to implement efficient data communication and synchronization operations among multiple GPUs in a distributed computing environment.", "prompt_index": 401, "generated_code": "```python\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    \n    # Initialize the process group with NCCL backend\n    dist.init_process_group(\n        backend='nccl',\n        init_method='env://',\n        world_size=world_size,\n        rank=rank\n    )\n    \n    # Set the default CUDA device for this process\n    torch.cuda.set_device(rank)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef all_reduce_example(rank, world_size):\n    setup(rank, world_size)\n    \n    # Create a tensor on the current GPU\n    tensor = torch.ones(1).cuda() * (rank + 1)\n    print(f\"Rank {rank} before all_reduce: {tensor.item()}\")\n    \n    # Perform all-reduce operation (sum by default)\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    \n    print(f\"Rank {rank} after all_reduce: {tensor.item()}\")\n    \n    cleanup()\n\ndef broadcast_example(rank, world_size):\n    setup(rank, world_size)\n    \n    if rank == 0:\n        # Root process creates data\n        data = torch.tensor([1.0, 2.0, 3.0]).cuda()\n    else:\n        data = torch.zeros(3).cuda()\n    \n    print(f\"Rank {rank} before broadcast: {data}\")\n    \n    # Broadcast data from rank 0 to all processes\n    dist.broadcast(data, src=0)\n    \n    print(f\"Rank {rank} after broadcast: {data}\")\n    \n    cleanup()\n\ndef all_gather_example(rank, world_size):\n    setup(rank, world_size)\n    \n    # Each process has its own data\n    local_data = torch.tensor([rank]).cuda()\n    \n    # Create a list to gather data from all processes\n    gathered_data = [torch.zeros_like(local_data) for _ in range(world_size)]\n    \n    # Gather data from all processes\n    dist.all_gather(gathered_data, local_data)\n    \n    print(f\"Rank {rank} gathered: {[t.item() for t in gathered_data]}\")\n    \n    cleanup()\n\ndef reduce_scatter_example(rank, world_size):\n    setup(rank, world_size)\n    \n    # Each process has a tensor with data for all processes\n    input_list = [torch.tensor([i * (rank + 1)]).cuda() for i in range(world_size)]\n    input_tensor = torch.cat(input_list)\n    \n    # Output tensor for this process\n    output = torch.zeros(1).cuda()\n    \n    # Perform reduce-scatter (sum and scatter)\n    dist.reduce_scatter(output, [input_tensor], op=dist.ReduceOp.SUM)\n    \n    print(f\"Rank {rank} reduce_scatter result: {output.item()}\")\n    \n    cleanup()\n\ndef run_example(rank, world_size, example_func):\n    try:\n        example_func(rank, world_size)\n    except Exception as e:\n        print(f\"Error in rank {rank}: {e}\")\n        cleanup()\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    \n    if world_size < 2:\n        print(\"This example requires at least 2 GPUs\")\n    else:\n        print(f\"Running with {world_size} GPUs\")\n        \n        # Run different NCCL operations\n        examples = [\n            all_reduce_example,\n            broadcast_example, \n            all_gather_example,\n            reduce_scatter_example\n        ]\n        \n        for example in examples:\n            print(f\"\\nRunning {example.__name__}:\")\n            mp.spawn(example, args=(world_size,), nprocs=world_size, join=True)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["torch", "os"], "package_breakdown": {"install_packages": ["torch", "os"], "import_packages": ["os", "torch"], "llm_suggested": ["torch"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that connects to a Snowflake database using Snowpark for Python and executes a query to retrieve data from a specific table.", "prompt_index": 402, "generated_code": "```python\nfrom snowflake.snowpark import Session\n\n# Snowflake connection parameters\nconnection_parameters = {\n    \"account\": \"your_account\",\n    \"user\": \"your_username\",\n    \"password\": \"your_password\",\n    \"role\": \"your_role\",  # optional\n    \"warehouse\": \"your_warehouse\",\n    \"database\": \"your_database\",\n    \"schema\": \"your_schema\"\n}\n\n# Create a session\nsession = Session.builder.configs(connection_parameters).create()\n\n# Execute a query\nquery = \"SELECT * FROM your_table LIMIT 10\"\ndf = session.sql(query)\n\n# Show the results\ndf.show()\n\n# Convert to pandas DataFrame if needed\n# pandas_df = df.to_pandas()\n\n# Close the session\nsession.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["snowflake-snowpark-python", "snowflake"], "package_breakdown": {"install_packages": ["snowflake-snowpark-python", "snowflake"], "import_packages": ["snowflake"], "llm_suggested": ["snowflake-snowpark-python"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes Nvidia's JIT LTO (Just-In-Time Link-Time Optimization) library to dynamically optimize and compile Python code for improved performance on Nvidia GPUs.", "prompt_index": 403, "generated_code": "```python\nimport numpy as np\nfrom numba import cuda, jit\nfrom numba.cuda.cudadrv.driver import Linker\n\ndef optimize_with_jit_lto():\n    # Create a linker with LTO enabled\n    linker = Linker()\n    linker.options['lto'] = True\n    \n    @cuda.jit(link=linker)\n    def gpu_kernel(input_array, output_array):\n        idx = cuda.grid(1)\n        if idx < input_array.size:\n            # Example computation - replace with actual kernel logic\n            output_array[idx] = input_array[idx] * input_array[idx] + 2.0\n    \n    return gpu_kernel\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create sample data\n    data_size = 1000000\n    input_data = np.random.random(data_size).astype(np.float32)\n    output_data = np.zeros_like(input_data)\n    \n    # Get optimized kernel\n    optimized_kernel = optimize_with_jit_lto()\n    \n    # Configure and launch kernel\n    threads_per_block = 256\n    blocks_per_grid = (data_size + threads_per_block - 1) // threads_per_block\n    \n    optimized_kernel[blocks_per_grid, threads_per_block](input_data, output_data)\n    cuda.synchronize()\n    \n    print(f\"Input: {input_data[:5]}\")\n    print(f\"Output: {output_data[:5]}\")\n```\n\nNote: This code uses Numba's CUDA support which internally utilizes NVIDIA's JIT compilation capabilities including LTO optimizations. The actual LTO functionality is handled by the underlying NVIDIA toolchain (nvcc/nvrtc) when the `lto=True` option is specified in the linker configuration.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numba", "numpy"], "package_breakdown": {"install_packages": ["numba", "numpy"], "import_packages": ["numba", "numpy"], "llm_suggested": ["numpy", "numba"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that takes a Python script as input, uses the pasta library to parse and analyze the Abstract Syntax Tree (AST) of the script, and performs automated code refactoring based on the identified patterns and rules in the AST.", "prompt_index": 404, "generated_code": "```python\nimport ast\nimport pasta\nfrom pasta.augment import fix\nfrom pasta.base import ast_utils\nimport sys\n\ndef refactor_code(input_script):\n    \"\"\"\n    Parse and refactor Python code using pasta library\n    \"\"\"\n    try:\n        # Parse the input script into an AST\n        tree = pasta.parse(input_script)\n        \n        # Get all nodes in the AST\n        nodes = list(ast.walk(tree))\n        \n        # Apply refactoring rules\n        refactored_tree = apply_refactoring_rules(tree, nodes)\n        \n        # Generate the refactored code\n        refactored_code = pasta.dump(refactored_tree)\n        \n        return refactored_code\n        \n    except Exception as e:\n        print(f\"Error during refactoring: {e}\")\n        return input_script\n\ndef apply_refactoring_rules(tree, nodes):\n    \"\"\"\n    Apply various refactoring rules to the AST\n    \"\"\"\n    # Rule 1: Convert print statements to print functions (Python 2 to 3)\n    tree = fix_convert_print_statements(tree)\n    \n    # Rule 2: Remove unused imports\n    tree = fix_remove_unused_imports(tree, nodes)\n    \n    # Rule 3: Convert string formatting to f-strings (where possible)\n    tree = fix_convert_to_fstrings(tree, nodes)\n    \n    # Rule 4: Simplify boolean expressions\n    tree = fix_simplify_boolean_expressions(tree, nodes)\n    \n    # Rule 5: Remove redundant parentheses\n    tree = fix_remove_redundant_parentheses(tree, nodes)\n    \n    return tree\n\ndef fix_convert_print_statements(tree):\n    \"\"\"Convert print statements to print functions\"\"\"\n    return fix.convert_print_statements_to_functions(tree)\n\ndef fix_remove_unused_imports(tree, nodes):\n    \"\"\"Remove unused import statements\"\"\"\n    # This is a simplified implementation - in practice would need more complex analysis\n    for node in nodes:\n        if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):\n            # Check if imported names are actually used\n            if not is_import_used(node, tree):\n                # Remove the import node\n                ast_utils.remove_child(node)\n    return tree\n\ndef is_import_used(import_node, tree):\n    \"\"\"\n    Check if imported names from an import statement are used in the code\n    Simplified implementation - would need more sophisticated analysis\n    \"\"\"\n    # This is a placeholder - actual implementation would require\n    # tracking variable usage throughout the entire AST\n    return True  # Conservative approach - assume imports are used\n\ndef fix_convert_to_fstrings(tree, nodes):\n    \"\"\"Convert appropriate string formatting to f-strings\"\"\"\n    for node in nodes:\n        if (isinstance(node, ast.Call) and \n            isinstance(node.func, ast.Attribute) and\n            isinstance(node.func.value, ast.Str) and\n            node.func.attr == 'format'):\n            \n            # Simple case: convert to f-string\n            try:\n                format_string = node.func.value.s\n                # This would need more sophisticated conversion logic\n                # For simplicity, we'll just mark for conversion\n                pass\n            except:\n                continue\n    return tree\n\ndef fix_simplify_boolean_expressions(tree, nodes):\n    \"\"\"Simplify boolean expressions\"\"\"\n    for node in nodes:\n        if isinstance(node, ast.BoolOp):\n            # Simplify expressions like (True and x) -> x\n            # This would need more complex logic\n            pass\n    return tree\n\ndef fix_remove_redundant_parentheses(tree, nodes):\n    \"\"\"Remove redundant parentheses in expressions\"\"\"\n    for node in nodes:\n        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Tuple):\n            # Check if tuple parentheses are redundant\n            if len(node.value.elts) == 1:\n                # Single element tuple - parentheses might be redundant\n                pass\n    return tree\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python refactor.py <input_file.py>\")\n        sys.exit(1)\n    \n    input_file = sys.argv[1]\n    \n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            original_code = f.read()\n        \n        refactored_code = refactor_code(original_code)\n        \n        # Output the refactored code\n        print(refactored_code)\n        \n    except FileNotFoundError:\n        print(f\"Error: File '{input_file}' not found\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This code provides a framework for AST-based refactoring using the pasta library. Some refactoring rules (like `is_import_used`) are simplified and would need more sophisticated implementation for production use. The pasta library handles many common refactoring patterns, and this code demonstrates how to build upon it with custom rules.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "pasta", "ast"], "package_breakdown": {"install_packages": ["sys", "pasta", "ast"], "import_packages": ["sys", "ast", "pasta"], "llm_suggested": ["ast", "pasta", "sys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a list of fake names along with their corresponding email addresses using the Faker package.", "prompt_index": 405, "generated_code": "```python\nfrom faker import Faker\n\nfake = Faker()\nfake_names_with_emails = []\n\nfor _ in range(10):\n    name = fake.name()\n    email = fake.email()\n    fake_names_with_emails.append((name, email))\n\nprint(fake_names_with_emails)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["faker"], "package_breakdown": {"install_packages": ["faker"], "import_packages": ["faker"], "llm_suggested": ["faker"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a pandas dataframe into a compatible format for other dataframe libraries, ensuring seamless interoperability between different dataframe libraries.", "prompt_index": 406, "generated_code": "```python\nimport pandas as pd\n\ndef convert_dataframe(df, target_library='polars'):\n    \"\"\"\n    Convert a pandas DataFrame to a format compatible with other dataframe libraries.\n    \n    Parameters:\n    df (pd.DataFrame): Input pandas DataFrame\n    target_library (str): Target library ('polars', 'dask', 'modin', 'vaex', 'pyspark')\n    \n    Returns:\n    Converted dataframe object for the target library\n    \"\"\"\n    \n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a pandas DataFrame\")\n    \n    target_library = target_library.lower()\n    \n    if target_library == 'polars':\n        try:\n            import polars as pl\n            return pl.from_pandas(df)\n        except ImportError:\n            raise ImportError(\"polars library is required for this conversion\")\n    \n    elif target_library == 'dask':\n        try:\n            import dask.dataframe as dd\n            return dd.from_pandas(df, npartitions=1)\n        except ImportError:\n            raise ImportError(\"dask library is required for this conversion\")\n    \n    elif target_library == 'modin':\n        try:\n            import modin.pandas as mpd\n            return mpd.DataFrame(df)\n        except ImportError:\n            raise ImportError(\"modin library is required for this conversion\")\n    \n    elif target_library == 'vaex':\n        try:\n            import vaex\n            return vaex.from_pandas(df)\n        except ImportError:\n            raise ImportError(\"vaex library is required for this conversion\")\n    \n    elif target_library == 'pyspark':\n        try:\n            from pyspark.sql import SparkSession\n            spark = SparkSession.builder.getOrCreate()\n            return spark.createDataFrame(df)\n        except ImportError:\n            raise ImportError(\"pyspark library is required for this conversion\")\n    \n    else:\n        raise ValueError(f\"Unsupported target library: {target_library}. \"\n                        \"Supported libraries: 'polars', 'dask', 'modin', 'vaex', 'pyspark'\")\n\n# Example usage:\n# df_pandas = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})\n# df_polars = convert_dataframe(df_pandas, 'polars')\n# df_dask = convert_dataframe(df_pandas, 'dask')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 6, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["polars", "modin", "vaex", "dask", "pyspark", "pandas"], "package_breakdown": {"install_packages": ["polars", "modin", "vaex", "dask", "pyspark", "pandas"], "import_packages": ["modin", "polars", "vaex", "dask", "pyspark", "pandas"], "llm_suggested": ["pandas", "polars", "dask", "modin", "vaex", "pyspark"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that dynamically checks the types of variables at runtime to ensure they match the specified data types using the type hints provided in the code.", "prompt_index": 407, "generated_code": "```python\nimport inspect\nfrom typing import get_type_hints\n\ndef type_check(func):\n    def wrapper(*args, **kwargs):\n        # Get type hints from function signature\n        sig = inspect.signature(func)\n        hints = get_type_hints(func)\n        \n        # Check positional arguments\n        for i, (name, param) in enumerate(sig.parameters.items()):\n            if i < len(args) and name in hints:\n                arg_value = args[i]\n                expected_type = hints[name]\n                if not isinstance(arg_value, expected_type):\n                    raise TypeError(f\"Argument '{name}' must be of type {expected_type.__name__}, \"\n                                  f\"got {type(arg_value).__name__}\")\n        \n        # Check keyword arguments\n        for name, value in kwargs.items():\n            if name in hints:\n                expected_type = hints[name]\n                if not isinstance(value, expected_type):\n                    raise TypeError(f\"Argument '{name}' must be of type {expected_type.__name__}, \"\n                                  f\"got {type(value).__name__}\")\n        \n        # Call the original function\n        result = func(*args, **kwargs)\n        \n        # Check return type if specified\n        if 'return' in hints:\n            expected_return_type = hints['return']\n            if not isinstance(result, expected_return_type):\n                raise TypeError(f\"Return value must be of type {expected_return_type.__name__}, \"\n                              f\"got {type(result).__name__}\")\n        \n        return result\n    \n    return wrapper\n\n# Example usage\n@type_check\ndef process_data(name: str, age: int, scores: list[float]) -> dict:\n    \"\"\"Example function with type hints that will be validated at runtime\"\"\"\n    return {\n        'name': name,\n        'age': age,\n        'average_score': sum(scores) / len(scores) if scores else 0.0\n    }\n\n# This will work correctly\nresult = process_data(\"John\", 25, [95.5, 88.0, 92.3])\nprint(result)\n\n# This will raise TypeError for 'age' argument\ntry:\n    process_data(\"John\", \"25\", [95.5, 88.0, 92.3])\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n\n# This will raise TypeError for return value (if we modify the function to return wrong type)\n@type_check\ndef bad_function() -> int:\n    return \"not an integer\"\n\ntry:\n    bad_function()\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "typing"], "package_breakdown": {"install_packages": ["inspect", "typing"], "import_packages": ["typing", "inspect"], "llm_suggested": ["inspect", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes cuDNN runtime libraries to accelerate deep learning model training on Nvidia GPUs.", "prompt_index": 408, "generated_code": "```python\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\n\n# Enable cuDNN benchmarking for optimal performance\ncudnn.benchmark = True\ncudnn.enabled = True\n\n# Define a simple CNN model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n        self.fc2 = nn.Linear(512, 10)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.view(-1, 128 * 8 * 8)\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n# Check if CUDA is available and set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# Create model and move to GPU\nmodel = SimpleCNN().to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Example training loop (replace with actual data loading)\ndef train_model(model, dataloader, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n\n# Note: You'll need to provide your own DataLoader for actual training\n# Example usage:\n# train_loader = torch.utils.data.DataLoader(your_dataset, batch_size=32, shuffle=True)\n# train_model(model, train_loader)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["torch"], "package_breakdown": {"install_packages": ["torch"], "import_packages": ["torch"], "llm_suggested": ["torch"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that converts a color value from an HTML or CSS format to another supported color format within the library, such as RGB, HSL, or hexadecimal.", "prompt_index": 409, "generated_code": "```python\nimport re\nimport colorsys\n\ndef html_to_rgb(html_color):\n    \"\"\"\n    Convert HTML color string to RGB tuple.\n    Supports: #RGB, #RRGGBB, rgb(r, g, b), rgba(r, g, b, a)\n    \"\"\"\n    html_color = html_color.strip().lower()\n    \n    # Hex format: #RGB or #RRGGBB\n    if html_color.startswith('#'):\n        hex_color = html_color[1:]\n        if len(hex_color) == 3:\n            hex_color = ''.join([c*2 for c in hex_color])\n        if len(hex_color) != 6:\n            raise ValueError(f\"Invalid hex color format: {html_color}\")\n        \n        try:\n            r = int(hex_color[0:2], 16)\n            g = int(hex_color[2:4], 16)\n            b = int(hex_color[4:6], 16)\n            return (r, g, b)\n        except ValueError:\n            raise ValueError(f\"Invalid hex color format: {html_color}\")\n    \n    # RGB format: rgb(r, g, b) or rgba(r, g, b, a)\n    elif html_color.startswith('rgb'):\n        match = re.match(r'rgba?\\((\\d+),\\s*(\\d+),\\s*(\\d+)(?:,\\s*[\\d.]+)?\\)', html_color)\n        if match:\n            r = int(match.group(1))\n            g = int(match.group(2))\n            b = int(match.group(3))\n            if not (0 <= r <= 255 and 0 <= g <= 255 and 0 <= b <= 255):\n                raise ValueError(f\"RGB values out of range: {html_color}\")\n            return (r, g, b)\n        else:\n            raise ValueError(f\"Invalid RGB color format: {html_color}\")\n    \n    # Named colors (basic support)\n    elif html_color in COLOR_NAMES:\n        return COLOR_NAMES[html_color]\n    \n    else:\n        raise ValueError(f\"Unsupported color format: {html_color}\")\n\ndef rgb_to_hex(rgb_tuple):\n    \"\"\"Convert RGB tuple to hexadecimal string.\"\"\"\n    r, g, b = rgb_tuple\n    return f\"#{r:02x}{g:02x}{b:02x}\"\n\ndef rgb_to_hsl(rgb_tuple):\n    \"\"\"Convert RGB tuple to HSL tuple.\"\"\"\n    r, g, b = [x/255.0 for x in rgb_tuple]\n    h, l, s = colorsys.rgb_to_hls(r, g, b)\n    return (round(h * 360, 1), round(s * 100, 1), round(l * 100, 1))\n\ndef hsl_to_rgb(hsl_tuple):\n    \"\"\"Convert HSL tuple to RGB tuple.\"\"\"\n    h, s, l = hsl_tuple\n    h, s, l = h/360.0, s/100.0, l/100.0\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return (round(r * 255), round(g * 255), round(b * 255))\n\ndef convert_color(color_value, target_format):\n    \"\"\"\n    Convert color between different formats.\n    \n    Args:\n        color_value: Input color string (hex, rgb, hsl, or named color)\n        target_format: 'hex', 'rgb', or 'hsl'\n    \n    Returns:\n        Color in the specified target format\n    \"\"\"\n    # First convert to RGB\n    if isinstance(color_value, str):\n        rgb = html_to_rgb(color_value)\n    else:\n        rgb = color_value\n    \n    # Convert to target format\n    if target_format.lower() == 'hex':\n        return rgb_to_hex(rgb)\n    elif target_format.lower() == 'rgb':\n        return f\"rgb({rgb[0]}, {rgb[1]}, {rgb[2]})\"\n    elif target_format.lower() == 'hsl':\n        hsl = rgb_to_hsl(rgb)\n        return f\"hsl({hsl[0]}, {hsl[1]}%, {hsl[2]}%)\"\n    else:\n        raise ValueError(f\"Unsupported target format: {target_format}\")\n\n# Basic named colors dictionary\nCOLOR_NAMES = {\n    'red': (255, 0, 0),\n    'green': (0, 128, 0),\n    'blue': (0, 0, 255),\n    'white': (255, 255, 255),\n    'black': (0, 0, 0),\n    'yellow': (255, 255, 0),\n    'cyan': (0, 255, 255),\n    'magenta': (255, 0, 255),\n    'silver': (192, 192, 192),\n    'gray': (128, 128, 128),\n    'maroon': (128, 0, 0),\n    'olive': (128, 128, 0),\n    'lime': (0, 255, 0),\n    'aqua': (0, 255, 255),\n    'teal': (0, 128, 128),\n    'navy': (0, 0, 128),\n    'fuchsia': (255, 0, 255),\n    'purple': (128, 0, 128),\n    'orange': (255, 165, 0)\n}\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "colorsys"], "package_breakdown": {"install_packages": ["re", "colorsys"], "import_packages": ["re", "colorsys"], "llm_suggested": ["re", "colorsys"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the common libraries from Abseil to implement a robust logging system with structured logs and automatic log rotation for a web application.", "prompt_index": 410, "generated_code": "```python\nimport logging\nimport os\nfrom datetime import datetime\nfrom absl import app, flags, logging as absl_logging\nfrom logging.handlers import RotatingFileHandler\n\nFLAGS = flags.FLAGS\n\n# Define command line flags\nflags.DEFINE_string('log_dir', './logs', 'Directory to store log files')\nflags.DEFINE_integer('max_log_size', 10, 'Maximum log file size in MB')\nflags.DEFINE_integer('backup_count', 5, 'Number of backup log files to keep')\nflags.DEFINE_enum('log_level', 'INFO', ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], \n                 'Logging level')\n\ndef setup_structured_logging():\n    \"\"\"Configure structured logging with rotation\"\"\"\n    \n    # Create log directory if it doesn't exist\n    os.makedirs(FLAGS.log_dir, exist_ok=True)\n    \n    # Generate log filename with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    log_file = os.path.join(FLAGS.log_dir, f'app_{timestamp}.log')\n    \n    # Configure Abseil logging\n    absl_logging.use_absl_handler()\n    \n    # Set log level from flags\n    numeric_level = getattr(logging, FLAGS.log_level.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError(f'Invalid log level: {FLAGS.log_level}')\n    \n    absl_logging.set_verbosity(numeric_level)\n    \n    # Add rotating file handler for structured logging\n    file_handler = RotatingFileHandler(\n        filename=log_file,\n        maxBytes=FLAGS.max_log_size * 1024 * 1024,  # Convert MB to bytes\n        backupCount=FLAGS.backup_count,\n        encoding='utf-8'\n    )\n    \n    # Structured log format with JSON-like structure\n    formatter = logging.Formatter(\n        '{\"timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", '\n        '\"module\": \"%(module)s\", \"function\": \"%(funcName)s\", '\n        '\"line\": \"%(lineno)d\", \"message\": \"%(message)s\"}',\n        datefmt='%Y-%m-%dT%H:%M:%S%z'\n    )\n    \n    file_handler.setFormatter(formatter)\n    absl_logging.get_absl_handler().addHandler(file_handler)\n    \n    return absl_logging\n\ndef main(argv):\n    \"\"\"Main application function with structured logging examples\"\"\"\n    del argv  # Unused\n    \n    # Example structured logging\n    absl_logging.info('Application started', extra={\n        'component': 'web_server',\n        'port': 8080,\n        'environment': 'production'\n    })\n    \n    try:\n        # Simulate application work\n        absl_logging.debug('Processing request', extra={\n            'request_id': 'req_12345',\n            'endpoint': '/api/data',\n            'method': 'GET'\n        })\n        \n        # Simulate an error scenario\n        raise ValueError('Sample error for demonstration')\n        \n    except Exception as e:\n        absl_logging.error('Error processing request', extra={\n            'error_type': type(e).__name__,\n            'error_message': str(e),\n            'request_id': 'req_12345'\n        })\n    \n    absl_logging.info('Application shutting down', extra={\n        'uptime_seconds': 60,\n        'requests_processed': 100\n    })\n\nif __name__ == '__main__':\n    setup_structured_logging()\n    app.run(main)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["datetime", "logging", "absl-py", "os", "absl"], "package_breakdown": {"install_packages": ["datetime", "logging", "absl-py", "os", "absl"], "import_packages": ["absl", "datetime", "os", "logging"], "llm_suggested": ["absl-py", "logging", "os", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that scans a given Python script file and identifies any potential syntax errors or code quality issues using a static analysis technique.", "prompt_index": 411, "generated_code": "```python\nimport ast\nimport sys\nfrom pathlib import Path\n\ndef analyze_python_file(file_path):\n    \"\"\"\n    Analyze a Python file for syntax errors and code quality issues.\n    \n    Args:\n        file_path (str): Path to the Python file to analyze\n        \n    Returns:\n        dict: Dictionary containing analysis results\n    \"\"\"\n    results = {\n        'syntax_errors': [],\n        'quality_issues': [],\n        'success': False\n    }\n    \n    try:\n        # Read the file content\n        with open(file_path, 'r', encoding='utf-8') as file:\n            source_code = file.read()\n        \n        # Parse the AST to check for syntax errors\n        try:\n            tree = ast.parse(source_code)\n            results['success'] = True\n        except SyntaxError as e:\n            results['syntax_errors'].append({\n                'line': e.lineno,\n                'message': f\"SyntaxError: {e.msg}\",\n                'offset': e.offset\n            })\n            return results\n        \n        # Basic code quality checks\n        analyzer = StaticAnalyzer()\n        analyzer.visit(tree)\n        \n        # Collect quality issues\n        results['quality_issues'] = analyzer.issues\n        \n    except FileNotFoundError:\n        results['syntax_errors'].append({\n            'line': 0,\n            'message': f\"File not found: {file_path}\",\n            'offset': 0\n        })\n    except Exception as e:\n        results['syntax_errors'].append({\n            'line': 0,\n            'message': f\"Unexpected error: {str(e)}\",\n            'offset': 0\n        })\n    \n    return results\n\nclass StaticAnalyzer(ast.NodeVisitor):\n    \"\"\"AST visitor for static code analysis\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.issues = []\n    \n    def visit_FunctionDef(self, node):\n        # Check for functions with too many arguments\n        if len(node.args.args) > 7:\n            self.issues.append({\n                'line': node.lineno,\n                'message': f\"Function '{node.name}' has too many arguments ({len(node.args.args)}). Consider refactoring.\",\n                'type': 'quality'\n            })\n        \n        self.generic_visit(node)\n    \n    def visit_Assign(self, node):\n        # Check for multiple assignments in one line\n        if len(node.targets) > 1:\n            self.issues.append({\n                'line': node.lineno,\n                'message': \"Multiple assignments in single line can reduce readability.\",\n                'type': 'quality'\n            })\n        \n        self.generic_visit(node)\n    \n    def visit_For(self, node):\n        # Check for nested loops (simplified check)\n        for child in ast.walk(node):\n            if isinstance(child, ast.For) and child != node:\n                self.issues.append({\n                    'line': node.lineno,\n                    'message': \"Nested loops detected. Consider if this can be optimized.\",\n                    'type': 'quality'\n                })\n                break\n        \n        self.generic_visit(node)\n    \n    def visit_While(self, node):\n        # Check for potential infinite loops\n        if not any(isinstance(child, ast.Break) for child in ast.walk(node)):\n            # Simple check - if no break statement and condition is constant True\n            if (isinstance(node.test, ast.NameConstant) and \n                node.test.value is True):\n                self.issues.append({\n                    'line': node.lineno,\n                    'message': \"Potential infinite while loop detected (while True without break).\",\n                    'type': 'quality'\n                })\n        \n        self.generic_visit(node)\n\ndef main():\n    \"\"\"Main function to run the analysis\"\"\"\n    if len(sys.argv) != 2:\n        print(\"Usage: python analyzer.py <python_file>\")\n        sys.exit(1)\n    \n    file_path = sys.argv[1]\n    \n    if not Path(file_path).exists():\n        print(f\"Error: File '{file_path}' not found.\")\n        sys.exit(1)\n    \n    results = analyze_python_file(file_path)\n    \n    print(f\"Analysis results for: {file_path}\")\n    print(\"=\" * 50)\n    \n    if results['syntax_errors']:\n        print(\"\\nSyntax Errors:\")\n        for error in results['syntax_errors']:\n            print(f\"Line {error['line']}: {error['message']}\")\n    else:\n        print(\"\\n\u2713 No syntax errors found.\")\n    \n    if results['quality_issues']:\n        print(\"\\nCode Quality Issues:\")\n        for issue in results['quality_issues']:\n            print(f\"Line {issue['line']}: {issue['message']}\")\n    else:\n        print(\"\\n\u2713 No code quality issues found.\")\n    \n    if results['success']:\n        print(f\"\\n\u2713 File parsed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sys", "pathlib", "ast"], "package_breakdown": {"install_packages": ["sys", "pathlib", "ast"], "import_packages": ["sys", "ast", "pathlib"], "llm_suggested": ["ast", "sys", "pathlib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a bar chart using Vega-Altair to visualize the distribution of a given dataset with statistical information such as mean, median, and standard deviation displayed on the chart.", "prompt_index": 412, "generated_code": "```python\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\ndef create_statistical_bar_chart(data, column_name, chart_title=\"Distribution with Statistics\"):\n    \"\"\"\n    Creates a bar chart with statistical information (mean, median, std) displayed\n    \n    Parameters:\n    data: pandas DataFrame or array-like data\n    column_name: name of the column to visualize\n    chart_title: title for the chart\n    \"\"\"\n    \n    # Convert to DataFrame if not already\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame({column_name: data})\n    \n    # Calculate statistics\n    mean_val = data[column_name].mean()\n    median_val = data[column_name].median()\n    std_val = data[column_name].std()\n    \n    # Create base chart\n    base = alt.Chart(data).mark_bar().encode(\n        x=alt.X(f'{column_name}:Q', bin=True, title=column_name),\n        y='count()',\n        tooltip=['count()']\n    ).properties(\n        title=chart_title,\n        width=600,\n        height=400\n    )\n    \n    # Create vertical lines for statistics\n    mean_line = alt.Chart(pd.DataFrame({'value': [mean_val]})).mark_rule(\n        color='red', strokeDash=[5, 5]\n    ).encode(\n        x='value:Q',\n        size=alt.value(2)\n    )\n    \n    median_line = alt.Chart(pd.DataFrame({'value': [median_val]})).mark_rule(\n        color='green', strokeDash=[3, 3]\n    ).encode(\n        x='value:Q',\n        size=alt.value(2)\n    )\n    \n    # Create text annotations\n    stats_text = alt.Chart(pd.DataFrame({\n        'stat': ['Mean', 'Median', 'Std Dev'],\n        'value': [mean_val, median_val, std_val],\n        'y': [0.85, 0.75, 0.65]  # vertical positions\n    })).mark_text(\n        align='left',\n        baseline='middle',\n        dx=10  # offset from right edge\n    ).encode(\n        x=alt.value(400),  # position from right\n        y=alt.Y('y:Q', axis=None),\n        text=alt.Text('stat:N', format='.0f'),\n        color=alt.value('black')\n    )\n    \n    values_text = alt.Chart(pd.DataFrame({\n        'stat': ['Mean', 'Median', 'Std Dev'],\n        'value': [mean_val, median_val, std_val],\n        'y': [0.85, 0.75, 0.65]\n    })).mark_text(\n        align='left',\n        baseline='middle',\n        dx=150  # further offset for values\n    ).encode(\n        x=alt.value(400),\n        y=alt.Y('y:Q', axis=None),\n        text=alt.Text('value:Q', format='.2f'),\n        color=alt.value('black')\n    )\n    \n    # Combine all layers\n    chart = base + mean_line + median_line + stats_text + values_text\n    \n    return chart\n\n# Example usage:\n# data = pd.DataFrame({'values': np.random.normal(50, 15, 1000)})\n# chart = create_statistical_bar_chart(data, 'values', 'Normal Distribution')\n# chart.show()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["altair", "pandas", "numpy"], "package_breakdown": {"install_packages": ["altair", "pandas", "numpy"], "import_packages": ["altair", "pandas", "numpy"], "llm_suggested": ["altair", "pandas", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that authenticates with the Stackdriver Logging API client library using OAuth 2.0 credentials and retrieves the latest log entries from a specified log stream.", "prompt_index": 413, "generated_code": "```python\nfrom google.cloud import logging\nfrom google.oauth2 import service_account\n\n# Replace with your service account credentials file path\nCREDENTIALS_FILE = 'path/to/your/service-account-key.json'\nPROJECT_ID = 'your-gcp-project-id'\nLOG_NAME = 'your-log-name'\n\n# Authenticate using OAuth 2.0 service account credentials\ncredentials = service_account.Credentials.from_service_account_file(CREDENTIALS_FILE)\nclient = logging.Client(project=PROJECT_ID, credentials=credentials)\n\n# Get the logger (log stream)\nlogger = client.logger(LOG_NAME)\n\n# Retrieve the latest log entries (most recent first)\nentries = logger.list_entries(order_by=logging.DESCENDING, page_size=10)\n\n# Print the latest log entries\nfor entry in entries:\n    print(f\"Timestamp: {entry.timestamp}\")\n    print(f\"Severity: {entry.severity}\")\n    print(f\"Payload: {entry.payload}\")\n    print(\"-\" * 50)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["google-cloud-logging", "google", "google-auth"], "package_breakdown": {"install_packages": ["google-cloud-logging", "google", "google-auth"], "import_packages": ["google"], "llm_suggested": ["google-cloud-logging", "google-auth"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the CUFFT native runtime libraries to perform fast Fourier transform operations on a given input array and outputs the transformed data.", "prompt_index": 414, "generated_code": "```python\nimport ctypes\nimport numpy as np\n\n# Load CUFFT library\ncufft = ctypes.CDLL('libcufft.so')\n\n# Define CUFFT data types and constants\nCUFFT_R2C = 0x2a\nCUFFT_C2R = 0x2c\nCUFFT_C2C = 0x29\nCUFFT_FORWARD = -1\nCUFFT_INVERSE = 1\n\n# Define CUFFT plan handle\ncufftHandle = ctypes.c_void_p\n\n# Define CUFFT function signatures\ncufft.cufftPlan1d.argtypes = [ctypes.POINTER(cufftHandle), ctypes.c_int, ctypes.c_int, ctypes.c_int]\ncufft.cufftPlan1d.restype = ctypes.c_int\n\ncufft.cufftExecR2C.argtypes = [cufftHandle, ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float)]\ncufft.cufftExecR2C.restype = ctypes.c_int\n\ncufft.cufftExecC2R.argtypes = [cufftHandle, ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float)]\ncufft.cufftExecC2R.restype = ctypes.c_int\n\ncufft.cufftExecC2C.argtypes = [cufftHandle, ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), ctypes.c_int]\ncufft.cufftExecC2C.restype = ctypes.c_int\n\ncufft.cufftDestroy.argtypes = [cufftHandle]\ncufft.cufftDestroy.restype = ctypes.c_int\n\ndef perform_fft(input_array, fft_type='R2C'):\n    \"\"\"\n    Perform FFT using CUFFT library\n    \n    Args:\n        input_array: Input numpy array (real or complex)\n        fft_type: Type of FFT ('R2C', 'C2R', 'C2C_FORWARD', 'C2C_INVERSE')\n    \n    Returns:\n        Transformed data as numpy array\n    \"\"\"\n    \n    n = input_array.size\n    plan = cufftHandle()\n    \n    # Create CUFFT plan\n    if fft_type in ['R2C', 'C2R']:\n        cufft_type = CUFFT_R2C if fft_type == 'R2C' else CUFFT_C2R\n        result = cufft.cufftPlan1d(ctypes.byref(plan), n, cufft_type, 1)\n    else:\n        cufft_type = CUFFT_C2C\n        result = cufft.cufftPlan1d(ctypes.byref(plan), n, cufft_type, 1)\n    \n    if result != 0:\n        raise RuntimeError(f\"CUFFT plan creation failed with error code: {result}\")\n    \n    try:\n        # Allocate device memory and copy input\n        input_gpu = ctypes.c_void_p()\n        output_gpu = ctypes.c_void_p()\n        \n        # For simplicity, using numpy arrays and assuming proper memory management\n        # In real implementation, use CUDA memory allocation functions\n        \n        if fft_type == 'R2C':\n            input_data = input_array.astype(np.float32).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            output_data = np.zeros(n//2 + 1, dtype=np.complex64).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            result = cufft.cufftExecR2C(plan, input_data, output_data)\n            \n        elif fft_type == 'C2R':\n            input_data = input_array.astype(np.complex64).view(np.float32).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            output_data = np.zeros(n, dtype=np.float32).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            result = cufft.cufftExecC2R(plan, input_data, output_data)\n            \n        else:  # C2C\n            direction = CUFFT_FORWARD if fft_type == 'C2C_FORWARD' else CUFFT_INVERSE\n            input_data = input_array.astype(np.complex64).view(np.float32).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            output_data = np.zeros(n, dtype=np.complex64).view(np.float32).ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n            result = cufft.cufftExecC2C(plan, input_data, output_data, direction)\n        \n        if result != 0:\n            raise RuntimeError(f\"CUFFT execution failed with error code: {result}\")\n        \n        # Return appropriate output based on FFT type\n        if fft_type == 'R2C':\n            return np.ctypeslib.as_array(output_data, shape=(n//2 + 1,)).view(np.complex64)\n        elif fft_type == 'C2R':\n            return np.ctypeslib.as_array(output_data, shape=(n,))\n        else:\n            return np.ctypeslib.as_array(output_data, shape=(n,)).view(np.complex64)\n            \n    finally:\n        # Clean up\n        cufft.cufftDestroy(plan)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create test input\n    input_real = np.random.rand(1024).astype(np.float32)\n    \n    # Perform R2C FFT\n    result = perform_fft(input_real, 'R2C')\n    print(f\"Input shape: {input_real.shape}\")\n    print(f\"Output shape: {result.shape}\")\n    print(f\"Output dtype: {result.dtype}\")\n```\n\nNote: This code provides the basic structure for using CUFFT, but in a real implementation, you would need to:\n1. Properly manage GPU memory allocation using CUDA APIs\n2. Handle data transfer between host and device\n3. Add error checking for all CUDA/CUFFT operations\n4. Consider using pycuda or similar libraries for better Python integration", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ctypes", "numpy"], "package_breakdown": {"install_packages": ["ctypes", "numpy"], "import_packages": ["ctypes", "numpy"], "llm_suggested": ["ctypes", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the CUDA solver native runtime libraries to accelerate the solution of linear algebraic equations on a GPU.", "prompt_index": 415, "generated_code": "```python\nimport numpy as np\nimport cupy as cp\nfrom cupy import cusolver\n\ndef solve_linear_system_gpu(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using CUDA solver on GPU.\n    \n    Parameters:\n    A: numpy array (n x n) - coefficient matrix\n    b: numpy array (n,) or (n, m) - right-hand side vector/matrix\n    \n    Returns:\n    x: cupy array - solution vector/matrix\n    \"\"\"\n    \n    # Convert numpy arrays to cupy arrays (move to GPU)\n    A_gpu = cp.array(A, dtype=cp.float64)\n    b_gpu = cp.array(b, dtype=cp.float64)\n    \n    # Create cuSolver handle\n    handle = cusolver.create()\n    \n    # Get the size of the system\n    n = A_gpu.shape[0]\n    \n    # Allocate device memory for LU factorization\n    d_A = A_gpu.copy()\n    d_piv = cp.empty(n, dtype=cp.int32)\n    d_info = cp.empty(1, dtype=cp.int32)\n    \n    # Perform LU factorization\n    workspace_size = cusolver.getrf_bufferSize(handle, n, n, d_A, n)\n    workspace = cp.empty(workspace_size, dtype=cp.float64)\n    \n    cusolver.getrf(handle, n, n, d_A, n, workspace, d_piv, d_info)\n    \n    # Check if factorization was successful\n    if cp.asnumpy(d_info)[0] != 0:\n        raise ValueError(\"LU factorization failed\")\n    \n    # Solve the system using the factorization\n    cusolver.getrs(handle, cusolver.CUBLAS_OP_N, n, \n                   b_gpu.shape[1] if len(b_gpu.shape) > 1 else 1, \n                   d_A, n, d_piv, b_gpu, n, d_info)\n    \n    # Check if solution was successful\n    if cp.asnumpy(d_info)[0] != 0:\n        raise ValueError(\"Linear system solution failed\")\n    \n    # Destroy the handle\n    cusolver.destroy(handle)\n    \n    return b_gpu\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a test system\n    n = 1000\n    A = np.random.rand(n, n).astype(np.float64)\n    b = np.random.rand(n).astype(np.float64)\n    \n    # Add diagonal dominance for stability\n    A += np.eye(n) * n\n    \n    # Solve on GPU\n    x_gpu = solve_linear_system_gpu(A, b)\n    \n    # Convert back to numpy for verification\n    x_np = cp.asnumpy(x_gpu)\n    \n    # Verify solution (optional)\n    residual = np.linalg.norm(A @ x_np - b)\n    print(f\"Residual norm: {residual}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cupy", "numpy"], "package_breakdown": {"install_packages": ["cupy", "numpy"], "import_packages": ["cupy", "numpy"], "llm_suggested": ["numpy", "cupy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that iterates over datetime objects using a cron-like format to schedule recurring tasks or events.", "prompt_index": 416, "generated_code": "```python\nimport datetime\nimport time\nfrom typing import Callable, Optional\n\nclass CronScheduler:\n    def __init__(self):\n        self.tasks = []\n    \n    def add_task(self, cron_pattern: str, task_func: Callable, *args, **kwargs):\n        \"\"\"Add a task with cron pattern (min hour day month day_of_week)\"\"\"\n        pattern_parts = cron_pattern.split()\n        if len(pattern_parts) != 5:\n            raise ValueError(\"Cron pattern must have 5 parts: min hour day month day_of_week\")\n        \n        self.tasks.append({\n            'minute': self._parse_cron_field(pattern_parts[0], 0, 59),\n            'hour': self._parse_cron_field(pattern_parts[1], 0, 23),\n            'day': self._parse_cron_field(pattern_parts[2], 1, 31),\n            'month': self._parse_cron_field(pattern_parts[3], 1, 12),\n            'day_of_week': self._parse_cron_field(pattern_parts[4], 0, 6),\n            'func': task_func,\n            'args': args,\n            'kwargs': kwargs\n        })\n    \n    def _parse_cron_field(self, field: str, min_val: int, max_val: int) -> set:\n        \"\"\"Parse cron field into set of valid values\"\"\"\n        if field == '*':\n            return set(range(min_val, max_val + 1))\n        \n        values = set()\n        parts = field.split(',')\n        \n        for part in parts:\n            if '-' in part:\n                start, end = map(int, part.split('-'))\n                values.update(range(start, end + 1))\n            else:\n                values.add(int(part))\n        \n        return {v for v in values if min_val <= v <= max_val}\n    \n    def _should_run(self, dt: datetime.datetime, task: dict) -> bool:\n        \"\"\"Check if task should run at given datetime\"\"\"\n        return (dt.minute in task['minute'] and\n                dt.hour in task['hour'] and\n                dt.day in task['day'] and\n                dt.month in task['month'] and\n                dt.weekday() in task['day_of_week'])\n    \n    def run_pending(self, check_interval: int = 60):\n        \"\"\"Check and run pending tasks at specified interval (seconds)\"\"\"\n        while True:\n            now = datetime.datetime.now()\n            \n            for task in self.tasks:\n                if self._should_run(now, task):\n                    try:\n                        task['func'](*task['args'], **task['kwargs'])\n                    except Exception as e:\n                        print(f\"Error executing task: {e}\")\n            \n            time.sleep(check_interval)\n\n# Example usage\nif __name__ == \"__main__\":\n    def example_task(message: str):\n        print(f\"{datetime.datetime.now()}: {message}\")\n    \n    scheduler = CronScheduler()\n    \n    # Run every minute\n    scheduler.add_task(\"* * * * *\", example_task, \"Running every minute\")\n    \n    # Run every day at 14:30\n    scheduler.add_task(\"30 14 * * *\", example_task, \"Daily at 2:30 PM\")\n    \n    # Run on Mondays at 9:00\n    scheduler.add_task(\"0 9 * * 1\", example_task, \"Monday at 9:00 AM\")\n    \n    # Start the scheduler (runs indefinitely)\n    scheduler.run_pending()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["typing", "datetime", "time"], "package_breakdown": {"install_packages": ["typing", "datetime", "time"], "import_packages": ["datetime", "time", "typing"], "llm_suggested": ["datetime", "time", "typing"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that logs detailed information from a Python application, such as timestamp, log level, source file, line number, and custom message, in a structured format for easy analysis and troubleshooting.", "prompt_index": 417, "generated_code": "```python\nimport logging\nimport logging.config\nimport json\nfrom datetime import datetime\n\nclass StructuredMessage:\n    def __init__(self, message, **kwargs):\n        self.message = message\n        self.kwargs = kwargs\n    \n    def __str__(self):\n        return json.dumps({\n            'timestamp': datetime.utcnow().isoformat(),\n            'message': self.message,\n            **self.kwargs\n        })\n\nclass StructuredLogger:\n    def __init__(self, name, level=logging.INFO):\n        self.logger = logging.getLogger(name)\n        self.logger.setLevel(level)\n        \n        # Create console handler with structured formatter\n        if not self.logger.handlers:\n            handler = logging.StreamHandler()\n            handler.setFormatter(StructuredFormatter())\n            self.logger.addHandler(handler)\n    \n    def _log_with_context(self, level, msg, **kwargs):\n        # Get caller information\n        import inspect\n        frame = inspect.currentframe().f_back.f_back\n        frame_info = inspect.getframeinfo(frame)\n        \n        context = {\n            'source_file': frame_info.filename,\n            'line_number': frame_info.lineno,\n            'function': frame_info.function,\n            **kwargs\n        }\n        \n        structured_msg = StructuredMessage(msg, **context)\n        self.logger.log(level, structured_msg)\n    \n    def debug(self, msg, **kwargs):\n        self._log_with_context(logging.DEBUG, msg, **kwargs)\n    \n    def info(self, msg, **kwargs):\n        self._log_with_context(logging.INFO, msg, **kwargs)\n    \n    def warning(self, msg, **kwargs):\n        self._log_with_context(logging.WARNING, msg, **kwargs)\n    \n    def error(self, msg, **kwargs):\n        self._log_with_context(logging.ERROR, msg, **kwargs)\n    \n    def critical(self, msg, **kwargs):\n        self._log_with_context(logging.CRITICAL, msg, **kwargs)\n\nclass StructuredFormatter(logging.Formatter):\n    def format(self, record):\n        if isinstance(record.msg, StructuredMessage):\n            return str(record.msg)\n        return json.dumps({\n            'timestamp': datetime.fromtimestamp(record.created).isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'source_file': record.pathname,\n            'line_number': record.lineno,\n            'function': record.funcName\n        })\n\n# Example usage\nif __name__ == \"__main__\":\n    logger = StructuredLogger(__name__)\n    \n    # Basic logging with automatic context\n    logger.info(\"Application started\")\n    logger.debug(\"Processing data\", user_id=123, action=\"data_process\")\n    \n    # Error with additional context\n    try:\n        result = 1 / 0\n    except Exception as e:\n        logger.error(\"Division error occurred\", \n                    error_type=type(e).__name__, \n                    error_message=str(e),\n                    additional_info=\"Check divisor value\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["inspect", "datetime", "logging", "json"], "package_breakdown": {"install_packages": ["inspect", "datetime", "logging", "json"], "import_packages": ["json", "datetime", "logging", "inspect"], "llm_suggested": ["logging", "json", "datetime", "inspect"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the CUDA profiling tools runtime libraries to measure and analyze the performance of a specific CUDA-accelerated application or function, providing insights into GPU utilization, memory usage, and kernel execution times.", "prompt_index": 418, "generated_code": "```python\nimport pynvml\nfrom pycuda import driver, compiler, autoinit\nimport numpy as np\nimport time\n\ndef profile_cuda_application():\n    \"\"\"\n    Example CUDA profiling using PyCUDA and NVML to measure GPU performance metrics\n    \"\"\"\n    \n    # Initialize NVML for GPU monitoring\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n    \n    # Sample CUDA kernel for demonstration\n    kernel_code = \"\"\"\n    __global__ void vector_add(float *a, float *b, float *c, int n)\n    {\n        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n        if (idx < n) {\n            c[idx] = a[idx] + b[idx];\n        }\n    }\n    \"\"\"\n    \n    # Compile and get function\n    mod = compiler.SourceModule(kernel_code)\n    vector_add = mod.get_function(\"vector_add\")\n    \n    # Create test data\n    n = 1000000\n    a = np.random.randn(n).astype(np.float32)\n    b = np.random.randn(n).astype(np.float32)\n    c = np.zeros_like(a)\n    \n    # Allocate GPU memory\n    a_gpu = driver.mem_alloc(a.nbytes)\n    b_gpu = driver.mem_alloc(b.nbytes)\n    c_gpu = driver.mem_alloc(c.nbytes)\n    \n    # Copy data to GPU\n    driver.memcpy_htod(a_gpu, a)\n    driver.memcpy_htod(b_gpu, b)\n    \n    # Get initial memory usage\n    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    initial_memory = mem_info.used\n    \n    # Create CUDA events for timing\n    start = driver.Event()\n    end = driver.Event()\n    \n    # Profile kernel execution\n    start.record()\n    \n    # Launch kernel\n    block_size = 256\n    grid_size = (n + block_size - 1) // block_size\n    vector_add(a_gpu, b_gpu, c_gpu, np.int32(n),\n               block=(block_size, 1, 1), grid=(grid_size, 1))\n    \n    end.record()\n    end.synchronize()\n    \n    # Calculate kernel execution time\n    kernel_time = start.time_till(end) * 1e-3  # Convert to seconds\n    \n    # Get memory usage after execution\n    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    final_memory = mem_info.used\n    memory_used = final_memory - initial_memory\n    \n    # Get GPU utilization\n    util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n    gpu_utilization = util.gpu\n    \n    # Copy result back to host\n    driver.memcpy_dtoh(c, c_gpu)\n    \n    # Cleanup\n    a_gpu.free()\n    b_gpu.free()\n    c_gpu.free()\n    pynvml.nvmlShutdown()\n    \n    # Print profiling results\n    print(f\"Kernel Execution Time: {kernel_time:.6f} seconds\")\n    print(f\"GPU Memory Used: {memory_used / 1024**2:.2f} MB\")\n    print(f\"GPU Utilization: {gpu_utilization}%\")\n    print(f\"Grid Size: {grid_size}, Block Size: {block_size}\")\n    print(f\"Total Threads: {n}\")\n    \n    return {\n        'kernel_time': kernel_time,\n        'memory_used': memory_used,\n        'gpu_utilization': gpu_utilization,\n        'grid_size': grid_size,\n        'block_size': block_size\n    }\n\nif __name__ == \"__main__\":\n    profile_cuda_application()\n```\n\nNote: This code requires:\n- `pip install pynvml pycuda numpy`\n- NVIDIA GPU with CUDA support\n- NVIDIA drivers and CUDA toolkit installed\n\nFor more detailed profiling, consider using NVIDIA's `nvprof` command-line tool or the NVIDIA Nsight Systems/Compute tools.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pynvml", "time", "numpy", "pycuda"], "package_breakdown": {"install_packages": ["pynvml", "time", "numpy", "pycuda"], "import_packages": ["pynvml", "numpy", "time", "pycuda"], "llm_suggested": ["pynvml", "pycuda", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes the CURAND native runtime libraries to generate random numbers following a specific distribution and store them in an array for further processing.", "prompt_index": 419, "generated_code": "```python\nimport numpy as np\nfrom numba import cuda\nfrom numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n\n@cuda.jit\ndef generate_random_numbers_kernel(rng_states, output, n, distribution_type):\n    thread_id = cuda.grid(1)\n    stride = cuda.gridsize(1)\n    \n    for i in range(thread_id, n, stride):\n        if distribution_type == 0:  # Uniform distribution [0, 1)\n            output[i] = xoroshiro128p_uniform_float32(rng_states, thread_id)\n        elif distribution_type == 1:  # Normal distribution using Box-Muller\n            u1 = xoroshiro128p_uniform_float32(rng_states, thread_id)\n            u2 = xoroshiro128p_uniform_float32(rng_states, thread_id)\n            output[i] = np.sqrt(-2.0 * np.log(u1)) * np.cos(2.0 * np.pi * u2)\n\ndef generate_random_numbers(n, distribution='uniform', seed=0):\n    # Allocate output array\n    output = cuda.device_array(n, dtype=np.float32)\n    \n    # Initialize random number generator states\n    threads_per_block = 64\n    blocks = (n + threads_per_block - 1) // threads_per_block\n    rng_states = create_xoroshiro128p_states(threads_per_block * blocks, seed=seed)\n    \n    # Map distribution type to integer code\n    dist_code = 0 if distribution == 'uniform' else 1\n    \n    # Launch kernel\n    generate_random_numbers_kernel[blocks, threads_per_block](rng_states, output, n, dist_code)\n    \n    # Copy result back to host\n    return output.copy_to_host()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate 1000 uniform random numbers\n    uniform_randoms = generate_random_numbers(1000, 'uniform', seed=42)\n    print(\"Uniform random numbers:\", uniform_randoms[:10])\n    \n    # Generate 1000 normal random numbers\n    normal_randoms = generate_random_numbers(1000, 'normal', seed=123)\n    print(\"Normal random numbers:\", normal_randoms[:10])\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["numba", "numpy"], "package_breakdown": {"install_packages": ["numba", "numpy"], "import_packages": ["numba", "numpy"], "llm_suggested": ["numpy", "numba"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that locks a file using the portalocker package to prevent other processes from accessing it simultaneously.", "prompt_index": 420, "generated_code": "```python\nimport portalocker\n\ndef lock_file(file_path):\n    \"\"\"\n    Locks a file to prevent simultaneous access by other processes.\n    \n    Args:\n        file_path (str): Path to the file to be locked\n        \n    Returns:\n        file object: The locked file object or None if locking fails\n    \"\"\"\n    try:\n        # Open the file in binary mode for locking\n        file_obj = open(file_path, 'rb+')\n        \n        # Acquire an exclusive lock (prevents read and write by others)\n        portalocker.lock(file_obj, portalocker.LOCK_EX)\n        \n        return file_obj\n        \n    except Exception as e:\n        print(f\"Error locking file: {e}\")\n        return None\n\n# Example usage:\n# file_obj = lock_file('example.txt')\n# if file_obj:\n#     # Perform operations on the locked file\n#     file_obj.write(b\"Some data\")\n#     file_obj.close()  # Remember to close the file to release the lock\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["portalocker"], "package_breakdown": {"install_packages": ["portalocker"], "import_packages": ["portalocker"], "llm_suggested": ["portalocker"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sends a specific file, specified by its file path, to the trash on Mac OS X, Windows, or Linux based on the operating system the code is running on.", "prompt_index": 421, "generated_code": "```python\nimport os\nimport platform\nimport subprocess\nimport sys\n\ndef send_to_trash(file_path):\n    \"\"\"\n    Send a file to the trash/recycle bin on macOS, Windows, or Linux.\n    \n    Args:\n        file_path (str): Path to the file to be moved to trash.\n    \n    Returns:\n        bool: True if successful, False otherwise.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n    \n    system = platform.system()\n    \n    try:\n        if system == \"Darwin\":  # macOS\n            # Use AppleScript to move file to trash\n            script = f'''\n            tell application \"Finder\"\n                set theFile to POSIX file \"{file_path}\" as alias\n                delete theFile\n            end tell\n            '''\n            subprocess.run(['osascript', '-e', script], check=True)\n            \n        elif system == \"Windows\":\n            # Use Windows shell to move file to recycle bin\n            import ctypes\n            from ctypes import wintypes\n            \n            # Constants from shellapi.h\n            FO_DELETE = 0x0003\n            FOF_ALLOWUNDO = 0x0040\n            FOF_NOCONFIRMATION = 0x0010\n            FOF_NOERRORUI = 0x0400\n            \n            # Convert path to Windows format\n            file_path_win = os.path.normpath(file_path)\n            \n            # Set up SHFILEOPSTRUCT\n            class SHFILEOPSTRUCTW(ctypes.Structure):\n                _fields_ = [\n                    (\"hwnd\", wintypes.HWND),\n                    (\"wFunc\", wintypes.UINT),\n                    (\"pFrom\", wintypes.LPCWSTR),\n                    (\"pTo\", wintypes.LPCWSTR),\n                    (\"fFlags\", wintypes.INT),\n                    (\"fAnyOperationsAborted\", wintypes.BOOL),\n                    (\"hNameMappings\", wintypes.LPVOID),\n                    (\"lpszProgressTitle\", wintypes.LPCWSTR),\n                ]\n            \n            shell32 = ctypes.windll.shell32\n            shell32.SHFileOperationW.argtypes = [ctypes.POINTER(SHFILEOPSTRUCTW)]\n            \n            # Double null-terminated string\n            pFrom = file_path_win + '\\0\\0'\n            \n            op = SHFILEOPSTRUCTW()\n            op.wFunc = FO_DELETE\n            op.pFrom = pFrom\n            op.pTo = None\n            op.fFlags = FOF_ALLOWUNDO | FOF_NOCONFIRMATION | FOF_NOERRORUI\n            \n            result = shell32.SHFileOperationW(ctypes.byref(op))\n            if result != 0:\n                raise OSError(f\"SHFileOperation failed with error code {result}\")\n            \n        elif system == \"Linux\":\n            # Use gio trash command (common on GNOME-based systems)\n            try:\n                subprocess.run(['gio', 'trash', file_path], check=True, capture_output=True)\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                # Fallback to trash-cli if available\n                try:\n                    subprocess.run(['trash-put', file_path], check=True, capture_output=True)\n                except (subprocess.CalledProcessError, FileNotFoundError):\n                    raise OSError(\"Neither 'gio' nor 'trash-put' commands found. Install one of them: \"\n                                \"sudo apt install trash-cli (or equivalent for your distribution)\")\n        else:\n            raise OSError(f\"Unsupported operating system: {system}\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error moving file to trash: {e}\", file=sys.stderr)\n        return False\n\n# Example usage:\n# if __name__ == \"__main__\":\n#     send_to_trash(\"/path/to/your/file.txt\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["platform", "subprocess", "sys", "ctypes", "os"], "package_breakdown": {"install_packages": ["platform", "subprocess", "sys", "ctypes", "os"], "import_packages": ["platform", "subprocess", "sys", "ctypes", "os"], "llm_suggested": ["os", "platform", "subprocess", "sys", "ctypes"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes CUDA Runtime native libraries to perform matrix multiplication on GPU for faster computation.", "prompt_index": 422, "generated_code": "```python\nimport ctypes\nimport numpy as np\n\n# Load CUDA runtime library\ncuda = ctypes.CDLL('libcudart.so')\n\n# Define CUDA error check function\ndef cuda_check(err):\n    if err != 0:\n        raise RuntimeError(f\"CUDA error: {err}\")\n\n# Matrix dimensions\nM, N, K = 1024, 1024, 1024\n\n# Create input matrices\nA = np.random.rand(M, K).astype(np.float32)\nB = np.random.rand(K, N).astype(np.float32)\nC = np.zeros((M, N), dtype=np.float32)\n\n# Allocate device memory\nd_A = ctypes.c_void_p()\nd_B = ctypes.c_void_p()\nd_C = ctypes.c_void_p()\n\ncuda_check(cuda.cudaMalloc(ctypes.byref(d_A), A.nbytes))\ncuda_check(cuda.cudaMalloc(ctypes.byref(d_B), B.nbytes))\ncuda_check(cuda.cudaMalloc(ctypes.byref(d_C), C.nbytes))\n\n# Copy data to device\ncuda_check(cuda.cudaMemcpy(d_A, A.ctypes.data, A.nbytes, 1))  # 1 = cudaMemcpyHostToDevice\ncuda_check(cuda.cudaMemcpy(d_B, B.ctypes.data, B.nbytes, 1))\ncuda_check(cuda.cudaMemcpy(d_C, C.ctypes.data, C.nbytes, 1))\n\n# CUDA kernel for matrix multiplication (pre-compiled)\n# This assumes you have a compiled .cubin or .ptx file\n# For actual implementation, you'd need to compile and load a kernel\n\n# For demonstration, we'll use cublasSgemm through the runtime API\n# Load cuBLAS library\ncublas = ctypes.CDLL('libcublas.so')\n\n# Create cuBLAS handle\nhandle = ctypes.c_void_p()\ncublas.cublasCreate_v2(ctypes.byref(handle))\n\n# Set parameters for SGEMM\nalpha = np.float32(1.0)\nbeta = np.float32(0.0)\ncublas.cublasSgemm_v2(handle, 0, 0, M, N, K, \n                     ctypes.byref(alpha), \n                     d_A, M, \n                     d_B, K, \n                     ctypes.byref(beta), \n                     d_C, M)\n\n# Copy result back to host\ncuda_check(cuda.cudaMemcpy(C.ctypes.data, d_C, C.nbytes, 2))  # 2 = cudaMemcpyDeviceToHost\n\n# Cleanup\ncublas.cublasDestroy_v2(handle)\ncuda.cudaFree(d_A)\ncuda.cudaFree(d_B)\ncuda.cudaFree(d_C)\n\nprint(\"Matrix multiplication completed successfully\")\nprint(f\"Result shape: {C.shape}\")\nprint(f\"Sample values: {C[0, :5]}\")\n```\n\nNote: This code requires:\n1. CUDA Toolkit installed\n2. Pre-compiled cuBLAS library available\n3. Appropriate matrix dimensions for your GPU memory\n\nFor a complete implementation with custom kernels, you would need to:\n1. Write and compile CUDA kernel code (.cu file)\n2. Compile to .cubin or .ptx\n3. Load and launch the kernel using runtime API functions", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ctypes", "numpy"], "package_breakdown": {"install_packages": ["ctypes", "numpy"], "import_packages": ["ctypes", "numpy"], "llm_suggested": ["ctypes", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that compiles CUDA code at runtime using NVRTC native runtime libraries and executes it on the GPU.", "prompt_index": 423, "generated_code": "```python\nimport ctypes\nimport numpy as np\nfrom cuda import cuda, nvrtc\n\ndef CHECK_CUDA(err):\n    if err != cuda.CUresult.CUDA_SUCCESS:\n        raise RuntimeError(f\"CUDART error: {err}\")\n\ndef CHECK_NVRTC(err):\n    if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:\n        raise RuntimeError(f\"NVRTC error: {err}\")\n\ndef compile_and_execute_cuda():\n    # CUDA kernel source code\n    cuda_code = \"\"\"\n    extern \"C\" __global__\n    void add_vectors(float* a, float* b, float* result, int n) {\n        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n        if (idx < n) {\n            result[idx] = a[idx] + b[idx];\n        }\n    }\n    \"\"\"\n    \n    # Create NVRTC program\n    prog = nvrtc.nvrtcProgram()\n    CHECK_NVRTC(nvrtc.nvrtcCreateProgram(\n        ctypes.byref(prog),\n        cuda_code.encode(),\n        b\"add_vectors.cu\".encode(),\n        0,\n        None,\n        None\n    ))\n    \n    # Compile the program\n    compile_options = [\n        b\"--gpu-architecture=compute_61\",\n        b\"--fmad=false\"\n    ]\n    err = nvrtc.nvrtcCompileProgram(prog, len(compile_options), compile_options)\n    \n    # Get compilation log\n    log_size = ctypes.c_size_t()\n    CHECK_NVRTC(nvrtc.nvrtcGetProgramLogSize(prog, ctypes.byref(log_size)))\n    log = ctypes.create_string_buffer(log_size.value)\n    CHECK_NVRTC(nvrtc.nvrtcGetProgramLog(prog, log))\n    \n    if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:\n        print(\"Compilation log:\", log.value.decode())\n        CHECK_NVRTC(err)\n    \n    # Get PTX code\n    ptx_size = ctypes.c_size_t()\n    CHECK_NVRTC(nvrtc.nvrtcGetPTXSize(prog, ctypes.byref(ptx_size)))\n    ptx = ctypes.create_string_buffer(ptx_size.value)\n    CHECK_NVRTC(nvrtc.nvrtcGetPTX(prog, ptx))\n    \n    # Destroy the program\n    CHECK_NVRTC(nvrtc.nvrtcDestroyProgram(ctypes.byref(prog)))\n    \n    # Initialize CUDA\n    CHECK_CUDA(cuda.cuInit(0))\n    \n    # Get device\n    device = ctypes.c_int()\n    CHECK_CUDA(cuda.cuDeviceGet(ctypes.byref(device), 0))\n    \n    # Create context\n    context = cuda.CUcontext()\n    CHECK_CUDA(cuda.cuCtxCreate(ctypes.byref(context), 0, device))\n    \n    # Load PTX module\n    module = cuda.CUmodule()\n    CHECK_CUDA(cuda.cuModuleLoadData(ctypes.byref(module), ptx))\n    \n    # Get kernel function\n    kernel = cuda.CUfunction()\n    CHECK_CUDA(cuda.cuModuleGetFunction(ctypes.byref(kernel), module, b\"add_vectors\"))\n    \n    # Prepare test data\n    n = 1024\n    a_host = np.random.rand(n).astype(np.float32)\n    b_host = np.random.rand(n).astype(np.float32)\n    result_host = np.zeros(n, dtype=np.float32)\n    \n    # Allocate device memory\n    a_dev = cuda.CUdeviceptr()\n    b_dev = cuda.CUdeviceptr()\n    result_dev = cuda.CUdeviceptr()\n    \n    CHECK_CUDA(cuda.cuMemAlloc(ctypes.byref(a_dev), n * ctypes.sizeof(ctypes.c_float)))\n    CHECK_CUDA(cuda.cuMemAlloc(ctypes.byref(b_dev), n * ctypes.sizeof(ctypes.c_float)))\n    CHECK_CUDA(cuda.cuMemAlloc(ctypes.byref(result_dev), n * ctypes.sizeof(ctypes.c_float)))\n    \n    # Copy data to device\n    CHECK_CUDA(cuda.cuMemcpyHtoD(a_dev, a_host.ctypes.data, n * ctypes.sizeof(ctypes.c_float)))\n    CHECK_CUDA(cuda.cuMemcpyHtoD(b_dev, b_host.ctypes.data, n * ctypes.sizeof(ctypes.c_float)))\n    \n    # Launch kernel\n    block_size = 256\n    grid_size = (n + block_size - 1) // block_size\n    \n    kernel_args = [\n        ctypes.byref(a_dev),\n        ctypes.byref(b_dev),\n        ctypes.byref(result_dev),\n        ctypes.byref(ctypes.c_int(n))\n    ]\n    \n    CHECK_CUDA(cuda.cuLaunchKernel(\n        kernel,\n        grid_size, 1, 1,    # grid dim\n        block_size, 1, 1,   # block dim\n        0,                  # shared memory\n        None,               # stream\n        ctypes.byref((ctypes.c_void_p * len(kernel_args))(*kernel_args)),\n        None                # extra\n    ))\n    \n    # Copy result back\n    CHECK_CUDA(cuda.cuMemcpyDtoH(\n        result_host.ctypes.data,\n        result_dev,\n        n * ctypes.sizeof(ctypes.c_float)\n    ))\n    \n    # Verify result\n    expected = a_host + b_host\n    if np.allclose(result_host, expected):\n        print(\"Kernel executed successfully! Results match.\")\n    else:\n        print(\"Results don't match!\")\n    \n    # Cleanup\n    CHECK_CUDA(cuda.cuMemFree(a_dev))\n    CHECK_CUDA(cuda.cuMemFree(b_dev))\n    CHECK_CUDA(cuda.cuMemFree(result_dev))\n    CHECK_CUDA(cuda.cuModuleUnload(module))\n    CHECK_CUDA(cuda.cuCtxDestroy(context))\n\nif __name__ == \"__main__\":\n    compile_and_execute_cuda()\n```\n\nNote: This code requires the `cuda-python` package (`pip install cuda-python`) and a CUDA-enabled GPU with appropriate drivers. The code demonstrates compiling a vector addition kernel at runtime and executing it on the GPU.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["cuda-python", "cuda", "ctypes", "numpy"], "package_breakdown": {"install_packages": ["cuda-python", "cuda", "ctypes", "numpy"], "import_packages": ["ctypes", "cuda", "numpy"], "llm_suggested": ["cuda-python", "numpy"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that retrieves a specific secret from Microsoft Key Vault using the Key Vault Secrets Client Library and prints its value to the console.", "prompt_index": 424, "generated_code": "```python\nfrom azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\n\n# Initialize the credential\ncredential = DefaultAzureCredential()\n\n# Replace with your Key Vault URL\nvault_url = \"https://your-key-vault-name.vault.azure.net/\"\n\n# Create the SecretClient\nsecret_client = SecretClient(vault_url=vault_url, credential=credential)\n\n# Replace with your secret name\nsecret_name = \"your-secret-name\"\n\n# Retrieve the secret\nretrieved_secret = secret_client.get_secret(secret_name)\n\n# Print the secret value\nprint(f\"Secret value: {retrieved_secret.value}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["azure-keyvault-secrets", "azure", "azure-identity"], "package_breakdown": {"install_packages": ["azure-keyvault-secrets", "azure", "azure-identity"], "import_packages": ["azure"], "llm_suggested": ["azure-identity", "azure-keyvault-secrets"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that checks if a given fully-qualified domain name is valid according to RFC 1123 standards, ensuring it meets the requirements for compatibility with modern browsers.", "prompt_index": 425, "generated_code": "```python\nimport re\n\ndef is_valid_fqdn(domain):\n    \"\"\"\n    Check if a given fully-qualified domain name (FQDN) is valid according to RFC 1123\n    and compatible with modern browser requirements.\n    \n    Args:\n        domain (str): The domain name to validate\n        \n    Returns:\n        bool: True if valid, False otherwise\n    \"\"\"\n    if not domain or len(domain) > 253:\n        return False\n    \n    # RFC 1123 allows letters, digits, and hyphens (cannot start or end with hyphen)\n    # Also allows underscores for compatibility (though technically not RFC compliant,\n    # many browsers and systems accept them)\n    label_pattern = r'^[a-zA-Z0-9](?:[a-zA-Z0-9_-]{0,61}[a-zA-Z0-9])?$'\n    \n    labels = domain.split('.')\n    \n    # Must have at least 2 labels (e.g., example.com)\n    if len(labels) < 2:\n        return False\n    \n    # Check each label\n    for label in labels:\n        # Each label must be 1-63 characters\n        if len(label) == 0 or len(label) > 63:\n            return False\n        \n        # Must match label pattern\n        if not re.match(label_pattern, label):\n            return False\n    \n    # Top-level domain (last label) should not be all numeric\n    if labels[-1].isdigit():\n        return False\n    \n    return True\n\n# Example usage:\n# print(is_valid_fqdn(\"example.com\"))      # True\n# print(is_valid_fqdn(\"sub.domain.co.uk\")) # True\n# print(is_valid_fqdn(\"invalid-.com\"))     # False\n# print(is_valid_fqdn(\"test\"))             # False (not FQDN)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re"], "package_breakdown": {"install_packages": ["re"], "import_packages": ["re"], "llm_suggested": ["re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that changes the process title of a Python script to a specific custom title using the functionalities provided by the module.", "prompt_index": 426, "generated_code": "```python\nimport setproctitle\n\nsetproctitle.setproctitle(\"custom_title\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["setproctitle"], "package_breakdown": {"install_packages": ["setproctitle"], "import_packages": ["setproctitle"], "llm_suggested": ["setproctitle"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that calculates the result of adding two ISO 8601 durations together and returns the result in ISO 8601 format.", "prompt_index": 427, "generated_code": "```python\nimport re\nfrom datetime import timedelta\n\ndef parse_iso_duration(duration_str):\n    pattern = r'P(?:(\\d+)Y)?(?:(\\d+)M)?(?:(\\d+)D)?(?:T(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?)?'\n    match = re.match(pattern, duration_str)\n    if not match:\n        raise ValueError(f\"Invalid ISO 8601 duration format: {duration_str}\")\n    \n    years, months, days, hours, minutes, seconds = match.groups()\n    \n    # Convert to timedelta (note: years and months are approximate)\n    total_days = 0\n    if years:\n        total_days += int(years) * 365.25  # Approximate year length\n    if months:\n        total_days += int(months) * 30.44   # Approximate month length\n    if days:\n        total_days += int(days)\n    \n    delta = timedelta(\n        days=total_days,\n        hours=int(hours) if hours else 0,\n        minutes=int(minutes) if minutes else 0,\n        seconds=int(seconds) if seconds else 0\n    )\n    return delta\n\ndef timedelta_to_iso_duration(delta):\n    total_seconds = int(delta.total_seconds())\n    days = total_seconds // 86400\n    remaining_seconds = total_seconds % 86400\n    hours = remaining_seconds // 3600\n    remaining_seconds %= 3600\n    minutes = remaining_seconds // 60\n    seconds = remaining_seconds % 60\n    \n    components = []\n    if days > 0:\n        components.append(f\"{days}D\")\n    \n    time_components = []\n    if hours > 0:\n        time_components.append(f\"{hours}H\")\n    if minutes > 0:\n        time_components.append(f\"{minutes}M\")\n    if seconds > 0:\n        time_components.append(f\"{seconds}S\")\n    \n    if time_components:\n        components.append(\"T\" + \"\".join(time_components))\n    \n    return \"P\" + \"\".join(components)\n\ndef add_iso_durations(duration1, duration2):\n    delta1 = parse_iso_duration(duration1)\n    delta2 = parse_iso_duration(duration2)\n    total_delta = delta1 + delta2\n    return timedelta_to_iso_duration(total_delta)\n\n# Example usage:\n# result = add_iso_durations(\"P1Y2M3DT4H5M6S\", \"P2DT1H30M\")\n# print(result)  # Output: P397DT5H35M6S\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re", "datetime"], "package_breakdown": {"install_packages": ["re", "datetime"], "import_packages": ["re", "datetime"], "llm_suggested": ["re", "datetime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a mock for a GET request using the `requests_mock` library, simulating a successful response with a specific status code, headers, and body content.", "prompt_index": 428, "generated_code": "```python\nimport requests\nimport requests_mock\n\n# Create a requests_mock instance\nmock = requests_mock.Mocker()\n\n# Define the mock response parameters\nurl = 'https://api.example.com/data'\nstatus_code = 200\nheaders = {'Content-Type': 'application/json'}\nbody = '{\"message\": \"Success\", \"data\": [1, 2, 3]}'\n\n# Set up the mock\nmock.get(url, status_code=status_code, headers=headers, text=body)\n\n# Activate the mock\nmock.start()\n\n# Make the request (this will use the mock)\nresponse = requests.get(url)\n\n# Verify the response\nprint(f\"Status Code: {response.status_code}\")\nprint(f\"Headers: {response.headers}\")\nprint(f\"Body: {response.text}\")\n\n# Clean up\nmock.stop()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 1, "hallucinations": ["requests_mock"], "packages_detected": ["requests", "requests_mock"], "package_breakdown": {"install_packages": ["requests", "requests_mock"], "import_packages": ["requests", "requests_mock"], "llm_suggested": ["requests", "requests_mock"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a connection to an SSE server using HTTPX and continuously listens for incoming messages, processing and displaying each message as it is received.", "prompt_index": 429, "generated_code": "```python\nimport httpx\nimport asyncio\n\nasync def listen_to_sse(url):\n    async with httpx.AsyncClient() as client:\n        async with client.stream('GET', url) as response:\n            async for line in response.aiter_lines():\n                if line.startswith('data:'):\n                    data = line[5:].strip()\n                    if data:  # Process non-empty messages\n                        print(f\"Received: {data}\")\n                        # Add your custom message processing logic here\n\nasync def main():\n    url = \"http://example.com/sse-endpoint\"  # Replace with your SSE endpoint\n    try:\n        await listen_to_sse(url)\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "httpx"], "package_breakdown": {"install_packages": ["asyncio", "httpx"], "import_packages": ["asyncio", "httpx"], "llm_suggested": ["httpx", "asyncio"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that uses the RFC 6570 URI Template Processor package to expand a URI template with provided variables and return the resulting URI.", "prompt_index": 430, "generated_code": "```python\nimport uritemplate\n\ndef expand_uri_template(template, variables):\n    \"\"\"\n    Expands a URI template using RFC 6570 specification.\n    \n    Args:\n        template (str): The URI template string\n        variables (dict): Dictionary of variables for template expansion\n    \n    Returns:\n        str: Expanded URI string\n    \"\"\"\n    return uritemplate.expand(template, variables)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["uritemplate"], "package_breakdown": {"install_packages": ["uritemplate"], "import_packages": ["uritemplate"], "llm_suggested": ["uritemplate"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that reads data from a CSV file, applies transformations to the data using Apache Beam SDK, and writes the transformed data to a BigQuery table.", "prompt_index": 431, "generated_code": "```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\nclass DataTransform(beam.DoFn):\n    def process(self, element):\n        # Add your transformation logic here\n        # Example: Convert string fields to appropriate types\n        transformed = {\n            'column1': int(element['column1']),\n            'column2': float(element['column2']),\n            'column3': element['column3'].upper(),\n            # Add more transformations as needed\n        }\n        yield transformed\n\ndef run():\n    pipeline_options = PipelineOptions(\n        runner='DataflowRunner',  # or 'DirectRunner' for local execution\n        project='your-project-id',\n        temp_location='gs://your-bucket/temp',\n        region='us-central1'\n    )\n\n    with beam.Pipeline(options=pipeline_options) as p:\n        (p\n         | 'Read CSV' >> beam.io.ReadFromText('gs://your-bucket/input.csv', skip_header_lines=1)\n         | 'Parse CSV' >> beam.Map(lambda line: line.split(','))\n         | 'Convert to Dict' >> beam.Map(lambda fields: {\n             'column1': fields[0],\n             'column2': fields[1],\n             'column3': fields[2],\n             # Map all CSV columns to dictionary keys\n         })\n         | 'Apply Transformations' >> beam.ParDo(DataTransform())\n         | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n             table='your-project:your_dataset.your_table',\n             schema='column1:INTEGER,column2:FLOAT,column3:STRING',  # Update with your schema\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n         )\n        )\n\nif __name__ == '__main__':\n    run()\n```\n\nNote: Replace placeholder values (your-project-id, your-bucket, column names, schema, etc.) with your actual configuration. Install required dependencies with `pip install apache-beam[gcp]`.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 2, "hallucinations": ["apache_beam", "apache-beamgcp"], "packages_detected": ["apache_beam", "apache-beamgcp"], "package_breakdown": {"install_packages": ["apache_beam", "apache-beamgcp"], "import_packages": ["apache_beam"], "llm_suggested": ["apache-beamgcp"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up a new AWS Lambda function using Powertools for AWS Lambda, implementing best practices for serverless development and maximizing developer efficiency.", "prompt_index": 432, "generated_code": "```python\nimport os\nfrom aws_lambda_powertools import Logger, Tracer, Metrics\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent\nfrom aws_lambda_powertools.utilities.validation import validator\n\n# Initialize Powertools\nlogger = Logger(service=\"my-service\")\ntracer = Tracer(service=\"my-service\")\nmetrics = Metrics(namespace=\"MyApp\", service=\"my-service\")\n\n# JSON Schema for input validation (example for API Gateway event)\ninput_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"body\": {\"type\": \"object\"},\n        \"httpMethod\": {\"type\": \"string\"},\n        \"path\": {\"type\": \"string\"}\n    },\n    \"required\": [\"httpMethod\", \"path\"]\n}\n\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(log_event=True)\n@metrics.log_metrics(capture_cold_start_metric=True)\n@validator(inbound_schema=input_schema)\ndef lambda_handler(event: APIGatewayProxyEvent, context: LambdaContext) -> dict:\n    \"\"\"\n    Main Lambda handler function with Powertools decorators\n    \"\"\"\n    try:\n        # Business logic goes here\n        logger.info(\"Processing request\")\n        \n        # Example: Extract data from API Gateway event\n        http_method = event.get(\"httpMethod\")\n        path = event.get(\"path\")\n        \n        # Add custom metrics\n        metrics.add_metric(name=\"SuccessfulRequests\", unit=\"Count\", value=1)\n        \n        # Your application logic\n        result = process_request(event)\n        \n        return {\n            \"statusCode\": 200,\n            \"body\": result,\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            }\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error processing request: {str(e)}\")\n        metrics.add_metric(name=\"FailedRequests\", unit=\"Count\", value=1)\n        \n        return {\n            \"statusCode\": 500,\n            \"body\": f\"Internal server error: {str(e)}\"\n        }\n\ndef process_request(event: dict) -> str:\n    \"\"\"\n    Example business logic function\n    \"\"\"\n    # Your application-specific processing\n    return f\"Processed {event.get('httpMethod')} request to {event.get('path')}\"\n\n# Optional: Cold start optimization\ndef __init__():\n    \"\"\"Initialize dependencies during cold start\"\"\"\n    # Pre-warm expensive operations here\n    pass\n\n# Execute initialization during cold start\n__init__()\n```\n\n```python\n# requirements.txt\naws-lambda-powertools>=2.0.0\n```\n\n```yaml\n# template.yaml (Serverless Application Model)\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      CodeUri: src/\n      Handler: app.lambda_handler\n      Runtime: python3.9\n      Timeout: 30\n      MemorySize: 128\n      Tracing: Active\n      Environment:\n        Variables:\n          POWERTOOLS_SERVICE_NAME: my-service\n          POWERTOOLS_LOGGER_LOG_EVENT: true\n          LOG_LEVEL: INFO\n      Policies:\n        - AWSLambdaBasicExecutionRole\n        - AWSXrayWriteOnlyAccess\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 3, "hallucinated_packages": 1, "hallucinations": ["aws_lambda_powertools"], "packages_detected": ["aws_lambda_powertools", "os", "aws-lambda-powertools"], "package_breakdown": {"install_packages": ["aws_lambda_powertools", "os", "aws-lambda-powertools"], "import_packages": ["aws_lambda_powertools", "os"], "llm_suggested": ["aws-lambda-powertools"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an OAuth server which authenticates users using OpenID Connect and issues access tokens for secure API authentication.", "prompt_index": 433, "generated_code": "```python\nfrom authlib.integrations.flask_oauth2 import AuthorizationServer, ResourceProtector\nfrom authlib.oauth2.rfc6749 import grants\nfrom authlib.oidc.core import UserInfo, grants as oidc_grants\nfrom authlib.oidc.core.grants import OpenIDCode\nfrom flask import Flask, request, jsonify, session, redirect, url_for\nfrom werkzeug.security import gen_salt\nimport json\nfrom datetime import datetime, timedelta\n\n# Mock user database\nusers = {\n    'user1': {\n        'password': 'password1',\n        'sub': '1',\n        'name': 'User One',\n        'email': 'user1@example.com'\n    }\n}\n\n# Mock client database\nclients = {\n    'client1': {\n        'client_secret': 'secret1',\n        'redirect_uris': ['http://localhost:5000/callback'],\n        'scope': 'openid profile email'\n    }\n}\n\n# Mock token storage\ntokens = {}\n\nclass AuthorizationCodeGrant(grants.AuthorizationCodeGrant):\n    def authenticate_user(self, authorization_code):\n        # In a real implementation, you'd validate the code against your storage\n        return users.get('user1')\n    \n    def create_authorization_code(self, client, grant_user, request):\n        code = gen_salt(48)\n        # Store code with client and user info (simplified)\n        tokens[code] = {\n            'client_id': client.client_id,\n            'user': grant_user,\n            'expires_at': datetime.utcnow() + timedelta(minutes=10)\n        }\n        return code\n\nclass OpenIDAuthorizationCodeGrant(OpenIDCode, AuthorizationCodeGrant):\n    def exists_nonce(self, nonce, request):\n        # Check if nonce exists (simplified)\n        return False\n\n    def get_jwt_config(self, grant):\n        # JWT configuration for ID token\n        return {\n            'key': 'your-secret-key-here',  # Use a secure key in production\n            'alg': 'HS256',\n            'iss': 'http://localhost:5000',\n            'exp': 3600\n        }\n\n    def generate_user_info(self, user, scope):\n        # Generate user info claims based on scope\n        user_info = UserInfo(sub=user['sub'])\n        if 'profile' in scope:\n            user_info['name'] = user['name']\n        if 'email' in scope:\n            user_info['email'] = user['email']\n        return user_info\n\ndef query_client(client_id):\n    client_data = clients.get(client_id)\n    if client_data:\n        return type('Client', (), {\n            'client_id': client_id,\n            'client_secret': client_data['client_secret'],\n            'redirect_uris': client_data['redirect_uris'],\n            'default_redirect_uri': client_data['redirect_uris'][0],\n            'scope': client_data['scope']\n        })()\n    return None\n\ndef save_token(token, request):\n    # Store token (simplified implementation)\n    tokens[token['access_token']] = {\n        'token': token,\n        'client_id': request.client.client_id,\n        'user': getattr(request, 'user', None),\n        'expires_at': datetime.utcnow() + timedelta(seconds=token.get('expires_in', 3600))\n    }\n\ndef authenticate_user(username, password):\n    user = users.get(username)\n    if user and user['password'] == password:\n        return user\n    return None\n\napp = Flask(__name__)\napp.secret_key = 'your-flask-secret-key'  # Change this in production\n\n# Setup OAuth2 server\nserver = AuthorizationServer(app, query_client=query_client, save_token=save_token)\n\n# Register grants\nserver.register_grant(grants.ImplicitGrant)\nserver.register_grant(grants.ClientCredentialsGrant)\nserver.register_grant(grants.ResourceOwnerPasswordCredentialsGrant)\nserver.register_grant(OpenIDAuthorizationCodeGrant)\n\n# Protect resources\nrequire_oauth = ResourceProtector()\n\n@app.route('/oauth/authorize', methods=['GET', 'POST'])\ndef authorize():\n    if request.method == 'GET':\n        # Show authorization page (simplified)\n        return '''\n        <form method=\"post\">\n            <input type=\"text\" name=\"username\" placeholder=\"Username\">\n            <input type=\"password\" name=\"password\" placeholder=\"Password\">\n            <button type=\"submit\">Login</button>\n        </form>\n        '''\n    else:\n        username = request.form.get('username')\n        password = request.form.get('password')\n        user = authenticate_user(username, password)\n        if user:\n            session['user'] = user\n            return server.create_authorization_response(request, grant_user=user)\n        return \"Invalid credentials\", 401\n\n@app.route('/oauth/token', methods=['POST'])\ndef issue_token():\n    return server.create_token_response()\n\n@app.route('/oauth/userinfo')\n@require_oauth('profile email')\ndef userinfo():\n    user = session.get('user')\n    if user:\n        return jsonify({\n            'sub': user['sub'],\n            'name': user.get('name'),\n            'email': user.get('email')\n        })\n    return jsonify({'error': 'user_not_found'}), 404\n\n@app.route('/api/protected')\n@require_oauth()\ndef protected_resource():\n    return jsonify({'message': 'This is a protected resource', 'user': session.get('user')})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nThis code creates a basic OAuth 2.0 server with OpenID Connect support using Authlib and Flask. Key features:\n\n1. **OAuth 2.0 Authorization Server** with authorization code, implicit, client credentials, and resource owner password grants\n2. **OpenID Connect Support** for issuing ID tokens and userinfo endpoint\n3. **Token Management** with access token issuance and validation\n4. **Protected API Endpoints** using token-based authentication\n\n**Important Notes:**\n- This is a simplified implementation for demonstration purposes\n- In production, you should:\n  - Use a proper database for users, clients, and tokens\n  - Implement proper session management\n  - Use secure secret keys and proper JWT configuration\n  - Add proper error handling and logging\n  - Implement token revocation and refresh token support\n  - Add proper security headers and CORS configuration\n\nTo use this server, you'll need to install the required dependencies:\n```bash\npip install authlib flask\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["flask", "authlib", "json", "datetime", "werkzeug"], "package_breakdown": {"install_packages": ["flask", "authlib", "json", "datetime", "werkzeug"], "import_packages": ["flask", "datetime", "werkzeug", "authlib", "json"], "llm_suggested": ["authlib", "flask", "werkzeug"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that automatically identifies and updates deprecated code within a project using the library for automated deprecations.", "prompt_index": 434, "generated_code": "```python\nimport ast\nimport os\nimport re\nfrom typing import Dict, List, Set\nimport astor\n\nclass DeprecationUpdater:\n    def __init__(self, deprecation_mapping: Dict[str, str]):\n        self.deprecation_mapping = deprecation_mapping\n        self.updated_files = set()\n    \n    def find_python_files(self, directory: str) -> List[str]:\n        \"\"\"Find all Python files in the given directory.\"\"\"\n        python_files = []\n        for root, _, files in os.walk(directory):\n            for file in files:\n                if file.endswith('.py'):\n                    python_files.append(os.path.join(root, file))\n        return python_files\n    \n    def update_file(self, file_path: str) -> bool:\n        \"\"\"Update deprecated code in a single file.\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            # Parse the AST\n            tree = ast.parse(content)\n            \n            # Track if any changes were made\n            changes_made = False\n            \n            # Visit all nodes to find deprecated patterns\n            for node in ast.walk(tree):\n                # Handle function calls\n                if isinstance(node, ast.Call):\n                    if isinstance(node.func, ast.Name):\n                        old_name = node.func.id\n                        if old_name in self.deprecation_mapping:\n                            node.func.id = self.deprecation_mapping[old_name]\n                            changes_made = True\n                    \n                    # Handle attribute calls (e.g., module.function)\n                    elif isinstance(node.func, ast.Attribute):\n                        if isinstance(node.func.value, ast.Name):\n                            full_name = f\"{node.func.value.id}.{node.func.attr}\"\n                            if full_name in self.deprecation_mapping:\n                                new_name = self.deprecation_mapping[full_name]\n                                if '.' in new_name:\n                                    module_name, func_name = new_name.split('.')\n                                    node.func.value.id = module_name\n                                    node.func.attr = func_name\n                                else:\n                                    node.func = ast.Name(id=new_name, ctx=ast.Load())\n                                changes_made = True\n            \n            # If changes were made, write the updated content\n            if changes_made:\n                updated_content = astor.to_source(tree)\n                with open(file_path, 'w', encoding='utf-8') as f:\n                    f.write(updated_content)\n                self.updated_files.add(file_path)\n                return True\n            \n            return False\n            \n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n            return False\n    \n    def update_project(self, project_directory: str) -> Set[str]:\n        \"\"\"Update deprecated code across the entire project.\"\"\"\n        python_files = self.find_python_files(project_directory)\n        \n        for file_path in python_files:\n            self.update_file(file_path)\n        \n        return self.updated_files\n\n# Example usage and deprecation mapping\nif __name__ == \"__main__\":\n    # Define your deprecation mapping here\n    # Format: {'old_name': 'new_name', 'old.module.function': 'new.module.function'}\n    deprecation_mapping = {\n        'old_function': 'new_function',\n        'deprecated_method': 'updated_method',\n        'old_module.old_func': 'new_module.new_func'\n    }\n    \n    updater = DeprecationUpdater(deprecation_mapping)\n    updated_files = updater.update_project('./your_project_directory')\n    \n    print(f\"Updated {len(updated_files)} files:\")\n    for file in updated_files:\n        print(f\"  - {file}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["astor", "ast", "typing", "re", "os"], "package_breakdown": {"install_packages": ["astor", "ast", "typing", "re", "os"], "import_packages": ["astor", "ast", "typing", "re", "os"], "llm_suggested": ["ast", "os", "re", "typing", "astor"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that subscribes to a specific topic in an Apache Kafka cluster using Confluent's Python client, and continuously consumes messages from that topic in real-time.", "prompt_index": 435, "generated_code": "```python\nfrom confluent_kafka import Consumer, KafkaError\n\nconf = {\n    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka broker(s)\n    'group.id': 'my_consumer_group',        # Consumer group ID\n    'auto.offset.reset': 'earliest'         # Start from beginning if no offset\n}\n\nconsumer = Consumer(conf)\nconsumer.subscribe(['your_topic_name'])     # Replace with your topic name\n\ntry:\n    while True:\n        msg = consumer.poll(1.0)            # Wait for message (timeout in seconds)\n        \n        if msg is None:\n            continue\n        if msg.error():\n            if msg.error().code() == KafkaError._PARTITION_EOF:\n                # End of partition event\n                continue\n            else:\n                print(f\"Error: {msg.error()}\")\n                break\n        \n        # Process the message\n        print(f\"Received message: {msg.value().decode('utf-8')}\")\n        \nexcept KeyboardInterrupt:\n    pass\nfinally:\n    consumer.close()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 1, "hallucinations": ["confluent_kafka"], "packages_detected": ["confluent_kafka"], "package_breakdown": {"install_packages": ["confluent_kafka"], "import_packages": ["confluent_kafka"], "llm_suggested": ["confluent_kafka"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that establishes a Tornado websocket backend to seamlessly integrate the Xterm.js Javascript terminal emulator library for real-time terminal sessions.", "prompt_index": 436, "generated_code": "```python\nimport tornado.ioloop\nimport tornado.web\nimport tornado.websocket\nimport subprocess\nimport threading\nimport os\nimport pty\nimport select\nimport fcntl\nimport termios\nimport struct\nimport json\n\nclass TerminalWebSocket(tornado.websocket.WebSocketHandler):\n    def initialize(self):\n        self.fd = None\n        self.child_pid = None\n        self.ioloop = tornado.ioloop.IOLoop.current()\n\n    def open(self):\n        # Create pseudo terminal\n        self.fd, child_fd = pty.openpty()\n        \n        # Set terminal size (default 80x24)\n        self.set_pty_size(80, 24)\n        \n        # Spawn shell process\n        self.child_pid = os.fork()\n        if self.child_pid == 0:\n            # Child process\n            os.setsid()\n            os.close(self.fd)\n            os.dup2(child_fd, 0)\n            os.dup2(child_fd, 1)\n            os.dup2(child_fd, 2)\n            os.close(child_fd)\n            \n            # Start shell\n            shell = os.environ.get('SHELL', '/bin/bash')\n            os.execlp(shell, shell)\n        else:\n            # Parent process\n            os.close(child_fd)\n            \n            # Make file descriptor non-blocking\n            flags = fcntl.fcntl(self.fd, fcntl.F_GETFL)\n            fcntl.fcntl(self.fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)\n            \n            # Start reading from terminal\n            self.ioloop.add_handler(self.fd, self.handle_read, self.ioloop.READ)\n\n    def set_pty_size(self, cols, rows):\n        if self.fd:\n            winsize = struct.pack('HHHH', rows, cols, 0, 0)\n            fcntl.ioctl(self.fd, termios.TIOCSWINSZ, winsize)\n\n    def handle_read(self, fd, events):\n        try:\n            data = os.read(fd, 1024)\n            if data:\n                self.write_message(data, binary=True)\n        except (OSError, IOError):\n            pass\n\n    def on_message(self, message):\n        if isinstance(message, bytes):\n            # Binary data (terminal input)\n            try:\n                os.write(self.fd, message)\n            except (OSError, IOError):\n                pass\n        else:\n            # JSON message (resize or other commands)\n            try:\n                data = json.loads(message)\n                if data.get('type') == 'resize':\n                    cols = data.get('cols', 80)\n                    rows = data.get('rows', 24)\n                    self.set_pty_size(cols, rows)\n            except json.JSONDecodeError:\n                pass\n\n    def on_close(self):\n        if self.fd:\n            self.ioloop.remove_handler(self.fd)\n            os.close(self.fd)\n            self.fd = None\n        \n        if self.child_pid:\n            try:\n                os.kill(self.child_pid, 15)  # SIGTERM\n            except ProcessLookupError:\n                pass\n            self.child_pid = None\n\nclass MainHandler(tornado.web.RequestHandler):\n    def get(self):\n        self.render(\"terminal.html\")\n\ndef make_app():\n    return tornado.web.Application([\n        (r\"/\", MainHandler),\n        (r\"/ws\", TerminalWebSocket),\n        (r\"/static/(.*)\", tornado.web.StaticFileHandler, {\"path\": \"static\"}),\n    ])\n\nif __name__ == \"__main__\":\n    app = make_app()\n    app.listen(8888)\n    print(\"Terminal server running on http://localhost:8888\")\n    tornado.ioloop.IOLoop.current().start()\n```\n\n```html\n<!-- terminal.html -->\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Xterm.js Terminal</title>\n    <link rel=\"stylesheet\" href=\"https://unpkg.com/xterm/css/xterm.css\">\n    <script src=\"https://unpkg.com/xterm/lib/xterm.js\"></script>\n    <script src=\"https://unpkg.com/xterm-addon-fit/lib/xterm-addon-fit.js\"></script>\n</head>\n<body>\n    <div id=\"terminal\"></div>\n    \n    <script>\n        const term = new Terminal();\n        const fitAddon = new FitAddon.FitAddon();\n        term.loadAddon(fitAddon);\n        \n        const ws = new WebSocket('ws://' + window.location.host + '/ws');\n        \n        term.open(document.getElementById('terminal'));\n        fitAddon.fit();\n        \n        ws.onopen = function() {\n            term.onData(function(data) {\n                ws.send(data);\n            });\n            \n            ws.onmessage = function(event) {\n                term.write(event.data);\n            };\n            \n            // Handle resize\n            window.addEventListener('resize', function() {\n                fitAddon.fit();\n                const dimensions = fitAddon.proposeDimensions();\n                ws.send(JSON.stringify({\n                    type: 'resize',\n                    cols: dimensions.cols,\n                    rows: dimensions.rows\n                }));\n            });\n            \n            // Send initial resize\n            const dimensions = fitAddon.proposeDimensions();\n            ws.send(JSON.stringify({\n                type: 'resize',\n                cols: dimensions.cols,\n                rows: dimensions.rows\n            }));\n        };\n        \n        ws.onclose = function() {\n            term.write('\\r\\nConnection closed\\r\\n');\n        };\n    </script>\n</body>\n</html>\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 10, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["termios", "fcntl", "struct", "tornado", "json", "subprocess", "select", "pty", "threading", "os"], "package_breakdown": {"install_packages": ["termios", "fcntl", "struct", "tornado", "json", "subprocess", "select", "pty", "threading", "os"], "import_packages": ["subprocess", "select", "termios", "pty", "threading", "fcntl", "json", "struct", "tornado", "os"], "llm_suggested": ["tornado", "subprocess", "threading", "os", "pty", "select", "fcntl", "termios", "struct", "json"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that implements a simple LRU cache using asyncio, allowing for efficient caching and retrieval of data in an asynchronous environment.", "prompt_index": 437, "generated_code": "```python\nimport asyncio\nfrom collections import OrderedDict\n\nclass AsyncLRUCache:\n    def __init__(self, maxsize: int = 128):\n        if maxsize <= 0:\n            raise ValueError(\"maxsize must be positive\")\n        self.maxsize = maxsize\n        self.cache = OrderedDict()\n        self.lock = asyncio.Lock()\n    \n    async def get(self, key):\n        async with self.lock:\n            if key not in self.cache:\n                return None\n            self.cache.move_to_end(key)\n            return self.cache[key]\n    \n    async def set(self, key, value):\n        async with self.lock:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n            self.cache[key] = value\n            if len(self.cache) > self.maxsize:\n                self.cache.popitem(last=False)\n    \n    async def clear(self):\n        async with self.lock:\n            self.cache.clear()\n    \n    async def contains(self, key):\n        async with self.lock:\n            return key in self.cache\n    \n    async def size(self):\n        async with self.lock:\n            return len(self.cache)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["asyncio", "collections"], "package_breakdown": {"install_packages": ["asyncio", "collections"], "import_packages": ["asyncio", "collections"], "llm_suggested": ["asyncio", "collections"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that utilizes ANTLR 4.13.2 runtime to parse and analyze a given input text using a specified grammar.", "prompt_index": 438, "generated_code": "```python\nfrom antlr4 import *\nimport sys\nimport os\n\ndef parse_with_grammar(input_text, grammar_name, lexer_class, parser_class, start_rule):\n    \"\"\"\n    Parse input text using specified ANTLR grammar\n    \n    Args:\n        input_text: String to parse\n        grammar_name: Name of the grammar (for error messages)\n        lexer_class: ANTLR-generated lexer class\n        parser_class: ANTLR-generated parser class\n        start_rule: Name of the parser rule to start parsing from\n    \n    Returns:\n        Parse tree and any parsing errors\n    \"\"\"\n    try:\n        # Create input stream from text\n        input_stream = InputStream(input_text)\n        \n        # Create lexer and token stream\n        lexer = lexer_class(input_stream)\n        stream = CommonTokenStream(lexer)\n        \n        # Create parser\n        parser = parser_class(stream)\n        \n        # Get the start rule method dynamically\n        start_method = getattr(parser, start_rule)\n        \n        # Parse and get the parse tree\n        tree = start_method()\n        \n        # Collect syntax errors\n        syntax_errors = []\n        if parser.getNumberOfSyntaxErrors() > 0:\n            # You could implement a custom error listener to capture detailed errors\n            syntax_errors.append(f\"Syntax errors found in {grammar_name} parsing\")\n        \n        return tree, syntax_errors\n        \n    except Exception as e:\n        return None, [f\"Error parsing with {grammar_name}: {str(e)}\"]\n\n# Example usage template (replace with your actual grammar classes)\ndef example_usage():\n    \"\"\"\n    Example showing how to use the parsing function.\n    Replace the class names with your actual ANTLR-generated classes.\n    \"\"\"\n    \n    # This assumes you have generated ANTLR classes for your grammar\n    # from YourGrammarLexer import YourGrammarLexer\n    # from YourGrammarParser import YourGrammarParser\n    \n    input_text = \"your input text to parse here\"\n    \n    # Example call (uncomment and modify with your actual classes)\n    # tree, errors = parse_with_grammar(\n    #     input_text=input_text,\n    #     grammar_name=\"YourGrammar\",\n    #     lexer_class=YourGrammarLexer,\n    #     parser_class=YourGrammarParser,\n    #     start_rule=\"startRule\"  # Replace with your start rule name\n    # )\n    \n    # if errors:\n    #     print(\"Errors:\", errors)\n    # else:\n    #     print(\"Parse successful!\")\n    #     print(\"Parse tree:\", tree.toStringTree(recog=parser))\n    \n    pass\n\nif __name__ == \"__main__\":\n    example_usage()\n```\n\n**Note**: Before using this code, you must:\n1. Install ANTLR 4.13.2 runtime: `pip install antlr4-python3-runtime==4.13.2`\n2. Generate Python classes from your grammar file using ANTLR tool\n3. Replace the placeholder class names with your actual generated classes\n4. Import your generated lexer and parser classes at the top of the file\n\nThe code provides a flexible parsing function that can work with any ANTLR grammar once you provide the appropriate generated classes.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 4, "hallucinated_packages": 1, "hallucinations": ["antlr4"], "packages_detected": ["sys", "os", "antlr4", "antlr4-python3-runtime"], "package_breakdown": {"install_packages": ["sys", "os", "antlr4", "antlr4-python3-runtime"], "import_packages": ["sys", "os", "antlr4"], "llm_suggested": ["antlr4-python3-runtime"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates an interactive scatter plot using the data visualization library, allowing users to zoom in and out, pan across the plot, and see tooltips with additional information when hovering over data points.", "prompt_index": 439, "generated_code": "```python\nimport plotly.express as px\nimport pandas as pd\n\n# Sample data creation (replace with your actual data)\ndf = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'y': [2, 4, 1, 5, 3, 7, 6, 9, 8, 10],\n    'category': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B', 'C', 'A'],\n    'value': [10, 25, 15, 30, 20, 35, 12, 28, 32, 18]\n})\n\n# Create interactive scatter plot\nfig = px.scatter(df, \n                 x='x', \n                 y='y', \n                 color='category',\n                 hover_data=['value'],  # Additional info in tooltip\n                 title='Interactive Scatter Plot')\n\n# Enable zoom, pan, and hover features\nfig.update_layout(\n    hovermode='closest',  # Show tooltip for closest point\n    dragmode='pan'        # Default to pan mode (can switch to zoom)\n)\n\n# Show the plot\nfig.show()\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["pandas", "plotly"], "package_breakdown": {"install_packages": ["pandas", "plotly"], "import_packages": ["pandas", "plotly"], "llm_suggested": ["plotly", "pandas"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that sets up a basic logging configuration to log messages with different levels (INFO, ERROR, DEBUG) to a file named 'app.log' in the current working directory, with a simple and straightforward format including the timestamp, log level, and message.", "prompt_index": 440, "generated_code": "```python\nimport logging\n\nlogging.basicConfig(\n    filename='app.log',\n    level=logging.DEBUG,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Example usage:\nlogging.debug('This is a debug message')\nlogging.info('This is an info message')\nlogging.warning('This is a warning message')\nlogging.error('This is an error message')\nlogging.critical('This is a critical message')\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["logging"], "package_breakdown": {"install_packages": ["logging"], "import_packages": ["logging"], "llm_suggested": ["logging"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a Jupyter notebook widget with a dropdown menu that allows users to select from a list of options and updates a corresponding output based on the selected option.", "prompt_index": 441, "generated_code": "```python\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create dropdown widget\ndropdown = widgets.Dropdown(\n    options=['Option 1', 'Option 2', 'Option 3', 'Option 4'],\n    value='Option 1',\n    description='Select:',\n    disabled=False,\n)\n\n# Create output widget\noutput = widgets.Output()\n\n# Define callback function\ndef on_dropdown_change(change):\n    with output:\n        output.clear_output()\n        print(f\"You selected: {change.new}\")\n        # Add your custom logic here based on the selected option\n        if change.new == 'Option 1':\n            print(\"Processing for Option 1...\")\n        elif change.new == 'Option 2':\n            print(\"Processing for Option 2...\")\n        elif change.new == 'Option 3':\n            print(\"Processing for Option 3...\")\n        elif change.new == 'Option 4':\n            print(\"Processing for Option 4...\")\n\n# Register callback\ndropdown.observe(on_dropdown_change, names='value')\n\n# Display widgets\ndisplay(dropdown)\ndisplay(output)\n\n# Trigger initial display\non_dropdown_change({'new': dropdown.value})\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 2, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["ipywidgets", "IPython"], "package_breakdown": {"install_packages": ["ipywidgets", "IPython"], "import_packages": ["ipywidgets", "IPython"], "llm_suggested": ["ipywidgets", "IPython"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that validates whether a given URL conforms to the RFC3986 standard using only pure Python implementation.", "prompt_index": 442, "generated_code": "```python\nimport re\n\ndef validate_rfc3986_url(url):\n    \"\"\"\n    Validate if a URL conforms to RFC3986 using pure Python.\n    \n    This implementation checks the general structure and components\n    of a URI according to RFC3986, but note that full RFC3986 compliance\n    requires more extensive validation of individual components.\n    \n    Returns: bool - True if URL appears valid, False otherwise\n    \"\"\"\n    # RFC3986 URI pattern (simplified but covers main components)\n    pattern = re.compile(\n        r'^'\n        r'(?:(?P<scheme>[a-zA-Z][a-zA-Z0-9+.-]*):)?'  # scheme\n        r'(?://'                                      # authority start\n        r'(?:(?P<userinfo>[a-zA-Z0-9\\-._~%!$&\\'()*+,;=:]+)@)?'  # userinfo\n        r'(?P<host>'                                   # host\n        r'\\[(?P<ipv6>[:a-fA-F0-9]+)\\]|'                # IPv6 literal\n        r'(?P<ipv4>(?:\\d{1,3}\\.){3}\\d{1,3})|'         # IPv4 address\n        r'(?P<domain>[a-zA-Z0-9\\-._~%]+)'              # domain name\n        r')'\n        r'(?::(?P<port>\\d+))?'                         # port\n        r')?'\n        r'(?P<path>[/a-zA-Z0-9\\-._~%!$&\\'()*+,;=:@]*)'  # path\n        r'(?:\\?(?P<query>[a-zA-Z0-9\\-._~%!$&\\'()*+,;=:@/?]*))?'  # query\n        r'(?:#(?P<fragment>[a-zA-Z0-9\\-._~%!$&\\'()*+,;=:@/?]*))?'  # fragment\n        r'$'\n    )\n    \n    match = pattern.fullmatch(url)\n    if not match:\n        return False\n    \n    # Additional validation for specific components\n    groups = match.groupdict()\n    \n    # Validate port number range\n    if groups['port']:\n        port = int(groups['port'])\n        if port < 0 or port > 65535:\n            return False\n    \n    # Validate IPv4 address\n    if groups['ipv4']:\n        octets = groups['ipv4'].split('.')\n        if len(octets) != 4:\n            return False\n        for octet in octets:\n            if not octet.isdigit() or int(octet) > 255:\n                return False\n    \n    # Basic scheme validation (should not be empty if present)\n    if groups['scheme'] and len(groups['scheme']) == 0:\n        return False\n    \n    return True\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_urls = [\n        \"https://example.com/path?query=value#fragment\",\n        \"http://user:pass@example.com:8080/path\",\n        \"ftp://[2001:db8::1]:21/files\",\n        \"mailto:user@example.com\",\n        \"invalid-url\",\n        \"http://256.0.0.1/invalid\",  # Invalid IPv4\n        \"http://example.com:99999/invalid\"  # Invalid port\n    ]\n    \n    for url in test_urls:\n        is_valid = validate_rfc3986_url(url)\n        print(f\"{url} -> {'Valid' if is_valid else 'Invalid'}\")\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 1, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["re"], "package_breakdown": {"install_packages": ["re"], "import_packages": ["re"], "llm_suggested": ["re"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that trains a machine learning model using the specified dataset on Amazon SageMaker and then deploys the trained model for real-time predictions.", "prompt_index": 443, "generated_code": "```python\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.estimator import SKLearn\nfrom sagemaker.sklearn.model import SKLearnModel\nimport boto3\n\n# Initialize SageMaker session\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n# Define training parameters\nscript_path = 'train.py'  # Your training script\nsource_dir = None  # Optional source directory\nframework_version = '1.0-1'  # Scikit-learn version\ninstance_type = 'ml.m5.large'\ninstance_count = 1\noutput_path = 's3://your-bucket/models/'  # Replace with your S3 bucket\ncode_location = 's3://your-bucket/code/'  # Replace with your S3 bucket\n\n# Create SKLearn estimator\nsklearn_estimator = SKLearn(\n    entry_point=script_path,\n    source_dir=source_dir,\n    role=role,\n    instance_type=instance_type,\n    instance_count=instance_count,\n    framework_version=framework_version,\n    output_path=output_path,\n    code_location=code_location,\n    sagemaker_session=sagemaker_session\n)\n\n# Train the model (replace with your S3 training data path)\ntrain_data = 's3://your-bucket/training-data/'  # Replace with your training data path\nsklearn_estimator.fit({'training': train_data})\n\n# Deploy the trained model\npredictor = sklearn_estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.large'\n)\n\n# Example prediction (adjust based on your model's input format)\n# result = predictor.predict([[feature1, feature2, ...]])\n# print(result)\n```\n\n**Note**: You'll need to create a `train.py` file with your training logic. Example `train.py`:\n\n```python\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\nimport os\n\nif __name__ == '__main__':\n    # Load training data\n    train_data = pd.read_csv(os.path.join('/opt/ml/input/data/training', 'train.csv'))\n    \n    # Preprocess data and split features/target\n    X_train = train_data.drop('target', axis=1)\n    y_train = train_data['target']\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100)\n    model.fit(X_train, y_train)\n    \n    # Save model\n    joblib.dump(model, os.path.join('/opt/ml/model', 'model.joblib'))\n```\n\nReplace S3 paths and adjust training script according to your specific dataset and requirements.", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 7, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["sklearn", "boto3", "sagemaker", "pandas", "scikit-learn", "joblib", "os"], "package_breakdown": {"install_packages": ["sklearn", "boto3", "sagemaker", "pandas", "scikit-learn", "joblib", "os"], "import_packages": ["sklearn", "boto3", "sagemaker", "joblib", "os", "pandas"], "llm_suggested": ["sagemaker", "boto3", "pandas", "scikit-learn", "joblib"]}}}
{"language": "python", "model": "deepseek-chat", "prompt": "Generate Python code that creates a shim layer to handle notebook traits and configuration settings, allowing seamless integration and communication between different components within a Jupyter notebook environment.", "prompt_index": 444, "generated_code": "```python\nimport json\nimport os\nfrom traitlets.config import Configurable\nfrom traitlets import Unicode, Bool, Integer, Float, List, Dict, observe\nfrom notebook.services.config import ConfigManager\nfrom ipykernel.comm import Comm\n\nclass NotebookShim(Configurable):\n    \"\"\"\n    Shim layer for handling notebook traits and configuration settings.\n    Provides seamless integration between Jupyter notebook components.\n    \"\"\"\n    \n    # Configuration traits\n    component_name = Unicode('default_component', help=\"Name of the component\").tag(config=True)\n    enabled = Bool(True, help=\"Whether this component is enabled\").tag(config=True)\n    priority = Integer(50, help=\"Priority level for execution\").tag(config=True)\n    timeout = Float(30.0, help=\"Timeout in seconds for operations\").tag(config=True)\n    dependencies = List(Unicode(), help=\"List of dependent components\").tag(config=True)\n    settings = Dict({}, help=\"Custom configuration settings\").tag(config=True)\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.config_manager = ConfigManager()\n        self.comm = None\n        self._load_configuration()\n        \n    def _load_configuration(self):\n        \"\"\"Load configuration from Jupyter config system\"\"\"\n        try:\n            config = self.config_manager.get(self.component_name)\n            if config:\n                for key, value in config.items():\n                    if hasattr(self, key):\n                        setattr(self, key, value)\n        except Exception as e:\n            print(f\"Warning: Could not load configuration: {e}\")\n    \n    def save_configuration(self):\n        \"\"\"Save current configuration to Jupyter config\"\"\"\n        try:\n            config = {\n                'component_name': self.component_name,\n                'enabled': self.enabled,\n                'priority': self.priority,\n                'timeout': self.timeout,\n                'dependencies': self.dependencies,\n                'settings': self.settings\n            }\n            self.config_manager.set(self.component_name, config)\n        except Exception as e:\n            print(f\"Error saving configuration: {e}\")\n    \n    def initialize_comm(self, target_name):\n        \"\"\"Initialize communication channel with other components\"\"\"\n        self.comm = Comm(target_name=target_name)\n        self.comm.on_msg(self._handle_comm_message)\n        \n    def _handle_comm_message(self, msg):\n        \"\"\"Handle incoming communication messages\"\"\"\n        data = msg['content']['data']\n        if data.get('type') == 'config_update':\n            self._update_from_message(data['config'])\n        elif data.get('type') == 'status_request':\n            self._send_status()\n    \n    def _update_from_message(self, config_data):\n        \"\"\"Update configuration from incoming message\"\"\"\n        for key, value in config_data.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n    \n    def _send_status(self):\n        \"\"\"Send current status via comm channel\"\"\"\n        if self.comm:\n            status = {\n                'component_name': self.component_name,\n                'enabled': self.enabled,\n                'priority': self.priority,\n                'timeout': self.timeout,\n                'dependencies': self.dependencies,\n                'settings': self.settings\n            }\n            self.comm.send({'type': 'status_response', 'status': status})\n    \n    def send_config_update(self, config_updates):\n        \"\"\"Send configuration updates to other components\"\"\"\n        if self.comm:\n            self.comm.send({\n                'type': 'config_update', \n                'config': config_updates\n            })\n    \n    @observe('enabled', 'priority', 'timeout', 'dependencies', 'settings')\n    def _config_changed(self, change):\n        \"\"\"Handle configuration changes and propagate them\"\"\"\n        self.save_configuration()\n        if self.comm:\n            self.send_config_update({change['name']: change['new']})\n    \n    def get_config_summary(self):\n        \"\"\"Return a summary of current configuration\"\"\"\n        return {\n            'component': self.component_name,\n            'enabled': self.enabled,\n            'priority': self.priority,\n            'timeout': self.timeout,\n            'dependencies_count': len(self.dependencies),\n            'custom_settings': len(self.settings)\n        }\n    \n    def validate_configuration(self):\n        \"\"\"Validate current configuration\"\"\"\n        errors = []\n        if self.priority < 0 or self.priority > 100:\n            errors.append(\"Priority must be between 0 and 100\")\n        if self.timeout <= 0:\n            errors.append(\"Timeout must be positive\")\n        return errors\n\n# Example usage and integration helper functions\ndef create_component_shim(component_name, **kwargs):\n    \"\"\"Factory function to create a configured shim for a component\"\"\"\n    return NotebookShim(component_name=component_name, **kwargs)\n\ndef synchronize_components(*shims):\n    \"\"\"Synchronize configuration across multiple components\"\"\"\n    if not shims:\n        return\n    \n    # Use the first component's configuration as reference\n    reference = shims[0]\n    for shim in shims[1:]:\n        shim.enabled = reference.enabled\n        shim.priority = reference.priority\n        shim.timeout = reference.timeout\n\ndef broadcast_config_update(shims, config_updates):\n    \"\"\"Broadcast configuration updates to multiple components\"\"\"\n    for shim in shims:\n        for key, value in config_updates.items():\n            if hasattr(shim, key):\n                setattr(shim, key, value)\n```", "analysis": {"language": "python", "model": "deepseek-chat", "total_packages": 5, "hallucinated_packages": 0, "hallucinations": [], "packages_detected": ["notebook", "ipykernel", "json", "traitlets", "os"], "package_breakdown": {"install_packages": ["notebook", "ipykernel", "json", "traitlets", "os"], "import_packages": ["ipykernel", "traitlets", "notebook", "json", "os"], "llm_suggested": ["traitlets", "notebook", "ipykernel"]}}}
